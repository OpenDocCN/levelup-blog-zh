# 防止模型过度拟合的 6 个解决方案！

> 原文：<https://levelup.gitconnected.com/6-solutions-prevent-model-overfitting-9c73aa026c1>

![](img/977461a8f3beb2563fe27d5491510e57.png)

约书亚·索蒂诺在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

```
Source: Deep Learning Basics and Advanced, Extreme Market PlatformThis article **is about 2700 words** , it is recommended to read **6 minutes**This article summarizes and explains in detail several commonly used methods to prevent model overfitting.
```

其实正则化的本质很简单，它是一种手段或操作，对一个问题施加先验的限制或约束，以达到特定的目的。在算法中使用正则化的目的是防止模型过度拟合。谈到正则化，许多学生可能会立即想到常用的 L1 常模和 L2 常模。在总结之前，我们先来看看 LP 范数是什么？

# LP 范数

范数可以简单理解为表示向量空间中的距离，距离的定义很抽象。只要满足非负、自反、三角不等式，就可以称为距离。
LP 规范不是一个规范，而是一组规范，定义如下:

![](img/731db0c0e39ccd2db708ea0040025dd7.png)

p 的取值范围是[1，∞)。p 不是定义在范围(0，1)中的范数，因为违反了三角形不等式。
根据 pp 的变化，定额也有不同的变化。借用 P 范数的一个经典变化图如下:

![](img/2c2888dedf270a88d5072d0566676734.png)

上图显示了当 p 从 0 变化到正无穷大时，单位球是如何变化的。P 范数下定义的单位球都是凸集，但当 0

问题是，L0 范数是什么？
L0 范数表示向量中非零元素的个数，用公式表示如下:

![](img/392a32eeb3b1e01a041afe4f92c775e8.png)

我们可以通过最小化 L0 范数来找到最优稀疏特征项。但遗憾的是，L0 范数的优化问题是一个 NP 难问题(L0 范数也是非凸的)。因此，在实际应用中，我们经常对 L0 进行凸松弛。从理论上证明了 L1 范数是 L0 范数的最优凸逼近，所以通常使用 L1 范数而不是直接优化 L0 范数。

# L1 常模

根据 LP 范数的定义，我们很容易得到 L1 范数的数学形式:

![](img/1be25e61354bcbf23cf2ee74e8b94992.png)

从上式可以看出，L1 范数是向量的元素的绝对值之和，也称为“稀疏正则化算子”(Lasso 正则化)。所以问题是，我们为什么要稀疏？稀疏化有很多好处，其中两个最直接:

*   特征选择
*   可解释性

# L2 常模

L2 范数是最熟悉的，它是欧几里德距离，公式如下:

![](img/8ba4a475da32dfefdec345f8d6f0c2ca.png)

L2 规范有许多名称。有人称其回归为“岭回归”，有人称其为“体重衰减”。用 L2 范数作为正则项可以得到稠密解，即每个特征对应的参数 ww 很小，接近 0 但不为 0；此外，作为正则化项的 L2 范数会阻止模型适应训练集。过于复杂会造成过拟合，从而提高模型的泛化能力。

# L1 常模和 L2 常模的区别

介绍一个 PRML 的经典图来说明 L1 和 L2 范数的区别，如下图所示:

![](img/06da77935305bfa9d807f93befba0e0f.png)

如上图所示，蓝色圆圈代表问题的可能解范围，橙色圆圈代表正则项的可能解范围。而整个目标函数(原问题+正则项)有解当且仅当两个解范围相切。从上图中很容易看出，由于 L2 范数解的范围是一个圆，切点极有可能不在坐标轴上，而由于 L1 范数是一个菱形(顶点是凸的)，它的相位切点更有可能在坐标轴上，坐标轴上的点有一个特点，只有一个坐标分量不为零，其他坐标分量都为零，即稀疏。由此得出结论，L1 范数可以导致稀疏解，而 L2 范数可以导致稠密解。
从贝叶斯先验的角度来看，在训练一个模型的时候，仅仅依靠当前的训练数据集是不够的。为了达到更好的泛化能力，往往需要加入一个先验项，加入一个正则项就相当于加入了一个先验。

*   L1 范数相当于增加了一个拉普拉斯先验；
*   L2 范数相当于增加了一个高斯先验。

如下图所示:

![](img/ebc4e6be7e213b5efaca6f85dd53be5d.png)

# 拒绝传统社会的人

Dropout 是深度学习中经常使用的正则化方法。其做法可以简单理解为在 DNNs 训练过程中丢弃一些概率为 pp 的神经元，即使被丢弃神经元的输出为 0。可以按如下方式实例化 Dropout:

![](img/d94adede0219909ce31f8c18c02eacc0.png)

我们可以从两个方面直观地理解辍学的正则化效应:

*   每轮辍学训练中随机丢失神经元的操作相当于对多个 dnn 进行平均，所以用于预测时具有投票的效果。
*   减少神经元之间复杂的共同适应。当隐层神经元被随机删除时，全连通网络具有一定程度的稀疏性，从而有效降低了不同特征的协同效应。也就是说，一些特征可能依赖于固定关系的隐式节点的共同作用，通过 Dropout，它有效地组织了一些特征只有在其他特征存在的情况下才有效的情况，增加了神经网络的鲁棒性。太棒了。

# 批量标准化

批量归一化严格来说是一种归一化方法，主要用于加速网络的收敛，但也有一定程度的正则化效果。
以下是魏秀三博士知乎回答(https://www.zhihu.com/question/38102762)中关于协变量移位的解释
注:以下内容引自魏秀三博士知乎回答。大家都知道统计机器学习的一个经典假设是“源域和目标域的数据分布是一致的。”。如果没有，那么新的机器学习问题就产生了，比如迁移学习/领域适应等。协变量移位是不一致分布假设下的一个分支问题。这意味着源空间和目标空间的条件概率相同，但它们的边际概率不同。你想一想就会发现，的确，对于神经网络的每一层的输出，由于它们都经过了层内运算，所以它们的分布与每一层对应的输入信号的分布是明显不同的，而且这种差异会随着网络的深度而增大。大，但它们所能“指示”的样本标签仍然不变，符合协变量移位的定义。
BN 的基本思想其实挺直观的，因为非线性变换前的神经网络的激活输入值(X=WU+B，U 为输入)随着网络深度的加深而逐渐偏移或变化(即上述的协变量偏移)。训练收敛慢的原因是总体分布逐渐接近非线性函数的取值范围的上下限(对于 sigmoid 函数，意味着激活输入值 X=WU+B 是一个较大的负值或正值)，所以这导致了在反向传播时低层神经网络的梯度消失，这是训练一个深度神经网络收敛越来越慢的本质原因。BN 是通过一定的归一化方法，迫使神经网络各层中任意一个神经元的输入值的分布回归到均值为 0、方差为 1 的标准正态分布，从而避免激活函数带来的梯度分散问题。所以，与其说 BN 的作用是缓解协变量偏移，不如说 BN 可以缓解梯度分散的问题。

# 规范化、标准化和正规化

我们已经提到了正规化，这里简单提一下正规化和标准化。归一化:归一化的目标是找到一定的映射关系，将原始数据映射到[a，b]区间。一般来说，a，b 会取[1，1]，[0，1]这些组合。
一般有两种应用场景:

*   将数字转换为(0，1)之间的小数
*   将量纲数转换成无量纲数

常见最小-最大归一化:

![](img/311c5e9a5d5ba68955922592214c81ea.png)

标准化:使用大数法则将数据转换成标准的正态分布。标准化公式为:

![](img/36d873fa69d1e2160ba18ea4bca41213.png)

# 规范化和正常化的区别:

我们可以这样简单的解释:
归一化标度是“扁平化”的，统一到一个区间(仅由极值决定)，而归一化标度更具“弹性”和“动态性”，与整体样本的分布有很大关系。
值得注意的是:

*   标准化:缩放只与最大值和最小值之间的差异有关。
*   标准化:缩放与每个点相关，体现在方差中。与归一化相反，归一化中所有数据点都有贡献(通过平均值和标准偏差来贡献)。

# 为什么要标准化和常态化？

*   提高模型精度:归一化后，不同维度之间的特征在数值上具有可比性，可以大大提高分类器的精度。
*   加速模型收敛:标准化后，最优解的优化过程会明显变得更加平滑，更容易正确收敛到最优解。如下图所示:

![](img/f1c5925ea5e8eac65020c81caca9255c.png)![](img/1abdf73e0251c130f0aed71fdd62051f.png)

喜欢这篇文章吗？成为一个媒介成员，通过无限制的阅读继续学习。如果你使用[这个链接](https://machinelearningabc.medium.com/membership)成为会员，你将支持我，不需要额外的费用。提前感谢，再见！