<html>
<head>
<title>Fundamentals of Reinforcement Learning: Markov Decision Process</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的基础:马尔可夫决策过程</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/fundamental-of-reinforcement-learning-markov-decision-process-8ba98fa66060?source=collection_archive---------1-----------------------#2021-05-01">https://levelup.gitconnected.com/fundamental-of-reinforcement-learning-markov-decision-process-8ba98fa66060?source=collection_archive---------1-----------------------#2021-05-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bb24" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第1部分:解释马尔可夫决策过程和贝尔曼方程的概念</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/aa01f3d6150b5e60472b97b2fe00e136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8irf8y-s_TmtuoxyTIPWrA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Gert RDA valasevi it在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="345b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将重点介绍强化学习(RL)的基础——马尔可夫决策过程(MDP)公式以及值迭代、策略迭代和Q学习算法的一些重要组件。</p><p id="dfec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">马尔可夫决策过程(MDP)<br/></strong>【MDP】是RL的数学基础，如果我们想要完全理解RL算法，我们总是需要从MDP开始。MDP基本上是一个不确定情况下的决策框架。它可以提供一种计算最优决策策略的方法。首先，让我们介绍一下代理-环境接口。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/2ba3b8d97eaf3ab48e3d3e940f0e0b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Rq8m55-HKtZILCm82dNfw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">代理-环境界面。来源——斯坦福大学CME 241<a class="ae ky" href="https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/5-6-MDPs.pdf" rel="noopener ugc nofollow" target="_blank">讲座</a>。</figcaption></figure><p id="4404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们假设在离散的时间步长<strong class="lb iu"> t = 0，1，2，… </strong>只有一个智能体与环境交互。在每个时间步长<strong class="lb iu"> t </strong>时，智能体可以观察状态以了解状态<strong class="lb iu"> Sₜ ∈ S </strong>的情况。然后它会采取行动<strong class="lb iu"> Aₜ ∈ A(sₜ) </strong>。采取行动后会获得一定的奖励<strong class="lb iu"> R_{t+1} </strong>到达下一个状态<strong class="lb iu"> S_{t+1} ∈ S </strong>。然后，它将重复采取另一个行动，接受另一个奖励，并达到另一个状态，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/3e2f7082919d7a933f18fb4a7d8b48d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZtIbX3yV6M_I8Q91CWpMBA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">代理和环境的相互作用。来源——斯坦福大学<a class="ae ky" href="https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/5-6-MDPs.pdf" rel="noopener ugc nofollow" target="_blank">CME 241讲座</a>。</figcaption></figure><p id="7d69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是顺序决策。当前的行动将对未来产生影响，即代理人将获得多少总回报以及代理人将达到哪个未来状态。满足马尔可夫性质(转移性质只取决于当前状态，而不取决于以前的状态或历史，换言之，这意味着未来发生的事情只取决于当前状态)的序列决策问题称为马尔可夫决策过程。</p><p id="4744" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MDP的配方(组分)</p><ul class=""><li id="e5b5" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">马尔科夫状态从初始状态s0开始</li><li id="80e0" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu">动作</strong> a，在每个状态s都有动作集合A(s)</li><li id="12ff" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu">转换模型</strong> P(s'|s，a)，在当前状态s，如果代理采取一个动作到达另一个状态s '的概率。然而，它不是100%确定它将达到哪个状态，有一个概率分布，因此它是一个不确定情况下的决策框架。<br/>假设:从s到s’的概率只取决于s和a而不取决于任何其他过去的动作或状态(马尔可夫性质)</li><li id="2a7c" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu">奖励函数</strong> R(s)或R(s)</li></ul><p id="15a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在MDP中，目标是得到一个<strong class="lb iu">最优决策策略</strong><strong class="lb iu">【𝝅(s】</strong>，这是代理在任何给定状态下应该采取的行动。</p><p id="684e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们用一个具体的例子(游戏展示如下图所示)来解释MDP。这是一种我们在电视节目中看到的游戏，主持人会给参与者一系列问题，这些问题的难度和回报都在增加。在参与者成功回答一个问题后的每个时间点，他们有两个选择拿着钱离开或者继续下一个问题。如果参与者继续回答下一个问题，有两种可能:成功回答该问题并做出另一个决定，或者没有回答该问题并失去一切。</p><p id="94b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的例子共有4个问题，难度逐渐增加，第一个问题是100美元，第二个是1000美元，第三个是10000美元，最后一个是50000美元。在回答完每个问题后，参与者需要决定是退出游戏还是继续回答。显然，为了做出决定，参与者需要知道如果他们决定继续回答问题，他们可以获得的预期奖励，这取决于两个因素:正确回答问题的概率以及做出决定后发生的事情。基本上，这里我们需要做一些逆向归纳。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/091edd067cfaf9e6519f22839467d79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58CWsOT5ZZ1OqzYUhi_tFQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">游戏节目。来源——cs 440/ECE 448中伊利诺伊大学<a class="ae ky" href="http://isle.illinois.edu/speech_web_lg/coursematerials/ece448/17fall/ece448fa2017lecture25.pptx" rel="noopener ugc nofollow" target="_blank">讲座</a>。</figcaption></figure><p id="481c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，决定是否回答Q2将取决于回答Q2后发生的事情或回答Q2后得到多少奖励/金钱。同样，在决定是回答Q3还是Q4时，参与者需要知道在他们决定回答Q3或Q4后会发生什么。(逆向归纳)</p><p id="bac7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们首先考虑我们需要做出的最后一个决定，即参与者是否应该回答最后一个问题Q4。给出正确回答Q4的概率为0.1。如果参与者回答Q4，∴期望效用= 0.1 *(100+1000+10000+50000)+0.9 * 0 = 6110美元，如果他们不回答Q4，他们将获得11100美元。由于不回答的收益更大，在回答了Q3后，这里的最优决策是退出并获得11，100美元的奖励。</p><p id="515c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，对于Q3，不回答的收益是1，100美元，回答的预期收益是0.5 * 11，100+0.5 * 0 = 5，550美元。这里的最佳决策是通过回答Q3继续博弈。</p><p id="2c61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，Q2不回答的收益是100美元，回答的预期收益是0.75 * 5550+0.25 * 0 = 4162.5美元。这里的最优决策是通过回答Q2来继续博弈。</p><p id="58d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，对于Q1来说，不回答的收益是0美元，回答的期望收益是0.9 * 4，162.5+0.1 * 0 = 3，746.25美元。这里的最优决策是通过回答Q1来继续博弈。</p><p id="5482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这正是MDP问题，有许多状态(每个问题)和必须做出的决定(退出或继续)，过渡模型(成功回答问题并到达另一个问题或回答失败并带着0美元回家)和奖励(最佳收益)。解决方案是参与者在任何给定状态下应该采取的最优策略或行动。</p><p id="24dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，最优策略应该最大化给定轨迹上的累积回报，如𝜏 = <s a="" r="" s="" class="lb iu"> ₜ ，A <strong class="lb iu"> ₜ </strong>，R <strong class="lb iu"> ₜ &gt; </strong>在某些策略下:</s></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/80edc660b73f3cec4d630b26ac98a764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YcS3Ja4LlWB7_zsN0kJwGQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">累积奖励。来源——斯坦福大学CME 241<a class="ae ky" href="https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/5-6-MDPs.pdf" rel="noopener ugc nofollow" target="_blank">讲座</a>。</figcaption></figure><p id="ee2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在MDP，它关心的是总报酬，通常有一个叫做γ的贴现因子，取值在0到1之间。贴现因子是一个来自经济学的概念，即今天的钱比明天的钱更值钱，因为人们可以用这些钱来做投资。此外，通过使用折扣因子，累积奖励将是有界的，例如<strong class="lb iu"> ₜ ≤ </strong> R <strong class="lb iu"> ₘₐₓ </strong> /(1-γ)，这对算法有一些好的影响，特别是在理论分析中。折扣因子是可能影响学习算法的收敛或速度的超参数，通常在0.9到1之间。</p><p id="b7a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们定义一些重要的东西来解释RL算法。</p><ul class=""><li id="94aa" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">如果代理人从状态<strong class="lb iu"> s </strong>开始执行最优策略<strong class="lb iu"> 𝜋 </strong>，状态的真实值(通常用<strong class="lb iu">v(s)】</strong>表示)是期望的贴现回报的总和。<br/><strong class="lb iu">𝑉</strong>𝜋<strong class="lb iu">(𝑠)</strong>=<strong class="lb iu">𝔼</strong>𝜋<strong class="lb iu">【𝐺ₜ|𝑆ₜ=𝑠】</strong></li><li id="056e" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">同样，我们可以将一个状态-动作对的动作-值定义如下:<br/><strong class="lb iu">𝑄</strong>𝜋<strong class="lb iu">(𝑠,𝑎)</strong>=<strong class="lb iu">𝔼</strong>𝜋<strong class="lb iu">【𝐺ₜ|𝑆ₜ=𝑠，𝐴ₜ=𝑎】</strong></li><li id="72da" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">而q与v的关系如下:<br/><strong class="lb iu">𝑉</strong>𝜋<strong class="lb iu">(𝑠)</strong>=σ<strong class="lb iu">𝑄</strong>𝜋<strong class="lb iu">(𝑠,𝑎)</strong><strong class="lb iu">𝜋(𝑎|s)</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/95227ccb6a320761af8363b8b03126e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*7MYES-ga-INzeIhz3QkdcQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">如何推导贝尔曼方程的图解？来源——cs 440/ECE 448中的伊利诺伊大学<a class="ae ky" href="http://isle.illinois.edu/speech_web_lg/coursematerials/ece448/17fall/ece448fa2017lecture25.pptx" rel="noopener ugc nofollow" target="_blank">讲座</a>。</figcaption></figure><p id="e48d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图可以看出，在状态s，我们可以采取许多可能的行动(a、a’、a”或a”)。在采取行动a后，我们将到达某个状态s ',但以一定概率到达s ' '或s ' ' '仍有不确定性。回到游戏节目的例子，在状态可以采取的行动是参与者是否回答问题(a)或不回答(a’)。如果他们决定回答这个问题，有两种结果:正确回答这个问题并进入下一个问题，或者回答失败并回家。在状态<strong class="lb iu"> s </strong>中采取行动<strong class="lb iu"> a </strong>的期望值基本上是考虑参与者用转移概率可以达到的不同状态。如果我们到达状态<strong class="lb iu">s’</strong>我们将会得到一个奖励，这个状态也有一些值<strong class="lb iu">V(s’)</strong>:<strong class="lb iu"/>参与者到达状态s’时可以得到的累积奖励。<br/><strong class="lb iu">𝑄(𝑠,𝑎)=σp(s'|𝑠,𝑎)[r(s，a，s') + γ * V(s')] </strong></p><p id="b627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在回到上图中的状态<strong class="lb iu"> s </strong>，代理人可以在动作空间a中采取不同的动作(a，a’，a”，a’”)，所选择的最优动作<strong class="lb iu"> a </strong>是最大化上面提到的<strong class="lb iu"> Q(s，a)</strong>:<br/><strong class="lb iu">𝜋∗(𝑠)=argmax</strong>𝑎∈𝐴<strong class="lb iu">σp(s'|𝑠,𝑎【r(s，a，s’)+γ* v(s’)】</strong></p><p id="d111" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且<strong class="lb iu"> V(s) </strong>根据其后继状态的效用的递归表达式被称为<strong class="lb iu">贝尔曼方程</strong>:<br/><strong class="lb iu">v(𝑠)=max</strong>𝑎∈𝐴<strong class="lb iu">σp(s'|𝑠,𝑎【r(s，a，s’)+γ* v(s’)】</strong></p><p id="cbbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果状态空间和动作空间是有限的，我们可以通过求解线性方程组来求解MDP。但是它通常效率不高，因为它必须枚举所有的转换、所有的状态和动作。因此，有两种流行的算法叫做值迭代和策略迭代来解决MDP问题。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="146d" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">推荐阅读</h1><div class="nn no gp gr np nq"><a rel="noopener  ugc nofollow" target="_blank" href="/fundamentals-of-reinforcement-learning-value-iteration-and-policy-iteration-with-tutorials-a7ad0049c84f"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">强化学习的基础:价值迭代和政策迭代教程</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">第二部分:解释用于解决MDP问题的价值迭代和策略迭代的概念。</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ks nq"/></div></div></a></div><div class="nn no gp gr np nq"><a rel="noopener  ugc nofollow" target="_blank" href="/fundamental-of-reinforcement-learning-monte-carlo-algorithm-85428dc77f76"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">强化学习的基础:蒙特卡罗算法</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">第三部分:阐述了无模型RL算法的基本原理:蒙特卡罗算法</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="nz l"><div class="of l ob oc od nz oe ks nq"/></div></div></a></div></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="049f" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">参考</h1><p id="5a84" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">[1] <a class="ae ky" href="https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/5-6-MDPs.pdf" rel="noopener ugc nofollow" target="_blank">斯坦福大学CME241讲座:金融中随机控制问题的强化学习</a>，2021</p><p id="d4ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] <a class="ae ky" href="http://isle.illinois.edu/speech_web_lg/coursematerials/ece448/17fall/ece448fa2017lecture25.pptx" rel="noopener ugc nofollow" target="_blank">伊利诺伊大学CS440/ECE448讲座:人工智能</a>，2021</p></div></div>    
</body>
</html>