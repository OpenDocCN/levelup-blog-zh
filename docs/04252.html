<html>
<head>
<title>How Neural Networks Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络如何学习</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/how-neural-networks-learn-f602d6f77a13?source=collection_archive---------20-----------------------#2020-06-16">https://levelup.gitconnected.com/how-neural-networks-learn-f602d6f77a13?source=collection_archive---------20-----------------------#2020-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1076" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">推导出前馈反向传播算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1b603eea52e26d2be1485f44ab079002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ahaU6RPeJ2atMS1vdRNJ5w.png"/></div></div></figure><p id="fe1e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">注:这篇文章是关于神经网络从无到有</em>  <em class="lq">的</em> <a class="ae lr" href="https://medium.com/@karenovna.ak/neural-networks-from-scratch-series-c969ba5b4e2b" rel="noopener"> <em class="lq">系列的第三部分。这里的数学建立在第2部分— </em> </a><a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b" rel="noopener ugc nofollow" target="_blank"> <em class="lq">训练单个感知器</em> </a> <em class="lq">的数学基础上，因此我强烈建议你对那篇文章中的等式有一个坚实的理解。读者应该有微积分方面的经验。</em></p><p id="9563" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我的<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">上一篇文章</strong> </a> <strong class="kw iu"> </strong>中，我推导了训练单个感知器的数学方法。这里的文章是前一篇文章的延续，前一篇文章将训练单个感知器的概念扩展到训练多层和多感知器神经网络。<strong class="kw iu"> <em class="lq">这篇文章包括了推导前馈反向传播算法所需的所有数学知识，前馈反向传播算法是神经网络学习的核心。</em> </strong></p><div class="ls lt gp gr lu lv"><a href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd iu gy z fp ma fr fs mb fu fw is bi translated">训练单个感知器</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">它的学习规则是从零开始的</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">blog.usejournal.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj ks lv"/></div></div></a></div><p id="12db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">事不宜迟，我们直接进入内容。</p><ol class=""><li id="8667" class="mk ml it kw b kx ky la lb ld mm lh mn ll mo lp mp mq mr ms bi translated"><strong class="kw iu">单感知器模型快速回顾</strong></li></ol><p id="aedb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#a773" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">训练单个感知器</strong> </a>(单个节点、单层神经网络的最简单形式)涉及使用<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#b7ed" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">梯度下降</strong> </a>来迭代<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#b7ed" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">更新连接输入和感知器的权重</strong> </a>，使得<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#fe4f" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">误差函数</strong> </a> <strong class="kw iu"> </strong>最小化<strong class="kw iu">。</strong>误差函数测量在<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#7069" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">监督学习</strong> </a>设置中，感知器的实际输出与期望输出(标签)的偏差。然后，在<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#bfb0" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">感知器增量函数</strong> </a> <strong class="kw iu"> <em class="lq"> </em> </strong>的帮助下，权重根据我们观察到的误差进行更新——该函数量化当前权重需要改变多少，以获得更新的权重，从而进一步减少误差。为了简单起见，我将忽略这篇文章中的偏见。</p><p id="c1d7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">考虑下面的感知器模型，它有一个感知器<em class="lq"> j </em>和一个<em class="lq"> m </em>维输入向量<em class="lq"> x </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/9d372d35ee79a222ac772176d5c8451d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hxyi6iPneEwnEHuXPyRZ9w.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">图1:单一感知器模型</figcaption></figure><p id="076f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从现在开始，每当涉及数学讨论时，我将把感知器称为一个<em class="lq">节点</em>。向量<em class="lq"> x </em>通常被称为<strong class="kw iu">特征向量。</strong>这个特征向量包含<em class="lq"> m个</em>元素，每个元素代表描述单个数据点的单个特征。为了给这个节点应用渐变下降，我们需要计算<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#edc0" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"/></a><strong class="kw iu"/>。在这种情况下，梯度向量是一个<em class="lq"> m </em>维向量，其分量是误差函数<em class="lq"> E </em>相对于权重的偏导数。<em class="lq">因此，梯度向量的每个分量代表E在单个权重方向上变化的速率。</em></p><p id="821a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">回想一下，梯度矢量的每个分量可以利用<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#f626" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="lq">链式法则</em> </strong> </a>计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/a0507b2ebf715b7000e7b390ba137f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3M05d14cVzXM4k2RhBhxkw.png"/></div></div></figure><blockquote class="mz na nb"><p id="a6cd" class="ku kv lq kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">在单感知器模型中，梯度向量是这样的表达式，其中误差函数相对于将输入仅连接到一个节点的所有权重取偏导数。</em>T29】</strong></p></blockquote><p id="31a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们从最右边的层和函数移到边或权重:<em class="lq">E→E→y _ j</em>→<em class="lq">A _ j</em>→<em class="lq">w _ ij</em>。上述等式中所有具有某个指数j的因子可以方便地聚合成一个因子<em class="lq"> delta_j </em>。这不仅仅是数学上的方便，而且是反向传播算法的基础，我们将在后面看到。经过大量的数学运算，我们为单个权重导出了<a class="ae lr" href="https://blog.usejournal.com/training-a-single-perceptron-405026d61f4b#bfb0" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="lq">感知器增量函数</em> </strong> </a>，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ca"><img src="../Images/0b7043713592d8635a8f04367c877c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S1Dw5TxCSy8dnWgHHc3bUw.png"/></div></div></figure><p id="17cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是现在让我们来看一个网络，它有几个输入，这些输入馈入几个节点，这样每个节点的输出可以作为其他节点的输入。</p><p id="03a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 2。前馈反向传播算法(FFBP) </strong></p><p id="df8a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 2.1。前馈过程</strong></p><p id="a44f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于多层、多感知器网络，我们有一个<em class="lq">输入层</em>、一个<em class="lq">隐藏层</em>和一个<em class="lq">输出层。</em>让我们用一个具体的例子，输入层有<em class="lq"> m </em>个输入，隐藏层有<em class="lq"> n </em>个节点，输出层有一个节点<em class="lq"> k </em>，如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/6e88731126359bfa97e4cc3589afdd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIHtcJmIA8hKMpVwbw5Zug.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">图2:一个多感知器，多层神经网络。</figcaption></figure><p id="f497" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出层中的节点产生<em class="lq">实际输出</em>，然后通过误差函数将其与<em class="lq">期望输出</em>(标签)<em class="lq"> </em>进行比较。这个过程是这样的:</p><ol class=""><li id="f1bb" class="mk ml it kw b kx ky la lb ld mm lh mn ll mo lp mp mq mr ms bi translated">输入层<em class="lq">的输入通过</em>网络，在隐藏层的每个节点(节点1，2，…j，…n)产生一个输出</li><li id="4913" class="mk ml it kw b kx ng la nh ld ni lh nj ll nk lp mp mq mr ms bi translated">然后，隐藏层的输出(y1，y2，…y_j，…y_n)被用作输出层中节点<em class="lq"> k </em>节点的输入(x1，x2，…x_j，…x_n)。</li><li id="e8b9" class="mk ml it kw b kx ng la nh ld ni lh nj ll nk lp mp mq mr ms bi translated">输出层中的节点<em class="lq"> k </em>最终产生实际输出(y_k)。</li></ol><p id="db1c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个过程在算法的名称中被我们称为'<strong class="kw iu"> <em class="lq">前馈'</em> </strong>'，因为输入层的输入从左到右通过网络反馈，从而在输出层产生实际输出。你可能也听说过这个过程被称为<strong class="kw iu"> <em class="lq">前馈时期</em> </strong>。</p><p id="4e60" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里的示例网络可以被概括为在每个隐藏层和输出层中具有任意数量的隐藏层和任意数量的节点。我选择在输出层使用单个节点<em class="lq"> k </em>来处理简单的二进制分类问题。如果输出层包含不止一个节点，那么我们将会处理一个多类分类问题。</p><p id="7752" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 2.2。反向传播过程</strong></p><p id="58ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们通过前馈时期，并在节点<em class="lq"> k、</em>产生实际输出，我们就需要计算梯度向量，类似于我们对单感知器模型所做的。然而，网络中额外数量的节点和隐藏层大大增加了计算该梯度向量的复杂性。</p><blockquote class="mz na nb"><p id="be7a" class="ku kv lq kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">在多层多感知器模型中，梯度向量要复杂得多，因为误差函数的偏导数是相对于与所有输出层节点相关联的所有权重以及相对于所有隐藏层节点来计算的，这使得神经网络的工作在计算上非常昂贵。通常，如果网络总共有n个权重，那么在每次迭代中将有n个偏导数要计算，并且每个偏导数的计算在数学上非常复杂。</em>T29】</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/023b5e9ed56318445077be92158c0dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kg9mApqolVCUhSMCa1rsZQ.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">多层、多感知器神经网络的梯度向量表达式(也称为怪物表达式)</figcaption></figure><p id="2d28" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将把上面的表达式<strong class="kw iu"> <em class="lq">称为怪物表达式</em> </strong>，因为我以后需要引用它，并且因为它对我来说看起来像一个怪物。根据图2，在计算梯度向量时，我们需要考虑两个层:换句话说，梯度向量的元素首先被计算为相对于连接输出层和隐藏层的权重的误差函数的偏导数，然后被计算为相对于连接隐藏层和输入层的权重的偏导数，从右到左(向后)。我强烈建议继续参考图表来形象化这里所说的内容。</p><p id="1d2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">输出层到隐藏层</strong></p><p id="7428" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们只放大monster表达式中的一项，比如第二项，我们会看到误差函数相对于连接输出层和隐藏层的权重的偏导数，即相对于某个权重<em class="lq"> w_jk </em>(从右到左)。如您所见，我们从误差函数开始，在完成<em class="lq"> </em>前馈时期后，节点<em class="lq"> k </em>在输出层产生输出后，计算该误差函数。这是一个偏导数的表达式，它将隐藏层中的第j个节点连接到节点k。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/2d2771990cda8953b5fa71229658d2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhsFZkbtNNGs95KnXKvCnA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">等式1:用于计算关于连接输出层和隐藏层的权重的梯度向量元素的等式(从右到左)</figcaption></figure><p id="f583" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个方程是如何计算与最外层相关的梯度向量元素的一般化形式。在我们的特定示例中，由于我们有三个权重将节点<em class="lq"> k </em>连接到隐藏层中的三个节点，因此我们将计算三个偏导数。这三个偏导数是描述<em class="lq">输出层到隐藏层</em>连接的梯度向量的三个分量(或元素)。实际上，这个通用表达式计算梯度向量，就像我们之前在单感知器模型中所做的一样。唯一的区别是我们在这个例子中改变了下标。</p><blockquote class="mz na nb"><p id="b5d4" class="ku kv lq kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">不要混淆梯度向量和偏导数的概念。梯度向量是包含误差函数相对于神经网络内的权重的偏导数作为其元素的向量。神经网络的梯度向量具有与网络中的权重一样多的偏导数。</em>T19】</strong></p></blockquote><p id="2d77" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">隐藏层到输入层</strong></p><p id="992f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，让我们放大monster表达式中的一项(即偏导数),它与连接隐藏层和输入层的权重之一有关(同样，从右到左)。更具体地说，让我们看看误差函数相对于连接隐藏层中的第j个节点和输入层中的第I个节点的权重的偏导数。<em class="lq">输入层中的第I个节点实际上与第I个输入相同——即x_i. </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/f720726db049812a10885a42da15c8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tgGvHt9J1XUsmJxSSJA25A.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">等式2:用于计算关于连接隐藏层和输入层的权重的梯度向量元素的等式(从右到左)</figcaption></figure><p id="57ac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种情况下，因为我们不是从输出层开始，所以误差不会一次向后传播一层。这就是为什么这个学期我们只有三个因素，而不是四个。总的来说，</p><blockquote class="mz na nb"><p id="df33" class="ku kv lq kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated">关于将<strong class="kw iu"> <em class="it">隐藏层节点连接到输入层节点</em> </strong>(也称为输入)的权重的偏导数有三项。</p><p id="0870" class="ku kv lq kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated">关于将一个<strong class="kw iu"> <em class="it">隐藏层节点连接到另一个隐藏层节点</em> </strong> <em class="it"> </em>的权重的偏导数有三项(如果有多个隐藏层)。</p><p id="8931" class="ku kv lq kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated">关于将<strong class="kw iu"> <em class="it">隐藏层节点连接到输出层节点</em> </strong>的权重的偏导数具有4项。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/3f88f7d2a8fdfa9d12b286628b34b71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IkWMCn6fQyNFbsNERpTi8A.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">图3:我们神经网络的简图。</figcaption></figure><p id="67c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">回想一下，之前我一直提到“从右到左”。为了更好地描述这里讨论的过程，请阅读上面的图3。</p><ol class=""><li id="d4e7" class="mk ml it kw b kx ky la lb ld mm lh mn ll mo lp mp mq mr ms bi translated">我们可以使用在输出节点计算的<em class="lq"> delta_k </em>值来计算与误差函数相关的关于由直边表示的权重的偏导数。</li><li id="7528" class="mk ml it kw b kx ng la nh ld ni lh nj ll nk lp mp mq mr ms bi translated">偏导数允许我们计算隐藏层中节点处的<em class="lq"> delta_j </em>值。</li><li id="ceae" class="mk ml it kw b kx ng la nh ld ni lh nj ll nk lp mp mq mr ms bi translated">然后，因为我们现在有了<em class="lq"> delta_j </em>值，我们可以计算与卷曲边缘表示的权重相关的偏导数。</li><li id="267f" class="mk ml it kw b kx ng la nh ld ni lh nj ll nk lp mp mq mr ms bi translated">但是<em class="lq"> delta_j </em>的值依赖于<em class="lq"> delta_k、</em>的值，因此我们再次在网络中从右向左移动——或者换句话说，我们在后退。</li></ol><p id="82c5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是为什么我们把这个算法的过程叫做<em class="lq">’</em><strong class="kw iu"><em class="lq">‘反向传播’，</em> </strong>也叫做<strong class="kw iu"> <em class="lq">反向传播历元。</em> </strong></p><p id="983a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 2.3。完整的FFBP算法</strong></p><p id="c038" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我现在会在这里给你更多的数学知识，因为这对理解神经网络如何学习并不重要。重要的是要知道，在做了一些微积分和代数魔术之后，我们可以进一步简化等式2:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/9b47f6d511b4b5c897d94096f964dc49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jhG-U2njzrdJNuXhVFwJg.png"/></div></div></figure><p id="433e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于梯度向量的一个元素表示误差函数相对于给定权重下降最快的方向，我们可以将上面的表达式重写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/c2eb3c75f8413a2c4a2b079dfac608cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJsSyCpPtUvHX5BrRxK99A.png"/></div></div></figure><blockquote class="mz na nb"><p id="8846" class="ku kv lq kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated"><strong class="kw iu">框中的表达式是<em class="it"> FFBP算法，该算法量化了需要应用于当前权重(</em> w_ij <em class="it">)的变化(</em>δ<em class="it">)，以便减少算法的下一次迭代的误差。</em> </strong></p></blockquote><p id="f9a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">除了应用于具有更多节点和层的神经网络之外，您能看出这与我们为单个感知器模型导出的感知器delta函数非常相似吗？梯度向量要复杂得多，但这仅仅是因为增加了节点数量及其相关权重。唯一的细微差别是我们计算网络中不同层的梯度向量元素的方式略有不同(等式1和2)。</em></p><p id="3cbf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">顺便说一下，如果你真的想看我上面跳过的数学细节，请在下面给我留言，我会把它包括进去。</p><p id="06eb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 3。概括所有的数学难题</strong></p><p id="c3e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望你看到，一旦你理解了单个感知器是如何学习的，那么理解具有更复杂拓扑的神经网络就是将我们已经知道的单个感知器扩展到由多个感知器和多个层组成的网络。也就是说，FFBP算法本质上是应用于更复杂拓扑的感知器delta函数。我将再次敦促你，确保你理解我之前帖子中的所有内容，尤其是梯度下降是如何工作的，因为这是FFBP算法的关键部分。</p><p id="61ed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">概括地说，FFBP算法在每次迭代中有两个不同的阶段:前馈时期和反向时期。</p><ol class=""><li id="a261" class="mk ml it kw b kx ky la lb ld mm lh mn ll mo lp mp mq mr ms bi translated">在前馈时期期间，输入通过网络从最左侧层到最右侧层馈送到节点中，直到在输出节点产生输出。一旦产生输出，就计算输出的误差。</li><li id="a475" class="mk ml it kw b kx ng la nh ld ni lh nj ll nk lp mp mq mr ms bi translated">在后推历元期间，误差函数用于计算梯度向量，该梯度向量包含关于网络中每个权重的偏导数，这是用于最小化误差的梯度下降的关键部分。导数是在网络中反向计算的-从最右边的输出层到最左边的输入层。</li></ol><p id="79e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">1和2中的过程描述了FFBP算法的一次迭代。随着这样的迭代越来越多，网络以某种方式不断更新权重，使得误差函数继续变得最小。<strong class="kw iu"> <em class="lq">一旦误差函数被最小化，意味着我们已经找到了网络的最佳权重，我们就说神经网络已经被训练好了。</em>T15】</strong></p><p id="88b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本系列的下一篇文章将是FFBP算法实现的数值例子。希望这个例子能进一步加强我在这篇文章中提出的数学。</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="edd4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此内容摘自<a class="ae lr" href="http://page.mi.fu-berlin.de/rojas/neural/index.html.html" rel="noopener ugc nofollow" target="_blank"> <em class="lq"> Rojas，Raul (1996):神经网络:系统介绍。柏林:施普林格出版社。第7.1至7.3章。</em> </a> <em class="lq"> 4，第8.1–8 . 2 . 1章，摘自约翰霍普金斯大学教授M. Fleischer博士的课堂笔记《神经网络导论研究生课程》。</em></p></div></div>    
</body>
</html>