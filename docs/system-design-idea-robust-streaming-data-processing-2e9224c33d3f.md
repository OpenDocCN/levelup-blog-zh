# 系统设计理念:稳健的流数据处理

> 原文：<https://levelup.gitconnected.com/system-design-idea-robust-streaming-data-processing-2e9224c33d3f>

![](img/71c356ef10676a4fed92bdd433ec58e7.png)

资料来源:cloudplatform.googleblog.com

## **背景**

流数据处理虽然提供了诸如新鲜度和与它们的批处理对应部分相比更平滑的资源消耗的好处，但是历史上与诸如不可靠和具有近似结果的缺点相关联。然而，这些缺点并不是流数据处理本身的固有特性，而是它们以前是如何实现的。正如近年来[谷歌云数据流](https://cloud.google.com/dataflow)和[阿帕奇光束](https://beam.apache.org/)运动所示，流式处理可以像批处理一样健壮。在这篇博文中，我们将讨论如何使流数据处理变得健壮。这里的很多想法都是基于 [MillWheel](https://research.google/pubs/pub41378/) ，据说谷歌云数据流就是建立在这个基础上的。

## **问题分析**

人们对流数据处理健壮性的主要关注似乎集中在分布式系统中常见的部分故障场景，如机器故障和网络中断。对于批处理，我们总是可以在部分失败时重新运行整个操作，并且在重试之后批处理最终会成功。但是在流处理中，重试是什么样子的呢？它会丢弃数据还是创建重复数据？它会损害流处理的可伸缩性和性能吗？

为了回答这些问题，我们放大单个计算步骤。流数据处理可以被认为是由许多步骤组成的流水线。如果我们能够保证单个计算步骤的健壮性，我们就可以推广到整个流水线。注意，我们还需要一个逻辑上集中的控制服务来跟踪所有的计算并监控整个管道。但是我们没有时间详细讨论。

## **单一计算步骤**

让我们在抽象层次上看一个单独的计算步骤，这样我们讨论的结论可以应用于流数据处理中的任何工作。抽象地说，计算从上游获取输入，计算结果，并将结果发送到下游，如图 1 所示。

![](img/aaea92a8f22a964130e5719c723e5331.png)

图一

我们首先假设计算没有外部副作用，例如发送电子邮件、递增外部计数器等。否则，我们不能仅仅通过流处理本身来保证健壮性，因为我们必须能够在失败时重新运行计算。如果这样的外部副作用存在，它通常是一个坏设计的标志。有时，这是必要的，在这种情况下，用户需要注意消除流处理之外的副作用。既然我们已经建立了计算可以重试任意多次，我们只需要重新运行它，直到我们知道它成功。

接下来，我们需要确保不要在旧数据上重新运行它，因为这会产生重复的结果。我们为每个数据点分配一个唯一的 ID。该计算存储已处理的数据点 id。每当计算执行时，它将首先检查它的存储，以确保它以前没有看到过该数据点。如果计算之前已经看到了数据点，它会立即向上游确认，以发出数据点已被处理的信号。显然，我们必须清理旧的数据点 id 以节省存储空间。我们稍后会谈到这一点。作为延迟优化，我们可以依靠一个[布隆过滤器](https://en.m.wikipedia.org/wiki/Bloom_filter#:~:text=A%20Bloom%20filter%20is%20a,a%20member%20of%20a%20set.)来检查计算之前是否已经看到这个数据点。如果数据点 ID 不在布隆过滤器中，我们肯定知道它是新的数据点。布隆过滤器将大大减少对存储的查询。注意，该计算需要在重启时基于其存储来重新填充布隆过滤器。

最后，我们需要确保我们可靠地向下游交付结果。一旦计算完成处理数据点，它将结果存储在其存储中。请注意，计算必须将数据点 ID 和结果原子地放在一起进行检查，这一点非常重要。之后，它向上游确认，并向下游发送结果。计算重新尝试按顺序发送结果，直到下游确认。

一个计算的整个生命周期和潜在的失败场景见图-2。

![](img/ab8b55426b6760ffd9eddc94d21334a6.png)

图二。如果计算在(1)之前失败，则重新开始。对存储的写入是原子性的，因此下一个潜在故障点是(2)。之后，检查点被保存，计算可以通过将检查点读入内存来重新开始，并从(2)开始。

需要注意的一点是，在分布式系统中，没有可靠地检测到故障这样的事情。因此，即使计算被视为失败，它实际上仍然是活动的，以可能与新启动的实例冲突的方式写入存储。处理这种情况的一种方法是使用写租约来协调对存储的访问。控制服务为每个计算分配一个租约。如果控制服务认为某个计算失效，它会使租约失效，并为新实例生成一个新租约。由于租约无效，僵尸计算的剩余写入将被阻挡。

作为吞吐量优化，我们可以对数据点信息的处理和存储进行小批量处理。不需要一个一个做。至于底层存储系统， [BigTable](https://cloud.google.com/bigtable) 非常适合。在 BigTable 中写入单行是原子性的。我们可以把一个计算的所有内容写成一个文档。BigTable 不提供强一致性。但是我们不需要那个。我们只需要 BigTable 支持的“自己写自己读”的一致性，因为计算不需要从其他人存储的内容中读取。如果您想了解更多关于分布式系统一致性的知识，请查看这篇博文[ [链接](/system-design-interview-replicated-and-strongly-consistent-key-value-store-b690d8e15c9a)。

## **数据清理**

我们之前提到过，我们不想无限期地存储检查点(数据点 id 和结果)。所以我们需要一种机制来清理它们。结果清理很简单。一旦计算从下游接收到 ack，它可以丢弃相应的结果。数据点 id 清理有点棘手。

为了管理数据点 id 的生命周期，我们需要在流管道中建立时间戳的概念。每个数据点固有地与时间戳相关联。可能是外部来源的时间。例如，当流式管道将日志文件作为输入时，可以使用日志条目的时间戳。如果我们在流管道中生成数据点，也可能是内部的时间。例如，当计算产生一个结果数据点时，它可以将当前墙时间指定为时间戳。

告诉你一个好消息:不管我们愿意等多久，总会有迟交的数据。想想从手机上收集日志的用例，用户可能会在本地日志全部发出之前开启飞行模式。我们无法预测最后一班飞机什么时候着陆。

因此，在流管道的接收端，我们必须使用试探法来确定我们是否看到了特定时间戳之前的所有数据点。我们称之为低水位线。例如，我们说我们最多等待 3 天，在这种情况下，当前时间减去 3 天就是管道接收的低水位线。对于每个计算步骤，低水位线是来自前一步骤的低水位线的最小值和当前步骤中未处理数据点的最老时间戳。参见图 3 中的图解。

![](img/9b024cece87309828f1f47ff9d807622.png)

图 3

一旦低水位线超过存储数据点 id 的时间戳，就可以清除这些数据点 id。这意味着我们还需要存储每个数据点 ID 的时间戳。请注意，清理不需要对存储进行额外的写入。一旦计算对存储的数据点 id 和结果进行了更新，它就可以将其滚动到下一轮执行检查点。

## **缩放单个计算**

即使是一个计算步骤，我们也需要扩展到多台机器。在我们将上述单一计算扩展到在多台机器上并行运行之前，有一些实际的考虑。

首先，我们需要一种方法来确定性地将数据点分配/发送给当前计算步骤的实例。否则，前面步骤中的重新传输可能会导致数据点在不同的当前步骤实例中被多次处理，从而导致重复的结果。这可以通过获取数据点 ID 的[一致散列](https://en.wikipedia.org/wiki/Consistent_hashing#:~:text=In%20computer%20science%2C%20consistent%20hashing,is%20the%20number%20of%20slots.)来完成。显然，前面计算步骤的每个实例都需要知道当前步骤的所有实例的地址，以便能够适当地路由数据点。

当前步骤的每个实例负责一致散列空间中的一系列值。如果某个特定范围的值变得太受欢迎，相应的实例将会承受很大的负载。让我们看一下拆分热实例的过程。参见图 4 中的图解。

![](img/066bf83757954469e311e8a4ce071e7e.png)

图 4

1.  control service 首先告诉正在与该热实例通信的前面的步骤实例暂停向该热实例发送结果。
2.  然后它告诉热实例暂停处理。
3.  接下来，控制服务创建热实例的副本实例。拷贝实例将读取热实例的存储内容，因此它们具有相同的恢复点。但是从现在开始，两个实例都将为后续检查点写入新行。
4.  之后，控制服务通知两个实例恢复。对于以前未确认的结果，下游将从两个实例接收它们。但是正如我们之前讨论的，下游很容易对它们进行重复数据删除。
5.  最后，控制服务告诉暂停的前一步骤实例根据更新的一致散列范围分配发送到两个实例。

如果上述任何过程未能完成，控制服务将重试，直到成功。显而易见，添加更多实例或减少实例可以用几乎相同的方式完成。

## **遗言**

希望流数据处理现在不再神秘。你现在至少可以接受流式数据处理变得健壮的想法了。更多系统设计思路，请查看此列表[ [链接](https://github.com/eileen-code4fun/SystemDesignInterviews) ]。