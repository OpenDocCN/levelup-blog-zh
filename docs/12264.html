<html>
<head>
<title>What Kinds of Controls are Possible in Reinforcement Learning Problems?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在强化学习问题中，什么样的控制是可能的？</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/what-kinds-of-controls-are-possible-in-reinforcement-learning-problems-9a30530fc65?source=collection_archive---------11-----------------------#2022-05-26">https://levelup.gitconnected.com/what-kinds-of-controls-are-possible-in-reinforcement-learning-problems-9a30530fc65?source=collection_archive---------11-----------------------#2022-05-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/ed7f294fbaadc0a9f85a6447aeeabbec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nl5MKw_cxHwo0y8Cqh0qLg.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Terrence Bowen摄影:<a class="ae kf" href="https://www.pexels.com/photo/helicopter-cockpits-in-close-up-photography-6896138/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/helicopter-cockpit-in-特写-摄影-6896138/ </a></figcaption></figure><h1 id="d74d" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">1.什么是强化学习问题，为什么控制对他们很重要？</h1><p id="8557" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">强化学习是一个问题，其中一个代理人试图学习如何通过与其环境的互动来最大化其回报。决策，也称为控制，对强化学习问题很重要，因为它们帮助代理人了解在给定状态下哪种控制与最大回报相关。有许多不同的强化学习算法，有助于发现什么是最佳控制(那些导致最大回报的控制)；未来的教程将更深入地讨论这些算法。</p><p id="bfcd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">不同的强化学习控制算法可以结合起来克服它们各自的一些缺点。例如，强化学习算法可以鼓励在代理训练的早期进行探索，以避免陷入局部最优或次优策略。此外，强化学习控制算法可以用来帮助代理从错误中恢复并返回到手头的任务。适当调整强化学习控制选择是解决强化学习问题的一个重要部分。在一个智能体被充分训练后，它将转向利用它的知识在实现或测试阶段从任何给定的状态中获得尽可能多的回报。想了解更多关于state的信息，请阅读我之前的帖子:</p><p id="3489" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://medium.com/@CalebMBowyer/what-is-state-in-reinforcement-learning-it-is-what-the-engineer-says-it-is-47add99a1121" rel="noopener">https://medium . com/@ CalebMBowyer/what-is-state-in-reinforcement-learning-it-is-the-engineer-say-is-47 add 99 a 1121</a></p><p id="9855" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">所有可能控制的集合被称为控制空间。在大多数模型化的问题中，这个空间是有限的，尤其是对于游戏。然而，在需要连续控制的问题中，控制空间很容易是无限的。在强化学习中，状态控制空间对于智能体来说通常太大而不能彻底搜索，因此它必须使用某种形式的近似来集中搜索。强化学习算法主要有两种类型:基于值的和基于策略的。基于价值的方法旨在直接估计价值函数，该函数给出在给定状态下采取给定控制的预期回报。基于策略的方法旨在直接学习策略，即从状态到控制的映射。此外，策略可以是确定性的，也可以是随机的。两种类型的强化学习算法都有它们的优点和缺点，并且有大量正在进行的研究来开发新的和改进的强化学习控制算法。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/47110967e84ef85b78cdafd93f26e11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0AsDrMSFIfSJp9CAaLHQw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">里卡多·埃斯基维尔摄影:<a class="ae kf" href="https://www.pexels.com/photo/cockpit-dashboard-with-control-console-and-gauges-3850907/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/cockpit-dashboard-with control-console-and-gauges-3850907/</a></figcaption></figure><h1 id="3820" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">2.一些强化学习控制算法:</h1><p id="c8d6" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">强化学习算法可以用来解决各种不同类型的控制问题。一些最常见的强化学习控制算法包括:</p><ul class=""><li id="54bf" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">Q-learning:该算法用于在马尔可夫决策过程(MDP)中为代理寻找最优策略。Q-learning可以是无模型或基于模型的强化学习算法，这意味着它在其Q-函数更新中不使用环境的模型，或者使用环境的模型。</li><li id="3943" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">SARSA:SARSA算法类似于Q-learning，但是它使用稍微不同的更新规则。SARSA也是一种无模型的强化学习算法。</li><li id="79fb" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">蒙特卡洛方法:蒙特卡洛方法是强化学习算法，可用于求解MDP和部分可观测MDP(pomdp)。蒙特卡罗方法通常要么符合政策，要么不符合政策。基于策略的强化学习算法从从当前策略采样的经验中学习，而基于策略的强化学习算法可以从从任意策略采样的经验中学习，即使该策略不是最优策略。<br/>每一种强化学习算法都有自己的优势和劣势，所以为你试图解决的特定问题选择正确的算法是很重要的。</li></ul><h1 id="1ec3" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">3.如何为你的RL问题选择正确类型的控制算法？</h1><p id="8d83" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">强化学习控制算法有许多不同的类型，选择使用哪一种取决于需要解决的具体优化问题。例如，如果目标是找到通过迷宫的最佳路径，那么像表格Q-learning这样的算法将非常适合，因为问题可以分解为有限的状态集和有限的控制集。状态是迷宫的位置，控件可以向上、向下、向左或向右移动。</p><p id="4db0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">然而，如果目标是控制机械臂在工作空间中移动物体，那么不同的强化学习控制算法，例如逆强化学习算法，将会更合适。强化学习控制算法的选择是一个重要的问题，对于需要解决的具体优化问题，选择正确的算法非常重要。</p><p id="9014" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一些强化学习问题更适合于表格方法，而其他问题更适合于函数逼近方法，例如使用神经网络进行控制。为您的问题选择强化学习控制算法时，要考虑的第二件事是状态空间和控制空间的大小。表格法只对小的状态空间可行，而函数逼近法可以用于较大的状态空间。第三件要考虑的事情是奖励函数的性质。有的强化学习问题奖励稀疏，有的奖励密集。稀疏奖励的一个例子是，当一个代理人只因为赢了一场游戏而得到奖励，这场游戏持续了许多决策阶段，而其他阶段收到的奖励信号为零。</p><p id="8e15" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">最后，您应该考虑可用的计算资源。一些强化学习算法比其他算法计算量更大。如果资源有限，您可能需要使用计算强度较低的算法。例如，我最近一直在训练一些非常复杂的深度Q-网络，它们需要花很长时间在我的个人电脑上训练，所以我正在迁移我的神经网络代码，以使用谷歌的Colab工具运行。这是加速计算密集型复杂强化学习算法的一种选择。谷歌的Colab工具对于深度学习工程师和深度RL工程师来说是一个非常好的免费资源。</p><h1 id="1ea8" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">4.在现实世界的强化学习问题中如何使用不同控制的例子:</h1><p id="4084" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">强化学习控制可以用于各种现实世界的问题。例如，考虑一个需要被训练以触及特定对象的机械臂。在这种情况下，强化学习可以用来调整手臂的控制参数，使其以最高的概率到达目标。在机器人控制中，强化学习用于优化机器人的控制输入，以使其实现某个目标。例如，一个机器人可能被编程为在一个房间里四处走动并收集房间里的所有物品。强化学习算法将学习控制机器人马达的最佳方式，以便它能够尽快收集所有物体。</p><p id="41f2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在玩游戏中，强化学习用于训练人工智能(AI)玩游戏，如国际象棋或围棋。在这里，控制将在哪里放置什么游戏棋子，坚持游戏规则。人工智能代理采用强化学习算法编程，允许它从错误中学习，并逐渐更好地玩游戏。最终，人工智能代理将能够在游戏中击败人类对手。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/61bdb5a60133eda4a1f63d8286cbdea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrprXmHwe3yAThv9w_YuzA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">ALEXGTACAR的照片:<a class="ae kf" href="https://www.pexels.com/photo/shallow-focus-photography-of-blue-alpine-car-1592384/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/shallow-focus-photography-of-blue-alpine-car-1592384/</a></figcaption></figure><p id="21d5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">强化学习控制的另一个例子是自动驾驶。在这里，强化学习可以用来优化控制系统，使汽车安全高效地行驶。此外，对于自动驾驶，汽车还可以根据当前交通状况和目的地等信息，学习在交通中导航的最佳方式。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/1f9742f34166bdd23fc48b14d9a02cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7eRxEwW7N1pgVeoQ5JfPA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">本亚明·梅利西摄影:【https://www.pexels.com/photo/man-planting-plant-169523/ T2】</figcaption></figure><p id="de84" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在农业环境中，强化学习可以用来控制灌溉系统。该系统将根据天气条件和植物类型等因素，学习提供给植物的最佳水量。强化学习在园艺环境中的其他应用可能是在哪里放置种子以及在什么深度播种这些种子。</p><p id="51a3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在工厂环境中，强化学习可以用来控制装配线机器人。该系统将根据要组装的产品数量和组装顺序等数据，学习组装产品的最有效方式。强化学习是一种强大的工具，可用于各种不同的环境，包括工业环境。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/132af03d3ab9ab7e3080885bdea254e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_53fm2oRwGqardVf33gxrw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Anna Shvets摄:<a class="ae kf" href="https://www.pexels.com/photo/medical-equipment-on-an-operation-room-3844581/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/medical-equipment-on-a-operation-room-3844581/</a></figcaption></figure><p id="18f3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在医疗领域，强化学习也可以用于控制机器人假体。在这种应用中，强化学习可用于调整假体的控制参数，使其为用户发挥最佳功能。因此，强化学习控制可以有广泛的现实世界的应用。我对未来的研究感到非常兴奋，因为它似乎是探索最少的，但却是以无数方式帮助人类的最有用的方法之一。</p><p id="2246" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在神经网络训练中，强化学习用于优化神经网络的权重和偏差，以使其在某些任务中表现良好。例如，强化学习算法可以用来训练神经网络以高精度识别手写数字，就像MNIST数据集的情况一样。MNIST是一个基准数据集，虽然在这一点上这是一个玩具问题，但神经网络在更困难问题上的应用正在不断增长。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/b8797ca6bd630c82d097663398f5269f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uIqnFwBnhrMxVNSWD_D8cA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">照片由Blue Arauz拍摄:<a class="ae kf" href="https://www.pexels.com/photo/atlantis-space-shuttle-in-close-up-photography-11479850/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/Atlantis-space-shuttle-in-close-photography-11479850/</a></figcaption></figure><p id="7c26" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">总之，强化学习控制可以以多种方式用于优化性能或解决非常具有挑战性的问题的特定控制问题，其中最佳控制策略目前可能还不知道。例如，在给定一组约束和目标的情况下，它们可以用来计算任何系统的最优控制输入，或者找到控制一个可能非线性系统的最佳方法。因此，强化学习控制算法代表了一种强大的工具，可以用许多不同的方式来改善高度复杂的非线性、噪声或连续系统的性能。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="6236" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">直到下一次，</p><p id="2487" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">凯勒。</p><p id="8e68" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">考虑成为一个媒体成员，永远不会错过我的故事。无限制地访问我的作品和其他作者的作品:</p><div class="nk nl gp gr nm nn"><a href="https://medium.com/@CalebMBowyer/membership" rel="noopener follow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">加入我的介绍链接媒体-迦勒鲍耶</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">medium.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jz nn"/></div></div></a></div></div></div>    
</body>
</html>