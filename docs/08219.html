<html>
<head>
<title>Portfolio Allocation with TensorTrade: (Part 2/2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于TensorTrade的投资组合配置(下)</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/portfolio-allocation-with-tensortrade-part-2-2-9ac30a6bcbfe?source=collection_archive---------1-----------------------#2021-04-12">https://levelup.gitconnected.com/portfolio-allocation-with-tensortrade-part-2-2-9ac30a6bcbfe?source=collection_archive---------1-----------------------#2021-04-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="83b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae ko" rel="noopener ugc nofollow" target="_blank" href="/portfolio-allocation-with-tensortrade-part-1-2-1d0c3b126bf6">的上一篇文章</a>中，我讲述了如何使用TensorTrade库中提供的通用组件创建一个交易环境。本文将关注如何在构建的环境中调优、训练和评估代理，同时展示如何在Ray中创建定制模型和动作分布。不再拖延，让我们开始构建模型和动作分配吧！</p><h1 id="042a" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">光线分量</h1><p id="d58a" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">当使用RL环境时，定制培训过程的不同组件是很方便的。这是由于环境可能产生的数据类型、代理接收观察之前所需的预处理或探索过程的动作采样过程。Ray很好地支持定制许多不同RL算法的能力。下图显示了RL算法的工作过程，涉及到制作算法的组件:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/2800fb23821a2e0d443e9238a3c9704d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XCJ3a2Vp_hjDT7YHf6-xw.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">来源:射线文档</figcaption></figure><p id="e0f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图中可以看出，用户可以定制任何绿色的东西。我将使用近似策略优化(PPO)算法进行训练。因此，我们的目标是构建一个定制的模型和动作分布，用于与我们的环境进行交互。</p><h2 id="fc96" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">模型</h2><p id="eb19" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">这个模型的实现很简单，并演示了将来如何修改自己的模型。对于像DQN这样的简单RL算法，在探索方法中没有选择。不像DQN，PPO在你可以使用的行动类型和探索方法上给了你更多的自由。模型输出与动作分布有直接联系。事实上，模型的输出被用作动作分布的参数。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mu"><img src="../Images/19bf4436bf7787f2f4cfce2d3ec99849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkq10dX6f88y9SKtXosulA.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">行动分配探索</figcaption></figure><p id="47c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这使得代理能够保持接近确定性动作，同时保持探索其周围的局部区域的能力。下面是模型代码的实现:</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mv mw l"/></div></figure><h2 id="d593" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">动作分布</h2><p id="f396" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">给定在投资组合中定义了m个资产，投资组合分配问题的动作空间是m-单纯形。我们如何从这个探索空间中抽取动作样本？答案以<a class="ae ko" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布</a>的形式出现。这个分布非常适合这个问题，因为它支持概率单纯形。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mx"><img src="../Images/c56ac7b277bf3d140ae74ed6e2d9ddc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DraEnGrkwtgYp0tf.jpg"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">来源:<a class="ae ko" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Dirichlet_distribution</a></figcaption></figure><p id="9c71" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要在Ray中实现这一点，需要几个方法:</p><ul class=""><li id="8b07" class="my mz it js b jt ju jx jy kb na kf nb kj nc kn nd ne nf ng bi translated"><code class="fe nh ni nj nk b">deterministic_sample</code></li><li id="3e70" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated"><code class="fe nh ni nj nk b">logp</code></li><li id="c238" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated"><code class="fe nh ni nj nk b">entropy</code></li><li id="a8c6" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated"><code class="fe nh ni nj nk b">kl</code></li><li id="6a9c" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated"><code class="fe nh ni nj nk b">required_model_output_shape</code></li></ul><p id="fffe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为Dirichlet分布内置于PyTorch中，所以我们可以利用内置方法来实现该类。下面是基于Ray的<code class="fe nh ni nj nk b">TorchDistributionWrapper</code>类构建的自定义动作分布的代码:</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="c253" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实现了模型和动作分布之后，是时候开始调整算法了。</p><h2 id="01f1" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">调整</h2><p id="c0f0" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">RL算法面临的主要问题之一是在建立训练算法时选择超参数。我应该为多少个时代训练一个模型？批量应该是多少？在一个神经网络中应该使用多少个隐藏单元？幸运的是，有许多算法可以解决这个问题。在本教程中，我将使用基于群体的训练(PBT)算法。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mv mw l"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">调谐码</figcaption></figure><h2 id="d2f6" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">火车</h2><p id="2ae6" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">培训将按照调优脚本的相同“调”进行😉。唯一的区别是将超参数设置为调整后的参数，并根据平均剧集奖励设置更高的停止阈值。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mv mw l"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">列车代码</figcaption></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nq"><img src="../Images/d41496d860ff9b24e851175c1ffb9987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YEHsxbocwRTOA6qbvmjLXA.png"/></div></div></figure><p id="8e2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图表中，我们可以看到代理学会了如何成功地在环境中导航。有明显的低买高卖模式。也有代理人将资金分配给一种资产而不是另一种资产的时间点，因为价格在不同的方向移动。因为代理已经通过了我们给它的随机正弦曲线，让我们看看使用不同随机过程生成的价格曲线会发生什么。</p><h2 id="77a5" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">评价</h2><p id="91f6" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">为了做一些启发式评估，我用多元几何布朗运动生成了价格曲线。要查看实现，请访问<a class="ae ko" href="https://github.com/mwbrulhardt/penv" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库。为该算法计算的两个指标是夏普比率和最大每日压降。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="faea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我用多元几何布朗运动的三种实现方式渲染了三个不同环境场景的图像。以下是三集样片中的图片:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nq"><img src="../Images/9d06401d29ad84b51989aaa856aa86a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Q6RJLll20N2Picnj1NGYw.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">样本第1集</figcaption></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nq"><img src="../Images/9f600ff9ed05cf754121e2227e417bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwk7TWZ9dgxFjgPL-OhM1w.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">样本第2集</figcaption></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nq"><img src="../Images/73d013490504d49b914de47fb176200d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIpUbr3MfnUWZ7bbX_I30w.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">样本第3集</figcaption></figure><p id="baf8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">三个样本的结果相当中性。夏普比率分别为-0.01、0.04和0.01。在大多数情况下，代理人没有赚或亏太多的钱。平心而论，这个代理并不是在这种环境中直接训练出来的，所以我并不指望它能做得非常好。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="ab gu cl nr"><img src="../Images/457bc3006f87d58d5a819e4a5ae4d8d0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*wk43-4_PPHB92OZ3uCGkqw.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">随机代理</figcaption></figure><p id="e80c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">值得注意的是，与随机代理相比，代理实际上表现良好。平均而言，随机代理的夏普比率为-0.06，MDD为42%。因此，代理必须学会一些东西，以退出环境收支平衡！</p><h1 id="a9f3" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论</h1><p id="40d0" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在本系列中，我演示了用户如何定制TensorTrade组件来创建投资组合分配环境。此外，我们能够根据我们的期望定制Ray的组件。从本质上讲，这些教程是一个模板，用于结合使用TensorTrade和Ray来创建适合任何情况的特定需求的交易算法。最后，我总结了我们在本系列第1部分和第2部分中学到的所有东西。</p><h2 id="bc00" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">我们学到了什么？</h2><ul class=""><li id="aab8" class="my mz it js b jt ln jx lo kb ns kf nt kj nu kn nd ne nf ng bi translated">从TensorTrade通用组件构建一个定制的投资组合环境。</li><li id="e253" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated">在TensorTrade中实现一个自定义的连续<code class="fe nh ni nj nk b">ActionScheme</code>。</li><li id="c3d7" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated">在Ray中实现自定义模型和动作分布。</li><li id="957c" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated">实现使用多元几何布朗运动来模拟价格的流。</li><li id="592f" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated">使用Ray的调整库，通过PPO算法在自定义环境中执行超参数调整。</li><li id="a408" class="my mz it js b jt nl jx nm kb nn kf no kj np kn nd ne nf ng bi translated">计算剧集的两个不同指标。</li></ul><p id="00f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">期待以后能出新的教程，下次和大家再见！</p><h2 id="50b2" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">密码</h2><ul class=""><li id="14af" class="my mz it js b jt ln jx lo kb ns kf nt kj nu kn nd ne nf ng bi translated"><a class="ae ko" href="https://github.com/mwbrulhardt/penv" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ul><h2 id="f859" class="mi kq it bd kr mj mk dn kv ml mm dp kz kb mn mo ld kf mp mq lh kj mr ms ll mt bi translated">参考</h2><ul class=""><li id="ff73" class="my mz it js b jt ln jx lo kb ns kf nt kj nu kn nd ne nf ng bi translated">蒋、、徐迪星、梁金军。"金融投资组合管理问题的深度强化学习框架."ArXiv.org(2017年)。网络。&lt;<a class="ae ko" href="https://arxiv.org/abs/1706.10059" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.10059</a>T5。</li></ul></div></div>    
</body>
</html>