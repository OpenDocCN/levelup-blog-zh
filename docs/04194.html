<html>
<head>
<title>Lyrics Generator using LSTM: NLP Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LSTM的歌词生成器:NLP项目</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/lyrics-generation-using-lstm-5a5a0bcac4fa?source=collection_archive---------4-----------------------#2020-06-14">https://levelup.gitconnected.com/lyrics-generation-using-lstm-5a5a0bcac4fa?source=collection_archive---------4-----------------------#2020-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e37a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最近，我建立了一个单词预测模型，并使用该模型来生成歌词。我用来训练模型的数据包含了泰勒·斯威夫特歌曲中的歌词。我的人工智能看起来很沮丧，或者也许它只是一个心碎。结果是这样的:</p><div class="ko kp kq kr gt ab cb"><figure class="ks kt ku kv kw kx ky paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/25be0e978b5586f3997ddcff0117f396.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*5Jw7WUpV88tL6ypmqvgwUQ.png"/></div></figure><figure class="ks kt lf kv kw kx ky paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/d5780effa62b15665e3f6568420c9d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*no9_tSCKYXmb6-aMgsszMQ.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk lk di ll lm translated">这是我的模型预测的一个例子，当我给它提供一些词根的时候。</figcaption></figure></div><p id="18de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们来谈谈我是如何做到的——方法和工具。我们开始吧！</p><figure class="ko kp kq kr gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi ln"><img src="../Images/f13eddae150a7108786eba9c4dbeef21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiiwmmDyNj6LSDNnuGzKww.png"/></div></div></figure><h2 id="41ec" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated">读取数据</h2><p id="417a" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">我们将从一个简单的问题开始。如何读取数据？这很简单</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="4074" class="lo lp it mn b gy mr ms l mt mu">data = open('../filename.txt').read()</span></pre><p id="0d59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您的数据可能如下所示</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="4687" class="lo lp it mn b gy mr ms l mt mu">'He said the way my blue eyes shined<strong class="mn iu">\r\n</strong>Put those Georgia.......'</span></pre><p id="7658" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意“\r\n”？这些代表下一行或句子的结尾。有时，根据数据和读取数据的方式，它可能只是' \r '或' \n '。我们必须摆脱他们。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="97c9" class="lo lp it mn b gy mr ms l mt mu"># Splitting the string into sentences, while converting whole data into lowercase.</span><span id="8e96" class="lo lp it mn b gy mv ms l mt mu">corpus = data.lower().split("\r\n")</span><span id="663d" class="lo lp it mn b gy mv ms l mt mu"># To make sure no sentence appears twice in our corpus, we use set. Otherwise, it will make the model biased.</span><span id="548b" class="lo lp it mn b gy mv ms l mt mu">corpus = list(set(corpus))</span></pre><p id="5fb3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，语料库是一个包含你的数据的每个句子作为其元素的列表。因为我们使用set，所以它没有任何重复的值。</p><h2 id="6c2f" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated">组织数据</h2><p id="2ce4" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">因为我们的模型只接受数字作为输入，所以我们需要将单词转换成数字。</p><p id="02b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> Tokens: </strong>为此，我们将使用Tokenizer，这是Keras附带的一个非常有用的工具。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="3aaf" class="lo lp it mn b gy mr ms l mt mu">tokenizer = Tokenizer()</span><span id="a5af" class="lo lp it mn b gy mv ms l mt mu">tokenizer.fit_on_texts(corpus)</span><span id="eed1" class="lo lp it mn b gy mv ms l mt mu">total_words = len(tokenizer.word_index) + 1</span></pre><p id="e4d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们适合语料库的标记器时，我们将与语料库相关的数据存储在标记器本身中。</p><p id="e805" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你运行<em class="mw"> tokenizer.word_index </em>，它会返回如下结果:</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="0e92" class="lo lp it mn b gy mr ms l mt mu">{'you': 1,  'i': 2,  'and': 3,  'the': 4,  'to': 5,  'me': 6,  'a': 7,  'it': 8,  'in': 9,  'my': 10,  'your': 11,.......}</span></pre><p id="3e4a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它是一个字典，其中关键字代表语料库中的所有单词，每个关键字的值是其索引。</p><p id="e07a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总字数取为<em class="mw">len(tokenizer . word _ index)+1</em>。</p><p id="bda8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">“+1”用于为不属于我们语料库的未知单词做标记。</p><p id="c087" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输入序列:</strong>输入序列是单词排列方式的数字表示。继续读下去就知道是什么意思了。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="99d4" class="lo lp it mn b gy mr ms l mt mu"># create input sequences using list of tokens</span><span id="0aca" class="lo lp it mn b gy mv ms l mt mu">input_sequences = []</span><span id="0f8a" class="lo lp it mn b gy mv ms l mt mu">for line in corpus:<br/>    token_list = tokenizer.texts_to_sequences([line])[0]<br/>    <br/>    for i in range(1, len(token_list)):<br/>        n_gram_sequence = token_list[:i+1]<br/>    <br/>    input_sequences.append(n_gram_sequence)</span></pre><p id="6c71" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将对此进行逆向工程，看看这是如何工作的。</p><p id="623b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当你打印<em class="mw"> input_sequences，</em>我们会得到这样的东西:</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="85aa" class="lo lp it mn b gy mr ms l mt mu">[[125, 45],  <br/> [125, 45, 901],<br/> [125, 45, 901, 9],  <br/> [125, 45, 901, 9, 10],  <br/> [125, 45, 901, 9, 10, 36],  <br/> [125, 45, 901, 9, 10, 36, 96],  <br/> [125, 45, 901, 9, 10, 36, 96, 11],  <br/> [125, 45, 901, 9, 10, 36, 96, 11, 902],<br/> .....<br/> ......]</span></pre><p id="a2a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是语料库中第一句话的再现，即<br/><em class="mw">‘我背上仍有你的刀伤’。</em></p><p id="9085" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在您可以检查一下，在tokenizer.word_index中，still的索引是125。所有的字都是如此。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="6670" class="lo lp it mn b gy mr ms l mt mu">&gt;&gt;&gt; tokenizer.word_index["still"]<br/>Out: 125</span></pre><p id="94db" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同一个句子有不同长度的输入序列。它们只是代表了模型如何学习。首先，我们输入两个单词，并教它预测第三个单词。然后，我们输入3个单词，并期望它预测第4个单词。每个句子也是如此。我们的模型就是这样运作的。</p><p id="c610" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">填充序列:</strong>在我们的语料库中，每个句子都有不同的长度。因此输入序列也将具有不同的长度。这样的输入不能输入到模型中，因为它是不平衡的。因此，我们执行填充。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="10e2" class="lo lp it mn b gy mr ms l mt mu"># pad sequences</span><span id="2283" class="lo lp it mn b gy mv ms l mt mu">max_sequence_len = max([len(x) for x in input_sequences])</span><span id="edb7" class="lo lp it mn b gy mv ms l mt mu">input_sequences = np.array(pad_sequences(input_sequences,<br/>                       maxlen = max_sequence_len, padding='pre'))</span></pre><p id="86c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先我们定义，序列的最大长度是多少。然后，我们填充输入序列的每一行，使每一行中的元素数量相等。但是填充是如何工作的呢？</p><p id="f0cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设序列的最大长度是10。但是你的序列长度只有4。例:[125，45，901，9]。</p><p id="0fc1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">填充后，这个序列将变成:[0，0，0，0，0，0，125，45，901，9]。</p><p id="8ce6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输入序列现在看起来像这样:</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="5146" class="lo lp it mn b gy mr ms l mt mu">[[  0,   0,   0, ...,   0, 125,  45],        <br/> [  0,   0,   0, ..., 125,  45, 901],        <br/> [  0,   0,   0, ...,  45, 901,   9],<br/> ..........<br/> ...........</span></pre><p id="bc51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">预测器和标签:</strong>我们这里解决的问题是一个有监督的学习问题。因此，我们必须为模型提供一些标签，以便它可以概括用于预测的单词和预测的单词之间的关系。</p><p id="5a0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，我们将使用我们的输入序列，并使用所有序列的最后一个单词作为所有前面单词的标签。</p><blockquote class="mx my mz"><p id="5550" class="jq jr mw js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">用简单的语言来说，在句子<em class="it">‘我背上还有刀疤’:<br/></em><strong class="js iu">还有</strong>会被用来预测<strong class="js iu">有了</strong> <br/> <strong class="js iu">还有</strong>会被用来预测<strong class="js iu">有刀疤</strong> <br/> <strong class="js iu">还有</strong>会被用来预测<strong class="js iu">有</strong>等等。</p></blockquote><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="874d" class="lo lp it mn b gy mr ms l mt mu">predictors, label = input_sequences[:,:-1],input_sequences[:,-1]</span></pre><h2 id="232f" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated">构建模型</h2><p id="e57a" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">老实说，复杂的部分做完了。现在我们只需要用LSTM建立一个RNN模型。点击这里阅读更多关于LSTM的信息。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="7e11" class="lo lp it mn b gy mr ms l mt mu">model = Sequential()</span><span id="567d" class="lo lp it mn b gy mv ms l mt mu">model.add(Embedding(total_words, 50, input_length=max_sequence_len-1))</span><span id="bdb2" class="lo lp it mn b gy mv ms l mt mu"># Add an LSTM Layer<br/>model.add(Bidirectional(LSTM(150, return_sequences=True)))  </span><span id="1d30" class="lo lp it mn b gy mv ms l mt mu"># A dropout layer for regularisation<br/>model.add(Dropout(0.2))</span><span id="580e" class="lo lp it mn b gy mv ms l mt mu"># Add another LSTM Layer<br/>model.add(LSTM(100)) </span><span id="b1d3" class="lo lp it mn b gy mv ms l mt mu">model.add(Dense(total_words/2, activation='relu'))  </span><span id="4067" class="lo lp it mn b gy mv ms l mt mu"># In the last layer, the shape should be equal to the total number of words present in our corpus</span><span id="0529" class="lo lp it mn b gy mv ms l mt mu">model.add(Dense(total_words, activation='softmax'))</span><span id="90dc" class="lo lp it mn b gy mv ms l mt mu">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')  #(# Pick a loss function and an optimizer)</span><span id="1bfa" class="lo lp it mn b gy mv ms l mt mu">print(model.summary())</span></pre><p id="6737" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们将根据预测值和标签来拟合模型。您可以改变时期，以查看准确性的变化以及模型在何处过度拟合数据。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="290e" class="lo lp it mn b gy mr ms l mt mu">history = model.fit(predictors, label, epochs= 100, verbose=1)</span></pre><h2 id="3361" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated">生成歌词</h2><p id="8c74" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">我们现在将使用我们构建的模型生成歌词。</p><pre class="ko kp kq kr gt mm mn mo mp aw mq bi"><span id="614d" class="lo lp it mn b gy mr ms l mt mu">def make_lyrics(seed_text, next_words):<br/>    for _ in range(next_words):<br/>        token_list = tokenizer.texts_to_sequences([seed_text])[0]<br/>        token_list = pad_sequences([token_list],<br/>                     maxlen=max_sequence_len-1,padding='pre')</span><span id="1052" class="lo lp it mn b gy mv ms l mt mu">        predicted = model.predict_classes(token_list, verbose=0)</span><span id="3674" class="lo lp it mn b gy mv ms l mt mu">        output_word = ""</span><span id="1143" class="lo lp it mn b gy mv ms l mt mu">        for word, index in tokenizer.word_index.items():<br/>            if index == predicted:<br/>                output_word = word<br/>                break<br/>        seed_text += " " + output_word<br/>    print(seed_text)</span></pre><p id="a2bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的函数挺好理解的。<br/>输入是“seed_text ”,然后对其进行标记和填充，使其等于序列的最大长度。然后，模型使用该种子文本来预测一个数字。该数字表示预测单词的索引。</p><p id="2df4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，我们遍历word_index字典中的所有条目，找到预测的单词，并将其添加到种子单词中，以完成句子。但是这个方法预测的只有一个词。因此，我们将所有这些放入一个for循环中，并多次运行。你需要做的迭代次数等于你要预测的字数。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="5372" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢阅读。我希望你喜欢我的文章，并发现它是有帮助的。如果你有任何问题或建议，请随时写在评论区。你也可以在LinekdIn这里跟我连线:<a class="ae nd" href="https://www.linkedin.com/in/juyalishant/" rel="noopener ugc nofollow" target="_blank"> Ishant Juyal </a>。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="ce12" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated">参考资料:</h2><ol class=""><li id="7f9d" class="nl nm it js b jt mh jx mi kb nn kf no kj np kn nq nr ns nt bi translated">数据集:<a class="ae nd" href="https://www.kaggle.com/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums" rel="noopener ugc nofollow" target="_blank">Kaggle.com/Taylor雨燕歌词</a></li><li id="6962" class="nl nm it js b jt nu jx nv kb nw kf nx kj ny kn nq nr ns nt bi translated">视频教程:<a class="ae nd" href="https://youtu.be/Pe56OZ4aPds" rel="noopener ugc nofollow" target="_blank">https://youtu.be/Pe56OZ4aPds</a></li></ol></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><div class="ko kp kq kr gt nz"><a href="https://skilled.dev" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">编写面试问题</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">一个完整的平台，在这里我会教你找到下一份工作所需的一切，以及…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">技术开发</p></div></div><div class="oi l"><div class="oj l ok ol om oi on ld nz"/></div></div></a></div></div></div>    
</body>
</html>