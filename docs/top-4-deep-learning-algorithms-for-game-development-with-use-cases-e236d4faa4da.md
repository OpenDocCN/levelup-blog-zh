# 游戏开发的 4 大深度学习算法及用例

> 原文：<https://levelup.gitconnected.com/top-4-deep-learning-algorithms-for-game-development-with-use-cases-e236d4faa4da>

如何使用这 4 种深度学习方法创建现实中从未见过的游戏，其中一种是必须学习的

![](img/34f7e38cab56beed5a973e0f8bdb8d4e.png)

来自 Unsplash 的бодьсанал·布吉

**我预测大型语言模型(LLM)将对游戏的未来产生深远的影响，这些模型旨在理解和同情你，并试图使用情感智能属性来影响你。**

**由于 LLM 的进步，情商正在进入游戏领域。**

深度学习方法和 LLM 是相互关联的，不考虑具体算法优化应用中的差异。

我认为，游戏将成为测试生态系统，以了解用户和游戏景观之间的虚拟和增强现实体验，从而开发和部署真实世界的虚拟+增强现实。

人工智能(AI)自诞生以来就一直是游戏不可或缺的一部分，但近年来，随着游戏变得越来越复杂，它的影响力甚至变得更大。

![](img/359a142e81aa1398592343cb55a47e98.png)

来自 Unsplash 的泰勒·卡拉汉

## 考虑游戏中 NPC(非玩家角色)[1]的实现。NPC 是由电脑而不是玩家控制的角色。

他们可以是任何人，从提供信息或出售物品的简单店主，到任何游戏故事情节的要素。举例来说，为了让 NPC 看起来更逼真，可以使用人工智能来影响 NPC 的反应，以实现他们如何适应周围发生的事情的更现实主义(而不仅仅是等待玩家角色的特定提示)。

可以如何为 NPC 激活 AI 的一些示例包括以下:(1)寻路(即，在两点之间找到最短或最吸引人的路线)，(2)行为树(详细描述 NPC 在各种情况下应该如何响应的逻辑流程图)，(3)基于对话分析的决策，以及(4)使用自然语言处理算法与其他 NPC 的无脚本对话。

以下是四种深度学习能力。

![](img/4694494e080b8e2e88bbeabdf043e25f.png)

由来自 Unsplash 的[卡尔生](https://unsplash.com/@carltraw)

# **卷积神经网络**

使用 CNN 的主要优势之一是管理大量数据[2]。这是因为 CNN 可以在非常大的数据集上训练，并且仍然可以达到期望的精度水平。因为 CNN 倾向于对新数据进行很好的概括[6]，所以可以使用它来开发游戏，并且即使输入数据有微小的变化(例如，不同的照明条件或视角)，游戏也能够适应并表现良好。

CNN 可以学习图像和视频中的复杂模式[3]。举例来说，CNN 可以用来开发一个游戏，在近乎实时的视频镜头中识别人脸或物体。

CNN 在处理图像和视频时也很高效。这是因为它们可以在 GPU 上并行化[5]，这种方法可以缩短训练时间。

具有强化学习算法的 CNN(那些破解了如何以经验可重复的方式实现端到端的代码的人)将成为通知特殊创新的先锋。此外，这种组合可以使游戏对用户来说更加真实和具有挑战性，因为 AI 可以从错误中学习并随着时间的推移变得更好。

![](img/09de5ee589b8b60da8a744d7f1527dfc.png)

来自 Unsplash 的 Florian Olivo

## CNN 实施机会

— **物体检测和识别**:CNN 可以检测和识别游戏环境中的物体，从而实现更加真实和互动的游戏体验。例如，流行的移动游戏 Pokémon GO 可以使用 CNN 来识别玩家活动所捕捉的真实世界图像中的 PokeStops 和 Pokémon 角色:CNN 可以用于根据描述性(历史)数据预测游戏中的潜在事件。例如，这种方法可用于生成真实的 NPC 行为或预测用户动作以相应地预载资源。

— **寻路**:通过卷积理解 3D 环境的结构，NPC 可以更真实地导航，而不会陷入困境或走不切实际的捷径。

— **动作控制**:流畅的动作对于身临其境的体验至关重要。使用光流技术[8]，估计一个物体如何从视频镜头的一帧移动到另一帧是可能实现的。

![](img/9e2aaf1765ff00c8f7e0af7b1cf382f7.png)

来自 Unsplash 的 Lorenzo Herrera

# **递归神经网络(RNN)**

这是橡胶与创造更真实可信的角色、改进游戏机制和增加沉浸感的潜力相遇的地方。

— **人物的现实主义** : RNN 可以用来学习和模仿人类的行为、情感和表情。因此，游戏开发者可以创造出更可信、更逼真的数字角色。

— **改进的游戏机制** : RNN 可以通过理解玩家的行为和偏好来学习优化游戏。因此，开发者可以潜在地使用这些相应的数据来改善游戏中的体验，例如关卡设计或对手 AI 模式。

— **沉浸式，放大式**:这种网络架构可能允许游戏对玩家的输入做出动态响应，使体验感觉更加自然和流畅。此外，通过给予 NPC 现实的行为和反应，玩家可能更有动力留下来并继续在游戏生态系统中操作。

![](img/c136ee9d874d6b0fe49eb95c913f3887.png)

来自 Unsplash 的 Riho Kroll

## **RNN 各地实施**

—预测一盘棋的下一步棋。

—开发能够识别场景中的对象并跟踪其运动的计算机视觉算法。这种方法可以用于视频游戏或增强现实应用中的近实时对象检测。

—自然语言处理任务，如机器翻译或聊天机器人开发，创建具有更逼真对话的角色，或自动将游戏对话翻译成多种语言。

—玩家行为的预测模型。这可以让开发者设计更智能的 NPC，对玩家的行为做出真实的反应，或者推荐为每个玩家定制的产品和任务线。

## 更多与 RNN 相关的例子:

—国际足联:改进球员动画和动作

— PUBG:预测玩家接下来会去哪里

——上古卷轴在线:创建与 NPC 的真实对话

—堡垒之夜:推荐在物品商店购买的商品

——炉石:开发新的卡牌和机制

![](img/92eb968149074707b9f8f8e4981ae732.png)

来自 Unsplash 的米卡·鲍梅斯特

# **长短期记忆网络(LSTM)**

LSTM 网络是一种可以从序列数据中学习的神经网络[9]。这使它们成为手写识别和机器翻译问题的理想选择，在这些问题中，输入代表一系列的动作。

LSTMs 还被发现在建模文本数据方面是有效的[10]，使得它们对于诸如自然语言理解和生成的任务是有价值的。此外，因为 LSTMs 可以长时间保留信息[11]，所以它们可能是基于内存的游戏中实现的潜在候选对象。

## 机会

—提高准确性:与传统的人工神经网络相比，LSTM 网络可以更好地记住以前的输入[11]，有可能提供更准确的预测。应用程序可能包括需要预测复杂模式的游戏，例如实时策略游戏中的对手移动或开放世界沙盒游戏中的资源收集。

—改进的可解释性:因为 LSTMs 可以记住以前的输入(具体到在一段时间内保留基于时间的信息[11])，它们可以潜在地提供关于网络为什么做出某些决策的见解。在游戏开发的调试和测试阶段，当试图跟踪错误或确定潜在的改进领域时，这种改进的可解释性可以提供信息。

—更快的开发时间:LSTMs 通过减少正确工作所需的试错次数，有可能加快整体开发时间[12]。在某些情况下，甚至有可能使用使用机器学习技术(如 LSTMs 之上的强化学习)的工具来自动化部分设计过程。

![](img/dfb814c34b147634187fe44ffb4c5f26.png)

来自 Unsplash 的 Boukaih

## 可行的部署

—创建展现真实行为的 NPC。例如，一个 NPC 可能会记住一个玩家的名字玩家的介绍(当这被应用而不是简单地存储信息然后直接输出那个简单的洞察力)并且在未来的互动中应用那个知识。这种方法可以使用 NPC 来具有长的短期记忆网络，以存储关于玩家的信息，用于将来回忆。

—对手 AI 设计。通过让对手回忆玩家以前的动作，他们可以更好地预测他们的动作，并为玩家创造独特的挑战。例如，如果一个敌人记得一个玩家在攻击时喜欢向左躲闪，他们可能会开始将自己定位在更靠近右侧的位置，以便在下一次攻击时让他们措手不及。

—在游戏中创建可信、真实或真实的角色对话。如果角色能够记住之前在对话中说过的话，这些见解可以根据上下文产生原始的(即兴的，无脚本的)反应。

![](img/30b400c5c9a0b670d2f890e2e73839db.png)

来自 Unsplash 的 Hello 灯泡

# **深度 Q-网络**

## 这是一门必须学习的课程，因为我相信它最有希望进步。

这里的机会是在自主游戏中过度扩展(例如，由 NPC 指导的自主决策)。一个用例是为游戏中的代理开发控制策略[13]。例如，假设玩家正在控制一个需要穿越障碍的角色。在这种情况下，深度 Q 网络可以被训练用于潜在地最大化玩家奖励(即，他们分数的增加)的动作。其他用例包括使用深度 Q 网络来生成真实的 NPC 行为或路径查找算法。另外:

—开发控制策略:如上所述，一种关于深度 Q-网络可以集成到视频游戏开发中的方法是为 NPC 或玩家自己控制的角色开发控制策略。这可能涉及端到端地训练 DQN 代理，以便它接收原始像素输入[14]并将它们直接映射到动作输出概率。后一种方法允许非线性函数近似[15][16]，这可能导致在许多其他环境中的成功推广。

![](img/44a75005d728f77b9b1b9f5b4f91a72c.png)

你好，我是来自 Unsplash 的 Nik

—生成合理且现实的 NPC 行为:生成对手或 NPC 行为。也就是说，在一组输入上训练 DQN，这些输入描述了 NPC 可用的游戏状态和动作，并输出每个动作的概率。DQN 有可能在每个时间步长为 NPC 选择一个动作。通过应用这种方法，NPC 可以表现出潜在的更现实的行为，因为在做决策时可能会对其环境的不同元素有更大的考虑(而不是遵循预定的路径或脚本)。

—寻路算法[17]:寻找两个节点之间最短路径的一种方法是通过 Djikstra 的 alDjikstra 的 S8]；然而，这种方法可能不总是找到最佳路径(像固定点[19]可能对整个旅程的影响)。通过输入当前游戏状态和可用的动作，DQN 可以用来找到更好的路径，并且可能显示每个动作的输出(概率)。然后，具有最高概率的路径可以被选为最佳路径。通过以这种方式使用 DQN，在寻找路径时可以考虑不同的环境因素(例如，玩家与其游戏生态系统的交互)，这可以导致更有效或更真实的结果。

![](img/33523ce0d02db6a8533f40d7f70a9851.png)

你好，我是来自 Unsplash 的 Nik

# **离别的思念**

除了创建可信的 NPC，程序员还有机会利用深度学习技术从用户活动中学习，以开发游戏的各个方面，如实时策略游戏中的敌对行为，展示高度情商的移情体验，或预测更接近现实生活的经济活动系统，如模拟(如模拟城市)。

如果你对这篇文章的编辑有任何建议，或者对进一步扩展这个主题领域有什么建议，请和我分享你的想法。

## 另外，请考虑订阅我的每周简讯:

[](https://pventures.substack.com) [## 产品。风险时事通讯

### 产品和人工智能交汇处的想法。让我先读一下产品。风险投资时事通讯…

pventures.substack.com](https://pventures.substack.com) 

我写了以下与这篇文章相关的内容:他们可能与你有相似的兴趣:

## 深度学习必须知道的 5 个数学概念

[](/top-5-must-know-math-concepts-for-deep-learning-95fd09ef038b) [## 深度学习必须知道的 5 个数学概念

### 一个是必须学习的

levelup.gitconnected.com](/top-5-must-know-math-concepts-for-deep-learning-95fd09ef038b) 

## 深度学习的线性代数，简单解释

[](https://pub.towardsai.net/linear-algebra-for-deep-learning-simply-explained-e279998cfad1) [## 深度学习的线性代数，简单解释

### 了解在深度学习中应用线性代数的 4 个原因，并了解 4 个用例来演示…

pub.towardsai.net](https://pub.towardsai.net/linear-algebra-for-deep-learning-simply-explained-e279998cfad1) 

**参考文献:**

*1。科佩尔，&# 38；哈加斯。(2018 年 1 月 1 日)。为 3D 视频游戏中的非玩家角色实现 AI。斯普林格国际出版公司。*[*https://link . springer . com/chapter/10.1007/978-3-319-75417-8 _ 57*](https://link.springer.com/chapter/10.1007/978-3-319-75417-8_57)

*2。通过用更大的数据训练 CNN，进一步完善准确性。(未注明)。IEEE Xplore。检索到 2022 年 8 月 1 日，来自*[*https://ieeexplore.ieee.org/abstract/document/7814098*](https://ieeexplore.ieee.org/abstract/document/7814098)

*3。基于 CNN 的大规模图像数据特征漂移补偿的联合聚类和表示学习。(未注明)。IEEE Xplore。检索到 2022 年 8 月 1 日，来自*[【https://ieeexplore.ieee.org/abstract/document/8017517】T21](https://ieeexplore.ieee.org/abstract/document/8017517)

*4。基于 FAREC——CNN 的高效人脸识别技术。(未注明)。IEEE Xplore。检索到 2022 年 8 月 1 日，来自*[*https://ieeexplore.ieee.org/abstract/document/7831628*](https://ieeexplore.ieee.org/abstract/document/7831628)

*5。CNN 框架在 GPU 上的性能分析。(未注明)。IEEE Xplore。检索到 2022 年 8 月 1 日，来自*[*https://ieeexplore.ieee.org/abstract/document/7975270/*](https://ieeexplore.ieee.org/abstract/document/7975270/)

*6。优化层改进 CNN 泛化和转移学习，用于从 EEG 进行想象语音解码。(未注明)。IEEE Xplore。检索到 2022 年 8 月 1 日，转自*[](https://ieeexplore.ieee.org/abstract/document/8914246)

**7。皮尔克，波耶尔，霍尔津格，&居特尔。(2017 年 1 月 1 日)。在视频游戏中使用 leap 运动控制器进行基于手势的交互。斯普林格国际出版公司。*[*https://link . springer . com/chapter/10.1007/978-3-319-58071-5 _ 47*](https://link.springer.com/chapter/10.1007/978-3-319-58071-5_47)*

**8。里戈，共和党人，奥特罗，法官，第 38 位；拉尼拉，j .(未注明)。使用基于 GPU 的光流算法的低成本 3D 人机界面设备——IOS Press。综合计算机辅助工程，18(4)，391–400。*[*https://doi.org/10.3233/ICA-2011-0384*](https://doi.org/10.3233/ICA-2011-0384)*

**9。利普顿，Z. C .，凯尔，D. C .，埃尔坎，c .，&# 38；韦策尔河(2015 年 11 月 11 日)。用 LSTM 递归神经网络学习诊断。ArXiv.Org。*[*https://arxiv.org/abs/1511.03677*](https://arxiv.org/abs/1511.03677)*

**10。张，b，金，m，哈里里马纳，g，康，s，&# 38；金智威(2020)。提高文本分类准确率的双 LSTM 模型:结合 word2vec CNN 和注意机制。应用科学，10(17)。*[【https://doi.org/10.3390/app10175841】T21](https://doi.org/10.3390/app10175841)*

*11。马纳维。(2018 年 1 月 1 日)。RNN 和 LSTM。阿普瑞斯。[*https://link . springer . com/chapter/10.1007/978-1-4842-3516-4 _ 9*](https://link.springer.com/chapter/10.1007/978-1-4842-3516-4_9)*

**12。税，韦列尼克，罗莎，l .，&# 38；大仲马。(2017 年 1 月 1 日)。基于 LSTM 神经网络的预测业务过程监控。斯普林格国际出版公司。*[*https://link . springer . com/chapter/10.1007/978-3-319-59536-8 _ 30*](https://link.springer.com/chapter/10.1007/978-3-319-59536-8_30)*

**13。Oh，T. H .，Kim，J. W .，Son，S. H .，Kim，h .，Lee，k .，&# 38；李，J. M. (2021)。用深度 Q 网络实现模拟移动床过程的自动控制。色谱杂志 A，1647，462073。*[*https://doi.org/10.1016/j.chroma.2021.462073*](https://doi.org/10.1016/j.chroma.2021.462073)*

**14。王，H. Y .，查韦斯，k .，&# 38；洪，a .(2015 . 8 . 18)。分布式深度 q 学习。ArXiv.Org。*[*https://arxiv.org/abs/1508.04186*](https://arxiv.org/abs/1508.04186)*

*15。海德尔，哈维，王，&# 38；斯科特尼。(2021).基于高斯非线性函数逼近的强化学习。计算机科学，2(3)，1-12。[](https://doi.org/10.1007/s42979-021-00642-4)*

**16。Mnih，v .，Kavukcuoglu，k .，Silver，d .，Graves，a .，Antonoglou，I .，Wierstra，d .，&# 38；里德米勒，M. (2013 年 12 月 19 日)。用深度强化学习玩雅达利。ArXiv.Org。[*https://arxiv.org/abs/1312.5602*](https://arxiv.org/abs/1312.5602)**

***17。DQN 寻路训练中白盒 Q 表变异的对立范例建构。(未注明)。IEEE Xplore。检索到 2022 年 8 月 1 日，来自*[*https://ieeexplore.ieee.org/abstract/document/8411947/*](https://ieeexplore.ieee.org/abstract/document/8411947/)**

***18。一种基于深度 q 网络(DQN)的移动机器人路径规划方法。(未注明)。IEEE Xplore。检索到 2022 年 8 月 1 日，来自*[【https://ieeexplore.ieee.org/abstract/document/8812452】T21](https://ieeexplore.ieee.org/abstract/document/8812452)**

**19。走在最短的路径上:Dijkstra 的 AlDijkstra '缝成定点计算。(未注明)。信息处理信函，77(2–4)，197–200。[*https://doi . org/10.1016/s 0020-0190(00)00202-7*](https://doi.org/10.1016/S0020-0190(00)00202-7)**

**20。橡胶与路相遇的地方——免费词典中的习语。[*https://sizings . thefreedictionary . com/where+the+rubber+meets+the+road*](https://idioms.thefreedictionary.com/where+the+rubber+meets+the+road)**