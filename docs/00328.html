<html>
<head>
<title>Using Docker and PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Docker和PySpark</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/using-docker-and-pyspark-134cd4cab867?source=collection_archive---------0-----------------------#2019-01-10">https://levelup.gitconnected.com/using-docker-and-pyspark-134cd4cab867?source=collection_archive---------0-----------------------#2019-01-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="fc45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近，我玩了一下PySpark，决定写一篇关于使用PySpark和Spark SQL的博文。Spark是一个伟大的开源工具，用于跨分布式计算集群管理数据和机器学习。PySpark是python API to Spark。</p><p id="32ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在您的机器上启动并运行PySpark可能有点困难。Docker是让Spark环境在您的本地机器上工作的一种快速简单的方法，这也是我在本地机器上运行PySpark的方式。</p><h1 id="e8d0" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">Docker是什么？</h1><p id="c99e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我先介绍一下Docker。根据<a class="ae lo" href="https://en.wikipedia.org/wiki/Docker_(software)" rel="noopener ugc nofollow" target="_blank">维基百科</a>“Docker是一个<a class="ae lo" href="https://en.wikipedia.org/wiki/Computer_program" rel="noopener ugc nofollow" target="_blank">计算机程序</a>，执行<a class="ae lo" href="https://en.wikipedia.org/wiki/Operating-system-level_virtualization" rel="noopener ugc nofollow" target="_blank">操作系统级虚拟化</a>，也被称为‘容器化’”。为了大大简化，Docker创建了一个隔离的linux操作系统，在你机器的操作系统之上运行软件，称为容器。对于熟悉虚拟机的人来说，容器基本上就是一个没有虚拟机管理程序的虚拟机。这些容器可以用脚本预先配置，以安装特定的软件并提供定制的功能。Dockerhub是一个网站，包含各种预配置的docker容器，可以在您的计算机上快速运行。其中之一就是<a class="ae lo" href="https://hub.docker.com/r/jupyter/pyspark-notebook" rel="noopener ugc nofollow" target="_blank"> jupyter/pysparknotebook。</a>这是我们今天将使用的docker图像。</p><h1 id="b04f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">启动Docker容器:</h1><p id="44ed" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在本地机器上设置Docker容器非常简单。只需从<a class="ae lo" href="https://www.docker.com/get-started" rel="noopener ugc nofollow" target="_blank"> docker网站</a>下载docker，并在终端中运行以下命令:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="9a65" class="ly km iq lu b gy lz ma l mb mc">docker run -it -p 8888:8888 jupyter/pyspark-notebook</span></pre><p id="5d3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在浏览器中导航至<a class="ae lo" href="http://localhost:8888" rel="noopener ugc nofollow" target="_blank"> http://localhost:8888 </a>，您将看到以下屏幕:</p><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi md"><img src="../Images/2c41061fcdb925cf8ddb55393046e8c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fv0U7mbZu8JJdRYbaYcd5Q.png"/></div></div></figure><p id="8071" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在您的终端中，您应该会看到一个令牌:</p><figure class="lp lq lr ls gt me gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/70317e1958567c1bf9b9d39cc0d65e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*ayYvL0yfVks5aiSSGsqxig.png"/></div></figure><p id="75d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">复制并粘贴这个令牌，数字跟在“/？token= "，进入token textbook，并在新密码框中为Jupyter笔记本服务器设置密码。</p><p id="1482" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完成后，您就可以开始了！Spark已经安装在容器中。您已经准备好打开笔记本，开始编写一些Spark代码。我将包括笔记本的副本，但是我建议将本文中的代码输入到您本地计算机上的新Jupyter笔记本中。这有助于你学习。</p><p id="f095" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要停止docker容器和Jupyter笔记本服务器，只需在运行它的终端中输入control + c。</p><h1 id="c8be" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">PySpark基础</h1><p id="1ce0" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">Spark是一个开源的集群计算框架，主要用scala编写，API有R、python、scala和java。它主要是为大规模数据分析和机器学习而设计的，因为它们不适合本地内存。在这个简短的教程中，我不会使用太大而不适合内存的数据集。本教程借用官方入门指南:<a class="ae lo" href="https://spark.apache.org/docs/latest/sql-getting-started.html" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/latest/SQL-getting-started . html</a>。</p><h1 id="bf21" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">火花数据类型:</h1><p id="10b5" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">spark生态系统中有两种主要的数据类型，弹性分布式数据集或rdd(有点像python列表和字典的混合体)和数据帧(数据帧很像R和python中的数据帧)。spark中的两种数据类型都是分区的和不可变的(这意味着您不能更改对象，而是返回一个新的对象)。在本教程中，我将重点介绍dataframe数据类型。</p><h1 id="c8c9" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">数据集:</h1><p id="92af" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我将使用的数据集是来自<a class="ae lo" href="https://data.vermont.gov/Finance/Vermont-Vendor-Payments/786x-sbp3" rel="noopener ugc nofollow" target="_blank">佛蒙特州开放数据社会门户的一个有点大的佛蒙特州供应商数据集。</a>点击链接即可轻松下载。</p><h1 id="abec" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">设置Spark会话:</h1><p id="453b" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这段代码在docker容器中启动PySpark环境，并导入数值计算的基本库。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="1537" class="ly km iq lu b gy lz ma l mb mc"># import necessary libraries<br/>import pandas as pd <br/>import numpy<br/>import matplotlib.pyplot as plt <br/>from pyspark.sql import SparkSession</span><span id="5558" class="ly km iq lu b gy mm ma l mb mc"># create sparksession<br/>spark = SparkSession \<br/>    .builder \<br/>    .appName("Pysparkexample") \<br/>    .config("spark.some.config.option", "some-value") \<br/>    .getOrCreate()</span></pre><h1 id="01f9" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">在CSV中阅读:</h1><p id="ebe8" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我想从比较熊猫和Spark在CSV中的阅读开始。Spark在CSV中的阅读速度比熊猫快得多。这证明了Spark数据帧与熊猫数据帧相比要快得多。</p><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mn"><img src="../Images/0438022fbe7b39a5d09fb2a7bac611ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f852lSq0MwP8H807jcjb8Q.png"/></div></div></figure><p id="b557" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于这个分析，我将使用inferSchema选项读入数据，并将Amount列转换为double。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="caf2" class="ly km iq lu b gy lz ma l mb mc">df = spark.read.csv('Vermont_Vendor_Payments (1).csv', header='true', inferSchema = True)<br/>df = df.withColumn("Amount", df["Amount"].cast("double"))</span></pre><h1 id="a4d4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">基本火花方法:</h1><p id="9405" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">像熊猫一样，我们用。dataframe的列属性。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="f587" class="ly km iq lu b gy lz ma l mb mc">#we can use the columns attribute just like with pandas<br/>columns = df.columns<br/>print('The column Names are:')<br/>for i in columns:<br/>    print(i)</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/f54aad081116cbc802ef88593c88b7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*rz7O_aDOnCfzfJz8U225KA.png"/></div></figure><p id="dbae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以使用。count()方法，我们可以通过获取列名的长度来获得列数。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="1219" class="ly km iq lu b gy lz ma l mb mc">print('The total number of rows is:', df.count(), '\nThe total number of columns is:', len(df.columns))</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/bacd4e1da911d33f4c810a743c58fe18.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*ihyAjdmnLf5u5mMeLTBP1w.png"/></div></figure><p id="8ef8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">的。show()方法默认打印dataframe的前20行。我选择在本文中只打印5。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="0db6" class="ly km iq lu b gy lz ma l mb mc">#show first 5 rows<br/>df.show(5)</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mq"><img src="../Images/ffc41cfe1cba3c654d9767a2dd317f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiQcU9mJKWFn6xghso1dyQ.png"/></div></div></figure><p id="e33a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">的。head()方法也可以用来显示第一行。这个印在笔记本上好看多了。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="f68c" class="ly km iq lu b gy lz ma l mb mc">#show first row<br/>df.head()</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mr"><img src="../Images/c9ddee70b15b271da2db5b6f7e30edf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7zaa9WXBCBoBmjQ9c0vORw.png"/></div></div></figure><p id="2950" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">像在pandas中一样，我们可以调用describe方法来获得数据的基本数字摘要。我们需要使用show方法将其打印到笔记本上。这在笔记本上打印得不太好。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="9e3b" class="ly km iq lu b gy lz ma l mb mc">df.describe().show()</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ms"><img src="../Images/3d7a5863307be0ab49f753458a85d351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cvY2Ky7sVGT1RKbKZ_qiLg.png"/></div></div></figure><h1 id="382e" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">查询数据:</h1><p id="b401" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">Spark的优势之一是可以使用每种语言各自的Spark库或Spark SQL进行查询。我将使用pythonic和SQL选项演示几个查询。</p><p id="c6b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下代码注册临时表，并使用SQL语法选择一些列:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="059d" class="ly km iq lu b gy lz ma l mb mc"># I will start by creating a temporary table query with SQL<br/>df.createOrReplaceTempView('VermontVendor')<br/>spark.sql(<br/>'''<br/>SELECT `Quarter Ending`, Department, Amount, State FROM VermontVendor<br/>LIMIT 10<br/>'''<br/>).show()</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mt"><img src="../Images/d56e9061b862c9f75d768244a74af64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*YunX8UY--fyEmpY_Ca9log.png"/></div></div></figure><p id="96c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这段代码使用pythonic语法执行几乎相同的操作:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="605b" class="ly km iq lu b gy lz ma l mb mc">df.select('Quarter Ending', 'Department', 'Amount', 'State').show(10)</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/1ba7742ff006b3b5eea8c7fb55dbd176.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*hUp3IXbOuCWIUjv76nUVJA.png"/></div></figure><p id="e2e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">需要注意的一点是pythonic解决方案的代码非常少。我喜欢SQL和它的语法，所以比起pythonic，我更喜欢SQL接口。</p><p id="36e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我可以使用SQL WHERE子句过滤查询中选择的列</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="a4a1" class="ly km iq lu b gy lz ma l mb mc">spark.sql(<br/>'''</span><span id="e6e3" class="ly km iq lu b gy mm ma l mb mc">SELECT `Quarter Ending`, Department, Amount, State FROM VermontVendor <br/>WHERE Department = 'Education'<br/>LIMIT 10</span><span id="0a36" class="ly km iq lu b gy mm ma l mb mc">'''<br/>).show()</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/87e8f6c8ed8b2a3e7ee9870ac0652c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*MGNwBRaREWdDu76NCggBFw.png"/></div></figure><p id="19c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似的结果可以通过。python API中的filter()方法。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="252d" class="ly km iq lu b gy lz ma l mb mc">df.select('Quarter Ending', 'Department', 'Amount', 'State').filter(df['Department'] == 'Education').show(10)</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/cc33d4c79ebe310ebda600e164fdfe44.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*txBQB_82zGyR5yEIabzEWQ.png"/></div></figure><h1 id="4ab7" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">测绘</h1><p id="c8e7" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">不幸的是，人们不能用Spark数据框直接绘制图表。最简单的解决方案是简单地使用。toPandas()方法将Spark计算的结果转换成Pandas数据帧。下面我举几个例子。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="d49e" class="ly km iq lu b gy lz ma l mb mc">plot_df = spark.sql(<br/>'''</span><span id="54dd" class="ly km iq lu b gy mm ma l mb mc">SELECT Department, SUM(Amount) as Total FROM VermontVendor <br/>GROUP BY Department<br/>ORDER BY Total DESC<br/>LIMIT 10</span><span id="9505" class="ly km iq lu b gy mm ma l mb mc">'''<br/>).toPandas()</span><span id="5aa6" class="ly km iq lu b gy mm ma l mb mc"><br/>fig,ax = plt.subplots(1,1,figsize=(10,6))<br/>plot_df.plot(x = 'Department', y = 'Total', kind = 'barh', color = 'C0', ax = ax, legend = False)<br/>ax.set_xlabel('Department', size = 16)<br/>ax.set_ylabel('Total', size = 16)<br/>plt.savefig('barplot.png')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mx"><img src="../Images/6ad0f1aa6d3debdf1864523a2d109391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wf9dVFkqBDKL7kb1J8SLWQ.png"/></div></div></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="9852" class="ly km iq lu b gy lz ma l mb mc">import numpy as np<br/>import seaborn as sns<br/>plot_df2 = spark.sql(<br/>'''<br/>SELECT Department, SUM(Amount) as Total FROM VermontVendor <br/>GROUP BY Department<br/>'''<br/>).toPandas()<br/>plt.figure(figsize = (10,6))<br/>sns.distplot(np.log(plot_df2['Total']))<br/>plt.title('Histogram of Log Totals for all Departments in Dataset', size = 16)<br/>plt.ylabel('Density', size = 16)<br/>plt.xlabel('Log Total', size = 16)<br/>plt.savefig('distplot.png')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mx"><img src="../Images/3c0982346d733e7f4522ec3687905363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bonNNl75ZJG5djc_eadtzA.png"/></div></div></figure><h1 id="c6cf" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">再次启动docker容器:</h1><p id="dd60" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">一旦您第一次启动并退出docker容器，您将为将来的使用以不同的方式启动它，因为容器已经运行过了。</p><p id="f1d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">传递以下命令以返回所有容器名称:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="285b" class="ly km iq lu b gy lz ma l mb mc">docker ps -a</span></pre><p id="f45d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从终端获取容器id:</p><figure class="lp lq lr ls gt me gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/1d63e233d55b5bd6983dd5f9c71ea176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*Ao0Sr1PEIGoNf73INYlO3w.png"/></div></figure><p id="66d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后使用容器id运行docker start来启动容器:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="ab42" class="ly km iq lu b gy lz ma l mb mc">docker start 903f152e92c5</span></pre><p id="2ee4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，您的Jupyter笔记本服务器将再次运行在<a class="ae lo" href="http://localhost:8888" rel="noopener ugc nofollow" target="_blank"> http://localhost:8888 </a>上。</p><p id="9db6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">带有更多示例的完整代码可以在我的github上找到:</p><p id="8a90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">https://github.com/crocker456/PlayingWithPyspark<a class="ae lo" href="https://github.com/crocker456/PlayingWithPyspark" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="2041" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">来源:</h1><div class="my mz gp gr na nb"><a href="https://stackoverflow.com/questions/39652767/pyspark-2-0-the-size-or-shape-of-a-dataframe" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd ir gy z fp ng fr fs nh fu fw ip bi translated">PySpark 2.0数据帧的大小或形状</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">感谢贡献一个堆栈溢出的答案！你过去的一些回答不太受欢迎，你…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">stackoverflow.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np mj nb"/></div></div></a></div><div class="my mz gp gr na nb"><a href="https://spark.apache.org/docs/latest/sql-getting-started.html" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd ir gy z fp ng fr fs nh fu fw ip bi translated">入门- Spark 2.4.0文档</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">编辑描述</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">spark.apache.org</p></div></div><div class="nk l"><div class="nq l nm nn no nk np mj nb"/></div></div></a></div></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><figure class="lp lq lr ls gt me gh gi paragraph-image"><a href="https://levelup.gitconnected.com/"><div class="gh gi ny"><img src="../Images/ff5028ba5a0041d2d76d2a155f00f05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JaoZbi7tTKJ5vL7i2OAYMQ.png"/></div></a></figure><div class="my mz gp gr na nb"><a href="https://gitconnected.com/learn/python" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd ir gy z fp ng fr fs nh fu fw ip bi translated">学习Python -最佳Python教程(2019) | gitconnected</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">77大Python教程。课程由开发者提交并投票，让你找到最好的Python…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">gitconnected.com</p></div></div><div class="nk l"><div class="nz l nm nn no nk np mj nb"/></div></div></a></div></div></div>    
</body>
</html>