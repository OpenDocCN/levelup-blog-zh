<html>
<head>
<title>How to Run Spark With Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Docker运行Spark</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/how-to-run-spark-with-docker-c6287a11a437?source=collection_archive---------1-----------------------#2022-12-27">https://levelup.gitconnected.com/how-to-run-spark-with-docker-c6287a11a437?source=collection_archive---------1-----------------------#2022-12-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="86cf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Pyspak教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/015131458bc57e0adf54cf59834c78cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7HttLNFqkpdrD4eN1JRFw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇Spark </a>的官方logo</figcaption></figure><p id="96df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我将指导您在Docker容器中安装和运行Apache Spark和PySpark。</p><p id="29d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于不熟悉Spark的人来说，它是一个开源的分布式计算系统，可以快速处理大量数据。PySpark是Spark的Python接口，允许您使用Spark的强大功能和Python的简单性。</p><p id="4042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文包含以下几个部分:</p><ul class=""><li id="bbad" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">Spark和Docker的介绍，解释使用它们进行数据处理的好处；</li><li id="3c56" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在Docker容器中安装和运行Spark和PySpark的过程，包括设置必要的依赖项和配置。</li><li id="88db" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">最后，在第三部分，我将向您展示如何运行您的第一个PySpark脚本。</li></ul><p id="2030" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程结束时，您将拥有一个在Docker容器中运行的Spark和PySpark的工作安装，并准备好开始使用它们来完成您自己的数据处理任务。</p><p id="a6e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将使用最完整的配置，而不是使用“jupyter/pyspark-notebook”Docker映像的配置。</p><p id="d100" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，让我们开始吧！😁</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="a88b" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">1.Spark和Docker简介</h1><h1 id="2067" class="mq mr it bd ms mt ni mv mw mx nj mz na jz nk ka nc kc nl kd ne kf nm kg ng nh bi translated">火花/ Pyspark</h1><p id="0233" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">Apache Spark是一个流行的开源数据处理引擎，广泛应用于大数据领域。众所周知，它能够快速高效地处理大量数据，是数据科学家和分析师的宝贵工具。</p><p id="152b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们展示了Apache Spark的三大特征。</p><ul class=""><li id="883c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">分布式处理— </strong> Apache Spark可以跨多个分布式计算集群处理大量数据，使其高效且可扩展。</li><li id="4a08" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">内存计算— </strong> Apache Spark将数据存储在内存中，从而实现更快的处理和实时分析</li><li id="da8f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">支持多种语言</strong> — Apache Spark支持多种语言编程，包括Python、Java和Scala。</li></ul><h1 id="2fc1" class="mq mr it bd ms mt ni mv mw mx nj mz na jz nk ka nc kc nl kd ne kf nm kg ng nh bi translated">码头工人</h1><p id="d03c" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">一年前，我开始从事<strong class="lb iu"> Docker </strong>的工作，这是一套奇妙的平台软件(PAAS)产品，我现在是它的超级粉丝。</p><p id="2be9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该工具使用操作系统级虚拟化，允许对容器中的软件进行奇妙的定制，可以轻松地与您的同事或开发环境之间共享，“<em class="ns">并确保您共享的每个人都获得以相同方式工作的相同容器”</em>(由<a class="ae ky" href="https://docs.docker.com/get-started/overview/#:~:text=Docker%20provides%20the%20ability%20to,simultaneously%20on%20a%20given%20host." rel="noopener ugc nofollow" target="_blank">官方Docker网站</a>)。</p><p id="2663" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Docker的工作方式非常简单，因为它使用了客户机-服务器架构。"<em class="ns">Docker客户端与</em> <strong class="lb iu"> <em class="ns"> Docker守护进程</em> </strong> <em class="ns">对话，后者负责构建、运行和分发Docker容器。Docker客户机和守护进程可以在同一个系统上运行，或者您可以将Docker客户机连接到远程Docker守护进程。Docker客户机和守护程序使用REST API通过UNIX套接字或网络接口进行通信。另一个Docker客户端是Docker Compose，它让您可以使用由一组容器组成的应用程序。</em>(文字摘自Docker官方网站)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3252c2faaedbef84b250893a410f4011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0smA2Iy0J4sMdmHH"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Docker架构(来自Docker官方网站<a class="ae ky" href="https://docs.docker.com/get-started/overview/#:~:text=Docker%20provides%20the%20ability%20to,simultaneously%20on%20a%20given%20host." rel="noopener ugc nofollow" target="_blank"/>)</figcaption></figure><p id="be98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Docker最常见的方式是在一个名为<strong class="lb iu"> Dockerfile </strong>的文本文件中设置几个指令。该文件首先从公共Docker存储库中“调用”一个图像(例如Python图像、气流图像等)来设置基本图像。然后，它将运行几个用户定义的命令来定制您的新映像。</p><p id="47fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，在运行“docker build”命令后，一个新的映像被创建，整个上下文(递归地)被发送到守护进程。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="add7" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">3.使用Docker运行Spark/PySpark</h1><p id="16de" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">为了在Docker容器中运行Spark和Pyspark，我们需要开发一个Docker文件来运行定制的映像。</p><p id="08eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要从Docker Hub调用Python 3.9.1镜像:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="3cca" class="nz mr it nv b be oa ob l oc od">FROM python:3.9.1</span></pre><p id="050f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于接下来的步骤，您需要下载文件“fhvhv _ tripdata _ 2021–01 . CSV . gz ”,您可以在此<a class="ae ky" href="https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhv" rel="noopener ugc nofollow" target="_blank">链接中获得该文件。</a>该文件也可用于其他项目。</p><p id="8c78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来的步骤包括安装“curl”、“wget”和“pandas”。我们还将之前下载的文件复制到容器中。</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="1d83" class="nz mr it nv b be oa ob l oc od">RUN apt-get install curl wget<br/>RUN pip install pandas<br/><br/>COPY fhvhv_tripdata_2021-01.csv.gz .</span></pre><p id="7439" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得更有条理的配置，我们将设置要使用的Spark、Hadoop和Java版本:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="757f" class="nz mr it nv b be oa ob l oc od"># VERSIONS<br/>ENV SPARK_VERSION=3.3.1 \<br/>HADOOP_VERSION=3 \<br/>JAVA_VERSION=11</span></pre><p id="22b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们需要设置Java环境变量，下载JDK 11并使用以下命令安装它:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="1d38" class="nz mr it nv b be oa ob l oc od"># SET JAVA ENV VARIABLES<br/>ENV JAVA_HOME="/home/jdk-${JAVA_VERSION}.0.2"<br/>ENV PATH="${JAVA_HOME}/bin/:${PATH}"<br/><br/># DOWNLOAD JDk 11 AND INSTALL<br/>RUN DOWNLOAD_URL="https://download.java.net/java/GA/jdk${JAVA_VERSION}/9/GPL/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \<br/>    &amp;&amp; TMP_DIR="$(mktemp -d)" \<br/>    &amp;&amp; curl -fL "${DOWNLOAD_URL}" --output "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \<br/>    &amp;&amp; mkdir -p "${JAVA_HOME}" \<br/>    &amp;&amp; tar xzf "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" -C "${JAVA_HOME}" --strip-components=1 \<br/>    &amp;&amp; rm -rf "${TMP_DIR}" \<br/>    &amp;&amp; java --version</span></pre><p id="e489" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们需要从https://dlcdn.apache.org下载并安装Spark:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="0a67" class="nz mr it nv b be oa ob l oc od"># DOWNLOAD SPARK AND INSTALL<br/>RUN DOWNLOAD_URL_SPARK="https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \<br/>    &amp;&amp; wget --no-verbose -O apache-spark.tgz  "${DOWNLOAD_URL_SPARK}"\<br/>    &amp;&amp; mkdir -p /home/spark \<br/>    &amp;&amp; tar -xf apache-spark.tgz -C /home/spark --strip-components=1 \<br/>    &amp;&amp; rm apache-spark.tgz</span></pre><p id="50f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们设置一些额外的环境变量:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="f453" class="nz mr it nv b be oa ob l oc od"># SET SPARK ENV VARIABLES<br/>ENV SPARK_HOME="/home/spark"<br/>ENV PATH="${SPARK_HOME}/bin/:${PATH}"<br/><br/># SET PYSPARK VARIABLES<br/>ENV PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"<br/>ENV PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH"</span></pre><p id="d528" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将入口点设置为Python命令行:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="f5fc" class="nz mr it nv b be oa ob l oc od">ENTRYPOINT ["python" ]</span></pre><p id="2198" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">完整的Dockerfile文件配置如下:</strong></p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="352f" class="nz mr it nv b be oa ob l oc od">FROM python:3.9.1<br/><br/>RUN apt-get install curl wget<br/>RUN pip install pandas<br/><br/>COPY fhvhv_tripdata_2021-01.csv.gz .<br/><br/># VERSIONS<br/>ENV SPARK_VERSION=3.3.1 \<br/>HADOOP_VERSION=3 \<br/>JAVA_VERSION=11<br/><br/># SET JAVA ENV VARIABLES<br/>ENV JAVA_HOME="/home/jdk-${JAVA_VERSION}.0.2"<br/>ENV PATH="${JAVA_HOME}/bin/:${PATH}"<br/><br/># DOWNLOAD JAVA 11 AND INSTALL<br/>RUN DOWNLOAD_URL="https://download.java.net/java/GA/jdk${JAVA_VERSION}/9/GPL/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \<br/>    &amp;&amp; TMP_DIR="$(mktemp -d)" \<br/>    &amp;&amp; curl -fL "${DOWNLOAD_URL}" --output "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \<br/>    &amp;&amp; mkdir -p "${JAVA_HOME}" \<br/>    &amp;&amp; tar xzf "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" -C "${JAVA_HOME}" --strip-components=1 \<br/>    &amp;&amp; rm -rf "${TMP_DIR}" \<br/>    &amp;&amp; java --version<br/><br/># DOWNLOAD SPARK AND INSTALL<br/>RUN DOWNLOAD_URL_SPARK="https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \<br/>    &amp;&amp; wget --no-verbose -O apache-spark.tgz  "${DOWNLOAD_URL_SPARK}"\<br/>    &amp;&amp; mkdir -p /home/spark \<br/>    &amp;&amp; tar -xf apache-spark.tgz -C /home/spark --strip-components=1 \<br/>    &amp;&amp; rm apache-spark.tgz<br/><br/># SET SPARK ENV VARIABLES<br/>ENV SPARK_HOME="/home/spark"<br/>ENV PATH="${SPARK_HOME}/bin/:${PATH}"<br/><br/># SET PYSPARK VARIABLES<br/>ENV PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"<br/>ENV PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH"<br/><br/># Let's change to  "$NB_USER" command so the image runs as a non root user by default<br/>USER $NB_UID<br/><br/>ENTRYPOINT ["python" ]</span></pre><p id="f22d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了构建我们定制的Docker映像，我们需要运行docker build命令:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="932d" class="nz mr it nv b be oa ob l oc od">docker build -t spark_docker_v1 .</span></pre></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="5bc0" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">3.Pyspark的首次运行</h1><p id="3065" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">现在我们有了一个Docker映像，可以运行Spark了，因此也可以运行Pyspark了。</p><p id="669e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要设置容器运行，我们调用以下命令:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="0476" class="nz mr it nv b be oa ob l oc od">docker run --rm -it -p 4040:4040 spark_docker_v1</span></pre><p id="639c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将容器设置为运行后，您将获得Python命令行:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="d3e1" class="nz mr it nv b be oa ob l oc od">Python 3.9.1 (default, Feb  9 2021, 07:42:03)<br/>[GCC 8.3.0] on linux<br/>Type "help", "copyright", "credits" or "license" for more information.</span></pre><p id="6009" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在命令行上，您可以通过导入所需的包、启动Spark会话、读取导入的文件，然后显示Spark数据帧的前20行来测试Spark:</p><pre class="kj kk kl km gt nu nv nw bn nx ny bi"><span id="44a5" class="nz mr it nv b be oa ob l oc od">import pyspark<br/>from pyspark.sql import SparkSession<br/><br/>spark = SparkSession.builder \<br/>    .master("local[*]") \<br/>    .appName('tepst') \<br/>    .getOrCreate()<br/><br/>df = spark.read \<br/>    .option("header", "true") \<br/>    .csv('fhvhv_tripdata_2021-01.csv.gz')<br/><br/>df.show()</span></pre><p id="f4f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一切顺利，您将获得以下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/4c2bc6f02e0c34102487bd16643310b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6XgoU5ZIRAhaZp3r-NY8FA.png"/></div></div></figure><p id="9da8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="ns"> localhost:4040 </em>上，您将看到这幅图像，表示Spark正在工作:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/3d7a837752c6158cdc1475b1d7432240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*owoIsGl4jyyporp9BVadSw.png"/></div></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="b8b7" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">最后的想法</h1><p id="90a7" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">如果您想为Spark应用程序使用一致且隔离的环境，在Docker上运行Spark可能是一个不错的选择。</p><p id="fa7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在Docker上运行Spark可能并不总是最佳选择。例如，如果您有一个大型集群，拥有运行Spark的专用资源，那么直接在集群上运行Spark可能比使用Docker容器更有效。</p><p id="fe32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个好的替代方法可能是通过在Kubernetes集群上部署Spark，使用Kubernetes为Spark提供更多的资源。这允许您通过向集群添加更多节点来水平扩展Spark应用程序。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="ef8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你喜欢这篇文章吗？关注我更多关于<a class="ae ky" href="https://medium.com/@lgsoliveira" rel="noopener">媒体</a>的文章。</p><p id="5336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/@lgsoliveira/membership" rel="noopener">阅读路易斯·奥利维拉(以及媒体上成千上万的其他作家)的每一个故事</a></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="1c38" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">分级编码</h1><p id="fc30" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">感谢您成为我们社区的一员！在你离开之前:</p><ul class=""><li id="3ec3" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">👏为故事鼓掌，跟着作者走👉</li><li id="ae5e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">📰查看<a class="ae ky" href="https://levelup.gitconnected.com/?utm_source=pub&amp;utm_medium=post" rel="noopener ugc nofollow" target="_blank">升级编码出版物</a>中的更多内容</li><li id="73e0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">🔔关注我们:<a class="ae ky" href="https://twitter.com/gitconnected" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae ky" href="https://www.linkedin.com/company/gitconnected" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae ky" href="https://newsletter.levelup.dev" rel="noopener ugc nofollow" target="_blank">时事通讯</a></li></ul><p id="cbc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">🚀👉<a class="ae ky" href="https://jobs.levelup.dev/talent/welcome?referral=true" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">加入升级人才集体，找到一份神奇的工作</strong> </a></p></div></div>    
</body>
</html>