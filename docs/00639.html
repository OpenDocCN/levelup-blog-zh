<html>
<head>
<title>How to train your model using XGBoost and Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用XGBoost和Python训练您的模型</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/how-to-train-your-model-using-xgboost-and-python-670f62a2bbdf?source=collection_archive---------4-----------------------#2019-06-14">https://levelup.gitconnected.com/how-to-train-your-model-using-xgboost-and-python-670f62a2bbdf?source=collection_archive---------4-----------------------#2019-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f073c2093bebbd80fa8e96ac58e33a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ik3__OIwOvnBn8-e"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">克里斯里德在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="9755" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> XGBoost(极限梯度提升)</strong>是梯度提升算法的高级实现。它与其他增强算法的区别在于它的速度和准确性。</p><p id="7179" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了完全掌握XGBoost，您将需要大量的时间和不同的项目，但我们希望这是一个坚实的起点。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="31c6" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">I <strong class="ak">安装XGBoost </strong></h1><p id="a55e" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">要在Python中安装XGBoost库，只需在命令提示符下键入<strong class="kf ir"> pip install xgboost </strong>。</p><p id="d0be" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，我们将使用其他库，因此它们也是一样的。如果尚未安装，请在命令提示符下键入<strong class="kf ir">pip install[library name]</strong>。</p><h1 id="bea1" class="li lj iq bd lk ll ml ln lo lp mm lr ls lt mn lv lw lx mo lz ma mb mp md me mf bi translated"><strong class="ak">读取数据集</strong></h1><p id="b969" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">首先，我们需要从文件中读取数据，并为训练我们的模型做准备。为此，我们将使用<strong class="kf ir"> pandas </strong>库及其<strong class="kf ir"> read_csv </strong>方法。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="664f" class="mz lj iq mv b gy na nb l nc nd">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="dfff" class="mz lj iq mv b gy ne nb l nc nd">dataset = pd.read_csv('train.csv')</span></pre><p id="59b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们将数据集存储到DataFrame变量中，我们需要指定哪些列将用作输入参数(X ),哪些将用作输出(y)</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="8fae" class="mz lj iq mv b gy na nb l nc nd">X = dataset.iloc[:, 0:9].values<br/>y = dataset.iloc[:, 9].values</span></pre><p id="cde4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着我们的数据集有10列，索引为0-8的列将作为输入，而最后一列是输出。</p><h1 id="75a9" class="li lj iq bd lk ll ml ln lo lp mm lr ls lt mn lv lw lx mo lz ma mb mp md me mf bi translated">一个热编码</h1><p id="ace2" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">XGBoost算法适用于数值类型，如果数据集只包含数值，可以跳过这一步。当然，并不是所有的数据集都只包含数字，但幸运的是有一个解决方法。</p><p id="ab5f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基本上，一种热门的编码是将分类值表示为二进制向量。这需要将分类值映射到整数值。</p><p id="4e1a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，每个整数值都被表示为一个二进制向量，除了用1标记的整数索引之外，其他都是零值。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="d5a4" class="mz lj iq mv b gy na nb l nc nd">from sklearn.preprocessing import LabelEncoder, OneHotEncoder</span><span id="a66c" class="mz lj iq mv b gy ne nb l nc nd">labelencoder_X_1 = LabelEncoder()<br/>X[:, 5] = labelencoder_X_1.fit_transform(X[:, 5])</span><span id="e4cb" class="mz lj iq mv b gy ne nb l nc nd">labelencoder_X_2 = LabelEncoder()<br/>X[:, 7] = labelencoder_X_2.fit_transform(X[:, 7])</span><span id="eb5e" class="mz lj iq mv b gy ne nb l nc nd">onehotencoder = OneHotEncoder(categorical_features = [1])<br/>X = onehotencoder.fit_transform(X).toarray()</span></pre><p id="c26e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本例中，数据集中包含分类数据的列的索引是5和7。对于您希望编码的每一列，您必须使用一个<strong class="kf ir"> LabelEncoder </strong>及其<strong class="kf ir"><em class="nf">fit _ transform</em></strong>方法。您将希望转换为整数的列作为参数传递，方法会完成这项工作。只需将返回值赋给与你传递的完全相同的列，用整数覆盖字符串。</p><p id="b554" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦这一步完成，我们需要做的就是实例化<strong class="kf ir"> OneHotEncoder </strong>并对整个输入数据调用它的<strong class="kf ir"> fit_transform </strong>方法。</p><h1 id="a272" class="li lj iq bd lk ll ml ln lo lp mm lr ls lt mn lv lw lx mo lz ma mb mp md me mf bi translated">分割数据集</h1><p id="8804" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">现在是时候训练和测试我们的模型了。如果您已经有单独的数据集用于训练和测试，那就更好了。您将加载测试数据，类似于我们上面提到的训练数据。如果你只有一个数据集，不用担心，有一种方法可以把这个数据拆分成训练样本和测试样本。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="d6b1" class="mz lj iq mv b gy na nb l nc nd">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span></pre><p id="9051" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着我们将使用20%的数据集进行测试，其余的用于训练。这个参数<strong class="kf ir"> random_state = 0 </strong>意味着将随机选择哪个数据进入哪个样本。如果希望对该过程有更多的控制，请指定一个非零值。</p><h1 id="7f4a" class="li lj iq bd lk ll ml ln lo lp mm lr ls lt mn lv lw lx mo lz ma mb mp md me mf bi translated">培训和测试</h1><p id="0e80" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">首先，我们让XGBoost适合训练数据。注意，我们使用的是不带任何参数的<strong class="kf ir"> XGBClassifier </strong>，这意味着我们使用的是默认参数。如果你想获得更高的精度，你将不得不调整参数，但这是因数据集而异的事情，这里不讨论。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="82a3" class="mz lj iq mv b gy na nb l nc nd">import xgboost</span><span id="62ad" class="mz lj iq mv b gy ne nb l nc nd">classifier = xgboost.XGBClassifier()<br/>classifier.fit(X_train, y_train)</span></pre><p id="5730" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以进行测试了。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="55c2" class="mz lj iq mv b gy na nb l nc nd">y_pred = classifier.predict(X_test)</span></pre><p id="3d5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以评估准确性。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="cda8" class="mz lj iq mv b gy na nb l nc nd">from sklearn.metrics import accuracy_score</span><span id="578c" class="mz lj iq mv b gy ne nb l nc nd">accuracy = accuracy_score(y_test, y_pred, normalize=False)<br/>print(accuracy)</span></pre><p id="2669" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着我们正在比较存储在<strong class="kf ir"> y_test </strong>中的“基本事实”和存储在<strong class="kf ir"> y_pred </strong>中的预测。在<strong class="kf ir"> normalize=False </strong>的情况下，精确度将是一个介于0和1之间的值，理想情况下应该尽可能接近1。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="d6d8" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">结论</h1><p id="f914" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">如果你坚持到了最后，我们想感谢你阅读本教程，我们希望我们能够澄清XGBoost的基础知识。如果您想进一步探索这个算法，我们建议您先检查一下参数调整。</p></div></div>    
</body>
</html>