<html>
<head>
<title>Feature Selection in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的要素选择</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/feature-selection-in-python-predictive-hacks-8805b136955e?source=collection_archive---------4-----------------------#2020-10-01">https://levelup.gitconnected.com/feature-selection-in-python-predictive-hacks-8805b136955e?source=collection_archive---------4-----------------------#2020-10-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b3db" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="b622" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何选择最重要特性的实际例子</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/68337b614ba804c4dfeb28395ea86a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*v55UxaisIS-Sx4N0.png"/></div></div></figure><div class="ld le gp gr lf lg"><a href="https://jorgepit-14189.medium.com/membership" rel="noopener follow" target="_blank"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd jd gy z fp ll fr fs lm fu fw jc bi translated">用我的推荐链接加入媒体-乔治皮皮斯</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">阅读乔治·皮皮斯(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">jorgepit-14189.medium.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu lb lg"/></div></div></a></div><p id="cfa5" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们将提供一个演示示例，说明如何选择最重要的功能。对于这个例子，我们将处理一个分类问题，但是也可以通过调整函数的参数扩展到回归情况。</p><p id="4d2b" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们将使用<a class="ae mr" href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data?select=data.csv" rel="noopener ugc nofollow" target="_blank">乳腺癌</a>数据集。让我们开始:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="c5de" class="mx my it mt b gy mz na l nb nc">import pandas as pd<br/>import numpy as np<br/>from scipy import stats<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.feature_selection import SelectFpr, chi2, SelectKBest, SelectFwe, f_classif, SelectFdr</span><span id="cede" class="mx my it mt b gy nd na l nb nc">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="2cd6" class="mx my it mt b gy nd na l nb nc"># <a class="ae mr" href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data?select=data.csv" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/uciml/breast-cancer-wisconsin-data?select=data.csv</a><br/>df = pd.read_csv("data.csv")</span><span id="1090" class="mx my it mt b gy nd na l nb nc"># replace M with 1 and B with 0<br/>my_map = {<br/>          'M':1,<br/>          'B' :0<br/>         }</span><span id="4f6e" class="mx my it mt b gy nd na l nb nc">df['diagnosis'] = df['diagnosis'].map(my_map)</span><span id="172f" class="mx my it mt b gy nd na l nb nc"># remove the id column<br/>df.drop(['id'], axis=1, inplace=True)</span><span id="02f6" class="mx my it mt b gy nd na l nb nc">df</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/8408f880ccc4d05a8b4f990a2a419026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dN7WRu6nq5vzT6GV.png"/></div></div></figure><h1 id="801e" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">用t检验统计显著特征</h1><p id="6ae1" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">由于我们的目标是二进制的，我们可以通过应用<code class="fe ob oc od mt b">t-test</code>来比较每个组(0，1)的独立变量的值。</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="2208" class="mx my it mt b gy mz na l nb nc">my_important = []</span><span id="6207" class="mx my it mt b gy nd na l nb nc">for c in df.columns[1:]:<br/>    pvalue = stats.ttest_ind(df.loc[df.diagnosis==1][c], df.loc[df.diagnosis==0][c])[1]<br/>    if pvalue&lt;0.05:<br/>        my_important.append(c)<br/>        print(f'The variable {c} is statistically significant with a pvalue = {pvalue:.2}')<br/>    else:<br/>        print(f'The variable {c} is NOT statistically significant')</span></pre><p id="03c6" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们得到了:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="3e33" class="mx my it mt b gy mz na l nb nc">The variable radius_mean is statistically significant with a pvalue = 8.5e-96<br/>The variable texture_mean is statistically significant with a pvalue = 4.1e-25<br/>The variable perimeter_mean is statistically significant with a pvalue = 8.4e-101<br/>The variable area_mean is statistically significant with a pvalue = 4.7e-88<br/>The variable smoothness_mean is statistically significant with a pvalue = 1.1e-18<br/>The variable compactness_mean is statistically significant with a pvalue = 3.9e-56<br/>The variable concavity_mean is statistically significant with a pvalue = 1e-83<br/>The variable concave points_mean is statistically significant with a pvalue = 7.1e-116<br/>The variable symmetry_mean is statistically significant with a pvalue = 5.7e-16<br/>The variable fractal_dimension_mean is NOT statistically significant<br/>The variable radius_se is statistically significant with a pvalue = 9.7e-50<br/>The variable texture_se is NOT statistically significant<br/>The variable perimeter_se is statistically significant with a pvalue = 1.7e-47<br/>The variable area_se is statistically significant with a pvalue = 5.9e-46<br/>The variable smoothness_se is NOT statistically significant<br/>The variable compactness_se is statistically significant with a pvalue = 1e-12<br/>The variable concavity_se is statistically significant with a pvalue = 8.3e-10<br/>The variable concave points_se is statistically significant with a pvalue = 3.1e-24<br/>The variable symmetry_se is NOT statistically significant<br/>The variable fractal_dimension_se is NOT statistically significant<br/>The variable radius_worst is statistically significant with a pvalue = 8.5e-116<br/>The variable texture_worst is statistically significant with a pvalue = 1.1e-30<br/>The variable perimeter_worst is statistically significant with a pvalue = 5.8e-119<br/>The variable area_worst is statistically significant with a pvalue = 2.8e-97<br/>The variable smoothness_worst is statistically significant with a pvalue = 6.6e-26<br/>The variable compactness_worst is statistically significant with a pvalue = 7.1e-55<br/>The variable concavity_worst is statistically significant with a pvalue = 2.5e-72<br/>The variable concave points_worst is statistically significant with a pvalue = 2e-124<br/>The variable symmetry_worst is statistically significant with a pvalue = 3e-25<br/>The variable fractal_dimension_worst is statistically significant with a pvalue = 2.3e-15</span></pre><p id="c61f" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">因此，统计上显著的变量是:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="c8fc" class="mx my it mt b gy mz na l nb nc">my_important</span><span id="e6c6" class="mx my it mt b gy nd na l nb nc">['radius_mean',<br/> 'texture_mean',<br/> 'perimeter_mean',<br/> 'area_mean',<br/> 'smoothness_mean',<br/> 'compactness_mean',<br/> 'concavity_mean',<br/> 'concave points_mean',<br/> 'symmetry_mean',<br/> 'radius_se',<br/> 'perimeter_se',<br/> 'area_se',<br/> 'compactness_se',<br/> 'concavity_se',<br/> 'concave points_se',<br/> 'radius_worst',<br/> 'texture_worst',<br/> 'perimeter_worst',<br/> 'area_worst',<br/> 'smoothness_worst',<br/> 'compactness_worst',<br/> 'concavity_worst',<br/> 'concave points_worst',<br/> 'symmetry_worst',<br/> 'fractal_dimension_worst']</span></pre><h1 id="c390" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">随机森林特征重要性</h1><p id="57dd" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">我们还可以运行一个模型，如随机森林，看看哪些是最重要的功能。</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="5325" class="mx my it mt b gy mz na l nb nc">clf = RandomForestClassifier( n_estimators=50)</span><span id="d2dd" class="mx my it mt b gy nd na l nb nc">X = df.drop(['diagnosis'], axis=1)<br/>y = df.diagnosis</span><span id="bf84" class="mx my it mt b gy nd na l nb nc">model = clf.fit(X,y)<br/>feat_importances = pd.DataFrame(model.feature_importances_, index=X.columns, columns=["Importance"])<br/>feat_importances.sort_values(by='Importance', ascending=False, inplace=True)</span><span id="c221" class="mx my it mt b gy nd na l nb nc">feat_importances.plot(kind='bar')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eeeedbdfde20c55226a9ebae0d9f13c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gSwLdIFj5iLu_lXU.png"/></div></div></figure><p id="8883" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">让我们来看看随机森林的10个最重要的特征:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="dc46" class="mx my it mt b gy mz na l nb nc">feat_importances.index[0:10]</span></pre><p id="61ee" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们得到:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="155f" class="mx my it mt b gy mz na l nb nc">Index(['concave points_worst', 'perimeter_worst', 'radius_worst', 'concave points_mean', 'area_worst', 'perimeter_mean', 'area_mean', 'concavity_worst', 'area_se', 'perimeter_se'], dtype='object')</span></pre><h1 id="18a1" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">使用Scikit-Learn进行功能选择</h1><p id="42a2" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">我们可以用<code class="fe ob oc od mt b">scikit-learn</code>工作。您可以在<a class="ae mr" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection" rel="noopener ugc nofollow" target="_blank">文档中找到更多详细信息。</a>我们将提供一些例子:</p><h1 id="ce4e" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">k-best</h1><p id="0ef9" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">它选择k个最重要的特征。在我们的例子中，我们将使用卡方检验。请记住，<strong class="lx jd"> new_data </strong>是我们删除非重要变量后的最终数据。</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="e891" class="mx my it mt b gy mz na l nb nc">selector = SelectKBest(score_func=chi2, k=5)<br/>new_data = selector.fit_transform(X, y)</span><span id="bc57" class="mx my it mt b gy nd na l nb nc">mask = selector.get_support()<br/>new_features = X.columns[mask]<br/>new_features</span></pre><p id="9565" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们得到:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="594b" class="mx my it mt b gy mz na l nb nc">Index(['perimeter_mean', 'area_mean', 'area_se', 'perimeter_worst', 'area_worst'], dtype='object')</span></pre><h1 id="61b8" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">finished with engines 用车床精加工</h1><p id="3f72" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">这类似于我们一开始用t检验做的。这可以用<code class="fe ob oc od mt b">chi-square test</code>或<code class="fe ob oc od mt b">ANOVA</code>来完成(其中二进制情况与t检验相同)</p><p id="d686" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated"><strong class="lx jd">卡方检验</strong></p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="d74b" class="mx my it mt b gy mz na l nb nc"># chi-ssquare<br/>selector = SelectFwe(score_func=chi2, alpha=0.05)<br/>new_data = selector.fit_transform(X, y)</span><span id="11c0" class="mx my it mt b gy nd na l nb nc">mask = selector.get_support()<br/>new_features = X.columns[mask]<br/>new_features</span></pre><p id="44b9" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们得到:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="7797" class="mx my it mt b gy mz na l nb nc">Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',<br/>       'concavity_mean', 'concave points_mean', 'radius_se', 'perimeter_se',<br/>       'area_se', 'radius_worst', 'texture_worst', 'perimeter_worst',<br/>       'area_worst', 'compactness_worst', 'concavity_worst',<br/>       'concave points_worst'],<br/>      dtype='object')</span></pre><p id="d481" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated"><strong class="lx jd">方差分析</strong></p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="33d8" class="mx my it mt b gy mz na l nb nc">selector = SelectFwe(score_func=f_classif, alpha=0.05)<br/>new_data = selector.fit_transform(X, y)</span><span id="9c90" class="mx my it mt b gy nd na l nb nc">mask = selector.get_support()<br/>new_features = X.columns[mask]<br/>new_features</span></pre><p id="d20c" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们得到:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="5a66" class="mx my it mt b gy mz na l nb nc">Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',<br/>       'smoothness_mean', 'compactness_mean', 'concavity_mean',<br/>       'concave points_mean', 'symmetry_mean', 'radius_se', 'perimeter_se',<br/>       'area_se', 'compactness_se', 'concavity_se', 'concave points_se',<br/>       'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',<br/>       'smoothness_worst', 'compactness_worst', 'concavity_worst',<br/>       'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst'],<br/>      dtype='object')</span></pre><h1 id="690f" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">FranklinDelanoRoosevelt富兰克林.德兰诺.罗斯福（美国第三十二任总统）</h1><p id="5371" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">错误发现率(FDR)考虑了多重比较。</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="d37d" class="mx my it mt b gy mz na l nb nc">selector = SelectFdr(chi2, alpha=0.05)<br/>new_data = selector.fit_transform(X, y)</span><span id="f4fb" class="mx my it mt b gy nd na l nb nc">mask = selector.get_support()<br/>new_features = X.columns[mask]<br/>new_features</span></pre><p id="98ac" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们得到:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="5571" class="mx my it mt b gy mz na l nb nc">Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',<br/>       'compactness_mean', 'concavity_mean', 'concave points_mean',<br/>       'radius_se', 'perimeter_se', 'area_se', 'radius_worst', 'texture_worst',<br/>       'perimeter_worst', 'area_worst', 'compactness_worst', 'concavity_worst',<br/>       'concave points_worst'],<br/>      dtype='object')</span></pre><h1 id="f7c6" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">从模型中</h1><p id="bf2e" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">我们可以保留从模型中得到的最重要的特征。让我们再次考虑随机森林:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="6ed9" class="mx my it mt b gy mz na l nb nc">from sklearn.feature_selection import SelectFromModel<br/>selector = SelectFromModel(estimator=RandomForestClassifier(n_estimators=50)).fit(X, y)<br/>mask = selector.get_support()<br/>new_features = X.columns[mask]<br/>new_features</span></pre><p id="842c" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们得到:</p><pre class="ks kt ku kv gt ms mt mu mv aw mw bi"><span id="716e" class="mx my it mt b gy mz na l nb nc">ndex(['perimeter_mean', 'area_mean', 'concavity_mean', 'concave points_mean',<br/>       'radius_worst', 'perimeter_worst', 'area_worst',<br/>       'concave points_worst'],<br/>      dtype='object')</span></pre><h1 id="b270" class="nf my it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">讨论</h1><p id="fddc" class="pw-post-body-paragraph lv lw it lx b ly nw kd ma mb nx kg md me ny mg mh mi nz mk ml mm oa mo mp mq im bi translated">这是一个如何很好地了解哪些变量对您的模型最重要的例子。请记住，您应该始终运行探索性数据分析。对于二元情况，箱形图始终是合适的图。让我们看一下恶性肿瘤的<code class="fe ob oc od mt b">concave points_worst</code>的方框图。</p><p id="3343" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">最后可以找一个<a class="ae mr" href="https://stackoverflow.com/questions/57273694/how-selectkbest-chi2-calculates-score?fbclid=IwAR1GxhB8TpifAHOswQETOw2NvcDs8_f5Qomr3xq3ZLodAFYyvXuJLkh0iYQ" rel="noopener ugc nofollow" target="_blank">卡方检验的详细解释。</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/065c9699eabbc8ab2b3acee40fe11d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lkHM4qjyAiknIMoo.png"/></div></div></figure></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="6572" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated"><em class="om">最初发表于</em><a class="ae mr" href="https://predictivehacks.com/feature-selection-in-python/" rel="noopener ugc nofollow" target="_blank">T5【https://predictivehacks.com】</a><em class="om">。</em></p></div></div>    
</body>
</html>