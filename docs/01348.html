<html>
<head>
<title>Scaling scikit-learn with Apache Beam</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">缩放sci kit-使用Apache Beam学习</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/scaling-scikit-learn-with-apache-beam-251eb6fcf75b?source=collection_archive---------3-----------------------#2019-12-16">https://levelup.gitconnected.com/scaling-scikit-learn-with-apache-beam-251eb6fcf75b?source=collection_archive---------3-----------------------#2019-12-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/27e7b9b9f24363591ecdc5a4d822d717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9A-NsaAl6VNk1OxEO-iXZA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:https://www.pxfuel.com/en/free-photo-ozymu</figcaption></figure><div class=""/><div class=""><h2 id="9ab5" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">第7章摘自“生产中的数据科学”</h2></div><p id="dce2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> Apache Beam是一个开源项目，使数据科学家能够创作可以扩展到海量数据集的机器学习管道。我的书的这一章着重于用Beam构建批处理模型管道，它可以在使用云数据流的集群上运行。该节选展示了如何使用BigQuery作为数据源和数据宿来执行分布式模型应用，本章的完整源代码可在</em> <a class="ae jd" href="https://github.com/bgweber/DS_Production/blob/master/CH7.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> GitHub </em> </a> <em class="lr">上获得。有关设置JSON凭证文件的详细信息，请参见GCP </em> <a class="ae jd" href="https://cloud.google.com/iam/docs/creating-managing-service-account-keys" rel="noopener ugc nofollow" target="_blank"> <em class="lr">文档</em> </a> <em class="lr">和我的</em> <a class="ae jd" href="https://leanpub.com/ProductionDataScience" rel="noopener ugc nofollow" target="_blank"> <em class="lr">书样</em> </a> <em class="lr">。</em></p><div class="ip iq gp gr ir ls"><a href="https://leanpub.com/ProductionDataScience" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab fo"><div class="lu ab lv cl cj lw"><h2 class="bd jh gy z fp lx fr fs ly fu fw jf bi translated">生产中的数据科学</h2><div class="lz l"><h3 class="bd b gy z fp lx fr fs ly fu fw dk translated">从初创公司到数万亿美元的公司，数据科学在帮助组织最大化…</h3></div><div class="ma l"><p class="bd b dl z fp lx fr fs ly fu fw dk translated">leanpub.com</p></div></div><div class="mb l"><div class="mc l md me mf mb mg ix ls"/></div></div></a></div><p id="224d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Dataflow是一个用于构建数据管道的工具，可以在本地运行，也可以在托管环境中扩展到大型集群。虽然Cloud Dataflow最初是作为一个特定于GCP的工具在谷歌孵化的，但它现在建立在开源的Apache Beam库之上，使其可以在其他云环境中使用。该工具提供了不同数据源的输入连接器，如BigQuery和云存储上的文件，用于转换和聚合数据的操作符，以及云数据存储和BigQuery等系统的输出连接器。</p><p id="f2c4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一章中，我们将使用数据流构建一个管道，从BigQuery读入数据，应用sklearn模型来创建预测，然后将预测写入BigQuery和云数据存储。我们将首先在数据子集上本地运行管道，然后使用GCP扩展到更大的数据集。</p><p id="0ac3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据流旨在实现高度可扩展的数据管道，例如执行ETL工作，您需要在云部署中的不同系统之间移动数据。它还被扩展到构建ML管道，并且内置了对TensorFlow和其他机器学习方法的支持。结果是，Dataflow使数据科学家能够建立大规模管道，而不需要工程团队的支持来扩大生产规模。</p><p id="fd5a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据流中的核心组件是管道，它定义了作为工作流的一部分要执行的操作。数据流中的工作流是包括数据源、数据接收器和数据转换的DAG。以下是一些关键组件:</p><ul class=""><li id="22f2" class="mh mi jg kx b ky kz lb lc le mj li mk lm ml lq mm mn mo mp bi translated"><strong class="kx jh"> Pipeline: </strong>定义作为作业的一部分要执行的一组操作。</li><li id="f363" class="mh mi jg kx b ky mq lb mr le ms li mt lm mu lq mm mn mo mp bi translated"><strong class="kx jh">集合:</strong>工作流中不同阶段之间的接口。工作流中任何步骤的输入都是对象的集合，输出是新的对象集合。</li><li id="d3f6" class="mh mi jg kx b ky mq lb mr le ms li mt lm mu lq mm mn mo mp bi translated">DoFn: 对集合中的每个元素执行的操作，产生一个新的集合。</li><li id="0dbf" class="mh mi jg kx b ky mq lb mr le ms li mt lm mu lq mm mn mo mp bi translated"><strong class="kx jh">转换:</strong>对集合中的元素集执行的操作，例如聚合。</li></ul><p id="d521" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Dataflow支持多种语言，但在本书中，我们将重点关注Python实现。Python版本有一些注意事项，因为worker节点可能需要从源代码编译库，但它确实很好地介绍了Apache Beam中的不同组件。要使用Beam创建工作流，可以使用Python中的管道语法将不同的步骤链接在一起。结果是要执行的DAG操作可以分布在集群中的机器上。</p><p id="2033" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在数据流管道中转换数据的两种方式是<code class="fe mv mw mx my b">DoFn</code>和<code class="fe mv mw mx my b">Transform</code>步骤。一个<code class="fe mv mw mx my b">DoFn</code>步骤定义了对集合中的每个对象执行的操作。例如，我们将查询Natality公共数据集，得到的集合将包含dictionary对象。我们将定义一个<code class="fe mv mw mx my b">DoFn</code>操作，该操作使用sklearn为每个字典对象创建一个预测，并输出一个新的字典对象。一个<code class="fe mv mw mx my b">Transform</code>定义了在一组对象上执行的操作，例如执行特征生成以将原始跟踪事件聚合到用户级摘要中。这些类型的操作通常与分区转换步骤结合使用，以将对象集合划分成可管理的大小。我们不会在本书中探索这一过程，但可以使用转换来应用Featuretools，以作为数据流管道的一部分来执行自动化特征工程。</p><p id="718e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一章中，我们将着手构建可以在本地和完全管理的GCP集群中运行的数据流管道。我们将首先构建一个处理文本数据的简单管道，然后构建一个在分布式工作流中应用sklearn模型的管道。</p><p id="03ed" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Apache Beam是一个开源库，用于使用Java、Python和Go构建数据处理工作流。Beam工作流可以跨多个执行引擎执行，包括Spark、Dataflow和MapReduce。使用Beam，您可以使用<code class="fe mv mw mx my b">Direct Runner</code>在本地测试工作流，然后使用<code class="fe mv mw mx my b">Dataflow Runner</code>在GCP部署工作流。波束管道可以是批处理的，即执行工作流直到完成，也可以是流式的，即管道连续运行，在接收数据时近乎实时地执行操作。我们将在本章重点讨论批处理管道，在下一章讨论流式管道。</p><p id="832a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了开始运行，我们需要做的第一件事是安装Apache Beam库。从命令行运行下面显示的命令，以便安装库，并在本地运行测试管道。<code class="fe mv mw mx my b">pip</code>命令包括<code class="fe mv mw mx my b">gcp</code>注释，用于指定数据流模块也应该被安装。如果最后一步成功，管道将输出莎士比亚的李尔王的字数。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="24ca" class="nh ni jg my b gy nj nk l nl nm"><strong class="my jh"># install APache Bean<br/></strong>pip install --user apache-beam[gcp]</span><span id="c704" class="nh ni jg my b gy nn nk l nl nm"><strong class="my jh"># run the word count example <br/></strong>python3 -m apache_beam.examples.wordcount --output outputs</span></pre><p id="aec3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了执行这种计数逻辑，示例流水线执行多个不同的步骤。首先，管道以字符串对象集合的形式读入剧本，其中剧本中的每一行都是一个字符串。接下来，管道将每一行拆分成一个单词集合，然后传递给map和group转换，计算每个单词的出现次数。映射和分组操作是内置的Bean转换操作。最后一步是将字数统计集合写入控制台。</p><h1 id="4e9c" class="no ni jg bd np nq nr ns nt nu nv nw nx km ny kn nz kp oa kq ob ks oc kt od oe bi translated">7.2批量模型管道</h1><p id="ccb7" class="pw-post-body-paragraph kv kw jg kx b ky of kh la lb og kk ld le oh lg lh li oi lk ll lm oj lo lp lq ij bi translated">云数据流为将sklearn模型扩展到海量数据集提供了一个有用的框架。我们可以在流程函数中单独评估每条记录，并使用Apache Beam将这些输出流式传输到数据接收器，如BigQuery，而不是将所有输入数据放入dataframe。只要我们有一种跨工作节点分发模型的方法，我们就可以使用数据流来执行分布式模型应用程序。这可以通过将模型对象作为辅助输入传递给操作者或者从持久存储(比如云存储)中读取模型来实现。在本节中，我们将首先使用Jupyter环境训练一个线性回归模型，然后将结果存储到云存储中，以便我们可以在大型数据集上运行该模型，并将预测保存到BigQuery和云数据存储中。</p><h1 id="7744" class="no ni jg bd np nq nr ns nt nu nv nw nx km ny kn nz kp oa kq ob ks oc kt od oe bi translated">模型培训</h1><p id="9105" class="pw-post-body-paragraph kv kw jg kx b ky of kh la lb og kk ld le oh lg lh li oi lk ll lm oj lo lp lq ij bi translated">我们将要执行的建模任务是在给定一些因素的情况下，使用出生率公共数据集来预测孩子的出生体重。要使用sklearn构建模型，我们可以在将数据集加载到Pandas数据框架并拟合模型之前对其进行采样。下面的代码片段显示了如何从Jupyter笔记本中对数据集进行采样，并可视化记录的子集，如图<a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/1-2-batch-model-pipeline.html#fig:7-train"> 1.3 </a>所示。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="d001" class="nh ni jg my b gy nj nk l nl nm">from google.cloud import bigquery<br/>client = <strong class="my jh">bigquery.Client</strong>()</span><span id="e933" class="nh ni jg my b gy nn nk l nl nm">sql = """<br/>SELECT year, plurality, apgar_5min, <br/>       mother_age, father_age,    <br/>       gestation_weeks, ever_born<br/>       ,case when mother_married = true <br/>             then 1 else 0 end as mother_married<br/>       ,weight_pounds as weight<br/>  FROM  `bigquery-public-data.samples.natality`<br/>  order by rand() <br/>  limit 10000<br/>"""</span><span id="548b" class="nh ni jg my b gy nn nk l nl nm">natalityDF = <strong class="my jh">client.query</strong>(sql)<strong class="my jh">.to_dataframe</strong>()<strong class="my jh">.fillna</strong>(0)<br/><strong class="my jh">natalityDF.head</strong>()</span></pre><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/6a7dd67c2defe93e7415cdea8248bc56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NZs6M8nm_KLFF8AWOntHOw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图1.3:用于训练的抽样出生率数据集。</figcaption></figure><p id="1c02" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦我们有了用于训练的数据，我们就可以使用sklearn中的<code class="fe mv mw mx my b">LinearRegression</code>类来拟合模型。我们将使用完整的数据集进行拟合，因为维持数据是未被采样的数据集的剩余部分。一旦完成训练，我们可以使用<code class="fe mv mw mx my b">pickle</code>来序列化模型并保存到磁盘。最后一步是将模型文件从本地存储移动到云存储，如下所示。我们现在有一个经过训练的模型，它可以作为分布式模型应用程序工作流的一部分。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="71d2" class="nh ni jg my b gy nj nk l nl nm">from sklearn.linear_model import LinearRegression<br/>import pickle<br/>from google.cloud import storage</span><span id="1870" class="nh ni jg my b gy nn nk l nl nm"><em class="lr"># fit and pickle a model </em><br/>model = <strong class="my jh">LinearRegression</strong>()<br/><strong class="my jh">model.fit</strong>(natalityDF.iloc[:,1:8], natalityDF['weight'])<br/><strong class="my jh">pickle.dump</strong>(model, <strong class="my jh">open</strong>("natality.pkl", 'wb'))</span><span id="8a2d" class="nh ni jg my b gy nn nk l nl nm"><em class="lr"># Save to GCS</em><br/>bucket = <strong class="my jh">storage.Client</strong>()<strong class="my jh">.get_bucket</strong>('dsp_model_store')<br/>blob = <strong class="my jh">bucket.blob</strong>('natality/sklearn-linear')<br/><strong class="my jh">blob.upload_from_filename</strong>('natality.pkl')</span></pre><h1 id="08b5" class="no ni jg bd np nq nr ns nt nu nv nw nx km ny kn nz kp oa kq ob ks oc kt od oe bi translated">7.2.2大查询发布</h1><p id="5cf7" class="pw-post-body-paragraph kv kw jg kx b ky of kh la lb og kk ld le oh lg lh li oi lk ll lm oj lo lp lq ij bi translated">我们将首先构建一个Beam管道，它从BigQuery读入数据，应用一个模型，然后将结果写入BigQuery。在下一节中，我们将添加云数据存储作为管道的附加数据接收器。这个管道将比前一个例子稍微复杂一些，因为我们需要在流程函数中使用多个Python模块，这需要更多的设置。</p><p id="f032" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一次，我们将遍历管道的不同部分，以提供关于每个步骤的更多细节。第一项任务是定义构建和执行管道所需的库。我们还导入了json模块，因为我们需要它来创建指定输出BigQuery表结构的schema对象。像上一节一样，我们仍然在对数据集进行采样，以确保我们的管道在处理完整的数据集之前工作正常。一旦我们对管道充满信心，我们就可以删除<code class="fe mv mw mx my b">limit</code>命令并自动扩展集群来完成工作负载。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="2120" class="nh ni jg my b gy nj nk l nl nm">import apache_beam as beam<br/>import argparse<br/>from apache_beam.options.pipeline_options import PipelineOptions<br/>from apache_beam.options.pipeline_options import SetupOptions<br/>from apache_beam.io.gcp.bigquery import parse_table_schema_from_json<br/>import json</span><span id="f8ff" class="nh ni jg my b gy nn nk l nl nm">query = """<br/>    SELECT year, plurality, apgar_5min, <br/>    mother_age, father_age,    <br/>       gestation_weeks, ever_born<br/>       ,case when mother_married = true <br/>          then 1 else 0 end as mother_married<br/>      ,weight_pounds as weight<br/>      ,current_timestamp as time<br/>      ,GENERATE_UUID() as guid<br/>    FROM `bigquery-public-data.samples.natality` <br/>    rand() <br/>    limit 100    <br/>"""</span></pre><p id="1767" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们将定义一个实现<code class="fe mv mw mx my b">process</code>函数的<code class="fe mv mw mx my b">DoFn</code>类，并将sklearn模型应用于出生率数据集中的单个记录。与以前的变化之一是我们现在有了一个<code class="fe mv mw mx my b">init</code>函数，我们用它来实例化一组字段。为了引用我们需要在<code class="fe mv mw mx my b">process</code>函数中使用的模块，我们需要在类中将这些模块指定为字段，否则当在分布式工作节点上运行函数时，引用将是未定义的。例如，我们用<code class="fe mv mw mx my b">self._pd</code>来指代熊猫模块，而不是<code class="fe mv mw mx my b">pd</code>。对于模型，一旦需要，我们将使用惰性初始化从云存储中获取模型。虽然可以实现由<code class="fe mv mw mx my b">DoFn</code>接口定义的<code class="fe mv mw mx my b">setup</code>函数来加载模型，但是对于运行者调用这个函数有一些限制。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="a5f6" class="nh ni jg my b gy nj nk l nl nm">class <strong class="my jh">ApplyDoFn</strong>(beam.DoFn):</span><span id="0927" class="nh ni jg my b gy nn nk l nl nm">    def <strong class="my jh">__init__</strong>(self):<br/>        self._model = None<br/>        from google.cloud import storage<br/>        import pandas as pd<br/>        import pickle as pkl<br/>        self._storage = storage<br/>        self._pkl = pkl<br/>        self._pd = pd<br/>     <br/>    def <strong class="my jh">process</strong>(self, element):<br/>        <strong class="my jh">if</strong> self._model is None:<br/>            bucket = <strong class="my jh">self._storage.Client</strong>()<strong class="my jh">.get_bucket</strong>(<br/>                                                 'dsp_model_store')<br/>            blob = <strong class="my jh">bucket.get_blob</strong>('natality/sklearn-linear')<br/>            self._model =<strong class="my jh">self._pkl.loads</strong>(<strong class="my jh">blob.download_as_string</strong>())<br/>        <br/>        new_x = <strong class="my jh">self._pd.DataFrame.from_dict</strong>(element, <br/>                            orient = "index")<strong class="my jh">.transpose</strong>()<strong class="my jh">.fillna</strong>(0)<br/>        weight = <strong class="my jh">self._model.predict</strong>(new_x.iloc[:,1:8])[0]<br/>        return [ { 'guid': element['guid'], 'weight': weight, <br/>                                   'time': <strong class="my jh">str</strong>(element['time']) } ]<!-- --> </span></pre><p id="53a3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦模型对象被延迟加载到流程函数中，就可以使用它将线性回归模型应用到输入记录。在数据流中，从BigQuery检索到的记录作为字典对象的集合返回，我们的流程函数负责独立地操作每一个字典。我们首先将字典转换为熊猫数据框架，然后将其传递给模型以获得预测体重。process函数返回一个字典对象列表，它描述了要写入BigQuery的结果。返回的是一个列表而不是一个字典，因为Beam中的过程函数可以返回零个、一个或多个对象。</p><p id="4e3c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的清单显示了一个传递给流程函数的示例<code class="fe mv mw mx my b">element</code>对象。对象是字典类型，其中键是查询记录的列名，值是记录值。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="c3d9" class="nh ni jg my b gy nj nk l nl nm">{'year': 2001, 'plurality': 1, 'apgar_5min': 99, 'mother_age': 33, <br/>     'father_age': 40, 'gestation_weeks': 38, 'ever_born': 8, <br/>     'mother_married': 1, 'weight': 6.8122838958, <br/>     'time': '2019-12-14 23:51:42.560931 UTC', <br/>     'guid': 'b281c5e8-85b2-4cbd-a2d8-e501ca816363'}</span></pre><p id="9fa1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了将预测保存到BigQuery，我们需要定义一个模式，该模式定义了预测表的结构。我们可以使用一个实用函数将表模式的JSON描述转换成Beam BigQuery编写器所需的模式对象。为了简化这个过程，我首先创建了一个Python dictionary对象，并使用<code class="fe mv mw mx my b">dumps</code>命令生成JSON。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="98c0" class="nh ni jg my b gy nj nk l nl nm">schema = <strong class="my jh">parse_table_schema_from_json</strong>(<strong class="my jh">json.dumps</strong>({'fields':<br/>            [ { 'name': 'guid', 'type': 'STRING'},<br/>              { 'name': 'weight', 'type': 'FLOAT64'},<br/>              { 'name': 'time', 'type': 'STRING'} ]}))</span></pre><p id="21b1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下一步是创建管道并定义射束操作的DAG。这一次，我们不向管道提供输入或输出参数，而是将输入和输出目的地传递给BigQuery操作符。管道有三个步骤:从BigQuery读取、应用模型和写入BigQuery。为了读取BigQuery，我们传递查询并指定我们使用标准SQL。为了应用该模型，我们使用自定义类来构建预测。为了编写结果，我们将模式和表名传递给BigQuery编写器，并指定如果需要，应该创建一个新表，如果数据已经存在，应该将数据追加到表中。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="e7b2" class="nh ni jg my b gy nj nk l nl nm"><em class="lr"># set up pipeline options </em><br/>parser = <strong class="my jh">argparse.ArgumentParser</strong>()<br/>known_args, pipeline_args = <strong class="my jh">parser.parse_known_args</strong>(None)<br/>pipeline_options = <strong class="my jh">PipelineOptions</strong>(pipeline_args)</span><span id="0f95" class="nh ni jg my b gy nn nk l nl nm"><em class="lr"># define the pipeline steps</em><br/>p = <strong class="my jh">beam.Pipeline</strong>(options=pipeline_options)<br/>data = p | 'Read from BigQuery' &gt;<strong class="my jh">&gt;</strong> <strong class="my jh">beam.io.Read</strong>(<br/>       <strong class="my jh">beam.io.BigQuerySource</strong>(query=query, use_standard_sql=True))<br/>scored = data | 'Apply Model' &gt;<strong class="my jh">&gt;</strong> <strong class="my jh">beam.ParDo</strong>(<strong class="my jh">ApplyDoFn</strong>())<br/>scored | 'Save to BigQuery' &gt;<strong class="my jh">&gt;</strong> <strong class="my jh">beam.io.Write</strong>(<strong class="my jh">beam.io.BigQuerySink</strong>(<br/>                'weight_preds', 'dsp_demo', schema = schema,<br/>   create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,<br/>   write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))</span></pre><p id="6d04" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">脚本的最后一步是运行管道。虽然可以从Jupyter运行这个完整的代码清单，但是管道将无法完成，因为<code class="fe mv mw mx my b">project</code>参数需要作为命令行参数传递给管道。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="6738" class="nh ni jg my b gy nj nk l nl nm"><em class="lr"># run the pipeline</em><br/>result = <strong class="my jh">p.run</strong>()<br/><strong class="my jh">result.wait_until_finish</strong>()</span></pre><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/4f3c30f70797c73b86b780f44b0b76b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zEgRKkSyuj94cXycTZNfAA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图1.4:big query上的出生率预测表。</figcaption></figure><p id="0b78" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在数据流上运行管道之前，最好使用数据子集在本地运行管道。为了在本地运行管道，有必要将GCP项目指定为命令行参数，如下所示。使用BigQuery读写数据需要project参数。运行管道后，您可以通过导航到BigQuery UI并检查目标表中的数据来验证工作流是否成功，如图<a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/1-2-batch-model-pipeline.html#fig:7-bq"> 1.4 </a>所示。</p><p id="ecb1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要在云数据流上运行管道，我们需要传递一个参数，将数据流运行器标识为执行引擎。我们还需要传递项目名称和云存储上的暂存位置。我们现在传入一个需求文件，该文件将<code class="fe mv mw mx my b">google-cloud-storage</code>库标识为一个依赖项，并使用max workers参数设置一个集群大小限制。提交后，您可以通过导航到GCP控制台中的数据流UI来查看作业的进度，如图<a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/1-2-batch-model-pipeline.html#fig:7-scale"> 1.5 </a>所示。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="9bff" class="nh ni jg my b gy nj nk l nl nm"><em class="lr"># running locally </em><br/>python3 apply.py --project your_project_name</span><span id="0c4b" class="nh ni jg my b gy nn nk l nl nm"><em class="lr"># running on GCP </em><br/>echo $'google-cloud-storage==1.19.0' &gt; reqs.txt<br/>python3 apply.py \<br/>  --runner DataflowRunner \<br/>  --project your_project_name \<br/>  --temp_location gs:<strong class="my jh">//</strong>dsp_model_store/tmp/ \<br/>  --requirements_file reqs.txt \<br/>  --maxNumWorkers 5</span></pre><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/84c933ddff47e6226b4cf4dae76399cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKwR6j5cSW2VNyRzdDQ7KA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图1.5:使用自动缩放运行托管管道。</figcaption></figure><p id="57fe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在可以从管道中的查询中删除limit命令，并将工作负载扩展到整个数据集。当运行全面的管道时，关注这项工作以确保集群大小不会超出预期是很有用的。设置最大工作线程数有助于避免问题，但是如果您忘记设置此参数，集群大小可能会迅速扩大，并导致代价高昂的管道运行。</p><p id="39c0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将Python用于数据流管道的一个潜在问题是，初始化集群可能需要一段时间，因为每个工作节点将从源代码安装作业所需的库，这对于Pandas等库来说可能需要大量时间。为了避免长时间的启动延迟，避免在需求文件中包含已经包含在数据流SDK中的库是有帮助的。例如，Pandas 0.24.2包含在SDK版本2.16.0中，这是一个足够新的版本。</p><p id="7965" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">云数据流的一个有用方面是它是完全受管的，这意味着它处理硬件供应，在出现问题时处理故障，并可以自动扩展以满足需求。Apache Beam对于数据科学家来说是一个很好的框架，因为它支持使用相同的工具进行本地测试和云部署。</p></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="6927" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本·韦伯是Zynga的一名杰出的数据科学家。我们正在<a class="ae jd" href="https://www.zynga.com/job-listing-category/data-analytics-user-research/" rel="noopener ugc nofollow" target="_blank">招聘</a>！</p></div></div>    
</body>
</html>