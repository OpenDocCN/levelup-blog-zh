<html>
<head>
<title>Automated Twitter Bot for Blockchain Metrics in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中区块链指标的自动化Twitter机器人</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/automated-twitter-bot-for-blockchain-metrics-in-python-part-2-a551e5a84df1?source=collection_archive---------12-----------------------#2020-12-02">https://levelup.gitconnected.com/automated-twitter-bot-for-blockchain-metrics-in-python-part-2-a551e5a84df1?source=collection_archive---------12-----------------------#2020-12-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/7b0e86bb141fd8c9ad002bdca5164630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*07ynb5kYQc9EupOZ-a-Weg.jpeg"/></div></div></figure><p id="e2e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个机器人是一个项目，我已经断断续续地工作了几个月了。这意味着原始代码被重构了无数次，所以我决定重做这篇文章，并删除我发表的关于这个机器人的原始文章，以简化和刷新文档。</p><p id="1b45" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于那些没有阅读原始文章的人，让我简单解释一下这个机器人是如何出现的。</p><p id="438a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">早在2020年第四季度，我就在寻找项目创意，因为我正在构建一个投资组合，以配合我最近获得的数据科学认证。我对区块链理工大学和风靡全球的项目也有着好奇的兴趣。一天，在浏览最新的加密推特时，我看到一个来自将军会的请求，呼吁有技能的人为https://docs.decred.org/privacy/cspp/overview/ T2的硬币洗牌++制作一个推特机器人</p><p id="aaf9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">老实说，我不知道从哪里开始，我也不完全确定我是否能够构建它，但在经历了许多版本、重构和花费时间试图解决我遇到的所有问题以及整合社区要求的新机器人功能后，我最终拥有了一个相当稳定并按预期工作的机器人。</p><p id="cf5a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该脚本由云托管，在UTC时间每天午夜后一点点执行。最初我使用的原始数据来自<a class="ae ky" href="https://explorer.dcrdata.org/api/chart/privacy-participation?axis=time&amp;bin=day" rel="noopener ugc nofollow" target="_blank">https://explorer . DCR data . org/API/chart/privacy-participation？axis=time &amp; bin=day </a>但是由于数据发布/刷新的本质以及其他一些问题，这并不十分奏效。简而言之，通常原始数据每两天更新一次，我们需要新的数据，所以我决定每天从block explorer中抓取数据，而不是使用它。此外，我们需要的一些数据不是以原始格式提供的，所以即使发布及时，也是不够的。整个抓取过程大约需要4500秒，这取决于当天挖掘了多少块，之后它会处理数据并将其广播到twitter。</p><p id="22c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看看数据在哪里，我们到底需要什么…如果我们打开:<a class="ae ky" href="https://explorer.dcrdata.org/" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org/</a>这是我们得到的结果:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kz"><img src="../Images/b1cc8d46736d5d3fd96178d9b5f9a491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3aB8zuRWRXf9kiuN28JbmA.png"/></div></div></figure><p id="940b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在左上角，我们看到“提议”…这是最近开采的区块。Decred的平均封锁时间为5分钟，这意味着每天应该有大约288个新区块被开采。这些块显示在“高度”下，也在上面页面的左侧。可以说，我们要找的数据就在这些街区内。让我们深入其中一个:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kz"><img src="../Images/8dc8428cea442a86c7e99bc13f8c72e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tzXTl7sU9qH3-bBjkW7ECQ.png"/></div></div></figure><p id="9552" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">像这样的大量信息驻留在每个块中…我们的机器人需要的数据是:1)左上角的DCR“混合”量，在这种情况下是:2，195，553708 DCR。2)右侧的日期，在本例中为:2020年11月30日。3)“已发送总量”，这是在该特定块中交易的DCR总量。4)“Block”，这是与其“内部”数据相关联的块号。</p><p id="c96b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们快速分解一下脚本的逻辑:每天UTC午夜后几分钟，前一天过去了，所有的块都在explorer上，我们希望脚本开始抓取刚刚过去的一整天的数据。</p><p id="32eb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在00:15 UTC，脚本运行后，首先检查当前的“建议块”,以确定基准值。这个建议的区块是新一天的第一个区块之一，因此这是我们刮削范围的终点。现在我们调用一个函数，该函数只查看前一天存储的数据中的最后一块，这是我们抓取范围的起点。这样，我们就锁定了这个范围，我们知道我们需要的所有数据都会被捕获，然后我们开始处理和过滤这些数据。</p><p id="3b9a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们需要访问该范围内的所有URL，例如:<a class="ae ky" href="https://explorer.dcrdata.org/block/506889" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org/block/506</a>300到<a class="ae ky" href="https://explorer.dcrdata.org/block/506889" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org/block/506</a>600，并从每个URL中收集我们需要的数据。然后，我们使用这些数据并对其进行处理，以获得机器人将广播的最终值和图表。一旦数据被发布，脚本运行时就结束了，一天后这个过程又开始了。</p><p id="bd5d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们已经有了基本的逻辑，让我们看看脚本:</p><p id="5ae8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它由13个功能组成，有些简短而甜蜜，有些则稍显丰富。在我们看所说的“逻辑()”函数之前，让我们先看看所有从主“逻辑()”函数调用的“独立”函数。</p><p id="7350" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我删除了代码中的所有注释，以使它不那么混乱，更具可读性。我会在本文末尾添加源代码。由于这段代码除了记录这个机器人之外，并不打算用于任何其他目的，所以我不打算添加注释。希望我在文章中的解释能够满足我们对每个函数的理解。</p><p id="2c9b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们导入该脚本所需的库(第1–9行)。第12–14行我们创建了一个api对象，我们需要它来通过tweepy库与twitter交互。第一个独立函数称为‘proposed _ block _ URL()’。该函数使用漂亮的soup库来确定最新的块高度，在获得该值后，该函数在被调用时返回该值。我们有一个基本的错误处理机制，它只是在get请求由于某种原因产生错误的情况下，将解释器转移回主函数，而不是崩溃。然后，main函数应该再次调用这个函数，以便重试。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi le"><img src="../Images/6369de229b1a37ab4a0782550c346463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1mKEx1ktLohyLFmKOkG68A.png"/></div></div></figure><p id="fa3b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来我们有另一个独立的函数叫做“混合日期时间(路线)”，它对于一个截图来说太大了，所以我把它分成了两半。这里我们有与前面函数相同的错误处理逻辑。这个函数传递了一个名为“route”的参数，它是浏览器上一个块的URL，例如<a class="ae ky" href="https://explorer.dcrdata.org/block/506889" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org/block/506</a>300。这个函数基本上会被调用300次，以便从前一天开采的每个区块中提取我们需要的数据。首先，我们使用漂亮的soup库来获取数据，其中也内置了一些错误处理。原因是，如果日期是&lt;10号，那么字符串的长度就会短1个字符，我们需要适应这样的情况。我们还将html_main &amp; soup_main变量定义为全局变量，以便能够在其他函数中使用来自原始get请求的数据，而不是用新的get请求轰炸服务器。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lf"><img src="../Images/ce1b10b130a51071acfa41e6cd38f17b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NGKOkjulalLrbPeBWFOwA.png"/></div></div></figure><p id="f134" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这个函数中，我们调用一个名为:date_time(date)的帮助函数，并向它传递刚刚得到的“date”值。在我介绍这个日期时间(日期)函数之前，让我们快速看一下“混合日期时间(路线)”函数的第二部分:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lg"><img src="../Images/b6d5b70adc272a39fcc8bcdd0fb78006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQK7vwQz9RhMne7rMo-1SA.png"/></div></div></figure><p id="f8a9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">if else条件语句只是为了确保我们得到我们想要的值，而不是一个错误。由于混合器在时期中运行，并且仅每±20分钟混合一次，所以一些块将具有0 DCR混合，因此是第95行中的逻辑，等等。最终，一旦这个函数收集了它需要的所有数据，它将返回混合值、总事务处理值以及块号和每个块的日期。上面提到的辅助函数的快速峰值:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lh"><img src="../Images/a5eab27016f9f5b1f5ba01c55d505db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxObFnnt_9UICU7ElshoCw.png"/></div></div></figure><p id="d26b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该函数连接日期值，并使用来自get请求的数据(我们将其声明为全局变量)来抓取混合值、总事务处理值以及块号，然后返回到调用该函数的位置。</p><p id="983d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们有两个非常简单的函数:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi li"><img src="../Images/fa75c78cd366ddb4f7b43e6d5a94b61d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0LdsChRxWgz6La9tE524Q.png"/></div></div></figure><p id="3d14" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这非常简单，这两个函数从第111行看到的URL中提取硬币供应量和DCR汇率，以计算前一天以美元计算的DCR混合量。你可以在下面的屏幕截图右下方看到被刮去的实际值:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lj"><img src="../Images/ea53530043cb8ee649302b8746a8efea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbkWfg-bDHH5yk_3ZKGEeQ.png"/></div></div></figure><p id="f67b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来的两个函数称为“Twitter _ data()&amp; current _ utc _ time()”:</p><p id="7c53" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个机器人广播的每条推文都以日期开始，以便有一个参考点。第一个函数使用twitter api对象从链接到机器人的特定帐户中提取时间轴。一旦我们有了时间线，我们就争论出日期，然后我们声明它是全局的，以便在这个函数之外使用它。我们还将日期转换成可计算的时间戳，我们也将它声明为全局时间戳。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lk"><img src="../Images/df684515e1e9a51c6bdedd8311e192b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFXherkLB31Cu4ylPAp_qQ.png"/></div></div></figure><p id="b188" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第二个函数current_utc_time()就是这样做的。当被调用时，它获取当前的UTC时间，并将其作为可计算的时间戳返回。</p><p id="5971" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来我们有两个函数来决定前一天的DCR混合量是否是一个新的历史最高纪录。我们使用来自dcrdata.decred.org的原始数据以及我们收集的最新混合值来计算它。原始数据如下所示:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ll"><img src="../Images/3c0b3ee14bdee7d356514ed659a4fbc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3O-wyD2B_5VteWrFM5vABg.png"/></div></div></figure><p id="3e2e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">基本上，上面一行是未处理的混合值，下面一行是时间戳。这两行的索引是相同的，所以如果你从一行中截取某个值，从另一行中截取相同的索引，你将得到混合的值及其伴随的时间戳。</p><p id="c023" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好，首先我们得到json格式的原始数据。我们将未处理的混合值保存为变量“data_values”中的全局值。然后我们创建两个变量。1包含时间戳格式的前几天的“日期”,第194行的第二个包含原始数据中最新值的时间戳。然后，我们将另一个变量初始化为False，它将被传递给helper函数。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lm"><img src="../Images/cff36ad164cf6a0c070764422da256c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGeg2gTbPEf4cjId3E5Hrg.png"/></div></div></figure><p id="c124" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们调用helper函数，并将布尔值作为参数传递。如果原始数据上的最新条目==前几天的时间戳，我们知道原始数据是“新鲜的”,然后我们需要删除该最新值，以便在对数据排序后找到前一条记录，如第168–169行所示。然后，我们对它进行排序，找出理论上应该是前一个记录的最大值。我们把它处理成可读的格式，保留相关的数字。然后，该函数将先前的记录值返回给record_check()函数，该函数在被调用时将返回这个派生的(先前的)记录值。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lj"><img src="../Images/bed430ce0d5e80cf5c7a4122d8b5cc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*85iou3X-ixc0rEb3aelHUw.png"/></div></div></figure><p id="4e3a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面我们有一个函数，它确定并返回以美元为单位的DCR。我们只需刮下赌注的价值，然后乘以当前的美元汇率。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/60e9b234d749b318bd9c9c94f5c7c349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1lf3LLp6UOBX9KgyR6TqQw.png"/></div></div></figure><p id="651b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们有第一个函数，当这个脚本启动时，这个函数就会被调用。这个函数的作用是启动一个while循环，并且只在time == 00:15 UTC时中断循环一次，然后调用main logic()函数，这个函数是从属函数，也可以说是这个程序的主干。在我们介绍主逻辑函数之前，这里还有最后一个非常基本的函数，它决定了我们需要抓取的范围的起始块，称为start_block1()。</p><p id="7129" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们打开我们附加到每日的csv文件，并查看最后一个条目，它是前几天范围的最后一个块，该值是我们刮取范围的起点。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/0e70c8ef4acacef0e89b85fb22c89845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OydzG46hGA8wTsDklrahgQ.png"/></div></div></figure><p id="dbaa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">主logic()函数很长，所以我将把它分解开来，分成几个部分来解释:</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lo"><img src="../Images/f31582db9641d2ad3c7565a2a5b303e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRLSEvfUWabagTiwSjDGdg.png"/></div></div></figure><p id="5a45" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们创建两个空列表，我们将在整个函数中使用它们。然后，我们创建一个名为time value的变量，以时间戳格式计算最近一条tweet和当前UTC时间之间的时间差。第257行的第一个条件只是验证之前的tweet实际上比我们将要收集的最新数据早一天。然后，我们通过调用proposed_block()函数来创建抓取范围，然后我们创建一个循环，该循环将遍历我们刚刚确定的整个抓取范围，并将我们想要抓取的所有URL附加到前面创建的空列表之一中。接下来，在第270行中，我们遍历所有的URL，并调用mixed_date_time(route)函数来收集我们正在寻找的所有数据，然后将这些数据添加到前面创建的列表中。我们在每个get请求之间插入一个12秒的暂停期，只是为了尊重服务器，而不是一下子用太多的请求轰炸它。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/57a6553a881c67b5a61cdb4631834d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOp7lvF6NUSbnRQfWVe5Fg.png"/></div></div></figure><p id="e49d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第274行我们循环遍历包含所有数据的列表，去掉一些括号等等。让它更容易使用。第280行我们创建了一个时间戳值，它==最近一次tweet之后的一天。这在理论上是我们想要的“新”数据。由于抓取范围比我们实际需要的要宽，我们确实需要像这样过滤数据，否则我们可能会从我们不感兴趣的日期获得数据。我们将过滤后的数据以独立的形式添加到一个新列表中。</p><p id="e412" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们将这些隔离的数据附加到各自的。csv文件，以便拥有该数据的历史记录。我们还需要至少30天的数据来绘制我们的图表，我们将很快介绍这些数据。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/68c77a35887c87ed8bf1139ed01095a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yDuFjaEnyqYMjQ1WN4b0ag.png"/></div></div></figure><p id="3bce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第319–322行我们按块合计前几天的混合值，以获得前一天的总混合值。第324–327行我们得到混合金额的美元值，并计算混合流通供应量的百分比。我们还为漂亮的显示设置了值的格式。第333–343行我们用BTC术语计算当前的DCR值，类似于我们计算DCR的美元值。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/dcd8c4c709e90809765488bf16e59749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JE2GN_ai-655JVoNUAR1Tw.png"/></div></div></figure><p id="19df" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第345–352行我们使用一个基本条件并调用record_check()函数来获取前一个记录值。如果我们前一天的新合计混合值大于旧记录，那么我们只需在推文中添加这确实是一个新的历史高点，或者我们只需以正常格式发布推文。第354–356行我们在。csv文件，我们为它们创建熊猫数据框。我们将混合值转换成整数，并将数据帧组合成1。然后，我们按日期对其进行分组，只使用数据集的最后31个条目，也就是最近一个月的误差。这里还创建了一个列表，以累积的方式包含混合值。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/8863db5bf4c49c603cf30fca3c510718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lbmd4pxBdGiR6UPpP8-Xxg.png"/></div></div></figure><p id="7f8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第369–371行我们从数据框中提取列，并将它们转换为列表格式，以便能够以可计算的格式处理这些值。第380-391行我们循环遍历新创建的列表，找到我们需要的字符串的特定部分，并以更精确/可行的形式将它们添加到新列表中。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/7b140aa13097ef1a17160009ebdb731d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZYElXsnM-iPYx73IeJIig.png"/></div></div></figure><p id="9edd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第393–400行我们重建数据帧。第403–420行我们整理了轴数据、定位、标签等。并绘制图表。我们将图形图像保存为。名为“graph2.png”的png文件</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lr"><img src="../Images/eddcf9d7ce35e60d9d953a063090aef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F6gOHIwFHw52n93PbAODzg.png"/></div></div></figure><p id="0541" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第421行我们创建了一个变量，将图片上传到twitters服务器，然后我们在第427行广播数据。</p><p id="2b52" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第428行我们只是从我们的云目录中删除图形图像，因为一旦广播，它就变得多余了。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/fe7f484babf73e56469506c0fec0b74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oc4oAHKKak7FO8S3SZzY8g.png"/></div></div></figure><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ls"><img src="../Images/fa3712956a0448460c1c6841da33d2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nro41YWOLkg2JdoiX8vvQ.png"/></div></div></figure><p id="d631" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面的源代码或者访问<a class="ae ky" href="https://pastebin.com/BvR7gmDe" rel="noopener ugc nofollow" target="_blank">https://pastebin.com/BvR7gmDe</a>，干杯！</p><pre class="la lb lc ld gt lt lu lv lw aw lx bi"><span id="e66c" class="ly lz iq lu b gy ma mb l mc md">import time, os, requests, json<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import tweepy<br/>import pandas as pd<br/>from urllib.request import urlopen<br/>from bs4 import BeautifulSoup as bs<br/>from datetime import datetime, timezone<br/>from itertools import accumulate</span><span id="6407" class="ly lz iq lu b gy me mb l mc md">auth = tweepy.OAuthHandler('#############################', '######################################')<br/>auth.set_access_token('#################-#################', '###################################')<br/>api = tweepy.API(auth)</span><span id="d3ad" class="ly lz iq lu b gy me mb l mc md">def proposed_block_url():<br/>    url = '<a class="ae ky" href="https://explorer.dcrdata.org'" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org'</a><br/>    try:<br/>        page = urlopen(url)<br/>    except:<br/>        logic()<br/>    html = page.read().decode("utf-8")<br/>    soup = bs(html, "html.parser")<br/>    current_block = soup.find_all(class_="d-inline-block h3 position-relative")<br/>    current_block = [str(x) for x in str(current_block)]<br/>    first_smaller_than_index = current_block.index('&lt;')<br/>    current_block.pop(first_smaller_than_index)<br/>    s_t_index = current_block.index('&lt;')<br/>    b_t_index = current_block.index('&gt;')<br/>    proposed_block = current_block[b_t_index+1:s_t_index]<br/>    proposed_block = str(''.join(proposed_block))<br/>    return proposed_block</span><span id="c653" class="ly lz iq lu b gy me mb l mc md">def date_time(date):<br/>    date = str(''.join(date))<br/>    soup_main = bs(html_main, "html.parser")<br/>    mixed_val = soup_main.find_all(class_="int")<br/>    mixed_val = mixed_val[1]<br/>    mix_int = [str(x) for x in str(mixed_val)]<br/>    mix_int1 = mix_int[18:-7]<br/>    mix_int2 = str("".join(mix_int1))</span><span id="df6f" class="ly lz iq lu b gy me mb l mc md">total = soup_main.find_all(class_="int")<br/>    total = list(total)<br/>    total = total[0]<br/>    total = [str(x) for x in str(total)]<br/>    total.pop(0)<br/>    first_index = total.index('&gt;')<br/>    second_index = total.index('&lt;')<br/>    total = total[first_index + 1:second_index]<br/>    total_sent = str(''.join(total))<br/>    total_sent = total_sent.replace(',', '')<br/>    total_sent = round(float(total_sent))</span><span id="86fc" class="ly lz iq lu b gy me mb l mc md">block = soup_main.find_all(class_="h5 d-inline-block pl-2")<br/>    block = [str(x) for x in str(block)]<br/>    block = str(block[2:])<br/>    first_index = block.index('#')<br/>    second_index = block.index('&lt;')<br/>    block = block[first_index + 3:second_index - 1]<br/>    block = block[:-2]<br/>    block_nr = str(''.join(block))<br/>    block_nr  = block_nr.replace("'", '').replace(",", '').replace(" ", '')<br/>    block_nr = int(block_nr)<br/>    return mix_int2, date, total_sent, block_nr</span><span id="217d" class="ly lz iq lu b gy me mb l mc md">def mixed_date_time(route):<br/>    try:<br/>        page = urlopen(route)<br/>    except:<br/>        logic()<br/>    global html_main<br/>    html_main = page.read().decode("utf-8")<br/>    global soup_main<br/>    soup_main = bs(html_main, "html.parser")<br/>    date = soup_main.find_all(class_="fs18 font-weight-bold lh1rem d-inline-block pt-1")<br/>    date = [str(x) for x in str(date)]<br/>    date = date[-20:-8]<br/>    try:<br/>        if date[0] == '&gt;':<br/>            date.pop(0)<br/>        mix_int2 = date_time(date)[0]<br/>        total_sent = date_time(date)[2]<br/>        block_nr = date_time(date)[3]<br/>    except:<br/>        mix_int2 = date_time(date)[0]<br/>        total_sent = date_time(date)[2]<br/>        block_nr = date_time(date)[3]<br/>    date = date_time(date)[1]</span><span id="a132" class="ly lz iq lu b gy me mb l mc md">if len(mix_int2) &gt;= 5:<br/>        mix_int2 = mix_int2.replace(',','')<br/>    if mix_int2 != '0':<br/>        mixed_val2 = soup_main.find_all(class_="decimal")<br/>        mixed_val2 = mixed_val2[3]<br/>        mix_dec = [str(x) for x in str(mixed_val2)]<br/>        mix_dec = mix_dec[22:-7]<br/>        mix_dec = str("".join(mix_dec))<br/>        mixed_in_block = float(str(mix_int2) + '.' + str(mix_dec))<br/>    else:<br/>        zero = 0<br/>    try:<br/>        return mixed_in_block, total_sent, block_nr, date<br/>    except:<br/>        return zero, total_sent, block_nr, date</span><span id="c9f4" class="ly lz iq lu b gy me mb l mc md">def coin_supply():<br/>    url = '<a class="ae ky" href="https://explorer.dcrdata.org'" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org'</a><br/>    page = urlopen(url)<br/>    html = page.read().decode("utf-8")<br/>    soup = bs(html, "html.parser")<br/>    coin_supply = soup.get_text('Total Coin Supply')<br/>    coin_supply = coin_supply.split('\n')<br/>    coin_supply = coin_supply[-52]<br/>    coin_supply = coin_supply[17:27]<br/>    coin_supply = coin_supply.replace(',','')<br/>    coin_supply = float(coin_supply)<br/>    return coin_supply</span><span id="9736" class="ly lz iq lu b gy me mb l mc md">def usd_val(mixed_today):</span><span id="03b8" class="ly lz iq lu b gy me mb l mc md">url = '<a class="ae ky" href="https://explorer.dcrdata.org'" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org'</a><br/>    page = urlopen(url)<br/>    html = page.read().decode("utf-8")<br/>    soup = bs(html, "html.parser")<br/>    global usd_value<br/>    usd_value = soup.get_text('Exchange Rate')<br/>    usd_value = usd_value.split('\n')<br/>    usd_value = usd_value[-27]<br/>    usd_value = usd_value.lower()<br/>    usd_value = usd_value.split('e')<br/>    usd_value = float(usd_value[3])<br/>    global USD<br/>    USD = round((usd_value * mixed_today))</span><span id="616b" class="ly lz iq lu b gy me mb l mc md">def twitter_data():<br/>    api = tweepy.API(auth)<br/>    client_id = api.me()<br/>    x = api.user_timeline(id = client_id, count = 1, tweet_mode='extended')<br/>    tweet_body = [tweet.full_text for tweet in x]<br/>    tweet_body = tweet_body[0]<br/>    tweet_string = tweet_body.split(' ')<br/>    prev_time_stamp = tweet_string[0]<br/>    l_of_pts = prev_time_stamp.split('-')<br/>    global latest_tweet_date<br/>    latest_tweet_date = l_of_pts[2]<br/>    global current_tw_dt<br/>    current_tw_dt = datetime(int(l_of_pts[0]),int(l_of_pts[1]),int(l_of_pts[2])).timestamp()</span><span id="2478" class="ly lz iq lu b gy me mb l mc md">def current_utc_time():<br/>    now_utc = datetime.now(timezone.utc)<br/>    global now_utc_list<br/>    now_utc_list = str(now_utc)[11:16]<br/>    current_time_list = str(now_utc).split(' ')<br/>    cur_date = current_time_list[0]<br/>    l_of_d = cur_date.split('-')<br/>    current_timestamp = datetime(int(l_of_d[0]),int(l_of_d[1]),int(l_of_d[2])).timestamp()<br/>    return current_timestamp</span><span id="cc38" class="ly lz iq lu b gy me mb l mc md">def record_helper(test_var):<br/>    if test_var == True:<br/>        data_values.pop()<br/>    record_val = sorted(data_values)[-1]<br/>    record_val2 = [int(x) for x in str(record_val)]</span><span id="dc2f" class="ly lz iq lu b gy me mb l mc md">if len(str(record_val)) == 14:<br/>        record_val3 = record_val2[0:6]<br/>    elif len(str(record_val)) == 15:<br/>        record_val3 = record_val2[0:7]<br/>    elif len(str(record_val)) == 16:<br/>        record_val3 = record_val2[0:8]</span><span id="fbf7" class="ly lz iq lu b gy me mb l mc md">record_val4 = [str(i) for i in record_val3]<br/>    global record_value<br/>    record_value = int("".join(record_val4))<br/>    return record_value</span><span id="66dd" class="ly lz iq lu b gy me mb l mc md">def record_check():<br/>    url_2 = '<a class="ae ky" href="https://dcrdata.decred.org/api/chart/privacy-participation?axis=time&amp;bin=day'" rel="noopener ugc nofollow" target="_blank">https://dcrdata.decred.org/api/chart/privacy-participation?axis=time&amp;bin=day'</a><br/>    response = requests.get(url_2)<br/>    data = response.text<br/>    parsed = json.loads(data)<br/>    timestamp = parsed["t"]<br/>    global data_values<br/>    data_values = parsed["anonymitySet"]<br/>    test_time = current_utc_time() - 86400<br/>    ts_latest = timestamp[-1]<br/>    test_var = False<br/>    if ts_latest == test_time:<br/>        test_var = True<br/>        prev_rec = record_helper(test_var)<br/>    else:<br/>        prev_rec = record_helper(test_var)<br/>    return prev_rec</span><span id="52ca" class="ly lz iq lu b gy me mb l mc md">def staked_in_usd():<br/>    url = '<a class="ae ky" href="https://explorer.dcrdata.org'" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org'</a><br/>    page = urlopen(url)<br/>    html = page.read().decode("utf-8")<br/>    soup = bs(html, "html.parser")<br/>    stake = soup.find_all(class_="int")<br/>    stake = stake[5]<br/>    staked_list = [str(x) for x in str(stake)]<br/>    staked_list.pop(0)<br/>    staked_list.pop(-1)<br/>    stake_index1 = staked_list.index('&gt;')<br/>    stake_index2 = staked_list.index('&lt;')<br/>    staked_val = staked_list[stake_index1 +1:stake_index2]<br/>    staked_val = str("".join(staked_val))<br/>    staked_val = staked_val.replace(',','')<br/>    staked_usd = round(int(staked_val) * usd_value)<br/>    return staked_usd</span><span id="5a27" class="ly lz iq lu b gy me mb l mc md">def first_check():<br/>    weight = 1<br/>    while weight == 1:<br/>        current_utc_time()<br/>        if now_utc_list == '00:15':<br/>            weight = 2<br/>            logic()<br/>        else:<br/>            time.sleep(45)<br/>            print('First time-based conditional not yet satisfied..')</span><span id="2200" class="ly lz iq lu b gy me mb l mc md">def start_block1():<br/>    with open('blocks.csv', 'r') as f:<br/>        data = f.read()<br/>        data = data.split('\n')<br/>    return int(data[-2])</span><span id="cce3" class="ly lz iq lu b gy me mb l mc md">def logic():<br/>    mixed_list = []<br/>    url_block = []<br/>    list_of_300_urls = []<br/>    mixed_by_block = []<br/>    test_list = []<br/>    new_total = []<br/>    new_block = []<br/>    new_date = []<br/>    twitter_data()<br/>    time_value = current_utc_time() - current_tw_dt</span><span id="a058" class="ly lz iq lu b gy me mb l mc md">if time_value == 172800:<br/>        print('startup conditional satisfied')<br/>        proposed_block = proposed_block_url()<br/>        start_block = start_block1()<br/>        proposed_block = int(proposed_block) + 2</span><span id="bc98" class="ly lz iq lu b gy me mb l mc md">for i in range(start_block, proposed_block):<br/>            url_block.append(int(i) + 1)<br/>        print('determined block range')<br/>        for i in url_block:<br/>            list_of_300_urls.append('<a class="ae ky" href="https://explorer.dcrdata.org/block/'" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org/block/'</a> + str(i))<br/>        print('appended urls to list complete. Scraping urls...est 63min remaining if no errors occur..')<br/>        for route in list_of_300_urls:<br/>            mixed_by_block.append(list(mixed_date_time(route)))<br/>            time.sleep(12)<br/>        print('mixed by block data appended to list, wrangling data')<br/>        for i in mixed_by_block:<br/>            mixed_by_block = str(mixed_by_block).replace(',','').replace('[','')<br/>        mixed_by_block = mixed_by_block.split(']')<br/>        mixed_by_block.pop()<br/>        mixed_by_block.pop()</span><span id="13cc" class="ly lz iq lu b gy me mb l mc md">day_after_tweet = current_tw_dt + 86400<br/>        ts = datetime.fromtimestamp(day_after_tweet).strftime('%Y-%m-%d %I:%M:%S %p')<br/>        ts_list = ts.split(' ')<br/>        latest_date = ts_list[0]<br/>        day_of_latest_date = latest_date.split('-')<br/>        day_of_latest_data = day_of_latest_date[2]</span><span id="ffd4" class="ly lz iq lu b gy me mb l mc md">if day_of_latest_data[0] == '0':<br/>            day_of_latest_data = day_of_latest_data[1]<br/>        for i in mixed_by_block:<br/>            test_list.append(str(i).split(' '))<br/>        for i in test_list:<br/>            if i[-2] == day_of_latest_data:<br/>                mixed_list.append(i[-6])<br/>                new_block.append(i[-4])<br/>                new_total.append(i[-5])<br/>                new_date.append(i[-3:])</span><span id="48d9" class="ly lz iq lu b gy me mb l mc md">with open('dates.csv', 'a') as f:<br/>            for i in new_date:<br/>                f.write(str(i))<br/>                f.write("\n")</span><span id="86f4" class="ly lz iq lu b gy me mb l mc md">with open('mixed.csv', 'a') as f:<br/>            for i in mixed_list:<br/>                f.write(str(i))<br/>                f.write("\n")</span><span id="6d26" class="ly lz iq lu b gy me mb l mc md">with open('blocks.csv', 'a') as f:<br/>            for i in new_block:<br/>                f.write(str(i))<br/>                f.write("\n")</span><span id="527c" class="ly lz iq lu b gy me mb l mc md">with open('total.csv', 'a') as f:<br/>            for i in new_total:<br/>                f.write(str(i))<br/>                f.write("\n")</span><span id="0e15" class="ly lz iq lu b gy me mb l mc md">print('last conditional satisfied, preparing to broadcast data')<br/>        global mixed_today<br/>        mixed_today = 0<br/>        for i in mixed_list:<br/>            mixed_today += float(i)</span><span id="a514" class="ly lz iq lu b gy me mb l mc md">mixed_today = round(mixed_today)<br/>        usd_val(mixed_today)<br/>        mixed_by_supply = (mixed_today / coin_supply()) * 100<br/>        mixed_by_supply = round((mixed_by_supply), 2)<br/>        cspp_current = "{:,}".format(mixed_today)<br/>        usd_display = "{:,}".format(USD)<br/>        sup_display = "{:,}".format(mixed_by_supply)<br/>        staked_display = "{:,}".format(staked_in_usd())</span><span id="28eb" class="ly lz iq lu b gy me mb l mc md">url3 = '<a class="ae ky" href="https://explorer.dcrdata.org/market?chart=depth&amp;xc=aggregated&amp;bin=1h&amp;stack=1'" rel="noopener ugc nofollow" target="_blank">https://explorer.dcrdata.org/market?chart=depth&amp;xc=aggregated&amp;bin=1h&amp;stack=1'</a><br/>        page3 = urlopen(url3)<br/>        html3 = page3.read().decode("utf-8")<br/>        soup3 = bs(html3, "html.parser")<br/>        btc_value = soup3.find_all(class_="pl-3 fs16 py-2 text-right")<br/>        btc_sliced_val = list(btc_value[-1])<br/>        btc_sliced_val = [str(x) for x in str(btc_sliced_val)]<br/>        prior_index = btc_sliced_val.index('0')<br/>        post_index = prior_index + 7<br/>        btc_val = btc_sliced_val[prior_index:post_index]<br/>        btc_val_final = str(''.join(btc_val))</span><span id="5a03" class="ly lz iq lu b gy me mb l mc md">if mixed_today &gt; record_check():<br/>            daily = f"""{latest_date} CoinShuffle++ transaction volume was: {cspp_current} $DCR / {usd_display} $USD<br/>            {sup_display} % of Circulating Supply Mixed Yesterday *NEW ATH!! (1 DCR = {usd_value} USD / {btc_val_final} BTC)<br/>             Total staked in USD: {staked_display} #dcr $dcr #DCR #Decred #bitcoin #btc #DCRDEX"""<br/>        else:<br/>            daily = f"""{latest_date} CoinShuffle++ transaction volume was: {cspp_current} $DCR / {usd_display} $USD<br/>            {sup_display} % of Circulating Supply Mixed Yesterday (1 DCR = {usd_value} USD / {btc_val_final} BTC)<br/>             Total staked in USD: {staked_display} #dcr $dcr #DCR #Decred #bitcoin #btc #DCRDEX"""</span><span id="8a5b" class="ly lz iq lu b gy me mb l mc md">df_dates = pd.read_csv('dates.csv', delimiter = '\n')<br/>        df_mixed = pd.read_csv('mixed.csv', delimiter = '\n')<br/>        df_total = pd.read_csv('total.csv', delimiter = '\n')<br/>        df_total = df_total.div(1000000)</span><span id="e523" class="ly lz iq lu b gy me mb l mc md">df_mixed = df_mixed.astype(int)<br/>        df_mixed = df_mixed.div(1000000)<br/>        df_data = pd.concat([df_dates, df_mixed, df_total], axis=1)<br/>        df_agg = df_data.groupby(['Dates'], sort=False).sum()<br/>        df_agg = df_agg.tail(31)<br/>        df_agg = df_agg.reset_index()<br/>        df_agg['Dates'] = df_agg['Dates'].str.strip('[]')<br/>        df_agg['Dates'] = df_agg['Dates'].str.strip('""')<br/>        df_acc = list(accumulate(df_agg['Mixed']))</span><span id="4b1f" class="ly lz iq lu b gy me mb l mc md">m = df_agg['Mixed'].values.tolist()<br/>        t = df_agg['Total'].values.tolist()<br/>        d2 = df_agg['Dates'].values.tolist()<br/>        test_list2 = []<br/>        day = []<br/>        day2 = []<br/>        year = []<br/>        year2 = []<br/>        month = []<br/>        month2 = []</span><span id="bf67" class="ly lz iq lu b gy me mb l mc md">for i in d2:<br/>            test_list2.append(str(i).split(' '))<br/>        for i in test_list2:<br/>            day.append(i[1])<br/>            month.append(i[0])<br/>            year.append(i[2])<br/>        for i in month:<br/>            month2.append(i[1:-2])<br/>        for i in day:<br/>            day2.append(i[1:-2])<br/>        for i in year:<br/>            year2.append(i[1:-1])</span><span id="1092" class="ly lz iq lu b gy me mb l mc md">df_date = pd.DataFrame(columns=['day2', 'month2', 'year2'])<br/>        df_date['day2'], df_date['month2'], df_date['year2'] = day2, month2, year2<br/>        df_date = df_date.assign(Dates_new = df_date.day2.astype(str) + ' ' + df_date.month2.astype(str) + ' ' + df_date.year2.astype(str))<br/>        df_date.drop('month2', axis=1, inplace=True)<br/>        df_date.drop('year2', axis=1, inplace=True)<br/>        df_final = pd.concat([df_agg, df_date], axis=1)<br/>        df_final.drop('Dates', axis=1, inplace=True)<br/>        df_final['day2'] = df_final['day2'].astype(int)<br/>        print(df_final)</span><span id="5e2f" class="ly lz iq lu b gy me mb l mc md">dates_xaxis = df_final['day2'].values.tolist()<br/>        dates_new = df_final['Dates_new'].values.tolist()<br/>        f2 = dates_new[0]<br/>        l2 = dates_new[-1]<br/>        N = len(dates_new)<br/>        xloc = np.arange(N)<br/>        barWidth = 0.50<br/>        p1 = plt.bar(xloc, m, width=barWidth, color='dodgerblue', edgecolor='black')<br/>        p2 = plt.bar(xloc, t, bottom=m, width=barWidth, color='mediumspringgreen', edgecolor='black')<br/>        p3 = plt.plot(xloc, df_acc)<br/>        plt.ylabel('DCR (Million)')<br/>        plt.xlabel('Day of Month')<br/>        plt.xticks(rotation=60)<br/>        plt.xticks(np.arange(len(dates_xaxis)), dates_xaxis)<br/>        plt.title(f'{f2} - {l2}')<br/>        plt.grid(True)<br/>        plt.legend((p1[0], p2[0], p3[0]), ('mixed', 'normal', 'mixed(cumulative)'))<br/>        plt.savefig('graph2.png')<br/>        media = api.media_upload('graph2.png')</span><span id="afbf" class="ly lz iq lu b gy me mb l mc md">f = new_block[0]<br/>        l = new_block[-1]<br/>        print(f'data scraped by range {f} - {l}')</span><span id="2733" class="ly lz iq lu b gy me mb l mc md">api.update_status(status = daily, media_ids=[media.media_id])<br/>        os.remove("graph2.png")<br/>        print('data released.')</span><span id="55ba" class="ly lz iq lu b gy me mb l mc md">else:<br/>        pass</span><span id="4759" class="ly lz iq lu b gy me mb l mc md">if __name__ == '__main__':<br/>    first_check()</span></pre></div></div>    
</body>
</html>