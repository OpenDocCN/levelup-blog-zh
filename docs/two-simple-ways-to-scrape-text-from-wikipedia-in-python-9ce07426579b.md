# ç”¨ Python ä»ç»´åŸºç™¾ç§‘ä¸­æŠ“å–æ–‡æœ¬çš„ä¸¤ç§ç®€å•æ–¹æ³•

> åŸæ–‡ï¼š<https://levelup.gitconnected.com/two-simple-ways-to-scrape-text-from-wikipedia-in-python-9ce07426579b>

è¿™ç¯‡æ–‡ç« å°†é¦–å…ˆå±•ç¤ºä¸€ä¸ª**çš„ç®€å•æ–¹æ³•**ï¼Œç”¨å‡ è¡Œä»£ç ä»ç»´åŸºç™¾ç§‘ä¸­æŠ“å–æ–‡æœ¬ï¼Œç„¶åç”¨ä¸€ä¸ªæ›´**çš„é€šç”¨æ–¹æ³•**æå–ç›¸åŒçš„æ–‡æœ¬ï¼Œè¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ç½‘ç«™ã€‚

æˆ‘ä»¬å°†ä»â€œçº¦ç¿°Â·dÂ·äº¨ç‰¹â€çš„[ç»´åŸºç™¾ç§‘é¡µé¢ä¸ŠæŠ“å–æ–‡æœ¬å†…å®¹](https://en.wikipedia.org/wiki/John_D._Hunter)ğŸ’ä»–æ˜¯ Python å¼€å‘çš„ä¼ å¥‡å…ˆé©±ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ M [atplotlib](https://matplotlib.org/) çš„åˆ›é€ è€…ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¯è§†åŒ–è½¯ä»¶åŒ…ã€‚å¦‚æœä½ æƒ³æ›´å¤šåœ°äº†è§£ä»–å’Œä»–ä»ä¸­å¹´æ—¶æœŸå­¦åˆ°çš„ä¸œè¥¿ï¼Œçœ‹çœ‹è¿™ç¯‡æœ‰è§åœ°ã€æœ‰è§è¯†çš„[æ¼”è®²](https://www.youtube.com/watch?v=e3lTby5RI54)ã€‚å°½ç®¡è§†é¢‘è´¨é‡å¾ˆå·®ï¼Œæˆ‘è¿˜æ˜¯éå¸¸å–œæ¬¢çœ‹ä»–çš„æ¼”è®²ã€‚

![](img/c291e570fa8a18fddcae8ffec036d139.png)

æ¥è‡ª[çš„å•è¯äº‘â€œPython ä¸­çš„ç®€å•å•è¯äº‘â€](https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5)

# 0.Python è®¾ç½®ğŸ”§

æˆ‘å‡è®¾è¯»è€…(ğŸ‘€æ˜¯çš„ï¼Œä½ ï¼)å¯ä»¥è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœä½ æ˜¯ Python çš„æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚

æˆ‘å·²ç»åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨å¹¶æµ‹è¯•äº† Python 3.7.1 ä¸­çš„è„šæœ¬ã€‚

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰è®©æˆ‘ä»¬ç¡®å®šä½ å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“:
â—» **ç®€æ˜“æ–¹å¼:** *ç»´åŸºç™¾ç§‘ã€* â—» **é€šç”¨æ–¹å¼:** *urllib* å’Œ *BeautifulSoup*

# 1.ä½¿ç”¨ç»´åŸºç™¾ç§‘çš„ç®€å•æ–¹æ³•ğŸ°

ç»´åŸºç™¾ç§‘åŒ…æ—¨åœ¨ä½¿ä»ç»´åŸºç™¾ç§‘ä¸­æå–æ•°æ®å˜å¾—å®¹æ˜“å’Œç®€å•ï¼Œå®ƒç¡®å®åšåˆ°äº†è¿™ä¸€ç‚¹ã€‚ğŸ‘

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‡ è¡Œä»£ç ä»ç»´åŸºç™¾ç§‘é¡µé¢ä¸­æå–æ–‡æœ¬å†…å®¹:

```
# Import package
import wikipedia# Specify the title of the Wikipedia page
wiki = wikipedia.page('John D. Hunter')# Extract the plain text content of the page
text = wiki.content
text
```

![](img/5e8f48a6c7a973939a7d83c13d8d351d.png)

æ–‡æœ¬

Ta-daâ•æˆ‘ä»¬åˆšåˆšæå–äº†æ–‡æœ¬å†…å®¹ï¼çœ‹åˆ°æœ‰å¤šç®€å•äº†å—ï¼Ÿå¦‚æœæ‚¨åœ¨æœ€åä¸€è¡Œä½¿ç”¨`print(text)`è€Œä¸æ˜¯`text`ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°å®ƒè¢«å¾ˆå¥½åœ°æ ¼å¼åŒ–ä¸ºæ–°çš„è¡Œå’Œæ ‡é¢˜ã€‚

å‡è®¾æˆ‘ä»¬åªæƒ³ä¿ç•™æ¯ä¸ªæ®µè½çš„ä¸»ä½“ï¼Œè€Œä¸æƒ³ä¿ç•™å…¶ä»–å†…å®¹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»åšä¸€äº›æ¸…ç†:
â—¼ï¸åˆ é™¤ç”±â€œ==â€åŒ…å›´çš„æ ‡é¢˜:`re.sub(r'==.*?==+', '', text)` â—¼ï¸ç”¨â€œâ€(ä¸€ä¸ªç©ºå­—ç¬¦ä¸²)æ›¿æ¢â€œ\nâ€(ä¸€ä¸ªæ–°è¡Œ):`.replace('\n', '')` è¾“å‡º*æ–‡æœ¬*æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²(å¯ä»¥ä½¿ç”¨`type(text)`è¿›è¡Œæ£€æŸ¥)ï¼Œè¿™å…è®¸æˆ‘ä»¬åˆ©ç”¨å­—ç¬¦ä¸²æ–¹æ³•æˆ–ä»»ä½•å…¶ä»–å¯ä»¥åœ¨å­—ç¬¦ä¸²ä¸Šä½¿ç”¨çš„æ“ä½œã€‚

```
# Import package
import re# Clean text
text = re.sub(r'==.*?==+', '', text)
text = text.replace('\n', '')
text
```

![](img/db3f5c8cf3761ac5c48239d9e79cb3f9.png)

å°±è¿™æ ·ï¼Œæˆ‘ä»¬åˆšåˆšä»ç»´åŸºç™¾ç§‘ä¸Šæœé›†äº†ä¸€äº›æ–‡æœ¬ã€‚ç®€å•çš„æ–¹æ³•ï¼Œæ»´ç­”ã€‚âœ…

*å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºè¿™ä¸ªåŒ…çš„ä¿¡æ¯ï¼Œè¿™é‡Œçš„*[](https://wikipedia.readthedocs.io/en/latest/code.html#api)**å°±æ˜¯æ–‡æ¡£ã€‚**

# *2.ä½¿ç”¨ urllib å’Œ BeautifulSoup çš„ä¸€èˆ¬æ–¹æ³•ğŸœ*

*è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ç§æ›´é€šç”¨çš„æ–¹æ³•æ¥æå–ç›¸åŒçš„æ•°æ®ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥ç”¨äºå…¶ä»–ç½‘ç«™ã€‚*

```
*# Import packages
from urllib.request import urlopen
from bs4 import BeautifulSoup# Specify url of the web page
source = urlopen('[https://en.wikipedia.org/wiki/John_D._Hunter').read()](https://en.wikipedia.org/wiki/John_D._Hunter').read())# Make a soup 
soup = BeautifulSoup(source,'lxml')
soup*
```

*![](img/b793430a5c37d53472f8568f1f1d3965.png)*

*æ±¤çš„æ ·æœ¬æå–ç‰©*

**soup* çš„è¿™ç§çœ‹èµ·æ¥åƒèƒ¡è¨€ä¹±è¯­çš„è¾“å‡ºæœ‰ä»€ä¹ˆæ„ä¹‰å—ï¼Ÿè®©æˆ‘ä»¬ä¸€èµ·è¯•ç€ç†è§£å®ƒã€‚*

**æ±¤*åŒ…å«é¡µé¢ä¸Šçš„æ‰€æœ‰å†…å®¹ï¼Œç”±æ›´å°çš„å…ƒç´ ç»„æˆï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æ˜¯æˆ‘ä»¬ä¸éœ€è¦çš„(ä¾‹å¦‚è¡¨æ ¼ã€å¼•ç”¨)ã€‚ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾å‡º*æ±¤*ä¸­æœ‰å“ªç»„å…ƒç´ :*

```
*print(set([text.parent.name for text in soup.find_all(text=True)]))*
```

*![](img/ce2ef385cc60d4d13a15ee4908716f0c.png)*

*å…ƒç´ *

*åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯é‚£äº›è¢«æ ‡è®°ä¸º *< p >* çš„æ®µè½ï¼Œå®ƒä»£è¡¨*æ®µè½ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬æ‰¾åˆ°æ‰€æœ‰æ®µè½:**

**Psstï¼Œå¦‚æœä½ å¥½å¥‡æƒ³äº†è§£æ›´å¤šå…³äºå…¶ä»–å…ƒç´ çš„ä¿¡æ¯ï¼Œè¯•ç€ç”¨é›†åˆä¸­çš„å¦ä¸€ä¸ªå…ƒç´ æ¥æ”¹å˜â€˜pâ€™(å‚è€ƒä¸Šé¢çš„å…ƒç´ é›†åˆ)ã€‚**

```
*# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text

text*
```

*![](img/0d76f55a10036cec03cc32feb1d396f5.png)*

*æ–‡æœ¬*

*æˆ‘ä»¬å¿«åˆ°äº†ã€‚è®©æˆ‘ä»¬åšä¸€ç‚¹æ¸…ç†ï¼Œä»¥è·å¾—å‰ä¸€éƒ¨åˆ†çš„å‡†ç¡®è¾“å‡º:
â—¼ï¸åˆ é™¤æ‹¬å·ä¸­çš„è„šæ³¨ä¸Šæ ‡:`re.sub(r'\[.*?\]+', '', text)` â—¼ï¸ç”¨' '(ç©ºå­—ç¬¦ä¸²)æ›¿æ¢' \n '(æ–°è¡Œ):`.replace('\n', '')`*

```
*# Import package
import re# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text = text.replace('\n', '')
text*
```

*Yayâ•:æˆ‘ä»¬ç”¨ä¸€ç§æ›´é€šç”¨çš„æ–¹æ³•æå–äº†å®Œå…¨ç›¸åŒçš„æ–‡æœ¬ã€‚ä¸€èˆ¬æ–¹å¼ï¼Œæ‰“å‹¾ã€‚âœ…*

# *3.æœ€ç»ˆä»£ç ğŸ“ƒ*

*è¿™ä¸ªæœ€ç»ˆçš„ä»£ç æ˜¯ä¸€ä¸ªç•¥æœ‰ä¸åŒçš„ç‰ˆæœ¬ï¼Œå…¶ä¸­è„šæœ¬è¢«è°ƒæ•´ä¸ºä¿ç•™æ®µè½æ ‡é¢˜ï¼Œå› æ­¤è¿™ä¸¤ç§æ–¹æ³•éƒ½æœ‰ä¸¤ä¸ªå¯ç”¨çš„ç¤ºä¾‹å¯ä¾›å‚è€ƒã€‚è¿™å¯èƒ½æ¯”æ•´ç†å‰é¢ç« èŠ‚ä¸­å·²ç»æ˜¾ç¤ºçš„ä»£ç æ›´æœ‰ç”¨ã€‚*

*ç»“æŸä¹‹å‰çš„æœ€åä¸€ç‚¹:å¦‚æœæ‚¨æƒ³çŸ¥é“å¦‚ä½•åœ¨ gist çš„ç¬¬ 38 è¡Œä¸­æ‰¾åˆ°æ­£ç¡®çš„åç§°å’Œå±æ€§ï¼Œæˆ‘æ˜¯è¿™æ ·åšçš„:
1)æˆ‘è¿è¡Œä¸‹é¢çš„è„šæœ¬å¹¶æ»šåŠ¨è¾“å‡ºï¼Œç›´åˆ°æˆ‘æ‰¾åˆ°ä¸€äº›æ®µè½æ ‡é¢˜çš„æ–‡æœ¬ï¼Œæ¯”å¦‚è¯´ä¼ è®°ï¼Œå¹¶å‘ç°å®ƒä»¬çš„çˆ¶åç§°æ˜¯`span`ã€‚è¯·è®°ä½å¯»æ‰¾ä¼ è®°çš„ç¬¬äºŒä¸ªå®ä¾‹ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªå®ä¾‹å°†ç”¨äºå†…å®¹è¡¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒä»¬éƒ½æœ‰ç›¸åŒçš„æ ‡ç­¾ã€‚*

```
*for s in soup.find_all(text=True):
    # Check out the parent name
    print(f'Parent name: {s.parent.name}')
    # Check the text
    print(s)*
```

*![](img/6b9aeed97fe4c2d509c1c196f4f70534.png)*

*2)æˆ‘ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬ç ”ç©¶äº†æ‰€æœ‰çš„æ ‡ç­¾åŠå…¶ç»†èŠ‚ã€‚æˆ‘å†æ¬¡æ»šåŠ¨è¾“å‡ºï¼Œæ‰¾åˆ°ä¸€äº›æ®µè½æ ‡é¢˜çš„æ–‡æœ¬ï¼Œæ¯”å¦‚ä¼ è®°(è®°å¾—å‚è€ƒç¬¬äºŒä¸ªå®ä¾‹)ï¼Œå¹¶å‘ç°æ ‡é¢˜æœ‰`class="mw-editsection"`ã€‚*

```
*for s in soup.find_all('span'):
    # Check out the parent name
    print(f'Parent name: {s.parent.name}')
    # Check the text
    print(s)* 
```

*![](img/bfa9ec86f1a8c6e2a59b9760acfd66fc.png)*

*è¿™æ˜¯ä¸€ä¸ªå°è¯•å’Œé”™è¯¯ï¼Œä½†æˆ‘å¾ˆé«˜å…´è¿™æ ·åšã€‚æˆ‘å¸Œæœ›ä½ ä¹Ÿæ˜¯ï¼*

*![](img/8e10ac0252cc796d1069f8df8c9f3238.png)*

*ç…§ç‰‡ç”±[å¤šæ¢…å°¼ç§‘Â·æ´›äºš](https://unsplash.com/@domenicoloia?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) æ‹æ‘„*

**æ‚¨æƒ³è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœæ‚¨ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership) ï¼Œ*æˆä¸ºä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šè´¹å°†ç›´æ¥ç”¨äºæ”¯æŒæˆ‘ã€‚**

*è°¢è°¢ä½ çœ‹æˆ‘çš„å¸–å­ã€‚æˆ‘å¸Œæœ›ä½ å·²ç»å­¦åˆ°äº†ä¸€äº›ä¸œè¥¿ï¼Œâœ‚ï¸.å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œè¿™é‡Œæœ‰æˆ‘çš„å…¶ä»–å¸–å­çš„é“¾æ¥:
â—¼ï¸[Python ä¸­çš„ç®€å•å•è¯äº‘](https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5)
â—¼ï¸ï¸[NLP ç®€ä»‹-ç¬¬ 1 éƒ¨åˆ†:python ä¸­çš„é¢„å¤„ç†æ–‡æœ¬](https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)
â—¼ï¸[NLP ç®€ä»‹-ç¬¬ 2 éƒ¨åˆ†:è¯æ¡æ»¡è¶³å’Œè¯å¹²çš„åŒºåˆ«](https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)
â—¼ï¸[NLP ç®€ä»‹-ç¬¬ 3 éƒ¨åˆ†:TF-IDF è§£é‡Š](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)
â—¼ï¸[NLP ç®€ä»‹-ç¬¬ 4 éƒ¨åˆ†:ç›‘ç£](https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267)*

*å†è§ğŸƒğŸ’¨*