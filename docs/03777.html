<html>
<head>
<title>Tensor Operations — Basic Building Blocks of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量运算——深度学习的基本构件</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/tensor-operations-basic-building-blocks-of-deep-learning-244e11dce40e?source=collection_archive---------20-----------------------#2020-05-26">https://levelup.gitconnected.com/tensor-operations-basic-building-blocks-of-deep-learning-244e11dce40e?source=collection_archive---------20-----------------------#2020-05-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="56ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Pytorch和使用Pytorch的核心矩阵运算简介</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/16009b74fb7ecbe3e65d4e5cdb385741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VRxHiMsdhtFgr1PN.jpg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">2012 —当前:深度学习爆炸的时代</figcaption></figure><p id="0b90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi lb translated">在2012年的<a class="ae lk" href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/" rel="noopener ugc nofollow" target="_blank"> Imagenet </a>运动之后，深度学习取得了突飞猛进的发展。深度学习现在已经成为我们日常生活的一部分，当我们与语音助手交谈、使用家庭自动化系统、写电子邮件等时，许多算法都在运行。事实上，它的影响如此之大，以至于我们可以在:D的<a class="ae lk" href="https://www.amazon.com/Neural-Networks-Babies-Baby-University/dp/1492671207" rel="noopener ugc nofollow" target="_blank">亚马逊</a>上看到名为《婴儿神经网络》的书籍</p><h1 id="3d6f" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">“深度”学习是如何发生的？</strong></h1><p id="1a1a" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">从本质上来说，深度学习只不过是人脑如何工作的微缩版(忽略了我们人脑中存在的实际复杂性，这仍然非常非常难以复制)。计算机使用数百甚至数千个跨深层的神经元连接从其输入和输出中学习(因此有了“深度学习”一词)。这些神经元连接中的每一个都有不同的权重。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/575fe2f1f6158649a8697850f2e8c052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*0pMTHdXBhJMtNvYg"/></div></figure><p id="f9f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">权重在每次迭代中得到优化，使得预测的损失最小化，并且以最高的精度预测输出。计算机是机器，而机器只理解数字。因此，在地面上，我们讨论的所有这些权重都是n维矩阵或<strong class="jp ir">张量。</strong></p><p id="745a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于每个权重是n维矩阵或张量，因此权重的学习和优化涉及数百万次矩阵乘法。在过去的6-7年中，我们已经看到许多简化这项任务的DL框架出现。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mp"><img src="../Images/5a7a7a4401085032beb4c058d445cd03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ne9UmvoZ__0lUENP.jpg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">五大深度学习框架(Pytorch、Tensorflow、Keras、CNTK、Caffe)</figcaption></figure><h1 id="b1e0" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">py torch是什么？</strong></h1><p id="1fae" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">Pytorch是一个流行的深度学习框架，由脸书人工智能研究所(FAIR)开发和维护，用于处理张量。自2016年1.0.0版本发布以来，由于使用简单灵活，它越来越受欢迎。在本文中，我们将主要关注使用Pytorch的一些核心张量运算。你可能想浏览这个<a class="ae lk" href="https://medium.com/@bryant.kou/how-to-install-pytorch-on-windows-step-by-step-cc4d004adb2a" rel="noopener">博客</a>来获得关于如何安装PyTorch的详细说明。</p><ol class=""><li id="07a4" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk mv mw mx my bi translated">火炬.尺寸</li><li id="7a44" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">火炬</li><li id="cb01" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">torch.cat</li><li id="f46a" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">火炬. mul</li><li id="0aff" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">火炬.反转</li></ol></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><h1 id="bcc8" class="ll lm iq bd ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me np mg mh mi bi translated"><strong class="ak"> 1。Torch.size </strong></h1><p id="b421" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">Torch.size返回任何输入张量的维数。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="5a79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的例子中，我创建了一个3X2X4张量和torch.size返回3个维度。我们的张量在外括号中有3个元素，每个元素是一个矩阵。每个矩阵又有2个元素，每个元素是一个包含4个元素的列表。</p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><h1 id="2fbb" class="ll lm iq bd ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me np mg mh mi bi translated"><strong class="ak"> 2。Torch.mm </strong></h1><p id="414f" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">torch.mm返回任意两个输入矩阵mat1和mat2的矩阵乘法(非元素方式)</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="16d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的例子中，mat1和mat2的大小都是3X3。因此torch.mm的输出大小也是3X3。我们可以看到可以使用' @ '操作符代替torch.mm来执行相同的操作</p><p id="b417" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ns">使用torch时的注意事项. mm() </em></p><ul class=""><li id="8867" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk nt mw mx my bi translated">第一个输入矩阵的列大小应该等于第二个输入矩阵的行大小</li><li id="6837" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">对于不是矩阵或者维数大于2的张量，torch.mm不起作用，我们可以使用torch.mul进行逐元素乘法</li><li id="aedb" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">@ '运算符执行与torch.mm相同的操作</li></ul></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><h1 id="9b5f" class="ll lm iq bd ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me np mg mh mi bi translated"><strong class="ak"> 3。Torch.cat </strong></h1><p id="d72d" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">torch.cat可以水平或垂直连接两个张量</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">我们可以看到张量y已经堆叠在张量x下面了。</figcaption></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="751f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用dim参数并将其设置为1，我们可以水平连接2个张量。默认设置为0，表示垂直连接。</p><p id="810d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ns">使用torch.cat时的注意事项</em></p><ul class=""><li id="57df" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk nt mw mx my bi translated">使用dim = 1水平连接张量</li><li id="df8b" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">可以连接任意数量的张量，但是，要确保连接方向上的张量大小应该相同</li></ul></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><h1 id="affb" class="ll lm iq bd ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me np mg mh mi bi translated"><strong class="ak"> 4。Torch.mul </strong></h1><p id="ead8" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">torch.mul在两个张量之间执行元素级乘法</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="8cb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，对任何维数的张量都可以进行逐元素乘法。应用torch.mul的另一种方法是使用*运算符和a.mul(b ),其中a和b是输入张量</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="4f88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ns">使用torch.mul时的注意事项</em></p><ul class=""><li id="7d82" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk nt mw mx my bi translated">torch.mul类似于两个向量之间的点积。</li><li id="53f8" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">* '运算符或a.mul(b)也执行与torch.mm相同的操作</li><li id="ecd2" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">输入张量应满足广播条件</li></ul></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><h1 id="674e" class="ll lm iq bd ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me np mg mh mi bi translated"><strong class="ak"> 5。Torch.inverse </strong></h1><p id="44e4" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">torch.inverse计算任意张量的倒数</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="6d74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，我们使用randn函数创建了一个张量，用随机数填充4X4张量。然后torch.inverse计算X的逆，Inv(x) @ X给我们一个单位矩阵</p><p id="9efd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ns">使用火炬时的注意事项。反转:</em></p><ul class=""><li id="fcf5" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk nt mw mx my bi translated">torch.inverse只有在输入张量在所有维度上具有相同数量的元素时才能正常工作。</li><li id="972d" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">torch.inverse也可以应用于3d张量。然而，张量应该在所有方向上有相同的维数，或者它应该是一个平方张量</li></ul><h1 id="31a4" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">结论</h1><p id="7922" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">因此，在本文中，我介绍了以下内容:</p><ul class=""><li id="ce12" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk nt mw mx my bi translated">什么是深度学习？</li><li id="c666" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">Pytorch是什么？</li><li id="55e8" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">有哪些重要的张量运算？</li><li id="0736" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">如何实现执行上述张量运算的核心函数</li><li id="adea" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">使用这些功能时有哪些典型的注意点？</li></ul><h1 id="bcf7" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">参考链接</h1><ul class=""><li id="06b6" class="mq mr iq jp b jq mj ju mk jy nu kc nv kg nw kk nt mw mx my bi translated">torch.tensor的官方文档:<a class="ae lk" href="https://jovian.ml/outlink?url=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftensors.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/tensors.html</a></li><li id="a04b" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">Pytorch从无到有简介:<a class="ae lk" href="https://jovian.ml/outlink?url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2019%2F09%2Fintroduction-to-pytorch-from-scratch%2F" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/09/introduction-to-py torch-从无到有/ </a></li><li id="63c5" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk nt mw mx my bi translated">如何安装py torch:<a class="ae lk" href="https://medium.com/@bryant.kou/how-to-install-pytorch-on-windows-step-by-step-cc4d004adb2a" rel="noopener">https://medium . com/@ Bryant . kou/how-to-install-py torch-on-windows-step-by-step-cc 4d 004 ADB 2 a</a></li></ul></div></div>    
</body>
</html>