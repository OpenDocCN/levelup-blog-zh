<html>
<head>
<title>4 Advanced Spark Tips For Faster Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">4个先进的火花尖端，性能更快</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/4-advanced-apache-spark-tips-for-faster-performance-dd0a1cc829aa?source=collection_archive---------4-----------------------#2020-10-21">https://levelup.gitconnected.com/4-advanced-apache-spark-tips-for-faster-performance-dd0a1cc829aa?source=collection_archive---------4-----------------------#2020-10-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/3f696494afd19791646888b0368e5e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_62IajCWBVoSAD9DbK43Q.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Marc-Olivier Jodoin 在<a class="ae kf" href="https://unsplash.com/s/photos/fast?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="021d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Apache Spark是最流行的数据处理分布式框架，包括批处理和流处理。它是由来自300多家公司的众多开发人员构建的。自2009年以来，已有超过1200名开发者为Spark做出了贡献。它在许多组织中用于处理大型数据集。</p><p id="215c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深入了解Apache Spark可以帮助您解决Spark中的性能问题。这里有5个提升Spark性能的技巧</p><h1 id="a392" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">分组集合</h1><p id="bb72" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我在网上很少看到这方面的文档，但它对提高Spark的性能非常有用。让我们看看这是怎么回事。</p><p id="d8f1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设您有一个包含列的数据集</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="6cf4" class="mq lf it mm b gy mr ms l mt mu">ID, Age, Gender, Role, Salary</span></pre><p id="00f0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你需要创建这些集合</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="57d7" class="mq lf it mm b gy mr ms l mt mu">ID, Age, SUM(Salary)<br/>Age, Gender, Role, SUM(Salary)<br/>Role, SUM(Salary)</span></pre><p id="1d59" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一种方法是为每个数据框创建单独的数据框，然后合并最终的数据框。在执行union时，您还必须填充虚拟列。</p><p id="793d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">分组设置</strong>解决了这个问题。让我们看看如何</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="52f1" class="mq lf it mm b gy mr ms l mt mu">spark.sql("""<br/>SELECT grouping_id(),ID, Age, Gender, Role, sum(Salary) as salary<br/>FROM GrpSetDemo<br/>GROUP BY ID, Age, Gender, Role<br/>GROUPING SETS (<br/>  (ID, Age), <br/>  (Age, Gender, Role), <br/>  (Role)<br/>)<br/>order by grouping_id()<br/>""")</span></pre><p id="0ab4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是输出的样子</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="96f8" class="mq lf it mm b gy mr ms l mt mu">+-------------+----+----+------+----+------+<br/>|grouping_id()|ID  |Age |Gender|Role|salary|<br/>+-------------+----+----+------+----+------+<br/>|3            |3   |41  |null  |null|5200  |<br/>|3            |3   |34  |null  |null|6000  |<br/>|3            |3   |12  |null  |null|3500  |<br/>|3            |1   |12  |null  |null|5000  |<br/>...<br/>|3            |1   |22  |null  |null|3900  |<br/>|3            |3   |32  |null  |null|4200  |<br/>|8            |null|32  |M     |Tech|4200  |<br/>|8            |null|12  |F     |Tech|3500  |<br/>...<br/>|14           |null|null|null  |Tech|47100 |</span></pre><p id="bb0f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以看到它是如何为我们请求的每个组合创建一个独特的组的。<strong class="ki iu"> Grouping_id </strong>()是一种识别每个组合的方式。如果您不知道生成了grouping_id，也没关系。我会解释的</p><p id="7e4c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看表中select语句请求的列。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="a169" class="mq lf it mm b gy mr ms l mt mu">ID, Age, Gender, Role</span></pre><p id="d2ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果该列存在于组中，我们就给它赋值0，如果它不存在，我们就给它赋值1，这样这些组看起来就像这样</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="9fda" class="mq lf it mm b gy mr ms l mt mu">ID, Age, SUM(Salary) --&gt; 0011 (ID, Age, Gender, Role is the master)<br/>Age, Gender, Role, SUM(Salary) --&gt; 1000<br/>Role, SUM(Salary) --&gt; 1110</span></pre><p id="dd0d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">十进制表示是分组集合。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="8d44" class="mq lf it mm b gy mr ms l mt mu">0011 --&gt; 3<br/>1000 --&gt; 8<br/>1110 --&gt; 14</span></pre><p id="44b6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是使用分组集的优点。</p><p id="3cfa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">→ <strong class="ki iu">性能</strong> : Spark使用多种优化来运行分组集，比如从基本数据中访问数据等。</p><p id="5d90" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">→ <strong class="ki iu">干净的代码</strong>:编写多重并集不是一种干净的编码方式。</p><p id="3f9b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你还有任何疑问，请在评论区留下你的问题。我会回答他们。</p><h1 id="c3ba" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Scala / Java中的用户自定义函数</h1><p id="a8a0" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">使用UDF，您可以创建自己的转换来管理数据。虽然我们可以用Scala、Java或Python编写UDF，但建议用Scala或Java编写UDF。我来解释一下原因。</p><p id="28c7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果函数是用Python编写的，Spark会在worker上启动一个Python进程，将所有数据序列化为Python可以理解的格式，在Python进程中对这些数据逐行执行函数，最后将行操作的结果返回给JVM和Spark。如果这个函数是用Scala或Java编写的，那么您可以在Java虚拟机(JVM)中使用它</p><p id="837b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请参见下面的图示</p><figure class="mh mi mj mk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/1814a2011555ebe71ed027315e0597ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGDVCEJkjnypAKq3CtPT0w.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</figcaption></figure><p id="4d13" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">启动这个Python过程代价很高，但是真正的代价是将数据序列化为Python。这是昂贵的，原因有二</p><p id="02a8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">→这是一项昂贵的计算</p><p id="c94e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">→数据进入Python后，Spark无法管理worker的内存。如果资源受限，这可能会导致工作线程失败，因为JVM和Python都在同一台机器上竞争内存</p><p id="66d5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我还做了一个测试来确认这个性能问题。我在10次迭代中运行了简单和复杂两种类型的查询，并生成了这个测试结果。</p><figure class="mh mi mj mk gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/64b69ddef13d3841b9d7477f8e2fc5c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*FTbA6XMs0rLRItTwGoyHbQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">作者收集的统计数据</figcaption></figure><p id="b687" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们清楚地看到了Scala UDF和Python UDF之间的区别。自己尝试一下，留下你的回答。</p><h1 id="f1bc" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">分区和存储桶</h1><p id="5e83" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki iu">数据的分区</strong>指的是根据一个关键字将文件存储在单独的目录中，比如数据中的日期字段。像Hive这样的元数据管理器支持这个概念，Spark的许多内置数据源也是如此。当Spark只需要具有特定范围的键的数据时，对数据进行正确的分区可以让它跳过许多不相关的文件。让我们看一个例子</p><p id="328f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">不分区数据</strong></p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="ff25" class="mq lf it mm b gy mr ms l mt mu">val rawData= spark.read.parquet("&lt;location&gt;/part-0000*")<br/>rawData.filter(<em class="mx">col</em>("event_type") === 22).count()</span><span id="2c9a" class="mq lf it mm b gy my ms l mt mu">res5: Long = 168</span></pre><p id="49e5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看Spark UI是什么样子的</p><figure class="mh mi mj mk gt ju gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ad1de4ece8e9e07be4de835c6e991f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*9Qxybztipm72QIC8uHv0Mw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Spark UI —作者截图</figcaption></figure><p id="558e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">分割数据后</strong></p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="12be" class="mq lf it mm b gy mr ms l mt mu">val partionedData = rawData.write.partitionBy("event_type").parquet("&lt;locationOP&gt;")<br/><br/>val PartitionData = spark.read.parquet("&lt;locationOP&gt;")<br/>PartitionData.filter(<em class="mx">col</em>("event_type") === 22).count()</span><span id="884f" class="mq lf it mm b gy my ms l mt mu">res5: Long = 168</span></pre><figure class="mh mi mj mk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/d1905457a743b6d7e236dd489c8b87d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*LUh5tL4-hcCz0KPJweqZoA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Spark UI —作者截图</figcaption></figure><p id="6528" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两种情况下的输出是相同的。然而，在第二个例子中，Spark读取的数据较少(读取了168行，而第一个例子中读取了452K)，因此性能要快得多。</p><p id="93c6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以用<em class="mx">得到类似的结果。桶比</em></p><p id="5f4d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">分桶</strong>是一种文件组织方法，通过这种方法，您可以控制专门写入每个文件的数据。这有助于避免以后读取数据时的混乱，因为具有相同存储桶id的数据将全部被分组到一个物理分区中。</p><p id="1c6f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的示例中，我们为event_type列创建了6个存储桶。性能结果与上面类似。仅提取相关数据。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="6900" class="mq lf it mm b gy mr ms l mt mu">val bucketData = rawData.write.bucketBy('6',"event_type").saveAsTable("bucketFiles")<br/>spark.sql("select count(1) from bucketFiles where event_type == 22").show(10,false)</span></pre><p id="51cf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，只有Spark管理的表才支持分桶。</p><p id="86d4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是关于使用Spark UI的更多信息</p><div class="nb nc gp gr nd ne"><a href="https://medium.com/swlh/spark-ui-to-debug-queries-3ba43279efee" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">如何仅使用Spark UI调试查询</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">您已经有了调试查询所需的东西</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">medium.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns jz ne"/></div></div></a></div><h1 id="2f39" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">静态数据— Apache拼花地板</h1><p id="1395" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">通常情况下，当您写入和保存数据时，它会被多次读取，以便进行各种分析。确保您存储的数据能够有效读取，这对于成功的大数据项目是绝对必要的。</p><p id="dcd0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这包括选择您的存储系统、选择您的数据格式，以及利用某些存储格式中的数据分区等功能。</p><p id="1f14" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> Apache parquet </strong>是数据的最佳存储格式之一。Parquet是Hadoop生态系统中的一种开源文件格式。它是一种扁平的列存储格式，在存储和查询方面都具有很高的性能。</p><p id="7be8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些是拼花地板中的并行单元</p><ul class=""><li id="a58d" class="nt nu it ki b kj kk kn ko kr nv kv nw kz nx ld ny nz oa ob bi translated">MapReduce —文件/行组</li><li id="c0b5" class="nt nu it ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">IO —列块</li><li id="5812" class="nt nu it ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">编码/压缩—第页</li></ul><p id="e6f0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有这些都有助于优化对parquet文件的查询。拼花文件有一个页脚，用于保存文件中的数据信息。这有助于减少查询时读取的数据。如果一个目录中有100个文件，每个文件的页脚包含每个文件包含的内容的信息。这有助于过滤所需的数据。</p><p id="2174" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以使用<strong class="ki iu">拼花工具</strong>访问页脚信息</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="89e3" class="mq lf it mm b gy mr ms l mt mu"><strong class="mm iu">parquet-tools</strong> <strong class="mm iu">meta</strong> hdfs://host:port/&lt;location&gt;/file1<br/>creator:              parquet-mr version 1.10.0 (build abc)row group 1:          RC:12527 TS:179580--------------------------------------------------------------------<br/>country: BINARY SNAPPY DO:0 FPO:4 SZ:5240/5373/1.03 VC:12527 ENC:PLAIN_DICTIONARY,BIT_PACKED,RLE ST:[min: Afghanistan, max: Zimbabwe, num_nulls: 1307]</span></pre><p id="fb99" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在另一篇文章中详细介绍了拼花文件的内部原理。</p><div class="nb nc gp gr nd ne"><a href="https://medium.com/swlh/insights-into-parquet-storage-ac7e46b94ffe" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">对拼花储物的洞察</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">从事大数据工作的大多数人都会听说过parquet，以及它是如何针对存储等进行优化的。在这里我将…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">medium.com</p></div></div><div class="nn l"><div class="oh l np nq nr nn ns jz ne"/></div></div></a></div><p id="4b5e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在我的个人工作中使用了以上所有方法来提高绩效。成为Spark和数据工程的冠军。</p><p id="3ebf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您对索引的基础知识以及Druid如何使用它感兴趣，请查看我最近的文章</p><div class="nb nc gp gr nd ne"><a rel="noopener  ugc nofollow" target="_blank" href="/insights-into-indexing-using-bitmap-index-c28a3db1ad97"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">使用位图索引进行索引的见解</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">以及Apache Druid如何使用它进行超快速分析</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="nn l"><div class="oi l np nq nr nn ns jz ne"/></div></div></a></div></div></div>    
</body>
</html>