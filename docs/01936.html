<html>
<head>
<title>Setting Up Airflow Using Celery Executors in Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Docker中使用芹菜执行器设置气流</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/etting-up-airflow-using-celery-executors-in-docker-8fcb94407ccc?source=collection_archive---------7-----------------------#2020-02-07">https://levelup.gitconnected.com/etting-up-airflow-using-celery-executors-in-docker-8fcb94407ccc?source=collection_archive---------7-----------------------#2020-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/cddd08c459ff2685039c83065ce9eb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Us5uS0BrDfD_wmFl.png"/></div></div></figure><p id="d283" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我最近接到一个任务，要建立一个阿帕奇气流的概念验证。</p><h1 id="4db2" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">什么是阿帕奇气流？</h1><p id="afe0" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">Apache Airflow是一个开源工具，用于编排复杂的计算工作流和数据处理管道。气流工作流被设计为有向无环图(DAG)。这意味着，当创作一个工作流时，你应该考虑如何将它分成可以独立执行的任务。</p><h1 id="4658" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">我的目标是学什么？</h1><p id="48a5" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">概念验证的目的是找出是否可以实现:</p><ul class=""><li id="f003" class="mc md it kd b ke kf ki kj km me kq mf ku mg ky mh mi mj mk bi translated">高度可用</li><li id="9ef3" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">可恢复的</li><li id="6414" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">可攀登的</li><li id="b5b3" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">能够从不同的AWS帐户推送和提取数据，因此能够在一个角色下运行</li></ul><p id="ab6a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我需要在集装箱环境中设置气流，以便能够勾掉一些点。这相对简单，虽然官方的airflow docker映像不是最容易设置的，但我最终还是建立了自己的docker文件，使用<a class="ae mq" href="https://github.com/puckel/docker-airflow" rel="noopener ugc nofollow" target="_blank">https://github.com/puckel/docker-airflow</a>作为指导方针。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="1518" class="na la it mw b gy nb nc l nd ne">FROM python:3.7-slim-stretch</span><span id="854c" class="na la it mw b gy nf nc l nd ne">ENV DEBIAN_FRONTEND noninteractive<br/>RUN echo 1 &gt; /dev/null<br/>RUN apt-get update -yqq &amp;&amp; apt-get upgrade -yqq &amp;&amp; \<br/>    apt-get install -yqq apt-utils &amp;&amp; \<br/>    apt-get install -yqq --no-install-recommends \<br/>        freetds-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev libpq-dev git \<br/>        freetds-bin \<br/>        build-essential \<br/>        default-libmysqlclient-dev \<br/>        cron &amp;&amp; \<br/>    apt-get autoremove -yqq --purge &amp;&amp; \<br/>    apt-get clean &amp;&amp; \<br/>    rm -rf \<br/>        /var/lib/apt/lists/* \<br/>        /tmp/* \<br/>        /var/tmp/* \<br/>        /usr/share/man \<br/>        /usr/share/doc \<br/>        /usr/share/doc-base</span><span id="fe76" class="na la it mw b gy nf nc l nd ne">RUN useradd -ms /bin/bash -d /usr/local/airflow airflow &amp;&amp; \<br/>    pip install -U pip setuptools wheel &amp;&amp; \<br/>    pip install pytz pyOpenSSL ndg-httpsclient pyasn1 awscli boto3</span><span id="26a2" class="na la it mw b gy nf nc l nd ne">RUN pip install apache-airflow[crypto,celery,postgres,hive,jdbc,mysql,ssh,redis,dynamodb]</span><span id="447e" class="na la it mw b gy nf nc l nd ne">COPY airflow.cfg /usr/local/airflow/airflow/airflow.cfg<br/>RUN chown -R airflow: /usr/local/airflow<br/>COPY dags/* /usr/local/airflow/airflow/dags/</span><span id="df37" class="na la it mw b gy nf nc l nd ne">EXPOSE 8080 5555 8793</span><span id="5016" class="na la it mw b gy nf nc l nd ne">USER airflow<br/>WORKDIR /usr/local/airflow</span><span id="8dd3" class="na la it mw b gy nf nc l nd ne">USER root<br/>COPY init.sh /usr/local/airflow/init.sh<br/>RUN chmod 755 /usr/local/airflow/init.sh</span><span id="d494" class="na la it mw b gy nf nc l nd ne">USER airflow</span><span id="57da" class="na la it mw b gy nf nc l nd ne">ENTRYPOINT ["/bin/sh", "-c", "/usr/local/airflow/init.sh"]</span></pre><p id="7b09" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">默认情况下，airflow似乎是以支持在单个实例上运行其所有任务的方式构建的，我需要它支持跨多个节点运行。快速搜索后，我发现使用<code class="fe ng nh ni mw b">celery</code>运行是可能的，这是python开发人员熟悉的概念，但由于我没有编写大量的python，对我来说是新的！</p><h1 id="de99" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">芹菜是什么？</h1><p id="4dda" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">Celery是一个分布式任务队列，一个基于分布式消息传递的异步任务队列/作业队列。称为任务的执行单元是在一个或多个使用多处理、eventlet或gevent的工作服务器上并发执行的。</p><p id="f709" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在airflow中，可以使用redis或rabbitMQ进行工人和主人之间的交流。因为我的概念验证是在AWS中进行的，所以我很自然地使用了redis，因为AWS将redis作为一项服务提供，而rabbitMQ我必须托管自己的服务。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/f1755d514f33c5afa6ffaffc2bbbc810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UQxCc-ksicNhowrP.jpg"/></div></div></figure><h1 id="c60b" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">初始化脚本</h1><p id="5dd8" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">因为airflow使用相同的库和不同的可执行文件来运行worker、webserver等。我更新了init脚本，以便能够注入我想要运行的进程类型，但保持相同的基础映像。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="6f5b" class="na la it mw b gy nb nc l nd ne">if [ -z "$AIRFLOW_TYPE" ]<br/>then<br/>  echo "Set the airflow type to either 'WEBSERVER', 'FLOWER' or 'WORKER'"<br/>  exit 1<br/>fi</span><span id="b5f5" class="na la it mw b gy nf nc l nd ne"># start airflow<br/>if [ "$AIRFLOW_TYPE" = "WORKER" ]<br/>then<br/>  airflow worker</span><span id="4de2" class="na la it mw b gy nf nc l nd ne">elif [ "$AIRFLOW_TYPE" = "WEBSERVER" ]<br/>then<br/>  airflow scheduler &amp; airflow webserver -p8080</span><span id="5dc5" class="na la it mw b gy nf nc l nd ne">elif [ "$AIRFLOW_TYPE" = "FLOWER" ]<br/>then<br/>  airflow flower<br/>fi</span></pre><p id="b39c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有一个想法是，应该有一个或多个工作者，但只有一个网络服务器或花卉实例</p><h1 id="6328" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">调查的结果</h1><p id="c919" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">它很容易设置，只要您使用数据库后端并配置远程日志，它似乎涵盖了我们需要的所有领域，即</p><ul class=""><li id="465b" class="mc md it kd b ke kf ki kj km me kq mf ku mg ky mh mi mj mk bi translated">高度可用(在AWS的ecs中托管，具有99.9%的SLA)</li><li id="02d0" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">可恢复的——不可变的容器，都连接到一个数据库实例并通过redis共享工作</li><li id="eb90" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">可伸缩性——根据所需的工作负载无限增加容器数量的能力(根据运行容器的统计数据自动实现这一点应该很简单)</li><li id="94cf" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">从不同的AWS帐户推送和提取数据的能力——这部分来自任务本身，我在网上读过很多关于如何做到这一点的内容。所以技术上是可能的，虽然我还没有机会证明这一点。</li></ul><p id="427f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我继续构建一个示例DAG，它允许我从S3提取一个CSV文件，转换成JSON，然后将结果存储在dynamodb存储中。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="ab17" class="na la it mw b gy nb nc l nd ne">import datetime<br/>import csv<br/>import json<br/>import os<br/>import string<br/>import random<br/>from airflow import DAG<br/>from airflow.hooks.S3_hook import S3Hook<br/>from airflow.contrib.hooks.aws_dynamodb_hook import AwsDynamoDBHook<br/>from airflow.operators.python_operator import PythonOperator</span><span id="ff23" class="na la it mw b gy nf nc l nd ne">s3 = S3Hook(aws_conn_id="s3_bucket")<br/>dynamo_db = AwsDynamoDBHook(aws_conn_id="dynamo_db")<br/>bucket_name = "airflow-poc-data"<br/>csv_key_name = "example-data.csv"<br/>tmp_filename = "/tmp/example-data.json"<br/></span><span id="8f28" class="na la it mw b gy nf nc l nd ne">def random_string():<br/>    """ Generate a random string of 20 characters"""<br/>    letters = string.ascii_lowercase<br/>    return ''.join(random.choice(letters) for i in range(20))<br/></span><span id="f634" class="na la it mw b gy nf nc l nd ne">def s3_csv_to_json(**kwargs):<br/>    """ convert file from s3, from csv to json """<br/>    print(kwargs)<br/>    csv_content = s3.read_key(csv_key_name, bucket_name)<br/>    print("fetched " + csv_key_name + " from " + bucket_name + ". length is " + str(len(csv_content)))</span><span id="e4aa" class="na la it mw b gy nf nc l nd ne">    tmp_csv_filename = random_string() + ".tmp"<br/>    with open(tmp_csv_filename, 'w') as csv_file:<br/>        csv_file.write(csv_content)</span><span id="fd18" class="na la it mw b gy nf nc l nd ne">    s3_tmp_filename = random_string() + ".json.tmp"<br/>    tmp_json_filename = convert_csv_file_to_json_file(tmp_csv_filename)<br/>    with open(tmp_json_filename, 'rb') as file:<br/>        s3.load_file_obj(file, s3_tmp_filename, bucket_name)</span><span id="6be1" class="na la it mw b gy nf nc l nd ne">    # cleanup<br/>    os.remove(tmp_csv_filename)<br/>    os.remove(tmp_json_filename)<br/>    return s3_tmp_filename<br/></span><span id="bc2a" class="na la it mw b gy nf nc l nd ne">def convert_csv_file_to_json_file(csv_filename):<br/>    tmp_json_filename = random_string() + ".tmp"<br/>    with open(csv_filename, 'rt') as csv_file:<br/>        with open(tmp_json_filename, 'w') as json_file:<br/>            reader = csv.DictReader(csv_file.readlines(), ('Id', 'Name', 'Enabled', 'FavouriteNumber'))<br/>            for rows in reader:<br/>                json_file.write(json.dumps(rows))<br/>    return tmp_json_filename<br/></span><span id="7684" class="na la it mw b gy nf nc l nd ne">def save_to_dynamo(**kwargs):<br/>    """ save json from previous task to dynamo """<br/>    print(kwargs)<br/>    s3_tmp_filename = kwargs['ti'].xcom_pull(task_ids='fetch_file_from_s3')<br/>    print("fetching previous state: key:" + str(s3_tmp_filename) + " bucket:" + str(bucket_name))<br/>    json_data = s3.read_key(s3_tmp_filename, bucket_name)<br/>    print("processing json data. length is " + str(len(json_data)))<br/>    dynamo_db.write_batch_data(json_data)<br/>    s3.delete_objects(bucket_name, [s3_tmp_filename])<br/></span><span id="8a2e" class="na la it mw b gy nf nc l nd ne">args = {<br/>    'owner': 'Craig Godden-Payne',<br/>    'retries': 10,<br/>    'start_date': datetime.datetime(2019, 8, 15),<br/>    'retry_delay': datetime.timedelta(minutes=1)<br/>}</span><span id="9c04" class="na la it mw b gy nf nc l nd ne">with DAG(dag_id='csv_to_json_example', default_args=args) as dag:<br/>    convert_csv_to_json = PythonOperator(task_id='fetch_file_from_s3', provide_context=True, python_callable=s3_csv_to_json)<br/>    save_to_dynamo = PythonOperator(task_id='save_to_dynamo', provide_context=True, python_callable=save_to_dynamo)</span><span id="99cd" class="na la it mw b gy nf nc l nd ne">convert_csv_to_json &gt;&gt; save_to_dynamo</span></pre><h1 id="a8ed" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">逮到你了</h1><p id="ca12" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">我注意到了一些问题，值得一提；</p><ul class=""><li id="0f51" class="mc md it kd b ke kf ki kj km me kq mf ku mg ky mh mi mj mk bi translated">如果使用sqllite在本地运行，由于并行性问题，不能同时运行scheduler和webserver。</li><li id="1cb8" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">当添加新的Dag时，它需要一段时间才会出现在web服务器中，它不是即时的，也不是中断的！</li><li id="46d7" class="mc md it kd b ke ml ki mm km mn kq mo ku mp ky mh mi mj mk bi translated">调度器可以在多个工作线程上多次触发单个任务，因此使Dag幂等非常重要。在概念验证过程中，我并没有亲眼看到这一点，尽管我已经阅读了很多相关资料。</li></ul><p id="f97f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">写于2019年8月20日。</p><p id="d507" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最初发表于:<a class="ae mq" href="https://craig.goddenpayne.co.uk/airflow/" rel="noopener ugc nofollow" target="_blank">https://craig.goddenpayne.co.uk/airflow/</a></p></div></div>    
</body>
</html>