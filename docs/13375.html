<html>
<head>
<title>Neural Networks (Chapter 4: AI Handbook)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络(第四章:人工智能手册)</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/neural-networks-chapter-4-ai-handbook-24c5a569fa05?source=collection_archive---------14-----------------------#2022-08-30">https://levelup.gitconnected.com/neural-networks-chapter-4-ai-handbook-24c5a569fa05?source=collection_archive---------14-----------------------#2022-08-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3899" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简化的、基于当今应用解释的几个神经网络。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7a0cc8645428b688568db46edc8faa43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oVHFQD7i2RXbwwsmDOpc1w.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来自<a class="ae le" href="https://www.pexels.com/@pixabay/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><p id="e1be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经网络是一组“神经元”[2]，它们一起工作来处理信息，并且相互连接。它们被用于人工智能(AI)，因为它们可以学习和识别模式，并根据这些模式进行预测。</p><p id="4ca4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输入层、隐藏层和输出层是神经网络的典型组件。输入层接收数据，数据的行为类似于图像(作为一个例子)，然后被传递到隐藏层。使用被称为激活函数的方法，隐藏层从数据中提取信息。最后，输出层生成分类或预测结果(进一步扩展图像示例:识别图像中的对象)。</p><h1 id="1348" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">层层</strong></h1><p id="9cad" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">在神经网络中，输入层是初始层。它接受原始数据，并将其转发到隐藏层，即后续层。隐藏层分析数据并为输出层生成输出。输出层是神经网络的最后一层；它生成最终结果。</p><p id="4f28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输入层由从网络周围接收输入的神经元组成。用最简单的话来说，输入层使网络能够从例子中学习。</p><p id="cbd4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在神经网络中，输入层是数据进入系统的地方。这些数据可能包括传感器信号、照片、文本或任何可以转换成数字的东西。然后，输入层将这些数据传输到下一个系统层，通常称为隐藏层。隐藏层由神经元(处理单元)组成，这些神经元相互链接，并与输入层中的神经元链接。</p><p id="2267" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些单元间的连接允许信息在系统间传递。隐藏层中的每个神经元对其输入执行简单的计算，并作为结果生成输出信号。隐藏层是神经网络中大多数计算发生的地方。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/adf585c63f87f583c02fd82ec5a11687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GbkWSKyLpT0rrm577LoKQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来自<a class="ae le" href="https://medium.com/@aniltilbe" rel="noopener">作者</a> [1]</figcaption></figure><h1 id="4238" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">普通神经网络</strong></h1><p id="a825" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">存在不同种类的神经网络，例如卷积神经网络、递归神经网络以及具有长期和短期记忆的网络，并且它们都具有以下基本组件:</p><p id="37b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">— CNN(卷积神经网络)用于图像分类，可以认为是特征提取器。它检查图片的一小部分，以检测是否存在特定的模式。</p><p id="2498" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">— RNN(递归神经网络)用于文本数据，并考虑词序。这表明它可以处理语法规则之类的事情。</p><p id="65d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">— LSTM(长短期记忆)同样用于文本数据，但可能比其他神经网络更有效地记住长期依赖关系[3]。</p><p id="d250" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简单来说，CNN主要用于图像处理，而RNN和LSTM可以用于文本数据。CNN通过将卷积滤波器应用于输入的局部区域来工作，而rnn顺序地读取输入(例如，一次一个元素)。LSTMs是一种RNN，包括反馈循环，以更好地模拟长期的复杂依赖关系。</p><h1 id="66f5" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">看一眼LSTM </strong></h1><p id="cebc" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">序列数据的处理，比如时间序列或文本，是LSTMs真正的亮点。与其他类型的人工智能算法相比，LSTMs具有在更长时间内记住信息并从上下文中学习的能力，这使它们能够有效地解释自然语言。LSTMs是如何工作的？</p><p id="b2b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个独立的LSTM单元由一个遗忘门、一个输入门、一个输出门和一个单元状态组成。这四个基本组成部分构成了LSTM。遗忘门决定该单元应该从当前时间步之前的时间步保留多少信息；输入门决定该单元应该从当前时间步长中获取什么新信息；输出门决定该单元在当前时间步长应该计算什么信息作为输出；并且单元状态存储单元到目前为止已经观察到的所有信息。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/6c094b58ddc77d114e2fd64612ca2a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4HpnmyOt_lXaqRXOstUCYw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来自<a class="ae le" href="https://medium.com/@aniltilbe" rel="noopener">作者</a> [1]</figcaption></figure><h1 id="ee8f" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">对RNN的温柔描述</strong></h1><p id="121f" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">它们属于人工智能领域，广泛应用于序列数据的建模。rnn可以被训练来模拟多种形式的序列，包括文本、时间序列数据或音频信号。rnn被设计用来识别数据序列中的模式，并且可以被训练来模拟任何种类的序列。</p><p id="6276" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">RNNs的功能是通过包含递归神经网络的隐藏层顺序处理输入。隐藏层以状态的形式保存关于先前输入的信息，状态是向量表示。在每次迭代中，RNN接收一个输入并相应地修改其状态向量。然后，修改后的状态向量被转发到后续阶段，直到所有输入都已被处理。</p><p id="3949" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练期间学习的权重和偏差决定了RNN在每一步的输出。训练RNN的目的是最小化目标函数，例如预测输出和实际输出之间的交叉熵或均方误差。使用随机梯度下降[4]跨时间反向传播梯度是一种常见的训练方法。</p><h1 id="aa7a" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">离别的思绪</h1><p id="3aaf" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">神经网络可以潜在地识别人类可能错过或发现难以识别的复杂模式。例如，在计算机视觉编程中用于面部识别的软件中经常使用神经网络。此外，它们是可扩展的，这意味着当向它们添加更多数据时，它们可以提高它们在模式识别任务中的性能。</p><p id="0a21" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个困难是，如果没有足够的数据进行训练，模型的准确性将会下降。因此，他们的效率可能会因此降低。</p><p id="0de8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你对这篇文章有任何建议或拓宽主题的建议，我将非常感谢你的来信。另外，请考虑<a class="ae le" href="https://predictiveventures.substack.com" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">订阅我的简讯。</strong>T3】</a></p><p id="b8a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我在Medium上创建了以下“列表”，你可以访问它来查看这个<a class="ae le" href="https://aniltilbe.medium.com/list/b67f31a002b3" rel="noopener"> <strong class="js iu">【人工智能手册】</strong> </a>系列中的所有其他帖子。</p><p id="ea1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你喜欢看这样的故事，并且想支持我这个作家，可以考虑注册成为Medium会员，获得Medium上所有故事的无限使用权:<a class="ae le" href="https://medium.com/@AnilTilbe/membership" rel="noopener"><strong class="js iu">* * *订阅Medium*** </strong> </a> <strong class="js iu">。</strong></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="e56f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">此外，考虑阅读以下来自<em class="mq">人工智能手册</em>的帖子:</strong></p><div class="mr ms gp gr mt mu"><a rel="noopener  ugc nofollow" target="_blank" href="/artificial-intelligence-history-chapter-1-ai-handbook-ae5774ef8026"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd iu gy z fp mz fr fs na fu fw is bi translated">人工智能历史(第一章:人工智能手册)</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">开始、中间和现在。一个端到端的，但非常简单的介绍人工智能在这个多部分…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni ky mu"/></div></div></a></div><p id="35ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="mq">和</em> </strong></p><div class="mr ms gp gr mt mu"><a rel="noopener  ugc nofollow" target="_blank" href="/knowledge-representation-chapter-2-ai-handbook-f37da56d5868"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd iu gy z fp mz fr fs na fu fw is bi translated">知识表示(第2章:人工智能手册)</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">所有人工智能产品都采用的基本问题空间，这是一个非常简单的知识表示介绍…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="nd l"><div class="nj l nf ng nh nd ni ky mu"/></div></div></a></div></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="1eba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考资料:</p><ol class=""><li id="23b6" class="nk nl it js b jt ju jx jy kb nm kf nn kj no kn np nq nr ns bi translated">OpenAI协助开发了这一可视化工具</li><li id="bcb0" class="nk nl it js b jt nt jx nu kb nv kf nw kj nx kn np nq nr ns bi translated">蒂尔贝，阿尼尔。(2022年7月24日)。10个最重要的递归神经网络。<a class="ae le" href="https://medium.com/p/8de9989db315" rel="noopener">https://medium.com/p/8de9989db315</a></li><li id="bdfa" class="nk nl it js b jt nt jx nu kb nv kf nw kj nx kn np nq nr ns bi translated">Pal，s .，Ghosh，s .，Nag，a .(公元前1年)。基于LSTM递归神经网络的情感分析。国际合成情绪杂志(IJSE)，9(1)，33–39。<a class="ae le" href="https://doi.org/10.4018/IJSE.2018010103" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.4018/IJSE.2018010103</a></li><li id="7356" class="nk nl it js b jt nt jx nu kb nv kf nw kj nx kn np nq nr ns bi translated">陈，G. (2016年10月8日)。带误差反向传播的递归神经网络教程。ArXiv.Org。<a class="ae le" href="https://arxiv.org/abs/1610.02583" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.02583</a></li></ol><p id="53e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">阿尼尔·蒂尔贝</p></div></div>    
</body>
</html>