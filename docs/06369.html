<html>
<head>
<title>Explaining Deep Learning in a Regression-Friendly Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用回归友好的方式解释深度学习</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/a-tutorial-to-build-from-regression-to-deep-learning-b7354240d2d5?source=collection_archive---------1-----------------------#2020-11-19">https://levelup.gitconnected.com/a-tutorial-to-build-from-regression-to-deep-learning-b7354240d2d5?source=collection_archive---------1-----------------------#2020-11-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f00b98152516c85750f49ae429546f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZywNOXFY4N1D9NEFjVBf4A.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">(图片由作者提供)</figcaption></figure><p id="c40a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">通常深度学习或神经网络在他们的类别中以他们的行话呈现。学习者以类似大脑的解剖学为导向，以“想象”深度学习如何在大脑环境中发挥作用。学习者会看到神经元、互联性和复杂的神经网络系统。在我的讲座从回归到深度学习的过渡中，我不知何故感到有一瞬间的沉默——就像跳过一个深深的缺口。跳到深度学习需要很多准备，那些跳的成功的同学可能不会很容易的回归到回归(你可能喜欢这个双关语)。术语的差异也造成了知识差距。这篇文章试图用一种回归友好的方法来呈现深度学习。我将解释神经元、激活函数、层、优化器等等。我将向您展示如何在深度学习框架中构建逻辑回归。一旦你有了很好的理解，我将带你深入:</p><ul class=""><li id="1ad5" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">深度学习中的L1和L2正规化，以及</li><li id="61ab" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">深度学习中的辍学正规化。</li></ul><p id="f358" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">本文的目标不仅仅是建立一个深度学习模型来执行逻辑回归的工作。那就太夸张了。目标是非常详细地解释，以便您可以建立更复杂的深度学习模型。看完这篇帖子，你会对深度学习建模驾轻就熟。我也推荐你的姊妹文章“<a class="ae lr" href="https://dataman-ai.medium.com/a-technical-guide-on-rnn-lstm-gru-for-stock-price-prediction-bce2f7f30346?sk=fe59aa77f9a953211253ebb5fb6f8b15" rel="noopener">RNN/LSTM/GRU股价预测技术指南</a>”。</p><p id="c694" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">关于这个平台，<strong class="kh iu"> Keras </strong>和<strong class="kh iu"> PyTorch </strong>在过去几年里已经在MXNet等其他平台中获得了巨大的人气。Keras是低级库<strong class="kh iu"> TensorFlow </strong>和Theano的高级Python包装器。本文将带您通过Keras来构建模型。如果你想探索PyTorch，我推荐我以前的帖子“<a class="ae lr" href="https://medium.com/analytics-vidhya/not-torturing-in-learning-pytorch-b2f7f169923a" rel="noopener">用PyTorch进行深度学习不是折磨</a>”，它温和地带你了解如何用<strong class="kh iu"> PyTorch </strong>构建深度学习模型。另外，我写过一系列关于深度学习或者神经网络的文章。参见"<a class="ae lr" href="https://towardsdatascience.com/module-6-image-recognition-for-insurance-claim-handling-part-i-a338d16c9de0" rel="noopener" target="_blank">什么是图像识别？</a>、<a class="ae lr" href="https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6" rel="noopener" target="_blank">使用自动编码器的异常检测变得容易</a>、<a class="ae lr" href="https://towardsdatascience.com/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763" rel="noopener" target="_blank">用于图像降噪的卷积自动编码器</a>。您可以将摘要文章“<a class="ae lr" href="https://medium.com/analytics-vidhya/dataman-learning-paths-build-your-skills-drive-your-career-e1aee030ff6e" rel="noopener"> Dataman学习之路——培养您的技能，推动您的职业发展</a>”加入书签。</p><p id="f3b3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">整个笔记本可以在<a class="ae lr" href="https://github.com/dataman-git/codes_for_articles/blob/master/From%20logistic%20to%20deep%20learning.ipynb" rel="noopener ugc nofollow" target="_blank"> this Github </a>下载。</p><p id="f219" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (A.1)输入数据在深度学习中被称为“张量”</strong></p><p id="657f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在Y = XB + e回归公式中，Y是一维向量，X是2D矩阵。输入数据集称为<em class="ls">数据框。</em>数据帧可以包含数值、分类或文本值。</p><p id="d8fa" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在深度学习中，输入数据被称为<em class="ls">张量</em>。如果你知道“张量”只不过是一维向量，你会感到宽慰。为什么他们不直接叫它“向量”？我不知道。这个不常见的术语来自哪里？它来自拉丁语张量，意思是“伸展的东西”。对拉伸下材料拉伸的研究就是对<em class="ls">张量</em>的研究。一位名叫沃耳德玛·福格特的数学家(1898年)借用这个术语来表示一个数字向量。张量是1D矢量。数据框是2D矩阵或2D张量。假设一个EXCEL电子表格有一列Y，许多列合在一起称为X。一列Y是1D张量，X是2D张量。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/79d6d46440caee2de90e4c4e7c337f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*nSVyz7hW3YZ30Wg_ah8zaw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">(图片由作者提供)</figcaption></figure><p id="fef3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一个重要的提醒是，深度学习模型只能解释数字数据。如果数据框包含分类数据，则需要通过一键编码或其他编码方法将其转换为数值。帖子“<a class="ae lr" href="https://towardsdatascience.com/a-data-scientists-toolkit-to-encode-categorical-variables-to-numeric-d17ad9fae03f" rel="noopener" target="_blank">一个将分类变量编码成数字的数据科学家工具包</a>”提供了一个全面的编码方法列表。</p><p id="3f43" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">图像数据也需要转换成n维张量。这篇文章不打算讨论图像数据。对图像数据转换感兴趣的读者，建议参考“<a class="ae lr" href="http://Deep Learning with PyTorch Is Not Torturing" rel="noopener ugc nofollow" target="_blank">深度学习用PyTorch </a>”、“<a class="ae lr" href="https://towardsdatascience.com/module-6-image-recognition-for-insurance-claim-handling-part-i-a338d16c9de0" rel="noopener" target="_blank">什么是图像识别？</a>、<a class="ae lr" href="https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6" rel="noopener" target="_blank">自动编码器异常检测变得简单</a>、<a class="ae lr" href="https://towardsdatascience.com/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763" rel="noopener" target="_blank">卷积自动编码器用于图像降噪</a>。</p><p id="b3c3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (A.2)输入层中的神经元是输入变量</strong></p><p id="10e7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于回归学习者来说，神经网络中的术语<em class="ls">神经元</em>是陌生的，术语<em class="ls">层</em>令人望而生畏。然而，你可能见过许多类似下图(A)的神经网络图。所以让我一步步介绍这个网络。该图是一个<em class="ls">模型</em>。我们如何理解一个模型？回归是模拟X和y之间关系的框架。决策树是模拟X和y之间关系的框架。因此图(A)是模拟X和y之间关系的神经网络图。神经网络模型有一个输入层、一个输出层和隐藏层。将预测值X分配给输入图层，将目标值Y分配给输出图层来训练模型。如果没有隐藏层，神经网络只有输入层和输出层。这种最简单的形式变成了逻辑回归，我将在后面的部分向您展示。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/ed46d3cd11bcc903fccd70e8fc647231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*e-KmA4GuRIRPldX0f5jgqA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">图(A):(图片由作者提供)</figcaption></figure><p id="7b60" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">网络中的节点怎么样？它们被称为<em class="ls">神经元</em>。输入层的神经元是最容易理解的。输入层中的神经元是一个变量。图(A)中的X矩阵有4个变量，所以它有4个神经元(或4列/向量/张量)。如果有100个输入变量，输入层就有100个神经元。此外，请记住神经网络模型只能接受数值。如果有80个数值变量和20个分类变量，每个变量有10个类别，则输入层中的神经元数量将是80+20×10 = 280个神经元(或280列/向量/张量)。</p><p id="9517" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (A.3)隐藏层中的神经元</strong></p><p id="5ea5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一个神经元接收来自前一层神经元的输入。每个神经元是前一层中所有数据值的加权和，或者说，每个神经元是前一层中值的线性组合。<em class="ls">权重</em>是待训练模型的参数，如下图(B)中的<em class="ls"> w1-w4 </em>。就像Y= XB + e中的参数向量B是一个回归模型的解一样，这些权重<em class="ls"> w1-w4 </em>就是模型的秘方。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/944c4425b95a2d74734311ecce0122d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*N-CYwalGrSXM6nAd6LIuzA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">图(B):(图片由作者提供)</figcaption></figure><p id="3a97" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (A.4)激活功能</strong></p><p id="75b8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们就要使用神经网络框架建立逻辑回归了。但是还有一个组件叫做<em class="ls">激活功能</em>。它的工作方式类似于逻辑回归中的logit函数。为什么逻辑回归需要logit函数？在线性回归Y = XB + e中，自变量X和预测值Y可以取从负到正无穷大的任何值。因为逻辑回归是关于概率的，所以不可能用线性回归模型获得这样的输出，我们需要将输出转换为0到1之间的值。</p><p id="a8b7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> Sigmoid函数:</strong>所以类似的想法也适用于神经网络。输出值的范围将扩大到负无穷大或正无穷大。为了防止这种情况发生，神经网络应用了如下左图所示的sigmoid函数，将输出转换为0和1之间的值。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/192814be4464ff6aa10a669ec9f79a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*Z-I3Kw78_moIWGoGH0-HnA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">图(A.4)</figcaption></figure><p id="2f13" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> ReLU功能:</strong>另一个常用的激活功能是ReLU(整流线性单元)，显示在右侧面板中。它将任何负值转换为零。我将在文章的最后更详细地描述这一点。</p><p id="5d59" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (B.1)加载波士顿住房数据</strong></p><p id="2d95" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我选择了一个流行的数据集<a class="ae lr" href="https://www.kaggle.com/c/boston-housing" rel="noopener ugc nofollow" target="_blank">波士顿住房数据</a>。该数据集记录了具有多个属性的中值房价。房价<em class="ls"> medv </em>是<em class="ls">以千美元为单位的自有住房的中值</em>。如果房价高于中值，我创建一个二元目标变量“Y”= 1，否则为0。该数据帧有506行和14列。为了便于说明，我随意选择了以下六个预测值:</p><ul class=""><li id="37ef" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">各城镇人均犯罪率。</li><li id="2d80" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated"><em class="ls"> ZN: </em>面积超过25，000平方英尺的住宅用地比例。制成</li><li id="bf7b" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated"><em class="ls"> RM: </em>每个住宅的平均房间数。</li><li id="9535" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated"><em class="ls">楼龄:</em>1940年前建成的自住单位比例。</li><li id="4f6f" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated"><em class="ls"> DIS: </em>到波士顿五个就业中心的加权平均距离。</li><li id="b9c5" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated"><em class="ls"> LSAT: </em>人口的下层地位(百分比)。</li></ul><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="47bc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">标准化:</strong>上面的代码也缩放输入变量。请注意，只有训练数据用于拟合缩放器变换，然后缩放器用于变换测试输入数据。新手学习者常犯的一个错误是独立地衡量x_train和x_test。这是你想要避免的致命的罪。在文章“<a class="ae lr" href="https://towardsdatascience.com/avoid-these-deadly-modeling-mistakes-that-may-cost-you-a-career-b9b686d89f2c" rel="noopener" target="_blank">避免这些可能让你失去职业生涯的致命建模错误</a>中，我记录了数据科学中的常见错误。有兴趣的话，看看吧。</p><p id="530d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">深度学习不做正态性假设。为了运行一个标准的线性回归，人们做出以下假设:</strong></p><ul class=""><li id="1814" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">X和Y之间的关系是线性的</li><li id="9db8" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">误差呈正态分布</li><li id="c92e" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">误差的同方差(或者，线周围的等方差)。</li></ul><p id="9b78" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">深度学习模型中不需要这些假设。</p><p id="7b10" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (B.2)让我们建立一个标准的逻辑回归</strong></p><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="f8a0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">均方误差为0.1068。供您参考，这些系数是</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi md"><img src="../Images/030fc1caf5e2e3dfdacbb3966a11ce51.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*WkNGhmbg0AW_cWSt6xa9_Q.png"/></div></figure><p id="7ca1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (C)让我们建立深度学习:</strong></p><p id="7c86" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">神经网络比逻辑回归更通用，因此它可以覆盖逻辑回归。我们可以认为逻辑回归是神经网络的一个特例。</p><p id="194d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">神经网络中有三个核心思想:(1)层按顺序堆叠，(2)神经元进行计算，以及(3)在每层中使用激活函数来绑定结果。这三个过程对应于Keras类别<code class="fe me mf mg mh b">Sequential</code>、<code class="fe me mf mg mh b">Dense</code>和<code class="fe me mf mg mh b">Activation</code>，如图(C)所示。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/2b711edd2a80ff3280af9e529124092c.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*PeXTRYc-grCRTjDe70N2Iw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">(图片由作者提供)</figcaption></figure><ul class=""><li id="8efe" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated"><code class="fe me mf mg mh b">Sequential</code>声明一个我们将要堆叠层的空模型。请参见下面代码中的第3行。</li><li id="ad29" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated"><code class="fe me mf mg mh b">add()</code>添加一层。</li><li id="c1f8" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated"><code class="fe me mf mg mh b">Dense(1,input_dim=6)</code>密集层是计算发生的地方。它执行矩阵乘法(点积)<em class="ls">加上噪声向量</em>。它被称为“密集”层，因为它压缩了前一层和当前层神经元之间的每一个可能的连接。矩阵中使用的值是可以在反向传播(稍后解释)的帮助下训练和更新的<em class="ls">权重</em>。在本例中，当前层有一个神经元，前一层有6个输入神经元。所以语法比较密集(<strong class="kh iu"> 1 </strong>，input_dim= <strong class="kh iu"> 6 </strong>)。见第4行。</li></ul><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><ul class=""><li id="6469" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated"><code class="fe me mf mg mh b">Activation(activation='sigmoid'))</code>:这将应用sigmoid函数来产生y。注意，第4行和第5行可以合并成一行代码，如第8行所示。</li></ul><p id="7667" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们已经完成了深度学习模型的模型规范，以执行逻辑回归所做的事情。该模型将使用优化器<code class="fe me mf mg mh b">compile()</code>和<code class="fe me mf mg mh b">fit()</code>进行训练。模型结果为['loss '，' mean_squared_error'] = [0.1031，0.1031]。MSE与逻辑回归的MSE相同。</p><p id="8a29" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (E)优化器</strong></p><p id="a8ca" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">什么是损失函数，什么是优化器？损失函数是测量实际值和预测值之间的误差的度量。优化器是一种改变神经元权重以追求最小误差的算法。一个流行的优化器是随机梯度下降(SGD)。文章《<a class="ae lr" href="https://medium.com/analytics-vidhya/a-lecture-note-on-random-forest-gradient-boosting-and-regularization-834fc9a7fa52" rel="noopener">我的随机森林、梯度推进、正则化、H2O.ai </a>讲义》对SGD进行了详细的描述。上面的代码指定<code class="fe me mf mg mh b">RMSprop</code>，均方根传播，作为优化器。</p><p id="2f86" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">你可能会问一个基本问题:“为什么回归不使用SGD这样的优化器来寻找最优参数，而机器学习技术需要它？”原因是回归可以通过解析或封闭形式的方程得出其最优解。损失函数是可微的。然而，并不是所有的损失函数都是可微的，以获得可跟踪的解决方案。因此，数值方法，如SGD或牛顿法被用来获得最优解。</p><p id="4bc2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面几行代码是一个基本神经网络的核心行。这些基本代码行可以扩展成非常复杂的形式。我将使用下面的例子来试验这个模型。</p><p id="8107" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (F)给深度学习模型增加更多层</strong></p><p id="c380" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">您可以在上述基本神经网络中添加任意数量的层和神经元。下面我用两个隐藏层做实验:</p><ul class=""><li id="5db7" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">第4行:第一个隐藏层有4个神经元。</li><li id="4de9" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">第5行:第二个隐藏层，有两个神经元。请注意，附加层不要求您指定前面层中神经元的数量。</li><li id="c479" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">第6行:输出层。</li></ul><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="ef0e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">['loss '，'均方误差'] = [0.1121，0.1121]。当您添加更多层时，模型可能会产生更好的结果，甚至会过度拟合数据。在广义线性模型(GLM)中，我们可以应用L1或L2正则化来控制过拟合。我们如何调整神经网络模型以减轻过度拟合？简而言之，有两种方法。首先，神经网络可以使用<code class="fe me mf mg mh b">regularizers.l1()</code>或<code class="fe me mf mg mh b">regularizers.l2()</code>进行L1和L2正则化。第二，神经网络可以用<code class="fe me mf mg mh b">dropout()</code>正则化。如果你不熟悉L1或L2正则化，请查看“<a class="ae lr" href="https://medium.com/analytics-vidhya/a-lecture-note-on-random-forest-gradient-boosting-and-regularization-834fc9a7fa52" rel="noopener">我的随机森林、梯度提升、正则化和H2O.ai </a>讲义”。</p><p id="b71b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (G)标准化数据以获得更好的模型性能</strong></p><p id="aaec" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于分类建模问题，在建模之前对数据进行标准化是一个很好的实践。这通常会产生模型性能。下面的代码和上面的一样，只是它是建立在标准化数据的基础上。结果似乎比上面的结果更好:['loss '，' mean_squared_error'] = [0.0981，0.0981]。</p><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="6ab7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (H)攻击过拟合</strong></p><p id="72df" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">过度拟合是机器学习中的一大罪过。当您根据定型数据定型模型并将其应用于测试数据时，测试数据的准确性通常低于定型数据的准确性。我们知道这是因为模型与训练数据拟合得太好，包括训练数据中的噪声。然而，如果过度拟合只是让你对测试数据的预测不那么有效，那又有什么大不了的呢？为什么学术界和从业者要投入几十年的工作来防止过度拟合？</p><p id="0f4a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">真正的问题是，过度拟合不仅会让你的模型效率低下，还会让你的预测非常错误。假设你的最终模型有十个变量，其中八个捕捉真实模式，另外两个变量是噪音。换句话说，这两个变量过度拟合噪声，是无用的。假设您要用十变量模型预测新数据，并且假设两个变量的新值很大。猜猜会发生什么？由于两个变量和新数据中的大值，您的预测将非常错误。所以过度拟合不仅会使你的模型无效，还会使你的预测非常错误。这就是为什么我们如此关注攻击过度拟合。如果您熟悉这些技术，您在数据科学方面的技术能力将会大大提高。</p><p id="c0bd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">(一)L1，L2转正&amp;退学转正</strong></p><p id="5cc9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">正则化将惩罚项添加到损失函数中，以惩罚大量的权重(参数)或大量的权重。深度学习提供了拉索(L1)和里奇(L2):</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/9853be4fe6ef898ad6beafb52312809c.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*MNaEh_8ddRexNtyX5OK9UQ.png"/></div></figure><p id="abeb" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">除了L1和L2，深度学习也有辍学正规化。建议您在深度学习模型中测试L1、L2和辍学正规化。</p><p id="c9af" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (I.1) L1 </strong></p><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="4f22" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的代码设置𝜆 = 0.01。结果是:['loss '，' mean_squared_error'] = [0.09434，0.09434]。</p><p id="fb4d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">(一. 2) L2 </strong></p><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="53a9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的代码设置𝜆 = 0.01。结果是:['loss '，' mean_squared_error'] = [0.09841，0.09841]。</p><p id="4fd9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (I.3)辍学正规化</strong></p><p id="34b6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">丢弃技术在每次迭代期间随机丢弃或停用一层的一些神经元。这就像一些权重被设置为零。因此，在每次迭代中，模型会查看自身稍微不同的结构来优化模型。</p><figure class="lu lv lw lx gt ju"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="0aaf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的代码将退出率设置为0.25，这意味着25%的神经元将被退出。结果是:['loss '，' mean_squared_error'] = [0.0974，0.0974]。</p><p id="5059" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (J)如何理解神经网络模型的复杂性？</strong></p><p id="61d9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果回归有许多参数，则被认为是复杂的。同样，如果深度学习有许多权重参数要训练，它就很复杂。</p><p id="f053" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我给你看两个模型，看看哪个模型更复杂(加权参数更多)。假设一个二进制分类问题有200列/变量和10亿行。这200个变量指的是150个数字变量和50个分类变量。50个分类变量中的每一个都有400个级别。有两种型号:</p><ul class=""><li id="faf9" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">模型1: 3个隐藏层，每个层有300个神经元。</li><li id="a036" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">模型2:具有500个神经元的1个隐藏层</li></ul><p id="00d8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们计算每个模型的权重数并进行比较。通常，我们将相邻层中的神经元数量相乘，然后将所有对相加。在这个问题中，分类变量必须是一个独立的神经元。因此，50个分类将被转换为50 x 400 = 2，000个虚拟变量。我们总共有150+2000 = 2150个输入神经元。输出有1个神经元。</p><ul class=""><li id="02a0" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">型号1:2150 x 300+300 x 300+300 x 300+300 x 1 = 825，300</li><li id="b71c" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">型号2: 2150 x 500 + 500 x 1 = 1，075，500</li></ul><p id="4764" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">所以模型2更复杂。</p></div></div>    
</body>
</html>