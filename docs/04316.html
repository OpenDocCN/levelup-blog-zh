<html>
<head>
<title>Natural Language Processing and Topic Modeling on User Review Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用户评论数据集上的自然语言处理和主题建模</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/natural-language-processing-and-topic-modeling-on-user-review-dataset-66599beaa50e?source=collection_archive---------5-----------------------#2020-06-20">https://levelup.gitconnected.com/natural-language-processing-and-topic-modeling-on-user-review-dataset-66599beaa50e?source=collection_archive---------5-----------------------#2020-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1d9bba849708e959d144cc7cf865f0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vRM3zfetxgWUJidhA7OVw.jpeg"/></div></div></figure></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h1 id="f42a" class="ki kj it bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">介绍</h1><p id="998f" class="pw-post-body-paragraph lg lh it li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自然语言处理是计算机科学中的一个热门话题。像句子这样的非结构化数据对人类来说很有意义，但对计算机来说却不是。</p><p id="a79e" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">为了让机器知道句子的意思，我们需要将人类可以理解的句子编码成机器可以理解的数字。这个过程有点像计算机如何存储图像，使用通道中的像素强度。</p><p id="de33" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">我今天要介绍的方法是术语频率-逆文档频率(TF-IDF)，这是一种数值统计，用来描述一个术语对集合中一个文档的重要性。</p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h1 id="6ef3" class="ki kj it bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">预处理</h1><h2 id="8dac" class="mj kj it bd kk mk ml dn ko mm mn dp ks lr mo mp kw lv mq mr la lz ms mt le mu bi translated">标记化</h2><p id="903c" class="pw-post-body-paragraph lg lh it li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了执行TF-IDF，我们需要首先对文档进行令牌化。</p><blockquote class="mv mw mx"><p id="06f7" class="lg lh my li b lj me ll lm ln mf lp lq mz mg lt lu na mh lx ly nb mi mb mc md im bi translated">标记化是将文本拆分成单个单词或单词序列(N元语法)的过程。</p></blockquote><p id="bc51" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">例如，正文是“汤姆和我上周末去了纽约”。在标记化之后，我们有一个这样的单词列表:</p><p id="7f08" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">《汤姆》、《和》、《我》、《去了》、《去了》、《纽约》、《最后》、《周末》</p><p id="2382" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">这是一个一词标记化的例子。您还可以执行两个字母组合，然后您可以在输出中保留像“New York”这样的短语。</p><p id="f90b" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">您可能还希望删除像“and”和“to”这样的停用词，并将这些词规范化为所有小写字母，因为大多数情况下停用词和大写字母不会传达太多信息，但会使我们的建模过程变得复杂。</p><p id="52f0" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">有时候，你可能会觉得一个单词的屈折形式很讨厌，比如“playing”和“played”。实际上，我们可以在这一步通过将“playing”和“played”转换为“play”来阻止单词。这样，我们限制了标记化单词列表中不必要的变化。</p><p id="684a" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">要完成这个过程，python包自然语言工具包(NLTK)非常有帮助。</p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="71ac" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">完成标记化后，我们可以看一看并深入研究TF-IDF。</p><h2 id="c4de" class="mj kj it bd kk mk ml dn ko mm mn dp ks lr mo mp kw lv mq mr la lz ms mt le mu bi translated"><strong class="ak"> <em class="ni">【词频-逆文档频(TF-IDF)】</em></strong></h2><p id="3e7a" class="pw-post-body-paragraph lg lh it li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们有了上一步中的标记化单词列表。我们可以对数据执行聚类算法或主题建模吗？答案是否定的，模型不能直接理解文字。</p><p id="5d86" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">我们需要一个加权因子来表示一个单词在文档集合中的重要性。而这里就出现了频率-逆文档频率(TF-IDF)这个术语。</p><p id="eba8" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">数学上，TF-IDF等于词频和逆文档频的乘积。</p><p id="f7a8" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">TF-IDF(t，d) = TF(t，d) * IDF(t)</p><p id="c6e3" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">对于<strong class="li iu">词频</strong> TF(t，d)，最简单的选择是文档中某个词的<em class="my">原始计数</em>除以文档中的总项数。</p><p id="5f36" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">TF(t，d) =文档d中术语t的计数/文档d中的总字数</p><p id="35d2" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><strong class="li iu">逆文档频率</strong> IDF(t)是对单词提供多少信息的度量，可以通过取文档总数除以包含该术语的文档数的对数来获得。</p><p id="cb74" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">IDF(t) = log(语料库中的文档总数/带有术语t的文档数)</p><p id="6de0" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">更常见的是，我们会在分母上加1，以避免被零除。</p><p id="1f43" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">我们可以使用sklearn库来帮助我们进行TF-IDF转换。</p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="8855" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">这样，我们可以将输入的评审数据矩阵转换成数字矩阵。太好了！最后，我们可以建立一个模型。</p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h1 id="da6f" class="ki kj it bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">建模</h1><p id="178c" class="pw-post-body-paragraph lg lh it li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">k均值和潜在狄利克雷分配(LDA)是无监督的机器学习算法，可以将输入聚类到不同的组中。这就是我们将要在用户评论数据集上做的，找出可能的主题组。</p><h2 id="0c2b" class="mj kj it bd kk mk ml dn ko mm mn dp ks lr mo mp kw lv mq mr la lz ms mt le mu bi translated"><strong class="ak"><em class="ni">——K的意思是</em> </strong></h2><p id="01d6" class="pw-post-body-paragraph lg lh it li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">K-means是一种流行的聚类算法，其目的是将观察值分成组。</p><p id="73e8" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">K-means中的主要步骤可以描述如下:</p><blockquote class="nj"><p id="615e" class="nk nl it bd nm nn no np nq nr ns md dk translated"><em class="ni">K均值算法</em></p></blockquote><ul class=""><li id="6316" class="nt nu it li b lj nv ln nw lr nx lv ny lz nz md oa ob oc od bi translated"><em class="my">随机选择K个点作为初始质心</em></li><li id="55b6" class="nt nu it li b lj oe ln of lr og lv oh lz oi md oa ob oc od bi translated"><strong class="li iu"> <em class="my">重复</em></strong><strong class="li iu"><em class="my">这两步，直到</em> </strong> <em class="my">质心不变或在公差范围内</em></li></ul><p id="97f4" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><em class="my"> —通过将所有点分配到最近的质心来形成K个聚类</em></p><p id="4115" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><em class="my"> —重新计算每个簇的质心</em></p><p id="be62" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">在实践中，我们可以将终止条件设置为最大迭代或容差。</p><p id="72c5" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">您可以使用python sklearn库实现Kmeans:</p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="ng nh l"/></div></figure><blockquote class="nj"><p id="e94a" class="nk nl it bd nm nn oj ok ol om on md dk translated">集群的总数是我们需要在模型中调整的一个超参数。如果给定两个聚类结果，我们如何知道哪一个更好？</p></blockquote><p id="a1e2" class="pw-post-body-paragraph lg lh it li b lj nv ll lm ln nw lp lq lr oo lt lu lv op lx ly lz oq mb mc md im bi translated"><strong class="li iu">如何评价聚类质量？</strong></p><p id="5d16" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">聚类质量有几种数值度量。</p><ul class=""><li id="9dc1" class="nt nu it li b lj me ln mf lr or lv os lz ot md oa ob oc od bi translated"><strong class="li iu"> <em class="my">平方和误差(WSSE) </em> </strong>是聚类同质性的度量，通过对每个点到其聚类质心的平方距离求和来计算。给定两个集群解决方案，WSSE较小的解决方案应该是首选。</li><li id="b52c" class="nt nu it li b lj oe ln of lr og lv oh lz oi md oa ob oc od bi translated"><strong class="li iu"><em class="my">【BSSE】</em></strong>之间的平方和误差是聚类分离的度量。BSSE是聚类质心和“大平均值”之间的距离的度量。给定两个集群解决方案，应首选BSSE较大的解决方案。</li><li id="eb59" class="nt nu it li b lj oe ln of lr og lv oh lz oi md oa ob oc od bi translated"><strong class="li iu"> <em class="my">剪影系数</em> </strong>结合了同质性和分离性。轮廓系数始终在-1和1之间。越接近1，聚类质量越好。</li></ul><p id="be2a" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">通过使用这些指标，我们能够评估我们的集群解决方案的质量。</p><h2 id="7238" class="mj kj it bd kk mk ml dn ko mm mn dp ks lr mo mp kw lv mq mr la lz ms mt le mu bi translated"><strong class="ak"> <em class="ni">【潜狄氏分配】</em></strong>【LDA<strong class="ak"><em class="ni">)</em></strong></h2><p id="bb92" class="pw-post-body-paragraph lg lh it li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">潜在狄利克雷分配(LDA)是主题建模的一个例子。它可以通过发现每个文档中的潜在主题将评论分成不同的组。</p><blockquote class="mv mw mx"><p id="240e" class="lg lh my li b lj me ll lm ln mf lp lq mz mg lt lu na mh lx ly nb mi mb mc md im bi translated">在自然语言处理中，<strong class="li iu">潜在狄利克雷分配</strong> ( <strong class="li iu"> LDA </strong>)是一种生成统计模型，它允许通过未观察到的组来解释观察集，这解释了为什么数据的某些部分是相似的。(维基百科)</p></blockquote><p id="718c" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">可以使用python sklearn库实现<strong class="li iu">潜在狄利克雷分配</strong>:</p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="ng nh l"/></div></figure></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h1 id="ce33" class="ki kj it bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">结果</h1><p id="1a44" class="pw-post-body-paragraph lg lh it li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是我们模型的可视化结果。基于分析，我们发现聚类0包含更多负面评论，而聚类2包含更多正面评论。第一组的评论更加中立。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ou"><img src="../Images/796c69b512e10aa8b3c441a70fe2fc19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpGfBguGXxV-wq-Icv4CsA.png"/></div></div></figure><p id="15e1" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">如果想了解更多的实现细节，可以在下面找到Github链接。感谢您的阅读！</p><p id="e5cb" class="pw-post-body-paragraph lg lh it li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><a class="ae ov" href="https://github.com/yanhan-si/NLP-and-Topic-Modeling-on-User-Review-Dataset" rel="noopener ugc nofollow" target="_blank">https://github . com/韩嫣-si/NLP-and-Topic-Modeling-on-User-Review-Dataset</a></p></div></div>    
</body>
</html>