<html>
<head>
<title>Easy Guide to Create a Custom Read Data Source in Apache Spark 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Apache Spark 3中创建自定义读取数据源的简单指南</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/easy-guide-to-create-a-custom-read-data-source-in-apache-spark-3-194afdc9627a?source=collection_archive---------0-----------------------#2020-10-19">https://levelup.gitconnected.com/easy-guide-to-create-a-custom-read-data-source-in-apache-spark-3-194afdc9627a?source=collection_archive---------0-----------------------#2020-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3735" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Apache Spark 3.0.x编写具有位置感知和多分区支持的自定义读取数据源的分步指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/014c56b839048dfcf892ef2286b5eda4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4NQEuI5kZrwfd6k1"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">安德里克·朗菲尔德在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="06f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Apache Spark是一个非常强大的分布式执行引擎。当我们通读它的文档和例子时，即使它有所有复杂的功能，我们发现它相对容易使用。当我们深入研究它并试图解决现实生活中的用例时，尽管它的特性打包了功能，我们还是需要编写一些定制。Apache Spark真正突出的一点是它内在支持这些定制的能力。Spark不仅提供了不同的接口来插入这种定制，而且还不断改进这些接口来克服限制。</p><h1 id="3d3e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">我们会学到什么？</strong></h1><ul class=""><li id="e99c" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">什么是数据源API及其重要性？</li><li id="e2ba" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">我们将讨论在Spark 3.0.x中实现读取数据源所需的所有接口</li><li id="dae7" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">这些接口的重要性及其在集群上的部署，驱动程序与执行器</li><li id="38a5" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">然后，我们将实现这些接口，并创建一个简单的CSV读取数据源</li><li id="5e7c" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">最后，我们将学习如何编写一个稍微复杂的位置敏感的多分区读数据源。</li></ul><p id="294e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，不浪费任何时间，让我们开始吧。</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="0e1f" class="ls lt iq bd lu lv nh lx ly lz ni mb mc jw nj jx me jz nk ka mg kc nl kd mi mj bi translated">数据来源是什么？</h1><p id="9d04" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">数据源是Apache Spark中非常流行的功能。许多开发人员广泛使用它来连接第三方应用程序和Apache Spark。从2.4.x版本开始，它有两个变体，V1和V2。</p><p id="0655" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">V1在v 2.3.x之前可用。V2 API在2.3.0中引入，并在2.4.0中修改。在3.0.0中，Spark在v2 APIs中引入了重大变化，但是，为了向后兼容，v1保持不变。</p><p id="8559" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在之前的一篇<a class="ae kv" href="https://medium.com/@aamargajbhiye/speed-up-apache-spark-job-execution-using-a-custom-data-source-fd791a0fa4b0" rel="noopener">文章</a>中，我讨论了如何使用v2数据源API创建定制数据源。在本文中，我们将了解如何为Apache Spark 3.0.x创建读数据源</p><h1 id="e790" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据源接口</h1><ul class=""><li id="98e1" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">表格提供者</li><li id="ad9a" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">桌子</li><li id="b257" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">扫描生成器</li><li id="2ace" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">扫描</li><li id="cf5f" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">一批</li><li id="61fb" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">输入分区</li><li id="5f8d" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">PartitionReaderFactory</li><li id="5007" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">分区阅读器</li></ul><p id="a6a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们详细讨论其中的每一项及其用途。</p><h2 id="c974" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">表格提供者</h2><p id="2d2b" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">在Spark 2.4.x中，数据源API中的主要接口是<code class="fe ob oc od oe b">DatasourceV2,</code>实现它或它的一个或其他专门化(如<code class="fe ob oc od oe b">ReadSupport</code>或<code class="fe ob oc od oe b">WriteSupport</code>)所需的所有定制数据源。这个接口在3.0.x中被移除了，取而代之的是引入了一个新的<a class="ae kv" href="http://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/sql/connector/catalog/TableProvider.html" rel="noopener ugc nofollow" target="_blank"> TableProvider </a>接口。它是所有不需要支持DDL的定制数据源的基本接口。此接口的实现应该有一个0参数的公共构造函数。让我们为我们的定制CSV数据源实现它，看看它看起来如何。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="947e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的数据源可以接受来自外部源的模式，比如通过<code class="fe ob oc od oe b">DataFrameReader.read()</code>的用户指定模式，我们从<code class="fe ob oc od oe b">supportsExternalMetadata</code>返回true，不会实现<code class="fe ob oc od oe b">inferschema</code>。</p><h2 id="60ef" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">桌子</h2><p id="1429" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">该接口定义了表示结构化数据的单个逻辑实体。它可以是基于文件系统的数据源的文件/文件夹，Kafka的主题，或者JDBC数据源的表。它可以与<code class="fe ob oc od oe b">SupportsRead</code>和<code class="fe ob oc od oe b">SupportsWrite</code>混合使用，以增加读写能力。在我们的例子中，CSV文件是一个表格。<code class="fe ob oc od oe b">capabilities</code> API返回表的所有功能。对于我们简单的read实现，让我们只返回BATCH_READ。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="d5bd" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">扫描</h2><p id="2cdd" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">对于我们简单的读取数据源，我们返回了<a class="ae kv" href="http://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/sql/connector/catalog/TableCapability.html#BATCH_READ" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> BATCH_READ </strong> </a>。其他读取能力有<code class="fe ob oc od oe b">MICRO_BATCH_READ</code> <strong class="ky ir"> </strong>和<strong class="ky ir"> </strong> <code class="fe ob oc od oe b">CONTINUOUS_READ</code> <strong class="ky ir">。</strong> SupportsRead接口有返回Scan的newScanBuilder() API。Scan表示数据源扫描的逻辑计划，可以是批处理、微批处理流或连续流。对于这个例子，我们将实现<code class="fe ob oc od oe b">toBatch()</code> <strong class="ky ir"> </strong> API，因为我们已经在表功能中添加了BATCH_READ作为功能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="b896" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">一批</h2><p id="2cea" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">批处理代表一个物理计划。在运行时，逻辑表扫描被转换为物理扫描。这是计划数据源分区的界面。它定义了一个工厂，该工厂被发送给执行器，为每个<code class="fe ob oc od oe b">InputPartition</code>创建一个<code class="fe ob oc od oe b">PartitionReader</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="98ea" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">输入分区和分区阅读器</h2><p id="2858" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">在使用所有已配置的选项后，应用运行时优化，如下推过滤器、列修剪等，创建物理规划，即<code class="fe ob oc od oe b">Batch</code>。这涉及到创建<code class="fe ob oc od oe b">InputPartition</code>和<code class="fe ob oc od oe b">PartitionReaderFactory</code> <strong class="ky ir">。他们被部署在执行者身上。在每个执行器上，对于每个分区，使用PartitionReaderFactory <strong class="ky ir">创建一个PartitionReader实例。</strong> <code class="fe ob oc od oe b">PartitionReader</code> <strong class="ky ir"> </strong>从数据存储器中读取数据，使用给定的<strong class="ky ir"> s </strong> chema <strong class="ky ir">将其转换为<code class="fe ob oc od oe b">InternalRow</code>。</strong></strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="1de4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，就这么定了。我们已经成功地定义了一个具有读取能力的简单定制数据源。这个读取数据源的完整源代码在这里分享<a class="ae kv" href="https://github.com/aamargajbhiye/big-data-projects/tree/master/Datasource%20spark3/src/main/java/com/bugdbug/customsource/csv" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="cfee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们利用所有这些知识，定义一个位置感知的多分区数据源。</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="e36f" class="ls lt iq bd lu lv nh lx ly lz ni mb mc jw nj jx me jz nk ka mg kc nl kd mi mj bi translated">具有首选位置的多分区数据源</h1><p id="761a" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">我们已经定义了一个基本的CSV数据源，它支持外部元数据，并且只有一个分区。现在，让我们看看如何定义一个带有多个分区的定制JDBC数据源，而不需要外部模式和对分区的首选位置支持。</p><p id="b958" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要来自用户输入的三样东西。</p><ol class=""><li id="39c1" class="mk ml iq ky b kz la lc ld lf oh lj oi ln oj lr ok ms mt mu bi translated">许多分区</li><li id="7a65" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr ok ms mt mu bi translated">应该在其上对数据进行分区的表列。</li><li id="88f3" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr ok ms mt mu bi translated">每个分区的位置信息</li></ol><p id="a104" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">JDBC可以推断出一个模式，因此<code class="fe ob oc od oe b">TableProvider</code>实现将从<code class="fe ob oc od oe b">inferSchema</code> API实现返回模式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="c8c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ob oc od oe b">ReadSupport</code>、<code class="fe ob oc od oe b">ScanBuilder</code>和<code class="fe ob oc od oe b">Scan</code>接口的实现是相似的。<code class="fe ob oc od oe b">Batch</code>规划分区的实施会有所不同。<code class="fe ob oc od oe b">JdbcBatch</code>将使用给定的输入来规划输入分区，即多个分区和分区列。它将获取分区列数据，并将其划分为若干分区。因此，JdbcInputPartition的每个实例都将获得一大块分区列数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="b1a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<code class="fe ob oc od oe b">JdbcInputPartition</code>的每个实例，将创建<code class="fe ob oc od oe b">JdbcPartitionReader</code>。反过来，读取器的每个实例将只获取表的一部分数据，并将这些id作为过滤器传递。每个分区读取并行运行，因此数据读取会更快。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="7418" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">什么是位置感知？</h2><p id="4acf" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">假设，我们正在为像HDFS这样的分布式、多节点、分区存储编写自定义数据源。如果我们在更靠近HDFS分区的地方运行数据读取，它的执行速度会快得多。如果它运行在同一个节点上，它将在本地读取数据，而不是通过网络读取数据。考虑到数据存储的局部性，运行每个数据源分区读取数据的方式越接近越好。</p><p id="7673" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分区位置信息可以通过<code class="fe ob oc od oe b">InputPartition</code>接口上的<code class="fe ob oc od oe b">preferredLocation</code> <strong class="ky ir"> </strong> API指定。它可以是主机地址或主机名。根据API文档，Apache spark试图在给定的位置运行分区，但不能保证这一点。在集群中的不同节点上运行之前，它使用<code class="fe ob oc od oe b"><strong class="ky ir">spark.locality.wait</strong></code> <strong class="ky ir"> </strong>配置。</p><p id="f31e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用<code class="fe ob oc od oe b">preferredLocation</code>在离数据源更近的地方运行数据获取并加快进程，但是不能保证局部性。因此，当分区在不同的节点上运行时，读取逻辑不应该是位置相关的，也不应该失败。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="479d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们在集群上运行它。这里分享了在windows上设置Apache Spark独立集群的步骤<a class="ae kv" href="https://medium.com/@aamargajbhiye/apache-spark-setup-a-multi-node-standalone-cluster-on-windows-63d413296971" rel="noopener"/></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="527b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个例子的完整源代码在这里分享<a class="ae kv" href="https://github.com/aamargajbhiye/big-data-projects/tree/master/Datasource%20spark3/src/main/java/com/bugdbug/customsource/jdbc" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="cb25" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">到目前为止我们讨论了什么？</h1><p id="16bc" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">我们讨论了如何创建单分区读数据源，以及如何创建多分区的位置感知数据源。</p><p id="137a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用<code class="fe ob oc od oe b">SupportsPushDownFilters</code>增加过滤能力和使用<code class="fe ob oc od oe b">SupportsPushDownRequiredColumns.</code>增加列修剪能力，可以进一步改进这个例子</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><p id="2ab8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读。你喜欢这篇文章吗？请在评论中告诉我你的想法。</p><p id="51b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一篇文章中，我们将讨论如何在Apache Spark 3中创建一个写数据源。</p><h1 id="918f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><div class="ol om gp gr on oo"><a href="http://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/sql/connector/read/package-summary.html" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">org . Apache . Spark . SQL . connector . read(Spark 3 . 0 . 0 JavaDoc)</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">编辑描述</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">spark.apache.org</p></div></div></div></a></div><div class="ol om gp gr on oo"><a href="https://github.com/aamargajbhiye/big-data-projects/tree/master/Datasource%20spark3" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">aamargajbhiye/大数据项目</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">github.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc kp oo"/></div></div></a></div></div></div>    
</body>
</html>