<html>
<head>
<title>K-Means Step-by-Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-表示循序渐进</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/k-means-manual-implementation-9f6fd9375b86?source=collection_archive---------5-----------------------#2019-01-24">https://levelup.gitconnected.com/k-means-manual-implementation-9f6fd9375b86?source=collection_archive---------5-----------------------#2019-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d76e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">聚类算法有助于将数据分组为相似的组或簇。有许多不同的聚类方法。在这篇文章中，我们来看看最流行的聚类算法之一:K-Means。该算法的目标是通过质心识别每个聚类。接下来的动机是理解K-Means算法是如何实现的。包含的代码不是为了用于生产，而是为了理解算法如何学习聚类。</p><p id="5285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们首先导入我们将使用的相关库。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="24b6" class="ku kv iq kq b gy kw kx l ky kz">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import KMeans</span></pre><p id="afe9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个算法都需要某种初始化。结果表明，K-Means对质心的初始化非常敏感。有很多方法可以初始化质心𝜇_k.，我们实现了文献中所说的<em class="la"> K-Means++ </em>初始化。我们通过随机选择一个数据点来初始化第一质心𝜇_1:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lb"><img src="../Images/0f07741488f7a5fad4757841f6acf485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4sUs2J0Rn9gql4yGaKBZjQ.png"/></div></div></figure><p id="73d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每个后续质心，我们希望选择一个数据点，该数据点很有可能远离前一个质心。假设已经选择了前k-1个质心:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/14b43bbf02b74988c2d25a35cf7bbb17.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*dx6_WKe8RmuH19YGu9ZKig.png"/></div></figure><p id="0ebe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们要选择下一个，让它大概率远离上一个。每次我们选择一个质心，我们从原始数据集中删除相应的数据点。</p><p id="39c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们开发了集合{1，2，…，N-k}上的分布𝜈(其中数据集已经被重新索引)。这种分布使得选择第I个数据点的概率与其自身和最后选择的质心之间的平方距离成比例。</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/1b71c8839c49c56ecf4ec04d0bc40a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*OYR-6od-vjDUkmqlIGmttg.png"/></div></figure><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="5fb8" class="ku kv iq kq b gy kw kx l ky kz">def initialize_centers(X,K):<br/>    '''Implements K-Means++ initialization given the data matrix X and the number of cluster K<br/>    Input:<br/>        X - shape (N, D) - data matrix containing N examples of D-dimensional data<br/>        K - a positive integer - the number of clusters<br/>    <br/>    Output:<br/>        mu - shape(K,D) - K centroids each of which is a D-dimensional vector<br/>    '''<br/>    <br/>    N,D = X.shape #find out the number of example and the dimesion of the data<br/>    mu = np.zeros((K,D)) #initialize the variable that will store the K (D-dimensional) centroids<br/>    idx = np.random.randint(0,X.shape[0]) #choose an index at random<br/>    mu[0] = X[idx] #assign the respective data point as the first centroid<br/>    X = np.delete(X,idx,axis=0) #remove the chosen data point from the data set.<br/>    for k in range(1,K): #for each of the subsequent clusters<br/>        #compute the square-norm of the different between each data point and the previous centroid<br/>        nu = np.linalg.norm(X-mu[k-1],axis=1)**2 <br/>        #divide these square-norms by their sum to get a distribution<br/>        nu = nu / np.sum(nu)<br/>        #choose an index at random from the distribution<br/>        idx = np.random.choice(np.array(range(X.shape[0])),1,replace=False,p=nu)<br/>        #assign the centroid of the k^th cluster as the respective data point<br/>        mu[k] = X[idx]<br/>        #remove the selected data point from the data matrix<br/>        X = np.delete(X,idx,axis=0)<br/>    #return the centroids<br/>    return mu</span></pre><p id="6939" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能知道，许多机器学习算法通过优化“好”(最大化)或“坏”(最小化)的某种衡量标准来工作。在K-Means算法中，我们定义了一个我们想要最小化的损失函数(“坏性”)。这个函数计算每个集群中的“坏性”。这是通过计算分配给一个聚类的每个数据点与该聚类的质心之间的平方距离来实现的。失真损耗就是所有这些平方距离的总和。因此，如果一个数据点离质心不够近，它将对损耗产生很大的影响。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="f26d" class="ku kv iq kq b gy kw kx l ky kz">def distorsion_loss(X,mu, Z):<br/>    '''Given the data matrix X, the current set of centroids mu<br/>    and the assignment-matrix Z (also called a latent variable),<br/>    computes the distrortion-loss<br/>    <br/>    Input:<br/>        X - shape (N, D) - data matrix containing N examples of D-dimensional data<br/>        mu - shape(K,D) - K centroids each of which is a D-dimensional vector<br/>        Z - shape (N, K) - each element in the row contains exactly one 1 and the rest zero<br/>            indicating the assignment of the respective data point to the cluster.<br/>            For example Z_{53}=1 means that the 5th data point belongs to cluster 3 (4th cluster)<br/>    '''<br/>    K = mu.shape[0] #get the number of cluster by looking at how many centroids there are<br/>    loss = 0 #initialize the loss to zero<br/>    for k in range(K): #for each cluster<br/>        #get the data points assigned to that cluster<br/>        X_k = X[Z[:,k]==1]<br/>        #compute the square-dstiance between each of these data points and the centroid<br/>        D=np.linalg.norm(X_k-mu[k],axis = 1)**2<br/>        #accumulate the loss<br/>        loss += np.sum(D)<br/>    #return<br/>    return loss</span></pre><p id="9d8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在已经准备好训练K-Means算法。培训将按如下方式进行:</p><ol class=""><li id="4be3" class="ll lm iq jp b jq jr ju jv jy ln kc lo kg lp kk lq lr ls lt bi translated">初始化质心:mu_init = initialize_centers(X，K)</li><li id="21d5" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">更新分配矩阵。每个数据点被分配到最近的质心。</li><li id="6afe" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">更新质心。新的质心是它们各自聚类内的数据点的平均值。</li><li id="16d2" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">记录distorion_loss。如果当前损失和先前损失之间的变化小于容差，或者我们已经达到最大迭代次数，则停止，否则转到步骤2。</li></ol><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="4d10" class="ku kv iq kq b gy kw kx l ky kz">def train_kmeans(X,K,mu_init,max_iter=10,tol=1e-4):<br/>    '''Train the K-Means algorithm using the data matrix X, the number of clusters K,<br/>    and the initial centroids mu_init<br/>    <br/>    Input:<br/>        X - shape (N, D) - data matrix containing N examples of D-dimensional data<br/>        K - a positive integer - the number of clusters<br/>        mu_init - shape(K,D) - initial K centroids each of which is a D-dimensional vector<br/>        max_iter - positive integer (default 10) the maximum number of iterations<br/>        tol - positive number (default 1e-4) the tolerance which determines that<br/>                the distorion_loss has converged.<br/>    '''<br/>    N = X.shape[0] #read in the number of data points<br/>    mu = mu_init #initialize mu with mu_init<br/>    loss_trace = [] #initalize an array to keep track of the losses<br/>    mu_trace = [] #initialize an array to keep track of the centroids<br/>    Z_trace = [] #initialize an array to keep track of the assignment matrices<br/>    for it in range(max_iter): #for each iteration<br/>        #Update cluster indicators<br/>        Z = np.zeros((N,K)) #initialize the assignment matrix<br/>        for i in range(N): #for each data point<br/>            #compute the square distance between the datapoint and all the centroids<br/>            d = np.linalg.norm(X[i]-mu,axis=1)**2<br/>            #select the index of the smallest distance (corresponding to the closest centroid)<br/>            j = np.argmin(d)<br/>            #update the assingment matrix by assigning the i^th data point to the j^th cluster<br/>            Z[i,j]=1<br/><br/>        #Update centroids<br/>        N_k = np.sum(Z,axis=0) #count the number of data points in each cluster<br/>        for j in range(K): #for each cluster<br/>            #sum up the data points<br/>            mu[j] = np.sum(X[Z[:,j]==1],axis=0)<br/>            if N_k[j] &gt; 0: #if there is at least one data point in the cluster<br/>                mu[j] = mu[j] / N_k[j] #divide to get the average, the centroid<br/>        loss_ = distorsion_loss(X,mu,Z) #compute the loss<br/>        loss_trace.append(loss_) #record the loss<br/>        mu_trace.append(mu) #record the current value of mu<br/>        Z_trace.append(Z) #record the current assignment matrix<br/>        if len(loss_trace) &gt; 1: #if we have at least 2 iterations<br/>            if abs(loss_trace[-2]-loss_trace[-1]) &lt; tol: #check convergence<br/>                return mu, Z, loss_trace, mu_trace, Z_trace #return if loss has converged<br/>    #return<br/>    return mu, Z, loss_trace, mu_trace, Z_trace</span></pre><p id="935c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们找到了。K均值算法。让我们生成一些玩具数据来看看K-Means的表现。</p><p id="5524" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将生成以下列点为中心的3个集群:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/2565c02fd64d499edca0a765bb6b1637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*WCaPxwqezvBnpIgm2Y8N_A.png"/></div></figure><p id="7cf0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些中心周围的所有数据点将根据具有单位(各向同性)协方差矩阵的多元正态分布进行分布:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ma"><img src="../Images/326465aee216c22f56dcceb05bf2ce09.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*KO131po4Wb16v4q7NmediA.png"/></div></div></figure><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="729d" class="ku kv iq kq b gy kw kx l ky kz">#centroids<br/>mu_1_actual = np.array([2,2])<br/>mu_2_actual = np.array([-2,-2])<br/>mu_3_actual = np.array([10,0])<br/>#covariance matrix<br/>sigma = np.eye(2)<br/><br/>#Clusters<br/>X1 = np.random.multivariate_normal(mu_1_actual,sigma,100)<br/>X2 = np.random.multivariate_normal(mu_2_actual,sigma,100)<br/>X3 = np.random.multivariate_normal(mu_3_actual,sigma,100)<br/><br/>#Data matrix:<br/>X = np.concatenate([X1,X2,X3],axis=0)</span><span id="b112" class="ku kv iq kq b gy mb kx l ky kz">#Visualize the data and visualize the actual clusters.<br/><br/>fig = plt.figure(figsize=(20,5))<br/>plt.subplot(121)<br/>plt.scatter(X[:,0],X[:,1])<br/>plt.subplot(122)<br/>plt.scatter(X1[:,0],X1[:,1],c='r')<br/>plt.scatter(X2[:,0],X2[:,1],c='g')<br/>plt.scatter(X3[:,0],X3[:,1],c='b')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/8719a813883d7861266d427010c0b193.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MJ0Txx9D_d5IfG1ZRB_P_A.png"/></div></figure><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="89af" class="ku kv iq kq b gy kw kx l ky kz">mu_init = initialize_centers(X,3)<br/>mu,Z,loss_trace, mu_trace, Z_trace =train_kmeans(X,3,mu_init,max_iter=300,tol=1e-4)</span><span id="11a7" class="ku kv iq kq b gy mb kx l ky kz">fig = plt.figure(figsize=(20,5))<br/>plt.subplot(121)<br/>plt.scatter((X[Z[:,0]==1])[:,0],(X[Z[:,0]==1])[:,1],color='r')<br/>plt.scatter((X[Z[:,1]==1])[:,0],(X[Z[:,1]==1])[:,1],color='g')<br/>plt.scatter((X[Z[:,2]==1])[:,0],(X[Z[:,2]==1])[:,1],color='b')<br/>plt.scatter(mu[:,0],mu[:,1],color='black',marker='o',s=200,alpha=0.75)<br/>plt.title('K-Means with K=3')<br/>plt.xlabel('x_1')<br/>plt.ylabel('x_2')<br/>plt.subplot(122)<br/>plt.plot(range(len(loss_trace)),loss_trace)<br/>plt.title('Distorion Loss')<br/>plt.xlabel('Iteration')<br/>plt.ylabel('Loss')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/8b93f9eba0cbbe8ae5de880e34f3c6b2.png" data-original-src="https://miro.medium.com/v2/format:webp/1*eRSPKQMr0hgya4ud2rARrQ.png"/></div></figure><p id="eac6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还不错。看起来算法收敛了一些迭代。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="895c" class="ku kv iq kq b gy kw kx l ky kz">fig = plt.figure(figsize=(20,25))<br/>for i in range(len(loss_trace)):<br/>    plt.subplot(521+i)<br/>    Z_i = Z_trace[i]<br/>    mu_i = mu_trace[i]<br/>    plt.scatter((X[Z_i[:,0]==1])[:,0],(X[Z_i[:,0]==1])[:,1],color='r')<br/>    plt.scatter((X[Z_i[:,1]==1])[:,0],(X[Z_i[:,1]==1])[:,1],color='g')<br/>    plt.scatter((X[Z_i[:,2]==1])[:,0],(X[Z_i[:,2]==1])[:,1],color='b')<br/>    plt.scatter(mu_i[:,0],mu_i[:,1],color='black',marker='o',s=200, alpha=0.75)<br/>    plt.title('Iteration:' + str(i+1) +'\nLoss:'+str(round(100*loss_trace[i])/100))<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/cb80c2512837e14e9c13ff7357ebed2b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*TwCdmtxDGrRNBAYurUP-XA.png"/></div></figure><p id="f239" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在事情可能没这么美好了。K-Means对初始化非常敏感。事实上，如果我们用不同的初始质心再运行一次，它看起来可能不会这么漂亮。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="7c2b" class="ku kv iq kq b gy kw kx l ky kz">mu_init2 = initialize_centers(X,3)<br/>mu2,Z2,loss_trace2, mu_trace2, Z_trace2 =train_kmeans(X,3,mu_init2,max_iter=300,tol=1e-4)</span><span id="6a09" class="ku kv iq kq b gy mb kx l ky kz">fig = plt.figure(figsize=(20,5))<br/>plt.subplot(121)<br/>plt.scatter((X[Z2[:,0]==1])[:,0],(X[Z2[:,0]==1])[:,1],color='r')<br/>plt.scatter((X[Z2[:,1]==1])[:,0],(X[Z2[:,1]==1])[:,1],color='g')<br/>plt.scatter((X[Z2[:,2]==1])[:,0],(X[Z2[:,2]==1])[:,1],color='b')<br/>plt.scatter(mu2[:,0],mu2[:,1],color='black',marker='o',s=200,alpha=0.75)<br/>plt.title('K-Means with K=3')<br/>plt.xlabel('x_1')<br/>plt.ylabel('x_2')<br/>plt.subplot(122)<br/>plt.plot(range(len(loss_trace2)),loss_trace2)<br/>plt.title('Distorion Loss')<br/>plt.xlabel('Iteration')<br/>plt.ylabel('Loss')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/9759cd832edf71204847be65c6a06038.png" data-original-src="https://miro.medium.com/v2/format:webp/1*cfvl4M34X5O8V4HNsrGI2A.png"/></div></figure><p id="fecb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意算法如何收敛到更高的损失值，并且聚类不是我们想要的。现在在实践中，我们不知道集群应该是什么样子，这就是为什么我们首先要运行这个算法。那么我们如何克服这个障碍呢？实际发生的是distortion_loss函数有许多局部最小值。此外，K-Means算法不能保证收敛到全局最小值。因此，在实践中，我们将使用不同的初始值运行K-Means几次，并选择具有最低收敛损失的结果。正如你在上面看到的，损失集中在587左右，而在第二次运行中，它集中在2200左右。</p><h2 id="6e2a" class="ku kv iq bd md me mf dn mg mh mi dp mj jy mk ml mm kc mn mo mp kg mq mr ms mt bi translated">k-表示使用Sci-Kit学习</h2><p id="fb1a" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy mw ka kb kc mx ke kf kg my ki kj kk ij bi translated">为了进行比较，让我们使用机器学习领域的黄金标准SciKit-Learn软件包来运行K-Means算法。这实际上做了我们需要的。它会多次运行K-Means，每次使用不同的初始值，然后选择最佳结果。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="fb38" class="ku kv iq kq b gy kw kx l ky kz">kmeans = KMeans(n_clusters=3, random_state=0,verbose=0).fit(X)</span><span id="2bf5" class="ku kv iq kq b gy mb kx l ky kz">clusters=kmeans.predict(X)<br/>cluster_centers = kmeans.cluster_centers_<br/>Z_sklearn = np.zeros((X.shape[0],3))<br/>for i in range(X.shape[0]):<br/>    Z_sklearn[i,clusters[i]]=1</span><span id="5f3f" class="ku kv iq kq b gy mb kx l ky kz">plt.scatter((X[Z_sklearn[:,0]==1])[:,0],(X[Z_sklearn[:,0]==1])[:,1],color='r')<br/>plt.scatter((X[Z_sklearn[:,1]==1])[:,0],(X[Z_sklearn[:,1]==1])[:,1],color='g')<br/>plt.scatter((X[Z_sklearn[:,2]==1])[:,0],(X[Z_sklearn[:,2]==1])[:,1],color='b')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/42715bd48ef314d1295c877c4aa66c95.png" data-original-src="https://miro.medium.com/v2/format:webp/1*dUWUoB6gS8hKskwWCmObFQ.png"/></div></figure><p id="f0a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">顺便说一下，有一种更简单的方法来给集群着色..</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="dd28" class="ku kv iq kq b gy kw kx l ky kz">plt.scatter(X[:,0],X[:,1],c=clusters)<br/>plt.scatter(cluster_centers[:,0],cluster_centers[:,1],color='black',marker='o',s=200,alpha=0.75)<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/189436aab46fbfe19401dd848f781678.png" data-original-src="https://miro.medium.com/v2/format:webp/1*l6btJA3BKBSSvOHDFqNvkg.png"/></div></figure><h2 id="7e5f" class="ku kv iq bd md me mf dn mg mh mi dp mj jy mk ml mm kc mn mo mp kg mq mr ms mt bi translated">比较结果…</h2><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="d333" class="ku kv iq kq b gy kw kx l ky kz">#converged loss from sklearn<br/>distorsion_loss(X,cluster_centers,Z_sklearn)</span><span id="e0bc" class="ku kv iq kq b gy mb kx l ky kz">586.9973311742276</span><span id="8427" class="ku kv iq kq b gy mb kx l ky kz">#converged loss from our good result</span><span id="7099" class="ku kv iq kq b gy mb kx l ky kz">distorsion_loss(X,mu,Z)</span><span id="aaeb" class="ku kv iq kq b gy mb kx l ky kz">586.9973311742275</span><span id="1dc2" class="ku kv iq kq b gy mb kx l ky kz">#difference between our centroids and the ones found by sklearn<br/>np.linalg.norm(np.sort(mu,axis=0)-np.sort(cluster_centers,axis=0),axis=1)</span><span id="252a" class="ku kv iq kq b gy mb kx l ky kz">array([1.25607397e-15, 5.20417043e-18, 1.83102672e-15])</span></pre><p id="06d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">都几乎为零。</p><p id="2060" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望这篇文章对K-Means有所启发，K-Means是当今无监督学习中最流行的聚类算法之一。在我的下一篇文章中，我们将考虑使用高斯混合的软聚类算法。</p><p id="be22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是完整的jupyter笔记本。</p><p id="956d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你喜欢这篇文章，请花点时间给我一些掌声。</p></div></div>    
</body>
</html>