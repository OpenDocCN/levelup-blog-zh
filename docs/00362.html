<html>
<head>
<title>K-Means Step-by-Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-è¡¨ç¤ºå¾ªåºæ¸è¿›</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://levelup.gitconnected.com/k-means-manual-implementation-9f6fd9375b86?source=collection_archive---------5-----------------------#2019-01-24">https://levelup.gitconnected.com/k-means-manual-implementation-9f6fd9375b86?source=collection_archive---------5-----------------------#2019-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d76e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">èšç±»ç®—æ³•æœ‰åŠ©äºå°†æ•°æ®åˆ†ç»„ä¸ºç›¸ä¼¼çš„ç»„æˆ–ç°‡ã€‚æœ‰è®¸å¤šä¸åŒçš„èšç±»æ–¹æ³•ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹æœ€æµè¡Œçš„èšç±»ç®—æ³•ä¹‹ä¸€:K-Meansã€‚è¯¥ç®—æ³•çš„ç›®æ ‡æ˜¯é€šè¿‡è´¨å¿ƒè¯†åˆ«æ¯ä¸ªèšç±»ã€‚æ¥ä¸‹æ¥çš„åŠ¨æœºæ˜¯ç†è§£K-Meansç®—æ³•æ˜¯å¦‚ä½•å®ç°çš„ã€‚åŒ…å«çš„ä»£ç ä¸æ˜¯ä¸ºäº†ç”¨äºç”Ÿäº§ï¼Œè€Œæ˜¯ä¸ºäº†ç†è§£ç®—æ³•å¦‚ä½•å­¦ä¹ èšç±»ã€‚</p><p id="5285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æˆ‘ä»¬å°†ä½¿ç”¨çš„ç›¸å…³åº“ã€‚</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="24b6" class="ku kv iq kq b gy kw kx l ky kz">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import KMeans</span></pre><p id="afe9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ¯ä¸ªç®—æ³•éƒ½éœ€è¦æŸç§åˆå§‹åŒ–ã€‚ç»“æœè¡¨æ˜ï¼ŒK-Meanså¯¹è´¨å¿ƒçš„åˆå§‹åŒ–éå¸¸æ•æ„Ÿã€‚æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥åˆå§‹åŒ–è´¨å¿ƒğœ‡_k.ï¼Œæˆ‘ä»¬å®ç°äº†æ–‡çŒ®ä¸­æ‰€è¯´çš„<em class="la"> K-Means++ </em>åˆå§‹åŒ–ã€‚æˆ‘ä»¬é€šè¿‡éšæœºé€‰æ‹©ä¸€ä¸ªæ•°æ®ç‚¹æ¥åˆå§‹åŒ–ç¬¬ä¸€è´¨å¿ƒğœ‡_1:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lb"><img src="../Images/0f07741488f7a5fad4757841f6acf485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4sUs2J0Rn9gql4yGaKBZjQ.png"/></div></div></figure><p id="73d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å¯¹äºæ¯ä¸ªåç»­è´¨å¿ƒï¼Œæˆ‘ä»¬å¸Œæœ›é€‰æ‹©ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œè¯¥æ•°æ®ç‚¹å¾ˆæœ‰å¯èƒ½è¿œç¦»å‰ä¸€ä¸ªè´¨å¿ƒã€‚å‡è®¾å·²ç»é€‰æ‹©äº†å‰k-1ä¸ªè´¨å¿ƒ:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/14b43bbf02b74988c2d25a35cf7bbb17.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*dx6_WKe8RmuH19YGu9ZKig.png"/></div></figure><p id="0ebe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘ä»¬è¦é€‰æ‹©ä¸‹ä¸€ä¸ªï¼Œè®©å®ƒå¤§æ¦‚ç‡è¿œç¦»ä¸Šä¸€ä¸ªã€‚æ¯æ¬¡æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªè´¨å¿ƒï¼Œæˆ‘ä»¬ä»åŸå§‹æ•°æ®é›†ä¸­åˆ é™¤ç›¸åº”çš„æ•°æ®ç‚¹ã€‚</p><p id="39c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†é›†åˆ{1ï¼Œ2ï¼Œâ€¦ï¼ŒN-k}ä¸Šçš„åˆ†å¸ƒğœˆ(å…¶ä¸­æ•°æ®é›†å·²ç»è¢«é‡æ–°ç´¢å¼•)ã€‚è¿™ç§åˆ†å¸ƒä½¿å¾—é€‰æ‹©ç¬¬Iä¸ªæ•°æ®ç‚¹çš„æ¦‚ç‡ä¸å…¶è‡ªèº«å’Œæœ€åé€‰æ‹©çš„è´¨å¿ƒä¹‹é—´çš„å¹³æ–¹è·ç¦»æˆæ¯”ä¾‹ã€‚</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/1b71c8839c49c56ecf4ec04d0bc40a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*OYR-6od-vjDUkmqlIGmttg.png"/></div></figure><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="5fb8" class="ku kv iq kq b gy kw kx l ky kz">def initialize_centers(X,K):<br/>    '''Implements K-Means++ initialization given the data matrix X and the number of cluster K<br/>    Input:<br/>        X - shape (N, D) - data matrix containing N examples of D-dimensional data<br/>        K - a positive integer - the number of clusters<br/>    <br/>    Output:<br/>        mu - shape(K,D) - K centroids each of which is a D-dimensional vector<br/>    '''<br/>    <br/>    N,D = X.shape #find out the number of example and the dimesion of the data<br/>    mu = np.zeros((K,D)) #initialize the variable that will store the K (D-dimensional) centroids<br/>    idx = np.random.randint(0,X.shape[0]) #choose an index at random<br/>    mu[0] = X[idx] #assign the respective data point as the first centroid<br/>    X = np.delete(X,idx,axis=0) #remove the chosen data point from the data set.<br/>    for k in range(1,K): #for each of the subsequent clusters<br/>        #compute the square-norm of the different between each data point and the previous centroid<br/>        nu = np.linalg.norm(X-mu[k-1],axis=1)**2 <br/>        #divide these square-norms by their sum to get a distribution<br/>        nu = nu / np.sum(nu)<br/>        #choose an index at random from the distribution<br/>        idx = np.random.choice(np.array(range(X.shape[0])),1,replace=False,p=nu)<br/>        #assign the centroid of the k^th cluster as the respective data point<br/>        mu[k] = X[idx]<br/>        #remove the selected data point from the data matrix<br/>        X = np.delete(X,idx,axis=0)<br/>    #return the centroids<br/>    return mu</span></pre><p id="6939" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä½ å¯èƒ½çŸ¥é“ï¼Œè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡ä¼˜åŒ–â€œå¥½â€(æœ€å¤§åŒ–)æˆ–â€œåâ€(æœ€å°åŒ–)çš„æŸç§è¡¡é‡æ ‡å‡†æ¥å·¥ä½œã€‚åœ¨K-Meansç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦æœ€å°åŒ–çš„æŸå¤±å‡½æ•°(â€œåæ€§â€)ã€‚è¿™ä¸ªå‡½æ•°è®¡ç®—æ¯ä¸ªé›†ç¾¤ä¸­çš„â€œåæ€§â€ã€‚è¿™æ˜¯é€šè¿‡è®¡ç®—åˆ†é…ç»™ä¸€ä¸ªèšç±»çš„æ¯ä¸ªæ•°æ®ç‚¹ä¸è¯¥èšç±»çš„è´¨å¿ƒä¹‹é—´çš„å¹³æ–¹è·ç¦»æ¥å®ç°çš„ã€‚å¤±çœŸæŸè€—å°±æ˜¯æ‰€æœ‰è¿™äº›å¹³æ–¹è·ç¦»çš„æ€»å’Œã€‚å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªæ•°æ®ç‚¹ç¦»è´¨å¿ƒä¸å¤Ÿè¿‘ï¼Œå®ƒå°†å¯¹æŸè€—äº§ç”Ÿå¾ˆå¤§çš„å½±å“ã€‚</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="f26d" class="ku kv iq kq b gy kw kx l ky kz">def distorsion_loss(X,mu, Z):<br/>    '''Given the data matrix X, the current set of centroids mu<br/>    and the assignment-matrix Z (also called a latent variable),<br/>    computes the distrortion-loss<br/>    <br/>    Input:<br/>        X - shape (N, D) - data matrix containing N examples of D-dimensional data<br/>        mu - shape(K,D) - K centroids each of which is a D-dimensional vector<br/>        Z - shape (N, K) - each element in the row contains exactly one 1 and the rest zero<br/>            indicating the assignment of the respective data point to the cluster.<br/>            For example Z_{53}=1 means that the 5th data point belongs to cluster 3 (4th cluster)<br/>    '''<br/>    K = mu.shape[0] #get the number of cluster by looking at how many centroids there are<br/>    loss = 0 #initialize the loss to zero<br/>    for k in range(K): #for each cluster<br/>        #get the data points assigned to that cluster<br/>        X_k = X[Z[:,k]==1]<br/>        #compute the square-dstiance between each of these data points and the centroid<br/>        D=np.linalg.norm(X_k-mu[k],axis = 1)**2<br/>        #accumulate the loss<br/>        loss += np.sum(D)<br/>    #return<br/>    return loss</span></pre><p id="9d8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘ä»¬ç°åœ¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒK-Meansç®—æ³•ã€‚åŸ¹è®­å°†æŒ‰å¦‚ä¸‹æ–¹å¼è¿›è¡Œ:</p><ol class=""><li id="4be3" class="ll lm iq jp b jq jr ju jv jy ln kc lo kg lp kk lq lr ls lt bi translated">åˆå§‹åŒ–è´¨å¿ƒ:mu_init = initialize_centers(Xï¼ŒK)</li><li id="21d5" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">æ›´æ–°åˆ†é…çŸ©é˜µã€‚æ¯ä¸ªæ•°æ®ç‚¹è¢«åˆ†é…åˆ°æœ€è¿‘çš„è´¨å¿ƒã€‚</li><li id="6afe" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">æ›´æ–°è´¨å¿ƒã€‚æ–°çš„è´¨å¿ƒæ˜¯å®ƒä»¬å„è‡ªèšç±»å†…çš„æ•°æ®ç‚¹çš„å¹³å‡å€¼ã€‚</li><li id="16d2" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">è®°å½•distorion_lossã€‚å¦‚æœå½“å‰æŸå¤±å’Œå…ˆå‰æŸå¤±ä¹‹é—´çš„å˜åŒ–å°äºå®¹å·®ï¼Œæˆ–è€…æˆ‘ä»¬å·²ç»è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåˆ™åœæ­¢ï¼Œå¦åˆ™è½¬åˆ°æ­¥éª¤2ã€‚</li></ol><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="4d10" class="ku kv iq kq b gy kw kx l ky kz">def train_kmeans(X,K,mu_init,max_iter=10,tol=1e-4):<br/>    '''Train the K-Means algorithm using the data matrix X, the number of clusters K,<br/>    and the initial centroids mu_init<br/>    <br/>    Input:<br/>        X - shape (N, D) - data matrix containing N examples of D-dimensional data<br/>        K - a positive integer - the number of clusters<br/>        mu_init - shape(K,D) - initial K centroids each of which is a D-dimensional vector<br/>        max_iter - positive integer (default 10) the maximum number of iterations<br/>        tol - positive number (default 1e-4) the tolerance which determines that<br/>                the distorion_loss has converged.<br/>    '''<br/>    N = X.shape[0] #read in the number of data points<br/>    mu = mu_init #initialize mu with mu_init<br/>    loss_trace = [] #initalize an array to keep track of the losses<br/>    mu_trace = [] #initialize an array to keep track of the centroids<br/>    Z_trace = [] #initialize an array to keep track of the assignment matrices<br/>    for it in range(max_iter): #for each iteration<br/>        #Update cluster indicators<br/>        Z = np.zeros((N,K)) #initialize the assignment matrix<br/>        for i in range(N): #for each data point<br/>            #compute the square distance between the datapoint and all the centroids<br/>            d = np.linalg.norm(X[i]-mu,axis=1)**2<br/>            #select the index of the smallest distance (corresponding to the closest centroid)<br/>            j = np.argmin(d)<br/>            #update the assingment matrix by assigning the i^th data point to the j^th cluster<br/>            Z[i,j]=1<br/><br/>        #Update centroids<br/>        N_k = np.sum(Z,axis=0) #count the number of data points in each cluster<br/>        for j in range(K): #for each cluster<br/>            #sum up the data points<br/>            mu[j] = np.sum(X[Z[:,j]==1],axis=0)<br/>            if N_k[j] &gt; 0: #if there is at least one data point in the cluster<br/>                mu[j] = mu[j] / N_k[j] #divide to get the average, the centroid<br/>        loss_ = distorsion_loss(X,mu,Z) #compute the loss<br/>        loss_trace.append(loss_) #record the loss<br/>        mu_trace.append(mu) #record the current value of mu<br/>        Z_trace.append(Z) #record the current assignment matrix<br/>        if len(loss_trace) &gt; 1: #if we have at least 2 iterations<br/>            if abs(loss_trace[-2]-loss_trace[-1]) &lt; tol: #check convergence<br/>                return mu, Z, loss_trace, mu_trace, Z_trace #return if loss has converged<br/>    #return<br/>    return mu, Z, loss_trace, mu_trace, Z_trace</span></pre><p id="935c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘ä»¬æ‰¾åˆ°äº†ã€‚Kå‡å€¼ç®—æ³•ã€‚è®©æˆ‘ä»¬ç”Ÿæˆä¸€äº›ç©å…·æ•°æ®æ¥çœ‹çœ‹K-Meansçš„è¡¨ç°ã€‚</p><p id="5524" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘ä»¬å°†ç”Ÿæˆä»¥ä¸‹åˆ—ç‚¹ä¸ºä¸­å¿ƒçš„3ä¸ªé›†ç¾¤:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/2565c02fd64d499edca0a765bb6b1637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*WCaPxwqezvBnpIgm2Y8N_A.png"/></div></figure><p id="7cf0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¿™äº›ä¸­å¿ƒå‘¨å›´çš„æ‰€æœ‰æ•°æ®ç‚¹å°†æ ¹æ®å…·æœ‰å•ä½(å„å‘åŒæ€§)åæ–¹å·®çŸ©é˜µçš„å¤šå…ƒæ­£æ€åˆ†å¸ƒè¿›è¡Œåˆ†å¸ƒ:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ma"><img src="../Images/326465aee216c22f56dcceb05bf2ce09.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*KO131po4Wb16v4q7NmediA.png"/></div></div></figure><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="729d" class="ku kv iq kq b gy kw kx l ky kz">#centroids<br/>mu_1_actual = np.array([2,2])<br/>mu_2_actual = np.array([-2,-2])<br/>mu_3_actual = np.array([10,0])<br/>#covariance matrix<br/>sigma = np.eye(2)<br/><br/>#Clusters<br/>X1 = np.random.multivariate_normal(mu_1_actual,sigma,100)<br/>X2 = np.random.multivariate_normal(mu_2_actual,sigma,100)<br/>X3 = np.random.multivariate_normal(mu_3_actual,sigma,100)<br/><br/>#Data matrix:<br/>X = np.concatenate([X1,X2,X3],axis=0)</span><span id="b112" class="ku kv iq kq b gy mb kx l ky kz">#Visualize the data and visualize the actual clusters.<br/><br/>fig = plt.figure(figsize=(20,5))<br/>plt.subplot(121)<br/>plt.scatter(X[:,0],X[:,1])<br/>plt.subplot(122)<br/>plt.scatter(X1[:,0],X1[:,1],c='r')<br/>plt.scatter(X2[:,0],X2[:,1],c='g')<br/>plt.scatter(X3[:,0],X3[:,1],c='b')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/8719a813883d7861266d427010c0b193.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MJ0Txx9D_d5IfG1ZRB_P_A.png"/></div></figure><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="89af" class="ku kv iq kq b gy kw kx l ky kz">mu_init = initialize_centers(X,3)<br/>mu,Z,loss_trace, mu_trace, Z_trace =train_kmeans(X,3,mu_init,max_iter=300,tol=1e-4)</span><span id="11a7" class="ku kv iq kq b gy mb kx l ky kz">fig = plt.figure(figsize=(20,5))<br/>plt.subplot(121)<br/>plt.scatter((X[Z[:,0]==1])[:,0],(X[Z[:,0]==1])[:,1],color='r')<br/>plt.scatter((X[Z[:,1]==1])[:,0],(X[Z[:,1]==1])[:,1],color='g')<br/>plt.scatter((X[Z[:,2]==1])[:,0],(X[Z[:,2]==1])[:,1],color='b')<br/>plt.scatter(mu[:,0],mu[:,1],color='black',marker='o',s=200,alpha=0.75)<br/>plt.title('K-Means with K=3')<br/>plt.xlabel('x_1')<br/>plt.ylabel('x_2')<br/>plt.subplot(122)<br/>plt.plot(range(len(loss_trace)),loss_trace)<br/>plt.title('Distorion Loss')<br/>plt.xlabel('Iteration')<br/>plt.ylabel('Loss')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/8b93f9eba0cbbe8ae5de880e34f3c6b2.png" data-original-src="https://miro.medium.com/v2/format:webp/1*eRSPKQMr0hgya4ud2rARrQ.png"/></div></figure><p id="eac6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¿˜ä¸é”™ã€‚çœ‹èµ·æ¥ç®—æ³•æ”¶æ•›äº†ä¸€äº›è¿­ä»£ã€‚</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="895c" class="ku kv iq kq b gy kw kx l ky kz">fig = plt.figure(figsize=(20,25))<br/>for i in range(len(loss_trace)):<br/>    plt.subplot(521+i)<br/>    Z_i = Z_trace[i]<br/>    mu_i = mu_trace[i]<br/>    plt.scatter((X[Z_i[:,0]==1])[:,0],(X[Z_i[:,0]==1])[:,1],color='r')<br/>    plt.scatter((X[Z_i[:,1]==1])[:,0],(X[Z_i[:,1]==1])[:,1],color='g')<br/>    plt.scatter((X[Z_i[:,2]==1])[:,0],(X[Z_i[:,2]==1])[:,1],color='b')<br/>    plt.scatter(mu_i[:,0],mu_i[:,1],color='black',marker='o',s=200, alpha=0.75)<br/>    plt.title('Iteration:' + str(i+1) +'\nLoss:'+str(round(100*loss_trace[i])/100))<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/cb80c2512837e14e9c13ff7357ebed2b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*TwCdmtxDGrRNBAYurUP-XA.png"/></div></figure><p id="f239" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ç°åœ¨äº‹æƒ…å¯èƒ½æ²¡è¿™ä¹ˆç¾å¥½äº†ã€‚K-Meanså¯¹åˆå§‹åŒ–éå¸¸æ•æ„Ÿã€‚äº‹å®ä¸Šï¼Œå¦‚æœæˆ‘ä»¬ç”¨ä¸åŒçš„åˆå§‹è´¨å¿ƒå†è¿è¡Œä¸€æ¬¡ï¼Œå®ƒçœ‹èµ·æ¥å¯èƒ½ä¸ä¼šè¿™ä¹ˆæ¼‚äº®ã€‚</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="7c2b" class="ku kv iq kq b gy kw kx l ky kz">mu_init2 = initialize_centers(X,3)<br/>mu2,Z2,loss_trace2, mu_trace2, Z_trace2 =train_kmeans(X,3,mu_init2,max_iter=300,tol=1e-4)</span><span id="6a09" class="ku kv iq kq b gy mb kx l ky kz">fig = plt.figure(figsize=(20,5))<br/>plt.subplot(121)<br/>plt.scatter((X[Z2[:,0]==1])[:,0],(X[Z2[:,0]==1])[:,1],color='r')<br/>plt.scatter((X[Z2[:,1]==1])[:,0],(X[Z2[:,1]==1])[:,1],color='g')<br/>plt.scatter((X[Z2[:,2]==1])[:,0],(X[Z2[:,2]==1])[:,1],color='b')<br/>plt.scatter(mu2[:,0],mu2[:,1],color='black',marker='o',s=200,alpha=0.75)<br/>plt.title('K-Means with K=3')<br/>plt.xlabel('x_1')<br/>plt.ylabel('x_2')<br/>plt.subplot(122)<br/>plt.plot(range(len(loss_trace2)),loss_trace2)<br/>plt.title('Distorion Loss')<br/>plt.xlabel('Iteration')<br/>plt.ylabel('Loss')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/9759cd832edf71204847be65c6a06038.png" data-original-src="https://miro.medium.com/v2/format:webp/1*cfvl4M34X5O8V4HNsrGI2A.png"/></div></figure><p id="fecb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¯·æ³¨æ„ç®—æ³•å¦‚ä½•æ”¶æ•›åˆ°æ›´é«˜çš„æŸå¤±å€¼ï¼Œå¹¶ä¸”èšç±»ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ç°åœ¨åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“é›†ç¾¤åº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é¦–å…ˆè¦è¿è¡Œè¿™ä¸ªç®—æ³•ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å…‹æœè¿™ä¸ªéšœç¢å‘¢ï¼Ÿå®é™…å‘ç”Ÿçš„æ˜¯distortion_losså‡½æ•°æœ‰è®¸å¤šå±€éƒ¨æœ€å°å€¼ã€‚æ­¤å¤–ï¼ŒK-Meansç®—æ³•ä¸èƒ½ä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚å› æ­¤ï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„åˆå§‹å€¼è¿è¡ŒK-Meanså‡ æ¬¡ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€ä½æ”¶æ•›æŸå¤±çš„ç»“æœã€‚æ­£å¦‚ä½ åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼ŒæŸå¤±é›†ä¸­åœ¨587å·¦å³ï¼Œè€Œåœ¨ç¬¬äºŒæ¬¡è¿è¡Œä¸­ï¼Œå®ƒé›†ä¸­åœ¨2200å·¦å³ã€‚</p><h2 id="6e2a" class="ku kv iq bd md me mf dn mg mh mi dp mj jy mk ml mm kc mn mo mp kg mq mr ms mt bi translated">k-è¡¨ç¤ºä½¿ç”¨Sci-Kitå­¦ä¹ </h2><p id="fb1a" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy mw ka kb kc mx ke kf kg my ki kj kk ij bi translated">ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„é»„é‡‘æ ‡å‡†SciKit-Learnè½¯ä»¶åŒ…æ¥è¿è¡ŒK-Meansç®—æ³•ã€‚è¿™å®é™…ä¸Šåšäº†æˆ‘ä»¬éœ€è¦çš„ã€‚å®ƒä¼šå¤šæ¬¡è¿è¡ŒK-Meansï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„åˆå§‹å€¼ï¼Œç„¶åé€‰æ‹©æœ€ä½³ç»“æœã€‚</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="fb38" class="ku kv iq kq b gy kw kx l ky kz">kmeans = KMeans(n_clusters=3, random_state=0,verbose=0).fit(X)</span><span id="2bf5" class="ku kv iq kq b gy mb kx l ky kz">clusters=kmeans.predict(X)<br/>cluster_centers = kmeans.cluster_centers_<br/>Z_sklearn = np.zeros((X.shape[0],3))<br/>for i in range(X.shape[0]):<br/>    Z_sklearn[i,clusters[i]]=1</span><span id="5f3f" class="ku kv iq kq b gy mb kx l ky kz">plt.scatter((X[Z_sklearn[:,0]==1])[:,0],(X[Z_sklearn[:,0]==1])[:,1],color='r')<br/>plt.scatter((X[Z_sklearn[:,1]==1])[:,0],(X[Z_sklearn[:,1]==1])[:,1],color='g')<br/>plt.scatter((X[Z_sklearn[:,2]==1])[:,0],(X[Z_sklearn[:,2]==1])[:,1],color='b')<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/42715bd48ef314d1295c877c4aa66c95.png" data-original-src="https://miro.medium.com/v2/format:webp/1*dUWUoB6gS8hKskwWCmObFQ.png"/></div></figure><p id="f0a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæœ‰ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•æ¥ç»™é›†ç¾¤ç€è‰²..</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="dd28" class="ku kv iq kq b gy kw kx l ky kz">plt.scatter(X[:,0],X[:,1],c=clusters)<br/>plt.scatter(cluster_centers[:,0],cluster_centers[:,1],color='black',marker='o',s=200,alpha=0.75)<br/>plt.show()</span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="ab gu cl mc"><img src="../Images/189436aab46fbfe19401dd848f781678.png" data-original-src="https://miro.medium.com/v2/format:webp/1*l6btJA3BKBSSvOHDFqNvkg.png"/></div></figure><h2 id="7e5f" class="ku kv iq bd md me mf dn mg mh mi dp mj jy mk ml mm kc mn mo mp kg mq mr ms mt bi translated">æ¯”è¾ƒç»“æœâ€¦</h2><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="d333" class="ku kv iq kq b gy kw kx l ky kz">#converged loss from sklearn<br/>distorsion_loss(X,cluster_centers,Z_sklearn)</span><span id="e0bc" class="ku kv iq kq b gy mb kx l ky kz">586.9973311742276</span><span id="8427" class="ku kv iq kq b gy mb kx l ky kz">#converged loss from our good result</span><span id="7099" class="ku kv iq kq b gy mb kx l ky kz">distorsion_loss(X,mu,Z)</span><span id="aaeb" class="ku kv iq kq b gy mb kx l ky kz">586.9973311742275</span><span id="1dc2" class="ku kv iq kq b gy mb kx l ky kz">#difference between our centroids and the ones found by sklearn<br/>np.linalg.norm(np.sort(mu,axis=0)-np.sort(cluster_centers,axis=0),axis=1)</span><span id="252a" class="ku kv iq kq b gy mb kx l ky kz">array([1.25607397e-15, 5.20417043e-18, 1.83102672e-15])</span></pre><p id="06d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">éƒ½å‡ ä¹ä¸ºé›¶ã€‚</p><p id="2060" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯¹K-Meansæœ‰æ‰€å¯å‘ï¼ŒK-Meansæ˜¯å½“ä»Šæ— ç›‘ç£å­¦ä¹ ä¸­æœ€æµè¡Œçš„èšç±»ç®—æ³•ä¹‹ä¸€ã€‚åœ¨æˆ‘çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä½¿ç”¨é«˜æ–¯æ··åˆçš„è½¯èšç±»ç®—æ³•ã€‚</p><p id="be22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¿™æ˜¯å®Œæ•´çš„jupyterç¬”è®°æœ¬ã€‚</p><p id="956d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·èŠ±ç‚¹æ—¶é—´ç»™æˆ‘ä¸€äº›æŒå£°ã€‚</p></div></div>    
</body>
</html>