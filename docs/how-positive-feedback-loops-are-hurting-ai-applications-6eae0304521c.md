# 正反馈循环是如何伤害人工智能应用的

> 原文：<https://levelup.gitconnected.com/how-positive-feedback-loops-are-hurting-ai-applications-6eae0304521c>

又名为什么我们现在比以往任何时候都更需要数据科学的“科学”部分？

正如我们在过去一年中发现的那样，“积极”并不总是一个好的形容词。在数据科学中的“积极”反馈循环的情况下，如果我们继续传播人类的错误和偏见，结果可能是毁灭性的。

正反馈回路被定义为当一个部件发生小扰动时，系统的部件双向增加彼此的值。通常正反馈循环与指数增长联系在一起(又一个不总是好的词)。最熟悉的应该是我们最近在新冠肺炎和医疗能力方面的经验。感染上升，医院不堪重负，由于缺乏能力，感染和死亡不断增加，医院更加不堪重负，等等。在全球化的疫情中，政府被迫采取措施对抗这种滚雪球效应，但在大多数情况下，没有人能阻止雪崩。有时我们甚至没有意识到。

![](img/f9a141294edfa422ab51927193eb2d30.png)

这与数据科学有什么关系？你可能会问。我想说这是现实世界中人工智能应用的最大问题之一。我们都从亚马逊上随机购买了一件商品，但不久之后就收到了更多这些商品的推荐，比如马桶座圈。这并不一定会导致反馈循环，因为你不会去买无限多的马桶座圈。但是人工智能系统中缺乏*常识*并不总是那么有趣。在大多数情况下，这是非常有害的。

犯罪特征分析工具是政府系统的现实。自然，这些系统并不完全了解一个城市中发生的每一起犯罪。如果他们有，我们一开始就不需要他们了。[相反，他们用“逮捕”来代替犯罪。一个城市的贫困地区有更高的犯罪率，因此他们的治安更好，这导致更多的逮捕，这导致更多的犯罪“评级”，这导致更多的治安，这导致更多的逮捕…你明白这一点。最终，机器学习算法变成了一个种族/社会剖析工具，造成了一个鸿沟。或者，石膏。](https://advances.sciencemag.org/content/4/1/eaao5580)

这些反馈循环的另一种有害方式是减少原创性。社交媒体平台上的广告就是从这种反馈循环中获利的。一个广告被显示出来，它得到互动，更多的相同的东西被放置，多样性被减少，你没有别的东西可以点击，所以算法看不到添加的替代物，更多的相同的东西被放置等等。

现在可以说，广告真的是我们最小的问题。类似的行为发生在每一个评分算法中。想象一个用于投资决策的系统。它确切地知道历史上什么是成功的，并在此基础上不断推荐决策。没有原创得到投资，更多相同的东西得到资助，变得“成功”等等。基本上是做空特斯拉(和 GameStop)的投资基金的一个更愚蠢的版本。因此，不仅仅是公众从消除这些偏见中受益，商业实体也有动力进行无偏见的评估(咄！).通常，这些实体是最有偏见的。

关于这些还有很多例子，但现在我离题了。以上所有(以及更多)的系统定义和分类可以在[这篇非凡的论文](https://arxiv.org/pdf/1901.10002.pdf)中找到。

**我们如何解决这个问题？**

![](img/711ff70a21ceee672fb2a3d2b6985b73.png)

# *在 ML* 的背景下，我们需要一种偏见和不公平的语言

我在上面链接的论文很好地以一种清晰的方式阐述了这个问题，并开始讨论这些问题。然而，到目前为止，这些定义都是学术性的。如果我们希望实践者能够从一开始就思考这些问题，我们需要有一种语言，因为正如维特根斯坦所说的“我的语言的限制意味着我的世界的限制。”。每个人都必须理解关于偏见、公平和传播的概念。

# 我们需要将科学带回数据科学

现实世界系统中的大部分数据科学应用都遵循这个管道:数据发现→数据收集→标记/注释→分析/建模、部署→可操作化→反馈到系统中并改进。

在这条管道的每一个点上，我们都能找到偏差的来源。任何模型都会传播训练数据中的偏差。数据科学家必须能够定义他们的数据源和人口，他们的数据收集采样技术，标签和注释阶段的假设(甚至是假定)。培训/验证和模型性能度量会传播和放大前面阶段产生的问题(如果没有很好地理解世界的动态，即使度量看起来很好，它们也可能是完全错误的)。

年复一年的科学研究试图涵盖实验和研究设计技术。实验设计、数据收集方案、抽样方法必须遵循科学的程序，以批判性地评估人工智能系统的局限性和缺点。例如，在最简单的情况下，大多数最大似然解都隐含假设数据中的缺失值是随机分布的。但在这些应用中，系统性的代表性不足(或在上述犯罪监测例子中的代表性过高)非常普遍。这是反馈回路的主要来源之一。如果你的数据管道不断强化这些偏见，它们将永远不会得到解决。大多数从业者无视适当的实验实践。因此，实践中很少有数据科学是真正的科学。

# 我们必须为审查腾出空间。

部署、运作和反馈也可能导致错误和偏见的产生。一旦部署，机器学习系统预计将与解决方案的其余部分进行交互。一旦操作化，它就被人类使用，可能有一些反馈机制来重新训练。

这些交互作用的每一点都可能以意想不到的(甚至是有害的)方式影响解决方案。假设您的机器学习解决方案依赖于外部系统的变量，而该变量的定义在您不知情的情况下发生了变化，这并不一定意味着您的系统会立即失败。甚至你的模型度量也可能保持大致相同。假设您构建了一个使用分类变量的模型，并且对现有变量进行了哑编码。其中一个类别被一分为二，70%保留了原来的类别名称，另外 30%使用了新名称。第一部分的虚拟值是正确的，剩下的 30%可以有多种表现。如果您已经预料到了这一点，并为此编写了异常处理，我为您鼓掌。如果没有并且*幸运的话，*系统会崩溃。如果不是，你就麻烦了。可以说，这是一个更容易解决的问题。

现在假设你的系统需要用户的反馈来改进算法，而用户完全误解/误用了这个工具。这是一个很难解决的问题，尤其是如果差别很小的话。

我们为决策支持系统构建的工具，无论是否智能，都必须接受与这些工具相关的每个人的审查。设计者、开发者、数据科学家、用户、决策者和公众。我们都必须了解收集和使用数据的过程。技术团队必须了解他们的工作是如何结合在一起的。用户必须理解工具在做什么，最好是如何做。决策者和用户必须理解决策的用例、含义和影响。最后，公众有权知道为什么需要这个工具，以及它对他们有什么影响。特别是对于公共治理中使用的工具，我们必须有透明的审查和重新审查程序，既解决技术问题，也解决软问题。监控您的解决方案是一个很好的想法，但这只是一个卫生水平的行为。我们需要超越监督，让决策民主化。