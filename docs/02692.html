<html>
<head>
<title>Principal Component Analysis for Dimensionality Reduction in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python语言降维的主成分分析</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/principal-component-analysis-for-dimensionality-reduction-in-python-4187d98f7cc1?source=collection_archive---------20-----------------------#2020-03-29">https://levelup.gitconnected.com/principal-component-analysis-for-dimensionality-reduction-in-python-4187d98f7cc1?source=collection_archive---------20-----------------------#2020-03-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b0c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文将重点介绍Python中的主成分分析。</p><p id="8110" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">目录:</strong></p><ul class=""><li id="c9bf" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">介绍</li><li id="1a87" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">主成分分析(综述)</li><li id="09ab" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">Python中的主成分分析</li><li id="30cf" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">结论</li></ul></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h1 id="bacb" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">介绍</h1><p id="4604" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">写这篇文章的一个主要原因是我痴迷于了解主成分分析(PCA)背后的细节、逻辑和数学。如今，大多数关于Python中主成分分析的在线教程和文章都专注于向学习者展示如何应用这种技术并可视化结果，而不是从头开始思考我们为什么需要它？我们的数据为什么需要减少要素的数量或对它们进行分组？</p><p id="9e7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们从头开始。即使不做任何降维处理，你打算用你拥有的数据集做什么？我猜你是想把它输入到机器学习算法中，对吗？</p><p id="e88d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们的第一步。我们的目标是有一个算法友好的数据集。我们这样说是什么意思？</p><p id="81e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当您拥有大量功能时，会有一些潜在的缺点:</p><ul class=""><li id="88fd" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">你的模型将会非常复杂</li><li id="a5c2" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">它们可能会产生很大的噪音</li><li id="240d" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">如果它们具有不同的比例，则会降低一些对比例敏感的算法的性能</li><li id="b5ce" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">n维空间中更复杂的可视化</li></ul><p id="71ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是PCA发挥作用的地方。它将通过提取/消除重要/不重要的特征来帮助我们降低数据集的维度。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h1 id="dc6b" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">主成分分析(综述)</h1><p id="102b" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">主成分分析(PCA)是一种线性降维技术。从数学上来说，PCA使用潜在相关特征到线性不相关的主成分的正交变换。</p><p id="a2d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，<strong class="js iu"> n </strong>个主成分的序列是按照它们所解释的原始数据集中的方差大小以降序排列的。这实质上意味着第一个主成分比第二个主成分解释了更多的差异，依此类推。</p><p id="4f14" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更好的做法是把它放在如何逐步计算主成分的背景下:</p><ol class=""><li id="6369" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn mm ku kv kw bi translated">计算协方差矩阵</li><li id="f90e" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn mm ku kv kw bi translated">计算特征值</li><li id="fc34" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn mm ku kv kw bi translated">计算特征向量</li><li id="4e9a" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn mm ku kv kw bi translated">按照特征值降序排列特征向量</li><li id="8062" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn mm ku kv kw bi translated">使用<strong class="js iu"> k </strong>特征向量将原始数据转换到<strong class="js iu"> k </strong>维</li></ol><p id="5298" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意:如前所述，根据您正在处理的数据和要素的缩放比例，您可能希望在对数据运行PCA之前对数据进行标准化。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h1 id="101e" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">Python中的主成分分析</h1><p id="b6f9" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">为了继续学习本教程，我们需要两个Python库:pandas、numpy、sklearn和matplotlib。</p><p id="5127" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您没有安装它们，请打开“命令提示符”(在Windows上)并使用以下代码安装它们:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="7c2b" class="mw lk it ms b gy mx my l mz na">pip install pandas<br/>pip install numpy<br/>pip install sklearn<br/>pip install matplotlib</span></pre><p id="5f64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">导入所需的库:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="fe6f" class="mw lk it ms b gy mx my l mz na">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import load_wine<br/>from sklearn.preprocessing<br/>import StandardScaler from sklearn.decomposition<br/>import PCA</span></pre><p id="926e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦下载、安装和导入了库，我们就可以继续Python代码实现了。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h2 id="22ee" class="mw lk it bd ll nb nc dn lp nd ne dp lt kb nf ng lx kf nh ni mb kj nj nk mf nl bi translated">步骤1:加载数据集</h2><p id="8781" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">在本教程中，我们将使用作为sklearn库一部分的葡萄酒识别数据集。该数据集包含13个特征，目标是3类葡萄酒。<br/>数据集的描述如下:</p><ul class=""><li id="0748" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">酒精</li><li id="e907" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">苹果酸</li><li id="3a90" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">灰</li><li id="f7c7" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">灰分的碱性</li><li id="f3d5" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">镁</li><li id="5f0b" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">总酚</li><li id="a9e9" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">黄酮类化合物</li><li id="8ff4" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">非类黄酮酚</li><li id="d92e" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">原花青素</li><li id="2c41" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">彩色亮度</li><li id="552c" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">顺化(越南城市)</li><li id="21e7" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">稀释葡萄酒的OD280/OD315</li><li id="0896" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">脯氨酸</li></ul><p id="8683" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个数据集对于说明分类问题特别有趣，对于我们展示PCA应用也非常有用。它有足够的特性来说明降维的真正好处。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="e06f" class="mw lk it ms b gy mx my l mz na">wine = load_wine()<br/>df = pd.DataFrame(wine.data, columns=wine.feature_names)<br/>print(df.iloc[:,0:4].head())</span></pre><p id="7271" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">来自sklearn的数据在导入时(<strong class="js iu"> wine </strong>)显示为数据集的容器对象。它类似于一个字典对象。然后我们将它转换成pandas数据帧，并使用特性名作为我们的列名。</p><p id="6502" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为我们有13个特性，所以要展示的范围很广，所以我们看一下前4列，以确保我们的代码有效。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h2 id="2143" class="mw lk it bd ll nb nc dn lp nd ne dp lt kb nf ng lx kf nh ni mb kj nj nk mf nl bi translated">步骤2:浏览数据集</h2><p id="cb5a" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">现在，让我们来看看这个数据框架的基本描述性统计数据。我们将继续只显示前4列，以节省一些输出空间。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a068" class="mw lk it ms b gy mx my l mz na">print(df.iloc[:,0:4].describe())</span></pre><p id="c08b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是我们在上面讨论过的4个特性的散点图矩阵。它在博客页面上的尺寸较小，因此不会减少页面加载时间。如果你想更深入地探索它，你可以很容易地通过一些参数调整<a class="ae nm" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-scatter-matrix" rel="noopener ugc nofollow" target="_blank">重新创建</a>。</p><figure class="mn mo mp mq gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nn"><img src="../Images/cd467e545d9ad4c91f0a88bc4f1f0327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UyL5zXIPLxHaezXu"/></div></div></figure><p id="9f9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您在散点图矩阵中看到的是描述性统计输出的可视化。我们将使用它与缩放后的数据进行比较，以显示标准化后每个要素的值的分布和分布保持不变。</p><p id="4b14" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们所看到的，每个特性的范围以及它们的均值和方差都非常不同。我们的下一步是通过减去每个值的平均值并除以其标准偏差来缩放每个特征的数据。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h2 id="4d63" class="mw lk it bd ll nb nc dn lp nd ne dp lt kb nf ng lx kf nh ni mb kj nj nk mf nl bi translated">步骤3:标准化数据集</h2><p id="e391" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">PCA主要关注最大化方差的特征。在当前未缩放数据的情况下，PCA将认为特征4(“alcalinity _ of _ ash”)支配最大方差度量，因为其范围比特征1:3大5-10倍。因此，得到的主成分可能与标准化数据非常不同。有关PCA缩放重要性的更多详细信息，请点击<a class="ae nm" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="5cdc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在扩展我们的数据时，我们将应用以下标准化方法:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="13f2" class="mw lk it ms b gy mx my l mz na">df = StandardScaler().fit_transform(df) df=pd.DataFrame(df,columns=wine.feature_names)</span></pre><p id="670a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们将StandatdScaler()应用于dataframe时，得到的转换对象是一个数组，然后我们将它转换回pandas dataframe对象。我们可以看到，与原始数据相比，转换后的要素在要素间的比例更加相似。</p><p id="f4ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了说明这意味着什么，让我们看一下缩放数据的描述性统计。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="bf70" class="mw lk it ms b gy mx my l mz na">print(df.iloc[:,0:4].describe())</span></pre><p id="4f3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，在缩放后的数据中，要素的平均值以0为中心，方差是单位方差。</p><p id="5335" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的矩阵散点图显示了上述观点。</p><figure class="mn mo mp mq gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nn"><img src="../Images/93a83f31a6a04f2285d8a7161d946722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2D_wmUe4SmRp-xXs"/></div></div></figure><p id="768f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以像之前的散点图矩阵一样<a class="ae nm" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-scatter-matrix" rel="noopener ugc nofollow" target="_blank">重新创建</a>。重要的是比较要素的分布和缩放值的分布，请注意它们是相同的。</p><p id="d1e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意:如果您有分类数据，您将需要在缩放之前对其进行<a class="ae nm" href="https://pyshark.com/label-encoding-in-python/" rel="noopener ugc nofollow" target="_blank">标签编码</a>。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h2 id="7b44" class="mw lk it bd ll nb nc dn lp nd ne dp lt kb nf ng lx kf nh ni mb kj nj nk mf nl bi translated">步骤4:在Python中应用主成分分析</h2><p id="9a6d" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">在缩放我们的数据后，我们就进入了本教程最有趣的部分。我们将继续对缩放后的数据集应用PCA。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="e6f4" class="mw lk it ms b gy mx my l mz na">pca = PCA(n_components=2)<br/>pca_model=pca.fit(df)<br/>df_trans=pd.DataFrame(pca_model.transform(df), columns=['pca1', 'pca2'])</span></pre><p id="178e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，将sklearn PCA的实例存储为具有2个组件的<strong class="js iu"> pca </strong>。我们特别选择2的原因只是为了本教程的简单和演示。在文章的后面，我们将讨论如何找到最佳主成分的数量。</p><p id="096a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> pca_model </strong>存储所应用技术的特征向量，该特征向量用于通过将其形状从13个原始特征缩减为由主分量表示的2个特征，将缩放数据集<strong class="js iu"> df </strong>转换为<strong class="js iu"> df_trans </strong>。</p><p id="635e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是这种降维的可视化(从13维到2维。让我们来看看:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="77cd" class="mw lk it ms b gy mx my l mz na">print(df_trans.head())</span></pre><p id="c5df" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面我们看到的是经过缩放的13要素数据集转换为2要素数据集，其中每个要素都是一个主成分。由于维数减少，它现在允许我们在二维空间中可视化13维数据集。</p><p id="7bc1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可视化转换后的数据<strong class="js iu"> df_trans </strong>:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="c380" class="mw lk it ms b gy mx my l mz na">plt.scatter(df_trans['pca1'], df_trans['pca2'], alpha=0.8)<br/>plt.xlabel('PCA 1')<br/>plt.ylabel('PCA 2')<br/>plt.show()</span></pre><figure class="mn mo mp mq gt no gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/f8e1ac549ae9e6b67378bb8020601f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/0*8jEhghGwGJ-1crU1"/></div></figure><p id="07e8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以将目标值作为颜色添加到上面的图中(有三个等级:1、2、3 ),得到下面的散点图:</p><figure class="mn mo mp mq gt no gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ef4123e4aab55e9cbf9fd5e7a39e15f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/0*7PoMGBAJHYO6JlGD"/></div></figure><p id="3ace" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在主成分变换之后，观察值被分组到可定义的群中。例如，如果您正在为分类构建预测算法(如KNN算法),当应用于转换后的数据集时，将会产生更高的准确性。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h2 id="9af4" class="mw lk it bd ll nb nc dn lp nd ne dp lt kb nf ng lx kf nh ni mb kj nj nk mf nl bi translated">步骤5:用Python解释主成分</h2><p id="4bc2" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">回到我们的主成分，它们实际上意味着什么？他们提供什么信息？</p><p id="8203" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">主成分与原始数据集的关系</strong></p><p id="cdff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">探索的第一部分将是每个主成分和来自原始数据集的特征之间的相关性。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5522" class="mw lk it ms b gy mx my l mz na">comp=pd.DataFrame(pca_model.components_, columns=wine.feature_names)<br/>print(comp)</span></pre><p id="bc7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个索引为0的行指的是第一个主成分。主成分和原始数据集特征之间的最高相关性(绝对值)是与“黄酮类化合物”特征的相关性(0.422934)。这意味着在<strong class="js iu"> trans_df </strong>中‘pc1’值大的葡萄酒‘黄素’值也高。<br/>对于第二个主成分，最高相关性(绝对值)是与‘颜色强度’特征(-0.529996)，这意味着在<strong class="js iu">反式_df </strong>中‘pc2’值大的葡萄酒‘颜色强度’值低。</p><p id="114a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">按照这种方法，您可以很好地理解每个主成分的值与特定数据集要素的值的相关程度。</p><p id="3302" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意:这与特征重要性不同(但它会根据哪个特征更重要来显示相似的结果)，而不是主成分和原始数据集之间的相关性概述。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><p id="c96d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">主成分解释方差</strong></p><p id="9814" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回想一下，这里有两个主成分，它们线性不相关。这些组件中的每一个都解释了原始数据集的一些变化。主成分分析的工作方式是根据每个成分解释的变化量，以降序找到主成分。让我们调查一下。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="daf3" class="mw lk it ms b gy mx my l mz na">print(pca_model.explained_variance_ratio_)</span><span id="df08" class="mw lk it ms b gy nw my l mz na">Output: [0.36198848 0.1920749 ]</span></pre><p id="b14f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上面可以看出，第一个主成分解释了原始数据集方差的大约36.2%，而第二个主成分解释了大约19.2%。</p><p id="56c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回想一下，在步骤4中创建<strong class="js iu"> pca </strong>实例时，我们将其限制为2个组件。这样做主要是为了展示该技术，并在本教程中简化解释。</p><p id="6a52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，可以有两个以上的主成分，数据科学家想要使用多少主成分实际上取决于他们想要解释多少方差。组件的最大数量受到特性数量的限制(在我们的例子中是13)。为了说明我们所指的是什么，我们将重复他的教程的几个步骤，而不限制主成分的数量为2。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="2226" class="mw lk it ms b gy mx my l mz na">pca = PCA()<br/>pca_model=pca.fit(df)</span></pre><p id="4ac0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于一个N维数据集，上面的代码将计算N个主成分。我们的葡萄酒数据集有13个特征，因此我们现在有13个主要成分。让我们看看他们各自解释的方差的分数:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="6545" class="mw lk it ms b gy mx my l mz na">print(pca_model.explained_variance_ratio_)</span><span id="d511" class="mw lk it ms b gy nw my l mz na">Output: [0.36198848 0.1920749 0.11123631 0.0706903 0.06563294 0.04935823 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233 0.00795215]</span></pre><p id="9726" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一个和第二个分量的值和之前一样。现在我们又添加了11个，它们都按降序排列。</p><p id="d9d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们如何选择需要将数据集缩减到多少主成分的一个重要因素是它们所解释的累积方差。对于每个后续的主成分，我们将把它解释的方差加上前面主成分解释的所有方差。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="d707" class="mw lk it ms b gy mx my l mz na">print(np.cumsum(pca_model.explained_variance_ratio_))</span><span id="2417" class="mw lk it ms b gy nw my l mz na">Output: [0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785 1. ]</span></pre><p id="5ccd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">前两个主成分解释了55.41%左右。随着我们添加更多的主成分，累积方差以递减的速率增加。</p><p id="23f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设，作为一名数据科学家，您的要求是80%的方差必须在PCA转换的数据中得到解释。这种水平的累积差异可以通过包含5个主成分来解释。让我们想象一下。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="2b8a" class="mw lk it ms b gy mx my l mz na">plt.plot(list(range(1,14)), pca.explained_variance_ratio_) plt.axis([0, 14, 0, max(pca.explained_variance_ratio_)+0.05]) plt.xticks(list(range(1,14)))<br/>plt.xlabel('Principal Components')<br/>plt.ylabel('Variance Explained')<br/>plt.show()</span></pre><figure class="mn mo mp mq gt no gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/433c27f2042f238334584b84cce0f824.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*CT3mU6wX1MqT4g8n"/></div></figure><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="353f" class="mw lk it ms b gy mx my l mz na">plt.plot(list(range(1,14)), np.cumsum(pca.explained_variance_ratio_))<br/>plt.axis([0, 14, 0, 1.1])<br/>plt.axhline(y=0.8, color='r', linestyle='--', linewidth=1) plt.xticks(list(range(1,14)))<br/>plt.xlabel('# of Principal Components')<br/>plt.ylabel('Cumulative Variance Explained')<br/>plt.show()</span></pre><figure class="mn mo mp mq gt no gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/87829516cf3ae8159c45d683ad784e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*kmi3IVrvtMrd02LA"/></div></figure></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><h1 id="7a05" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">结论</h1><p id="d81c" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">本文介绍了Python中主成分分析的理论和应用。<br/>您可以采取的进一步探索该技术优势的下一步是尝试在原始数据集和主成分数据集上应用一些机器学习算法，并比较您的准确性结果。</p><p id="acbc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你有任何问题或者对编辑有任何建议，请在下面留下你的评论。</p></div><div class="ab cl lc ld hx le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="im in io ip iq"><p id="22ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="nz">原载于2020年3月29日【https://pyshark.com】<a class="ae nm" href="https://pyshark.com/principal-component-analysis-in-python/" rel="noopener ugc nofollow" target="_blank"><em class="nz"/></a><em class="nz">。</em></em></p></div></div>    
</body>
</html>