<html>
<head>
<title>Dog Breed Classification using CNN and transfer learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于CNN和迁移学习的犬种分类</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/dog-breed-classification-using-cnn-and-transfer-learning-cc93a4497e90?source=collection_archive---------7-----------------------#2020-02-13">https://levelup.gitconnected.com/dog-breed-classification-using-cnn-and-transfer-learning-cc93a4497e90?source=collection_archive---------7-----------------------#2020-02-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d96c4fc2ee3a0c4527210f1620ee9ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9i6MyZLPCue2nCInqPUIrQ.jpeg"/></div></div></figure><h1 id="0a86" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">项目概述:</h1><p id="2a9c" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">本文主要是为了构建一个可以在web或移动应用程序中使用的管道，以处理真实世界中用户提供的图像。给定一张狗的图片，你的算法将识别出狗的品种。如果提供一个人的图像，代码将识别相似的狗品种。</p><h1 id="e547" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">评估指标:</h1><p id="2131" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">由于这是一个多类分类问题，所以我们使用准确度作为主要度量来评估模型性能。主要是因为这些类别在数据中几乎是平衡的。</p><h1 id="9766" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">路线图:</h1><blockquote class="lx ly lz"><p id="dff3" class="kz la ma lb b lc mb le lf lg mc li lj md me lm ln mf mg lq lr mh mi lu lv lw im bi translated">步骤0:导入数据集<br/>步骤1:检测人类<br/>步骤2:检测狗<br/>步骤3:创建CNN来分类狗的品种(从头开始)<br/>步骤4:使用CNN来分类狗的品种(使用迁移学习)<br/>步骤5:创建CNN来分类狗的品种(使用迁移学习)<br/>步骤6:编写您的算法<br/>步骤7:测试您的算法</p></blockquote></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="f496" class="kb kc it bd kd ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky bi translated">步骤0:</h1><p id="45d9" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们导入狗图像的数据集用于进一步建模。我们通过使用scikit-learn库中的<code class="fe mv mw mx my b">load_files</code>函数来填充一些变量。</p><p id="fcb9" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">导入狗数据:</p><ul class=""><li id="90cd" class="mz na it lb b lc mb lg mc lk nb lo nc ls nd lw ne nf ng nh bi translated"><code class="fe mv mw mx my b">train_files</code>、<code class="fe mv mw mx my b">valid_files</code>、<code class="fe mv mw mx my b">test_files</code> -包含图像文件路径的numpy数组</li><li id="897a" class="mz na it lb b lc ni lg nj lk nk lo nl ls nm lw ne nf ng nh bi translated"><code class="fe mv mw mx my b">train_targets</code>、<code class="fe mv mw mx my b">valid_targets</code>、<code class="fe mv mw mx my b">test_targets</code> -包含一个热编码分类标签的numpy数组</li><li id="512e" class="mz na it lb b lc ni lg nj lk nk lo nl ls nm lw ne nf ng nh bi translated"><code class="fe mv mw mx my b">dog_names</code> -用于翻译标签的字符串值犬种名称列表</li></ul><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="e74a" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">sklearn.datasets</strong> <strong class="my iu">import</strong> load_files       <br/><strong class="my iu">from</strong> <strong class="my iu">keras.utils</strong> <strong class="my iu">import</strong> np_utils<br/><strong class="my iu">import</strong> <strong class="my iu">numpy</strong> <strong class="my iu">as</strong> <strong class="my iu">np</strong><br/><strong class="my iu">from</strong> <strong class="my iu">glob</strong> <strong class="my iu">import</strong> glob<br/><br/><em class="ma"># define function to load train, test, and validation datasets</em><br/><strong class="my iu">def</strong> load_dataset(path):<br/>    data = load_files(path)<br/>    dog_files = np.array(data['filenames'])<br/>    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)<br/>    <strong class="my iu">return</strong> dog_files, dog_targets<br/><br/><em class="ma"># load train, test, and validation datasets</em><br/>train_files, train_targets = load_dataset('../../../data/dog_images/train')<br/>valid_files, valid_targets = load_dataset('../../../data/dog_images/valid')<br/>test_files, test_targets = load_dataset('../../../data/dog_images/test')<br/><br/><em class="ma"># load list of dog names</em><br/>dog_names = [item[20:-1] <strong class="my iu">for</strong> item <strong class="my iu">in</strong> sorted(glob("../../../data/dog_images/train/*/"))]<br/><br/><em class="ma"># print statistics about the dataset</em><br/>print('There are <strong class="my iu">%d</strong> total dog categories.' % len(dog_names))<br/>print('There are <strong class="my iu">%s</strong> total dog images.<strong class="my iu">\n</strong>' % len(np.hstack([train_files, valid_files, test_files])))<br/>print('There are <strong class="my iu">%d</strong> training dog images.' % len(train_files))<br/>print('There are <strong class="my iu">%d</strong> validation dog images.' % len(valid_files))<br/>print('There are <strong class="my iu">%d</strong> test dog images.'% len(test_files))</span></pre><p id="a5bc" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">导入人类数据</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="5a8c" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">import</strong> <strong class="my iu">random</strong><br/>random.seed(8675309)<br/><br/><em class="ma"># load filenames in shuffled human dataset</em><br/>human_files = np.array(glob("../../../data/lfw/*/*"))<br/>random.shuffle(human_files)<br/><br/><em class="ma"># print statistics about the dataset</em><br/>print('There are <strong class="my iu">%d</strong> total human images.' % len(human_files))</span></pre><h1 id="7de6" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第一步:探测人类</h1><p id="b0b8" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们使用OpenCV实现的<a class="ae oa" href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html" rel="noopener ugc nofollow" target="_blank">基于Haar特征的级联分类器</a>来检测图像中的人脸。OpenCV提供了许多预先训练好的人脸检测器，作为XML文件存储在github 上。我们已经下载了这些检测器中的一个，并将其存储在<code class="fe mv mw mx my b">haarcascades</code>目录中。</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="c0d3" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">import</strong> <strong class="my iu">cv2</strong>                <br/><strong class="my iu">import</strong> <strong class="my iu">matplotlib.pyplot</strong> <strong class="my iu">as</strong> <strong class="my iu">plt</strong>                        <br/>%matplotlib inline                               <br/><br/><em class="ma"># extract pre-trained face detector</em><br/>face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')<br/><br/><em class="ma"># load color (BGR) image</em><br/>img = cv2.imread(human_files[3])<br/><em class="ma"># convert BGR image to grayscale</em><br/>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/><br/><em class="ma"># find faces in image</em><br/>faces = face_cascade.detectMultiScale(gray)<br/><br/><em class="ma"># print number of faces detected in the image</em><br/>print('Number of faces detected:', len(faces))<br/><br/><em class="ma"># get bounding box for each detected face</em><br/><strong class="my iu">for</strong> (x,y,w,h) <strong class="my iu">in</strong> faces:<br/>    <em class="ma"># add bounding box to color image</em><br/>    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)<br/>    <br/><em class="ma"># convert BGR image to RGB for plotting</em><br/>cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/><br/><em class="ma"># display the image, along with bounding box</em><br/>plt.imshow(cv_rgb)<br/>plt.show()</span></pre><figure class="nn no np nq gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/483b34b52f4242831227a41b773fb541.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*-YC4CxxHEbbQ87mIV0DSXQ.png"/></div></figure><p id="4208" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">在使用任何面部检测器之前，标准程序是将图像转换为灰度。<code class="fe mv mw mx my b">detectMultiScale</code>函数执行<code class="fe mv mw mx my b">face_cascade</code>中存储的分类器，并将灰度图像作为参数。</p><p id="7917" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">在上面的代码中，<code class="fe mv mw mx my b">faces</code>是检测到的人脸的numpy数组，其中每一行对应一个检测到的人脸。每个检测到的面部是具有四个条目的1D阵列，其指定检测到的面部的边界框。数组中的前两个条目(在上面的代码中提取为<code class="fe mv mw mx my b">x</code>和<code class="fe mv mw mx my b">y</code>)指定了边界框左上角的水平和垂直位置。数组中的最后两个条目(这里提取为<code class="fe mv mw mx my b">w</code>和<code class="fe mv mw mx my b">h</code>)指定了盒子的宽度和高度。</p><h1 id="fa54" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">写一个人脸检测器</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="825f" class="nv kc it my b gy nw nx l ny nz"><em class="ma"># returns "True" if face is detected in image stored at img_path</em><br/><strong class="my iu">def</strong> face_detector(img_path):<br/>    img = cv2.imread(img_path)<br/>    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>    faces = face_cascade.detectMultiScale(gray)<br/>    <strong class="my iu">return</strong> len(faces) &gt; 0</span></pre><h1 id="7580" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第二步:探测狗</h1><p id="9aab" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在本节中，我们使用预训练的<a class="ae oa" href="http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006" rel="noopener ugc nofollow" target="_blank"> ResNet-50 </a>模型来检测图像中的狗。我们的第一行代码下载了ResNet-50模型，以及在<a class="ae oa" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上训练过的权重，ImageNet是一个非常大、非常受欢迎的数据集，用于图像分类和其他视觉任务。ImageNet包含超过1000万个URL，每个URL都链接到一个包含来自<a class="ae oa" href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" rel="noopener ugc nofollow" target="_blank"> 1000个类别</a>之一的对象的图像。给定一个图像，这个预训练的ResNet-50模型返回该图像中包含的对象的预测(从ImageNet中的可用类别中获得)。</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="cb60" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">keras.applications.resnet50</strong> <strong class="my iu">import</strong> ResNet50<br/><br/><em class="ma"># define ResNet50 model</em><br/>ResNet50_model = ResNet50(weights='imagenet')</span></pre><h1 id="a76b" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">预处理数据</h1><p id="f4a6" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">当使用TensorFlow作为后端时，Keras CNNs需要一个4D数组(我们也称之为4D张量)作为输入，其中形状<code class="fe mv mw mx my b">nb_samples</code>对应于图像(或样本)的总数，<code class="fe mv mw mx my b">rows</code>、<code class="fe mv mw mx my b">columns</code>和<code class="fe mv mw mx my b">channels</code>分别对应于每个图像的行数、列数和通道数。</p><p id="681e" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">下面的<code class="fe mv mw mx my b">path_to_tensor</code>函数将彩色图像的字符串值文件路径作为输入，并返回适合提供给Keras CNN的4D张量。该函数首先加载图像，并将其调整为像素的正方形图像。</p><p id="32f6" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">接下来，图像被转换成数组，然后数组被调整大小为4D张量。在这种情况下，由于我们正在处理彩色图像，每个图像有三个通道。同样，由于我们处理的是单个图像(或样本)，返回的张量总是有形状的</p><p id="d9dc" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated"><code class="fe mv mw mx my b">paths_to_tensor</code>函数将字符串值图像路径的numpy数组作为输入，并返回一个带有形状的4D张量</p><p id="b305" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">这里，<code class="fe mv mw mx my b">nb_samples</code>是所提供的图像路径阵列中的样本数或图像数。最好把<code class="fe mv mw mx my b">nb_samples</code>想象成你的数据集中3D张量的数量(其中每个3D张量对应一个不同的图像)！</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="aba3" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">tqdm</strong> <strong class="my iu">import</strong> tqdm<br/><br/><strong class="my iu">def</strong> path_to_tensor(img_path):<br/>    <em class="ma"># loads RGB image as PIL.Image.Image type</em><br/>    img = image.load_img(img_path, target_size=(224, 224))<br/>    <em class="ma"># convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)</em><br/>    x = image.img_to_array(img)<br/>    <em class="ma"># convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor</em><br/>    <strong class="my iu">return</strong> np.expand_dims(x, axis=0)<br/><br/><strong class="my iu">def</strong> paths_to_tensor(img_paths):<br/>    list_of_tensors = [path_to_tensor(img_path) <strong class="my iu">for</strong> img_path <strong class="my iu">in</strong> tqdm(img_paths)]<br/>    <strong class="my iu">return</strong> np.vstack(list_of_tensors)</span></pre><h1 id="2970" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">使用ResNet-50进行预测</h1><p id="1520" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">为ResNet-50和Keras中任何其他预训练模型准备好4D张量，需要一些额外的处理。首先，通过对通道重新排序，将RGB图像转换为BGR图像。所有预训练模型都有额外的归一化步骤，即必须从每个图像的每个像素中减去平均像素(以RGB表示，并根据ImageNet中所有图像的所有像素计算得出)。这在导入的函数<code class="fe mv mw mx my b">preprocess_input</code>中实现。如果你很好奇，可以在这里查看<code class="fe mv mw mx my b">preprocess_input</code> <a class="ae oa" href="https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py" rel="noopener ugc nofollow" target="_blank">的代码</a>。</p><p id="b95d" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">现在我们有了一种方法来格式化我们的图像以提供给ResNet-50，我们现在准备使用模型来提取预测。这是通过<code class="fe mv mw mx my b">predict</code>方法完成的，该方法返回一个数组</p><p id="540b" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">-第项是模型预测的图像属于</p><p id="3497" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">-第个ImageNet类别。这在下面的<code class="fe mv mw mx my b">ResNet50_predict_labels</code>函数中实现。</p><p id="3f62" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">通过获取预测概率向量的argmax，我们获得了与模型的预测对象类别相对应的整数，我们可以通过使用这个<a class="ae oa" href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" rel="noopener ugc nofollow" target="_blank">字典</a>来识别对象类别。</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="423f" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">keras.applications.resnet50</strong> <strong class="my iu">import</strong> preprocess_input, decode_predictions<br/><br/><strong class="my iu">def</strong> ResNet50_predict_labels(img_path):<br/>    <em class="ma"># returns prediction vector for image located at img_path</em><br/>    img = preprocess_input(path_to_tensor(img_path))<br/>    <strong class="my iu">return</strong> np.argmax(ResNet50_model.predict(img))</span></pre><h1 id="0aba" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">写一个狗探测器</h1><p id="2437" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在查看<a class="ae oa" href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" rel="noopener ugc nofollow" target="_blank">字典</a>时，您会注意到对应于狗的类别以不间断的顺序出现，并对应于字典关键字151-268，包括从<code class="fe mv mw mx my b">'Chihuahua'</code>到<code class="fe mv mw mx my b">'Mexican hairless'</code>的所有类别。因此，为了检查预训练的ResNet-50模型是否预测图像包含狗，我们只需要检查上面的<code class="fe mv mw mx my b">ResNet50_predict_labels</code>函数是否返回151和268之间(包括151和268)的值。</p><p id="7afc" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">我们使用这些想法来完成下面的<code class="fe mv mw mx my b">dog_detector</code>函数，如果在图像中检测到狗，则返回<code class="fe mv mw mx my b">True</code>(如果没有检测到狗，则返回<code class="fe mv mw mx my b">False</code>)。</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="a268" class="nv kc it my b gy nw nx l ny nz"><em class="ma">### returns "True" if a dog is detected in the image stored at img_path</em><br/><strong class="my iu">def</strong> dog_detector(img_path):<br/>    prediction = ResNet50_predict_labels(img_path)<br/>    <strong class="my iu">return</strong> ((prediction &lt;= 268) &amp; (prediction &gt;= 151))</span></pre><h1 id="d0f4" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第三步:创建一个CNN来分类狗的品种(从头开始)</h1><p id="f747" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">现在我们有了在图像中检测人类和狗的功能，我们需要一种从图像中预测品种的方法。在这一步中，您将创建一个对狗的品种进行分类的CNN。你必须从头开始创建你的CNN<em class="ma"/>(所以，你还不能使用转移学习<em class="ma"/>！)，并且您必须达到至少1%的测试精度。在本笔记的第5步中，您将有机会使用迁移学习来创建一个CNN，从而大大提高准确性。</p><p id="bc94" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">小心添加太多的可训练层！更多的参数意味着更长的训练时间，这意味着你更有可能需要一个GPU来加速训练过程。令人欣慰的是，Keras提供了一个方便的时间估计，每个纪元可能需要的时间；您可以推断出这一估计值，以计算出您的算法需要多长时间来训练。</p><p id="b5e8" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">我们提到，从图像中给狗分配品种的任务被认为是非常具有挑战性的。要知道为什么，想想<em class="ma">甚至人类</em>都很难区分布列塔尼犬和威尔士史宾格犬。</p><figure class="nn no np nq gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/f940111ab52289327e8fab6e44b121a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*raHbwDmkqsgkmoncFM_0hA.png"/></div></figure><p id="c97b" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">不难找到其他具有最小类间差异的犬种对(例如，卷毛寻回犬和美国水猎犬)。</p><figure class="nn no np nq gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/27fccae8eb08f7194ef24f3c04a8155a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*3wagzV4-MQNx2wpMpWGibw.png"/></div></figure><p id="8939" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">同样，回想一下拉布拉多有黄色、巧克力色和黑色。你的基于视觉的算法将不得不克服这种高类内变异，以确定如何将所有这些不同的色调归类为同一品种。</p><figure class="nn no np nq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/7f5e853b4200d546efc7fb338d8046fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U8P5MpI9q-VpYBCJUv2S-Q.png"/></div></div></figure><p id="23a0" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">我们还提到，随机机会提出了一个异常低的标准:抛开类别略有不平衡的事实，随机猜测将提供大约1/133的正确答案，这相当于不到1%的准确性。</p><h1 id="2966" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">预处理数据</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="3b5d" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">PIL</strong> <strong class="my iu">import</strong> ImageFile                            <br/>ImageFile.LOAD_TRUNCATED_IMAGES = <strong class="my iu">True</strong>                 <br/><br/><em class="ma"># pre-process the data for Keras</em><br/>train_tensors = paths_to_tensor(train_files).astype('float32')/255<br/>valid_tensors = paths_to_tensor(valid_files).astype('float32')/255<br/>test_tensors = paths_to_tensor(test_files).astype('float32')/255</span></pre><h1 id="d729" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">模型架构</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="d740" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">keras.layers</strong> <strong class="my iu">import</strong> Conv2D, MaxPooling2D, GlobalAveragePooling2D<br/><strong class="my iu">from</strong> <strong class="my iu">keras.layers</strong> <strong class="my iu">import</strong> Dropout, Flatten, Dense<br/><strong class="my iu">from</strong> <strong class="my iu">keras.models</strong> <strong class="my iu">import</strong> Sequential<br/><br/><em class="ma">### create the architecture using Sequential()</em><br/>model = Sequential()<br/><br/><em class="ma">### a convolution layer to extract features from the input image, </em><br/><em class="ma">### first CONV layer has 16 filters of size 3x3</em><br/><em class="ma">### Since this is the first layer we must input the dimension shape which is a 224 x 224 pixel image with depth = 3 (RGB).</em><br/>model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224,224,3)))<br/><br/><em class="ma">### The next layer will be a pooling layer with a 2 x 2 pixel filter to get the max element from the feature maps. </em><br/><em class="ma">### This reduces the dimension of the feature maps by half and is also known as sub sampling.</em><br/><em class="ma">### progressively reduce spatial size (width and height) of input </em><br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/><br/><em class="ma">### Create one more convolution layer and pooling layer like before, but without the input_shape</em><br/><em class="ma">### increase total number of filters learned</em><br/>model.add(Conv2D(32, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/><em class="ma">### Create one more convolution layer and pooling layer like before, but without the input_shape</em><br/><em class="ma">### increase total number of filters learned</em><br/>model.add(Conv2D(64, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/><br/>model.add(GlobalAveragePooling2D())<br/><br/>model.add(Dense(133, activation='relu'))<br/><em class="ma">### TODO: Define your architecture.</em><br/><br/>model.summary()</span></pre><figure class="nn no np nq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/353ca3e96e19bfa3a89b168ea8e21706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBLeRprYttBso4A8_V-C_w.png"/></div></div></figure><h1 id="183f" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">编译模型</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="a6b6" class="nv kc it my b gy nw nx l ny nz">model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])</span></pre><h1 id="acdb" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">训练模型</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="c879" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">keras.callbacks</strong> <strong class="my iu">import</strong> ModelCheckpoint  <br/><br/><em class="ma">### TODO: specify the number of epochs that you would like to use to train the model.</em><br/><br/>epochs = 50<br/><br/><em class="ma">### Do NOT modify the code below this line.</em><br/><br/>checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', <br/>                               verbose=1, save_best_only=<strong class="my iu">True</strong>)<br/><br/>model.fit(train_tensors, train_targets, <br/>          validation_data=(valid_tensors, valid_targets),<br/>          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)</span></pre><h1 id="181a" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">测试模型</h1><p id="3fcc" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在狗图像的测试数据集上尝试你的模型。</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="0361" class="nv kc it my b gy nw nx l ny nz"><em class="ma"># get index of predicted dog breed for each image in test set</em><br/>dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) <strong class="my iu">for</strong> tensor <strong class="my iu">in</strong> test_tensors]<br/><br/><em class="ma"># report test accuracy</em><br/>test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)<br/>print('Test accuracy: <strong class="my iu">%.4f%%</strong>' % test_accuracy)</span></pre><figure class="nn no np nq gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/4c3b0e8ca66616940879d336760c5c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*jyr5Q7DoPVWNjhjMdfKBAw.png"/></div></figure><h1 id="d5f2" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第四步:使用CNN对狗的品种进行分类</h1><p id="d093" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">为了在不牺牲准确性的情况下减少训练时间，我们向您展示如何使用迁移学习来训练CNN。</p><h1 id="5382" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">获取瓶颈特征</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="9676" class="nv kc it my b gy nw nx l ny nz">bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')<br/>train_VGG16 = bottleneck_features['train']<br/>valid_VGG16 = bottleneck_features['valid']<br/>test_VGG16 = bottleneck_features['test']</span></pre><h1 id="2d03" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">模型架构</h1><p id="a9db" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">该模型使用预训练的VGG-16模型作为固定特征提取器，其中VGG-16的最后卷积输出作为输入馈送到我们的模型。我们只添加了一个全局平均池层和一个全连接层，其中后者包含每个狗类别的一个节点，并配备了一个softmax。</p><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="6af3" class="nv kc it my b gy nw nx l ny nz">VGG16_model = Sequential()<br/>VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))<br/>VGG16_model.add(Dense(133, activation='softmax'))<br/><br/>VGG16_model.summary()</span></pre><h1 id="b747" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">编译模型</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="2584" class="nv kc it my b gy nw nx l ny nz">VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])</span></pre><h1 id="4eb0" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">训练模型</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="51ab" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">keras.callbacks</strong> <strong class="my iu">import</strong> ModelCheckpoint  <br/>checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', <br/>                               verbose=1, save_best_only=<strong class="my iu">True</strong>)<br/><br/>VGG16_model.fit(train_VGG16, train_targets, <br/>          validation_data=(valid_VGG16, valid_targets),<br/>          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)</span></pre><h1 id="d6b6" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">测试模型</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="3951" class="nv kc it my b gy nw nx l ny nz"><em class="ma"># get index of predicted dog breed for each image in test set</em><br/>VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) <strong class="my iu">for</strong> feature <strong class="my iu">in</strong> test_VGG16]<br/><br/><em class="ma"># report test accuracy</em><br/>test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)<br/>print('Test accuracy: <strong class="my iu">%.4f%%</strong>' % test_accuracy)</span></pre><figure class="nn no np nq gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/de64a17adfc2f7828f5cd97dded461c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*6f-vTjskOWMzxHkiKyFjCA.png"/></div></figure><h1 id="4a6e" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">用模型预测狗的品种</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="3d27" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">from</strong> <strong class="my iu">extract_bottleneck_features</strong> <strong class="my iu">import</strong> *<br/><br/><strong class="my iu">def</strong> VGG16_predict_breed(img_path):<br/>    <em class="ma"># extract bottleneck features</em><br/>    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))<br/>    <em class="ma"># obtain predicted vector</em><br/>    predicted_vector = VGG16_model.predict(bottleneck_feature)<br/>    <em class="ma"># return dog breed that is predicted by the model</em><br/>    <strong class="my iu">return</strong> dog_names[np.argmax(predicted_vector)]</span></pre><h1 id="eb0e" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第五步:创建一个CNN对狗的品种进行分类(使用迁移学习)</h1><p id="bdd7" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在第4步中，我们使用迁移学习来创建一个使用VGG-16瓶颈特征的CNN。在本节中，您必须使用不同的预训练模型中的瓶颈功能。为了方便您，我们预先计算了目前在Keras中可用的所有网络的功能:</p><ul class=""><li id="e3df" class="mz na it lb b lc mb lg mc lk nb lo nc ls nd lw ne nf ng nh bi translated"><a class="ae oa" href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz" rel="noopener ugc nofollow" target="_blank"> VGG-19 </a>的瓶颈特征</li><li id="b30c" class="mz na it lb b lc ni lg nj lk nk lo nl ls nm lw ne nf ng nh bi translated"><a class="ae oa" href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz" rel="noopener ugc nofollow" target="_blank"> ResNet-50 </a>瓶颈特性</li><li id="f241" class="mz na it lb b lc ni lg nj lk nk lo nl ls nm lw ne nf ng nh bi translated"><a class="ae oa" href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz" rel="noopener ugc nofollow" target="_blank">盗梦空间</a>瓶颈特性</li><li id="237b" class="mz na it lb b lc ni lg nj lk nk lo nl ls nm lw ne nf ng nh bi translated"><a class="ae oa" href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz" rel="noopener ugc nofollow" target="_blank">异常</a>瓶颈特性</li></ul><p id="5961" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">只需要遵循与第4步相同的步骤，那么我们的最终测试准确率为80.5024%。</p><h1 id="feae" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第六步:编写你的算法</h1><p id="cf1b" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">编写一个算法，该算法接受图像的文件路径，并首先确定图像是否包含人、狗或两者都不包含。然后，</p><ul class=""><li id="8056" class="mz na it lb b lc mb lg mc lk nb lo nc ls nd lw ne nf ng nh bi translated">如果在图像中检测到一只<strong class="lb iu">狗</strong>，返回预测的品种。</li><li id="d5cb" class="mz na it lb b lc ni lg nj lk nk lo nl ls nm lw ne nf ng nh bi translated">如果在图像中检测到一个<strong class="lb iu">人</strong>，返回相似的狗品种。</li><li id="5c5b" class="mz na it lb b lc ni lg nj lk nk lo nl ls nm lw ne nf ng nh bi translated">如果在图像中没有检测到<strong class="lb iu">或</strong>，则提供指示错误的输出。</li></ul><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="5aac" class="nv kc it my b gy nw nx l ny nz">%pylab inline<br/><strong class="my iu">import</strong> <strong class="my iu">matplotlib.pyplot</strong> <strong class="my iu">as</strong> <strong class="my iu">plt</strong><br/><strong class="my iu">import</strong> <strong class="my iu">matplotlib.image</strong> <strong class="my iu">as</strong> <strong class="my iu">mpimg</strong><br/><br/><em class="ma">### TODO: Write your algorithm.</em><br/><em class="ma">### Feel free to use as many code cells as needed.</em><br/><strong class="my iu">def</strong> image_detecter(img_path):<br/>    <br/>    img=mpimg.imread(img_path)<br/>    imgplot = plt.imshow(img)<br/>    plt.show()<br/>    <br/>    <strong class="my iu">if</strong> dog_detector(img_path) == <strong class="my iu">True</strong>:<br/>        dog_name = Resnet50_predict_breed(img_path)<br/>        print("a dog face is detected in the image and the predicted breed is <strong class="my iu">{}</strong>".format(dog_name.split(".")[1]))<br/>        <br/>    <strong class="my iu">elif</strong> face_detector(img_path) == <strong class="my iu">True</strong>:<br/>        resembling_breed = Resnet50_predict_breed(img_path)<br/>        print("a human face is detected in the image and the predicted resembling breed is <strong class="my iu">{}</strong>".format(resembling_breed.split(".")[1]))<br/>        <br/>    <strong class="my iu">else</strong>:<br/>        print("there is an error")</span></pre><h1 id="7865" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">步骤7:测试你的算法</h1><pre class="nn no np nq gt nr my ns nt aw nu bi"><span id="26ef" class="nv kc it my b gy nw nx l ny nz"><strong class="my iu">import</strong> <strong class="my iu">glob</strong><br/><br/><strong class="my iu">for</strong> filepath <strong class="my iu">in</strong> glob.iglob('images_for_step7/*.jpg'):<br/>    image_detecter(filepath)</span></pre><figure class="nn no np nq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/aa42e9a996aba1b6f9e760cfe18c8a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfAPahepTwSXpaa3QmxRPw.png"/></div></div></figure><figure class="nn no np nq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/b2653b8d50e800a6343ba293ffdb7ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f63aMA73UVyggZf4FkXJ6w.png"/></div></div></figure><figure class="nn no np nq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/e4216cf554fe85263c2adec5dedd4012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*82eLBYVD5XjF6zYMs7nnNA.png"/></div></div></figure><h1 id="7d8d" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结果:</h1><p id="4f08" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">最终的结果看起来相当准确，我们可以看到，狗的品种预测都是正确的，人类的相似品种是有意义的。</p><p id="5921" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">迁移学习比我从头构建的CNN模型更有效。主要是因为来自迁移学习的模型是由大量数据训练的，所以架构已经明白什么样的特征最能代表一幅图像，这使得分类过程变得容易得多，即使我们没有大量数据，我们也不需要牺牲准确性。</p><h1 id="6367" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结论:</h1><p id="1c67" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">端到端问题解决方案:本文提供了一个解决方案，它可以接收图像，然后返回狗的品种和类似的人类品种。</p><p id="2884" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">我在这个项目中发现的最有趣的方面是迁移学习的魔力，即使我们没有足够的数据，我们也能得到更好的结果，但我们仍然可以根据我们的目的训练预训练的模型，这是迁移学习最美妙的事情。</p><h1 id="e789" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">改进:</h1><p id="aa33" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">-获取更多数据来训练犬种分类模型<br/> -尝试通过使用不同的迁移学习模型来更多地调整模型<br/> -尝试向现有架构添加更多层</p><p id="cda6" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk me lm ln lo mg lq lr ls mi lu lv lw im bi translated">完整的编码请参考</p><div class="ol om gp gr on oo"><a href="https://github.com/liping97412/image_classifier/blob/master/dog_app.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">Liping 97412/图像分类器</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">permalink dissolve GitHub是4000多万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">github.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc jz oo"/></div></div></a></div></div></div>    
</body>
</html>