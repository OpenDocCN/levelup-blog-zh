<html>
<head>
<title>Movie review sentiment analysis with Naive Bayes | Machine Learning from Scratch (Part V)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于朴素贝叶斯的电影评论情感分析(五)</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/movie-review-sentiment-analysis-with-naive-bayes-machine-learning-from-scratch-part-v-7bb869391bab?source=collection_archive---------0-----------------------#2019-06-15">https://levelup.gitconnected.com/movie-review-sentiment-analysis-with-naive-bayes-machine-learning-from-scratch-part-v-7bb869391bab?source=collection_archive---------0-----------------------#2019-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0631" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何处理文本数据。创建通用文本分类器并预测IMDB电影评论的情感。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ad0f871b50d83f0862d7090e6af735fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0JKMLS0TQVHy1BsQqenjQ.jpeg"/></div></div></figure><blockquote class="kr"><p id="6040" class="ks kt iq bd ku kv kw kx ky kz la lb dk translated"><em class="lc">TL；DR使用Python从头开始构建朴素贝叶斯文本分类模型。使用该模型将IMDB电影评论分为正面或负面。</em></p></blockquote><p id="8cf2" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx lb ij bi translated">从你读到的推文到塞内卡的不朽著作，文本数据主宰着我们的世界。当我们越来越多地消费图片(看着你的Instagram)和视频时，你仍然每天多次阅读谷歌搜索结果。</p><p id="5817" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">文本数据经常出现的一个问题是<a class="ae md" href="https://en.wikipedia.org/wiki/Sentiment_analysis" rel="noopener ugc nofollow" target="_blank">情感分析</a>(分类)。想象一下你是一家很大的糖+水饮料公司。你想知道人们对你的产品的看法。您将搜索带有一些标签、徽标或名称的文本。然后，您可以使用情绪分析来判断意见是积极的还是消极的。当然，你会把那些消极的送到你在印度的报酬极低的支持中心去解决问题。</p><p id="30bb" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">在这里，我们将构建一个通用的文本分类器，将电影评论文本分为两类——负面或正面情绪。我们将简要地看一下贝叶斯定理，并使用天真的假设放松它的要求。</p><div class="me mf gp gr mg mh"><a href="https://colab.research.google.com/drive/1OPQDDJTKy0b40pziZWSsoBCmQV6HyXsm" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd ir gy z fp mm fr fs mn fu fw ip bi translated">谷歌联合实验室</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">基于朴素贝叶斯的电影评论分析——colab.research.google.com</h3></div></div><div class="mp l"><div class="mq l mr ms mt mp mu kp mh"/></div></div></a></div><h1 id="9a82" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak">处理文字</strong></h1><p id="ff0c" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">计算机不理解文本数据，尽管它们对数字处理得很好。<a class="ae md" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>提供了一套解决文本相关问题的方法，并将文本表示为数字。虽然NLP是一个广阔的领域，但我们将使用一些简单的预处理技术和<a class="ae md" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词包</a>模型。</p><h2 id="cdfd" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">数据</strong></h2><p id="d352" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">我们的数据来自一个Kaggle挑战——“一袋文字遇上一袋爆米花”<a class="ae md" href="https://www.kaggle.com/c/word2vec-nlp-tutorial" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="b205" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们有来自IMDB的<em class="oe"> 25，000条</em>电影评论，标记为正面或负面。您可能知道IMDB评级在<em class="oe">0–10</em>范围内。由数据集作者完成的附加预处理步骤将评级转换成二元情感(&lt; 5 —负面)。当然，一部电影可以有多条评论，但不超过<em class="oe"> 30条</em>。</p><h2 id="fa78" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">阅读评论</strong></h2><p id="e150" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">让我们在Pandas数据框中加载训练和测试数据:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="4560" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">探索</strong></h2><p id="cb99" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">让我们感受一下我们的数据。以下是训练数据的前5行:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/dbeeb9570da390b75432d363bb2d9bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*Q5OQhDmi5Su2zHKCHBBHWg.png"/></div></figure><p id="ec6d" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们将重点关注情绪和评论专栏。id列将电影id与一条评论的唯一编号相结合。这可能是现实世界场景中的一条重要信息，但我们将保持它的简单性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/b37ed7e5d06124c1f5fefd58adfe4e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDuSm53PQuPOTOvQQ117LQ.png"/></div></div></figure><p id="2c60" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">积极和消极的情绪同样存在。不需要额外的噱头来解决这个问题！</p><p id="cf41" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">以下是训练数据集评论中最常见的词:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/3e7dc7677125f3c5932b6ea182a096d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FWsSX4rpenO-_z9hZaI_3g.png"/></div></div></figure><p id="a014" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">嗯，那个<strong class="lf ir"> br </strong>看起来怪怪的，对吧？</p><h2 id="748f" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">清洁</strong></h2><p id="93cd" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">真实世界的文本数据真的很乱。它可以包含过多的标点符号、HTML标签(包括那个<strong class="lf ir"> br </strong>)、多个空格等等。我们将尝试移除/正常化大部分内容。</p><p id="0607" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们将使用[正则表达式](https://en . Wikipedia . org/wiki/Regular _ expression)进行大部分清理，但我们将使用两个库来处理HTML标签(令人惊讶地难以删除)和删除常用(停止)单词:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="ce27" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">首先，我们使用<a class="ae md" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>从文本中移除HTML标签。其次，我们删除所有不是字母或空格的内容(注意忽略了大写字符)。最后，我们用一个单独的间距替换多余的间距。</p><h2 id="0568" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">标记化</strong></h2><p id="c160" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">既然我们的评论是“干净的”，我们可以进一步为我们的单词袋模型做准备。让我们把它们转换成小写字母，并把它们分解成单个的单词。这个过程被称为<a class="ae md" href="https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" rel="noopener ugc nofollow" target="_blank">标记化</a>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="afb2" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们预处理的最后一步是使用那些在<a class="ae md" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>库中定义的移除<a class="ae md" href="https://en.wikipedia.org/wiki/Stop_words" rel="noopener ugc nofollow" target="_blank">停止字</a>。停用词通常是频繁出现的词，可能不会显著影响文本的含义。英语中的一些例子有:“是”、“the”、“and”。</p><p id="d6cb" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">删除停用词的另一个好处是加快了我们的模型，因为我们删除了大量的训练/测试数据。但是，在现实世界中，您应该考虑删除停用词是否合理。</p><p id="6a4c" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们将把我们的clean和tokenize函数放在一个名为<code class="fe ok ol om on b"><em class="oe">Tokenizer</em></code>(也称为Terminator)的类中。</p><h1 id="0b5a" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak">朴素贝叶斯</strong></h1><p id="28d1" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated"><strong class="lf ir"> <em class="oe">朴素贝叶斯</em> </strong>模型是概率分类器，它使用<a class="ae md" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>并强烈假设数据的特征是独立的。对于我们的例子，这意味着每个单词都是独立的。</p><p id="e44e" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">直觉上，这听起来可能是个愚蠢的想法。你知道(甚至从阅读中)前一个单词会影响当前和下一个单词。然而，这个假设简化了数学运算，并且<a class="ae md" href="https://www.cc.gatech.edu/~isbell/reading/papers/Rish.pdf" rel="noopener ugc nofollow" target="_blank">在实践中非常有效</a>！</p><p id="836c" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">贝叶斯定理被定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/e7e545c7f7cb603bf614a34672eaa3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*wIiRjb6thTR2xkTJs0sH0Q.png"/></div></figure><p id="4864" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">其中<strong class="lf ir"> <em class="oe"> A </em> </strong>和<strong class="lf ir"> <em class="oe"> B </em> </strong>是一些事件和<strong class="lf ir"> <em class="oe"> P(。)</em> </strong>是一个概率。</p><p id="33b8" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">这个等式给出了事件<strong class="lf ir"> <em class="oe"> A </em> </strong>发生的条件概率，假设<strong class="lf ir"> <em class="oe"> B </em> </strong>已经发生。为了找到这一点，我们需要计算<strong class="lf ir"><em class="oe"/></strong>发生的概率，假设<strong class="lf ir"><em class="oe"/></strong>已经发生，然后乘以<strong class="lf ir"><em class="oe"/></strong>(称为<em class="oe">先于</em>)发生的概率。这一切除以<strong class="lf ir"> <em class="oe"> B </em> </strong>自行发生的概率。</p><p id="974a" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">这个天真的假设允许我们将我们例子中的贝叶斯定理重新表述为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/3035845fb6c4d8b1d519b226efe5528b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*2aHJ2sqOQQcjZEmEyfgKUw.png"/></div></div></figure><p id="3b14" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们并不关心概率。我们只想知道一个给定的文本有积极的还是消极的情绪。我们可以完全跳过分母，因为它只是缩放分子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0dc2b5fef0bc3be73ae530db710909ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*bzjVCJlNjN-0ZwQQMjBrLw.png"/></div></figure><h1 id="802f" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak">实现多项式朴素贝叶斯</strong></h1><p id="3b1b" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">你可能已经猜到了，我们将文本分为两类——积极情绪和消极情绪。</p><p id="0cd1" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">多项式朴素贝叶斯允许我们将模型的特征表示为它们出现的频率(某个词在我们的评论中出现的频率)。换句话说，它告诉我们，我们使用的概率分布是<a class="ae md" href="https://en.wikipedia.org/wiki/Multinomial_distribution" rel="noopener ugc nofollow" target="_blank">多项式</a>。</p><h2 id="e80f" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">关于数值稳定性的说明</strong></h2><p id="73d1" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">我们的模型依赖于许多概率的相乘。这些可能会变得越来越小，我们的计算机可能会将它们四舍五入为零。为了解决这个问题，我们将使用对数概率，在我们的等式中取每一边的<em class="oe"> log </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/4a968ccb222d83ebd7bc02a75e406e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*euWPAgvUBxA_8hrm_d1sgQ.png"/></div></div></figure><p id="ea7d" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">注意，我们仍然可以使用我们的模型的最高分数来预测情绪，因为log是<a class="ae md" href="https://en.wikipedia.org/wiki/Monotonic_function" rel="noopener ugc nofollow" target="_blank">单调的</a>。</p><h2 id="f548" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">建立我们的模型</strong></h2><p id="23be" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">最后，是时候用Python实现我们的模型了。让我们从定义一些变量开始，并按类对数据进行分组，这样我们的训练代码会更整洁一些:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="0ef9" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们将实现一个通用的文本分类器，它不假设类的数量。你可以用它来进行新闻分类预测、情感分析、垃圾邮件检测等。</p><p id="f725" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">对于每个类，我们将找到其中的例子数量和对数概率(先验)。我们还将记录每个单词的出现次数，并创建一个词汇表，即我们在训练数据中看到的所有单词的集合:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="bf93" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">请注意，我们使用我们的<code class="fe ok ol om on b"><em class="oe">Tokenizer</em></code>和内置的类<a class="ae md" href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="noopener ugc nofollow" target="_blank">计数器</a>将评论转换为一袋单词。</p><p id="0478" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">如果你感兴趣，下面是<code class="fe ok ol om on b">group_by_class</code>的工作方式:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="4ca9" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">做预测</strong></h2><p id="b024" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">为了从文本数据中预测情感，我们将使用我们的类别先验和词汇:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="1017" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">对于每个评论，我们使用正面和负面情绪的对数先验，并标记文本。对于每个单词，我们检查它是否在词汇表中，如果不在，就跳过它。然后我们计算每个类这个词的对数概率。我们将班级分数相加，选择分数最高的班级作为我们的预测。</p><p id="4d2c" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated"><strong class="lf ir">拉普拉斯平滑</strong></p><p id="5d8b" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">注意<em class="oe"> log(0) </em>是<strong class="lf ir"> <em class="oe"> undefined </em> </strong>(而且不，我们这里不用JavaScript)。我们词汇中的一个单词完全有可能出现在一个类别中，而不在另一个类别中——在那个类别中找到这个单词的概率将是0！我们可以使用<a class="ae md" href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener ugc nofollow" target="_blank">拉普拉斯平滑</a>来修复这个问题。我们将简单地在分子上加1，同时在分母上增加我们的词汇量:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h1 id="b830" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak">预测情绪</strong></h1><p id="fbd8" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">既然你的模型可以被训练并做出预测，那么让我们用它来预测来自电影评论的情绪。</p><h2 id="a467" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">数据准备</strong></h2><p id="338b" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">如前所述，我们将只使用训练数据中的review和perspective列。让我们把它分成训练和测试两部分:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="02a2" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lm nx ny nh lq nz oa nj lu ob oc nl od bi translated"><strong class="ak">评估</strong></h2><p id="45e2" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">我们将把我们的<code class="fe ok ol om on b">fit</code>和<code class="fe ok ol om on b">predict</code>函数打包成一个名为<code class="fe ok ol om on b">MultinomialNaiveBayes</code>的类。让我们使用它:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="da43" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">我们的分类器接受一个可能的类列表和一个标记器作为参数。还有，API也挺好看的(感谢scikit-learn！)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><pre class="kg kh ki kj gt os on ot ou aw ov bi"><span id="8579" class="ns mw iq on b gy ow ox l oy oz">0.8556</span></pre><p id="5df9" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">这看起来不错。我们在测试集上获得了大约86%的准确率。</p><p id="cc3a" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">这是分类报告:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/565105b36d114e4ce873b836e7153221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*yUBpxC2aRfGfSSohChMbSg.png"/></div></div></figure><p id="3d19" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">和混淆矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/7410dbea963919bd558b479235c39968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MipPczz04CNHmHUSY57jHA.png"/></div></div></figure><p id="cb0c" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">总的来说，我们的分类器做得很好。将预测提交给Kaggle，并找出您在排行榜上的位置。</p><h1 id="fd16" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak">结论</strong></h1><p id="fca2" class="pw-post-body-paragraph ld le iq lf b lg nn jr li lj no ju ll lm np lo lp lq nq ls lt lu nr lw lx lb ij bi translated">干得好！您刚刚构建了一个多项式朴素贝叶斯分类器，它在情感预测方面做得非常好。您还学习了贝叶斯定理、文本处理和拉普拉斯平滑！另一种风格的朴素贝叶斯分类器性能会更好吗？</p><div class="me mf gp gr mg mh"><a href="https://colab.research.google.com/drive/1OPQDDJTKy0b40pziZWSsoBCmQV6HyXsm" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd ir gy z fp mm fr fs mn fu fw ip bi translated">谷歌联合实验室</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">基于朴素贝叶斯的电影评论分析——colab.research.google.com</h3></div></div><div class="mp l"><div class="mq l mr ms mt mp mu kp mh"/></div></div></a></div><p id="afac" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">接下来，您将从头开始构建一个推荐系统！</p></div><div class="ab cl pc pd hu pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="ij ik il im in"><p id="4aec" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated"><em class="oe">最初发表于</em><a class="ae md" href="https://www.curiousily.com/posts/movie-review-sentiment-analysis-with-naive-bayes/" rel="noopener ugc nofollow" target="_blank"><em class="oe">【https://www.curiousily.com】</em></a><em class="oe">。</em></p></div><div class="ab cl pc pd hu pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="ij ik il im in"><p id="2ad0" class="pw-post-body-paragraph ld le iq lf b lg ly jr li lj lz ju ll lm ma lo lp lq mb ls lt lu mc lw lx lb ij bi translated">喜欢你读的吗？你想了解更多关于机器学习的知识吗？提升你对ML的理解:</p><div class="me mf gp gr mg mh"><a href="https://leanpub.com/hmls" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd ir gy z fp mm fr fs mn fu fw ip bi translated">从零开始实践机器学习</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">“我不能创造的，我不理解”——理查德·费曼这本书将引导你走向更深的…</h3></div><div class="pj l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">leanpub.com</p></div></div><div class="mp l"><div class="pk l mr ms mt mp mu kp mh"/></div></div></a></div></div></div>    
</body>
</html>