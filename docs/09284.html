<html>
<head>
<title>Introduction to Natural Language Processing (NLP) in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中的自然语言处理(NLP)介绍</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/introduction-to-natural-language-processing-nlp-in-pytorch-8b7344c9dfec?source=collection_archive---------6-----------------------#2021-07-23">https://levelup.gitconnected.com/introduction-to-natural-language-processing-nlp-in-pytorch-8b7344c9dfec?source=collection_archive---------6-----------------------#2021-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6452c6a59f75fabb7944ab1cf7e1213b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5Vf07_3j_IfELV2mhiDRg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@pietrozj" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>在<a class="ae kc" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="44a5" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">单词嵌入</h1><p id="71b3" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">单词嵌入或单词向量提供了一种将单词从词汇表映射到低维空间的方法，在低维空间中，具有相似含义的单词靠得很近。让我们使用一组预先训练好的单词向量来熟悉它们的属性。存在多组预训练的单词嵌入；这里，我们使用ConceptNet Numberbatch，它以易于使用的格式(h5)提供了一个相对较小的下载。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="56e1" class="mi ke iq me b gy mj mk l ml mm"># Download word vectors<br/>from urllib.request import urlretrieve<br/>import os<br/>if not os.path.isfile('datasets/mini.h5'):<br/>    print("Downloading Conceptnet Numberbatch word embeddings...")<br/>    conceptnet_url = '<a class="ae kc" href="http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'" rel="noopener ugc nofollow" target="_blank">http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'</a><br/>    urlretrieve(conceptnet_url, 'datasets/mini.h5')</span></pre><p id="9a10" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">要读取h5文件，我们需要使用<code class="fe ms mt mu me b">h5py</code>包。如果您遵循了PyTorch在1A的安装说明，您应该已经下载了它。否则，您可以使用</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="8569" class="mi ke iq me b gy mj mk l ml mm"><em class="mv"># If you environment isn't currently active, activate it:</em><br/><em class="mv"># conda activate pytorch</em></span><span id="906b" class="mi ke iq me b gy mw mk l ml mm">pip install h5py</span></pre><p id="c68f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">您可能需要重新打开此笔记本，安装才能生效。</p><p id="1caa" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">下面，我们用包打开刚刚下载的<code class="fe ms mt mu me b">mini.h5</code>文件。我们从文件中提取一个utf-8编码的单词列表，以及它们的300维向量。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="cd77" class="mi ke iq me b gy mj mk l ml mm"># Load the file and pull out words and embeddings<br/>import h5py</span><span id="2a6f" class="mi ke iq me b gy mw mk l ml mm">with h5py.File('datasets/mini.h5', 'r') as f:<br/>    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]<br/>    all_embeddings = f['mat']['block0_values'][:]<br/>    <br/>print("all_words dimensions: {}".format(len(all_words)))<br/>print("all_embeddings dimensions: {}".format(all_embeddings.shape))</span><span id="fe8f" class="mi ke iq me b gy mw mk l ml mm">print("Random example word: {}".format(all_words[1337]))</span></pre><p id="5381" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">现在，<code class="fe ms mt mu me b">all_words</code>是一列𝑉字符串(我们称之为<em class="mv">词汇表</em>)，而<code class="fe ms mt mu me b">all_embeddings</code>是一个𝑉×300矩阵。琴弦的形式是<code class="fe ms mt mu me b">/c/language_code/word</code>——例如<code class="fe ms mt mu me b">/c/en/cat</code>和<code class="fe ms mt mu me b">/c/es/gato</code>。</p><p id="f40d" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们只对英语单词感兴趣。我们使用Python list comprehensions来提取英语单词的索引，然后只提取英语单词(去掉六个字符的<code class="fe ms mt mu me b">/c/en/</code>前缀)及其嵌入。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="3339" class="mi ke iq me b gy mj mk l ml mm"># Restrict our vocabulary to just the English words<br/>english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]<br/>english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]<br/>english_embeddings = all_embeddings[english_word_indices]</span><span id="ea05" class="mi ke iq me b gy mw mk l ml mm">print("Number of English words in all_words: {0}".format(len(english_words)))<br/>print("english_embeddings dimensions: {0}".format(english_embeddings.shape))</span><span id="b4f4" class="mi ke iq me b gy mw mk l ml mm">print(english_words[1337])</span></pre><p id="7628" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">单词向量的大小不如它的方向重要；数量可以被认为代表使用的频率，与单词的语义无关。在这里，我们将对语义感兴趣，所以我们<em class="mv">归一化</em>我们的向量，将每个向量除以它的长度。结果是，我们所有的单词向量的长度都是1，因此，位于单位圆上。两个向量的点积与它们之间角度的余弦成正比，并提供了相似性的度量(余弦越大，角度越小)。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/594e93f8496fd204ae3fe7c018e1e351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*VBY52u4xClPcZ3_cZGfN3Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图片来自作者</figcaption></figure><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="ebe7" class="mi ke iq me b gy mj mk l ml mm">import numpy as np</span><span id="d9ef" class="mi ke iq me b gy mw mk l ml mm">norms = np.linalg.norm(english_embeddings, axis=1)<br/>normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])</span></pre><p id="2143" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们想方便地查找单词，所以我们创建了一个字典，它将我们从一个单词映射到它在单词嵌入矩阵中的索引。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="d598" class="mi ke iq me b gy mj mk l ml mm">index = {word: i for i, word in enumerate(english_words)}</span></pre><p id="cdf8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">现在我们准备好测量单词对之间的相似度。我们用NumPy取点积。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="b419" class="mi ke iq me b gy mj mk l ml mm">def similarity_score(w1, w2):<br/>    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])<br/>    return score</span><span id="1ec2" class="mi ke iq me b gy mw mk l ml mm"># A word is as similar with itself as possible:<br/>print('cat\tcat\t', similarity_score('cat', 'cat'))</span><span id="9cbf" class="mi ke iq me b gy mw mk l ml mm"># Closely related words still get high scores:<br/>print('cat\tfeline\t', similarity_score('cat', 'feline'))<br/>print('cat\tdog\t', similarity_score('cat', 'dog'))</span><span id="8e21" class="mi ke iq me b gy mw mk l ml mm"># Unrelated words, not so much<br/>print('cat\tmoo\t', similarity_score('cat', 'moo'))<br/>print('cat\tfreeze\t', similarity_score('cat', 'freeze'))</span><span id="7d4d" class="mi ke iq me b gy mw mk l ml mm"># Antonyms are still considered related, sometimes more so than synonyms<br/>print('antonym\topposite\t', similarity_score('antonym', 'opposite'))<br/>print('antonym\tsynonym\t', similarity_score('antonym', 'synonym'))</span><span id="1d6c" class="mi ke iq me b gy mw mk l ml mm">int('antonym\tsynonym\t', similarity_score('antonym', 'synonym'))</span></pre><p id="a21c" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">例如，我们还可以找到与给定单词最相似的单词</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="42a3" class="mi ke iq me b gy mj mk l ml mm">def closest_to_vector(v, n):<br/>    all_scores = np.dot(normalized_embeddings, v)<br/>    best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))<br/>    return best_words[:n]</span><span id="fcf2" class="mi ke iq me b gy mw mk l ml mm">def most_similar(w, n):<br/>    return closest_to_vector(normalized_embeddings[index[w], :], n)</span><span id="c728" class="mi ke iq me b gy mw mk l ml mm">print(most_similar('cat', 10))<br/>print(most_similar('dog', 10))<br/>print(most_similar('duke', 10))</span></pre><p id="0f73" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们也可以使用<code class="fe ms mt mu me b">closest_to_vector</code>来寻找我们自己创造的“附近”的单词向量。这让我们能够解决类比。比如为了解决类比“男:弟::女:？”，我们可以计算一个新的向量<code class="fe ms mt mu me b">brother - man + woman</code>:兄弟的意思，减去男人的意思，加上女人的意思。然后，我们可以询问哪些单词在嵌入空间中最接近这个新向量。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="e7a4" class="mi ke iq me b gy mj mk l ml mm">def solve_analogy(a1, b1, a2):<br/>    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]<br/>    return closest_to_vector(b2, 1)</span><span id="dc38" class="mi ke iq me b gy mw mk l ml mm">print(solve_analogy("man", "brother", "woman"))<br/>print(solve_analogy("man", "husband", "woman"))<br/>print(solve_analogy("spain", "madrid", "france"))</span></pre><p id="77c8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">这三个结果都挺好的，但总的来说，这些类比的结果可能会令人失望。尝试其他类比，看看你是否能想出办法来解决你注意到的问题(例如，对<code class="fe ms mt mu me b">solve_analogy()</code>算法的修改)。</p><h1 id="beac" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">在深度模型中使用单词嵌入</h1><p id="229a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">单词嵌入很有趣，但它们的主要用途是让我们认为单词存在于连续的欧几里得空间中；然后，我们可以使用现有的具有连续数字数据的机器学习技术(如逻辑回归或神经网络)来处理文本。让我们来看一个特别简单的版本。我们将对一组电影评论进行<em class="mv">情感分析</em>:特别是，我们将尝试根据文本将电影评论分为正面或负面。</p><p id="3efb" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们将使用一个简单的单词嵌入模型来完成这个任务。我们将代表一篇评论，因为<em class="mv">表示单词在评论中的嵌入</em>。然后我们将训练一个两层MLP(一个神经网络)来将评论分为正面或负面。正如你可能猜到的那样，仅仅使用嵌入的平均值会丢弃句子中的大量信息，但是对于情感分析这样的任务，它会有惊人的效果。</p><p id="889c" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">如果您还没有，请下载<code class="fe ms mt mu me b">movie-simple.txt</code>文件。该文件的每一行包含</p><ol class=""><li id="74a4" class="my mz iq ld b le mn li mo lm na lq nb lu nc ly nd ne nf ng bi translated">数字0(表示负数)或数字1(表示正数)，后面跟着</li><li id="f100" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly nd ne nf ng bi translated">一个制表符(空白字符)，然后</li><li id="e88d" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly nd ne nf ng bi translated">评论本身。</li></ol><p id="8f04" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">让我们首先读取数据文件，将每一行解析成一个输入表示及其对应的标签。同样，因为我们使用SWEM，我们将把所有单词的单词嵌入的平均值作为我们的输入。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7559" class="mi ke iq me b gy mj mk l ml mm">import string<br/>remove_punct=str.maketrans('','',string.punctuation)</span><span id="aaa3" class="mi ke iq me b gy mw mk l ml mm"># This function converts a line of our data file into<br/># a tuple (x, y), where x is 300-dimensional representation<br/># of the words in a review, and y is its label.<br/>def convert_line_to_example(line):<br/>    # Pull out the first character: that's our label (0 or 1)<br/>    y = int(line[0])<br/>    <br/>    # Split the line into words using Python's split() function<br/>    words = line[2:].translate(remove_punct).lower().split()<br/>    <br/>    # Look up the embeddings of each word, ignoring words not<br/>    # in our pretrained vocabulary.<br/>    embeddings = [normalized_embeddings[index[w]] for w in words<br/>                  if w in index]<br/>    <br/>    # Take the mean of the embeddings<br/>    x = np.mean(np.vstack(embeddings), axis=0)<br/>    return x, y</span><span id="b5a1" class="mi ke iq me b gy mw mk l ml mm"># Apply the function to each line in the file.<br/>xs = []<br/>ys = []<br/>with open("datasets/movie-simple.txt", "r", encoding='utf-8', errors='ignore') as f:<br/>    for l in f.readlines():<br/>        x, y = convert_line_to_example(l)<br/>        xs.append(x)<br/>        ys.append(y)</span><span id="7ebf" class="mi ke iq me b gy mw mk l ml mm"># Concatenate all examples into a numpy array<br/>xs = np.vstack(xs)<br/>ys = np.vstack(ys)</span><span id="829a" class="mi ke iq me b gy mw mk l ml mm">print("Shape of inputs: {}".format(xs.shape))<br/>print("Shape of labels: {}".format(ys.shape))</span><span id="10a8" class="mi ke iq me b gy mw mk l ml mm">num_examples = xs.shape[0]</span></pre><p id="8e05" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">注意，在这个设置中，作为预处理的一部分，我们的输入单词已经被转换为向量。与学习单词嵌入相反，这实质上是在整个训练过程中将我们的单词嵌入锁定在适当的位置。学习单词嵌入，无论是从零开始还是从一些预先训练的初始化中进行微调，通常都是可取的，因为它使它们专门用于特定的任务。然而，因为我们的数据集相对较小，而且我们的计算预算也有限，所以我们将放弃学习这个模型的单词嵌入。我们稍后将再次讨论这个问题。</p><p id="a373" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">现在我们已经解析了数据，让我们保存20%的数据(四舍五入为整数)用于测试，其余的用于训练。我们加载的文件首先有所有的负面评价，然后是所有的正面评价，所以我们需要在将它拆分成训练和测试拆分之前对其进行洗牌。然后我们将数据转换成PyTorch张量，这样我们就可以将它们输入到我们的模型中。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="cb8b" class="mi ke iq me b gy mj mk l ml mm">print("First 20 labels before shuffling: {0}".format(ys[:20, 0]))</span><span id="2be8" class="mi ke iq me b gy mw mk l ml mm">shuffle_idx = np.random.permutation(num_examples)<br/>xs = xs[shuffle_idx, :]<br/>ys = ys[shuffle_idx, :]</span><span id="e3a3" class="mi ke iq me b gy mw mk l ml mm">print("First 20 labels after shuffling: {0}".format(ys[:20, 0]))</span><span id="71de" class="mi ke iq me b gy mw mk l ml mm">import torch</span><span id="a673" class="mi ke iq me b gy mw mk l ml mm">num_train = 4*num_examples // 5</span><span id="6065" class="mi ke iq me b gy mw mk l ml mm">x_train = torch.tensor(xs[:num_train])<br/>y_train = torch.tensor(ys[:num_train], dtype=torch.float32)</span><span id="94b8" class="mi ke iq me b gy mw mk l ml mm">x_test = torch.tensor(xs[num_train:])<br/>y_test = torch.tensor(ys[num_train:], dtype=torch.float32)</span></pre><p id="0c66" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们可以在将每一批数据输入到模型中时分别对其进行格式化，但是为了方便起见，让我们创建一个TensorDataset和DataLoader，就像我们过去对MNIST使用的那样。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="ff82" class="mi ke iq me b gy mj mk l ml mm">reviews_train = torch.utils.data.TensorDataset(x_train, y_train)<br/>reviews_test = torch.utils.data.TensorDataset(x_test, y_test)</span><span id="a188" class="mi ke iq me b gy mw mk l ml mm">train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)<br/>test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)</span></pre><p id="adcf" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">是时候在PyTorch中构建模型了。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="63e6" class="mi ke iq me b gy mj mk l ml mm">import torch.nn as nn<br/>import torch.nn.functional as F</span></pre><p id="279b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">首先，我们构建模型，组织成一个<code class="fe ms mt mu me b">nn.Module</code>。我们可以将MLP的输出数设为该数据集的类数(即2)。然而，由于我们这里只有两个输出类(“正”与“负”)，我们可以生成一个输出值，将所有大于00的称为“正”，将所有小于00的称为“负”。如果我们通过sigmoid运算传递此输出，则值被映射到[0，1][0，1]，0.50.5是分类阈值。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4e79" class="mi ke iq me b gy mj mk l ml mm">class SWEM(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.fc1 = nn.Linear(300, 64)<br/>        self.fc2 = nn.Linear(64, 1)</span><span id="94fa" class="mi ke iq me b gy mw mk l ml mm">def forward(self, x):<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.fc2(x)<br/>        return x</span></pre><p id="c04a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了训练模型，我们实例化模型。请注意，因为我们只进行二进制分类，所以我们使用二进制交叉熵(BCE)损失，而不是我们之前看到的交叉熵损失。为了数值稳定性，我们使用“带逻辑”的版本。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="06b9" class="mi ke iq me b gy mj mk l ml mm">## Training<br/># Instantiate model<br/>model = SWEM()</span><span id="c7aa" class="mi ke iq me b gy mw mk l ml mm"># Binary cross-entropy (BCE) Loss and Adam Optimizer<br/>criterion = nn.BCEWithLogitsLoss()<br/>optimizer = torch.optim.Adam(model.parameters(), lr=0.001)</span><span id="ad05" class="mi ke iq me b gy mw mk l ml mm"># Iterate through train set minibatchs <br/>for epoch in range(250):<br/>    correct = 0<br/>    num_examples = 0<br/>    for inputs, labels in train_loader:<br/>        # Zero out the gradients<br/>        optimizer.zero_grad()<br/>        <br/>        # Forward pass<br/>        y = model(inputs)<br/>        loss = criterion(y, labels)<br/>        <br/>        # Backward pass<br/>        loss.backward()<br/>        optimizer.step()<br/>        <br/>        predictions = torch.round(torch.sigmoid(y))<br/>        correct += torch.sum((predictions == labels).float())<br/>        num_examples += len(inputs)<br/>    <br/>    # Print training progress<br/>    if epoch % 25 == 0:<br/>        acc = correct/num_examples<br/>        print("Epoch: {0} \t Train Loss: {1} \t Train Acc: {2}".format(epoch, loss, acc))</span><span id="edbf" class="mi ke iq me b gy mw mk l ml mm">## Testing<br/>correct = 0<br/>num_test = 0</span><span id="2ae9" class="mi ke iq me b gy mw mk l ml mm">with torch.no_grad():<br/>    # Iterate through test set minibatchs <br/>    for inputs, labels in test_loader:<br/>        # Forward pass<br/>        y = model(inputs)<br/>        <br/>        predictions = torch.round(torch.sigmoid(y))<br/>        correct += torch.sum((predictions == labels).float())<br/>        num_test += len(inputs)<br/>    <br/>print('Test accuracy: {}'.format(correct/num_test))</span></pre><p id="d052" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们现在可以检查我们的模型已经学习了什么，看看它如何响应不同单词的单词向量:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7060" class="mi ke iq me b gy mj mk l ml mm"># Check some words<br/>words_to_test = ["exciting", "hated", "boring", "loved"]</span><span id="eeb8" class="mi ke iq me b gy mw mk l ml mm">for word in words_to_test:<br/>    x = torch.tensor(normalized_embeddings[index[word]].reshape(1, 300))<br/>    print("Sentiment of the word '{0}': {1}".format(word, torch.sigmoid(model(x))))</span></pre><p id="4b2b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">试试自己的一些话吧！你也可以尝试改变模型，重新训练它，看看结果如何变化。能否修改架构以获得更好的性能？或者，你能在不牺牲太多准确性的情况下简化模型吗？如果你尝试直接对均值嵌入进行分类呢？</p><h1 id="7332" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">学习单词嵌入</h1><p id="f8c8" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在前面的例子中，我们使用了预先训练的单词嵌入，但没有学习它们。单词嵌入是预处理的一部分，并且在整个训练中保持不变。如果我们有足够的数据，我们可能更喜欢学习单词embeddings和我们的模型。预训练单词嵌入通常是在具有无监督目标的大型语料库上训练的，并且通常是非特定的。如果我们有足够的数据，我们可能更喜欢学习单词嵌入，要么从零开始，要么通过微调，因为使它们特定于任务可能会提高性能。</p><p id="6a24" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们如何学习单词嵌入？为此，我们需要让它们成为我们模型的一部分，而不是加载数据的一部分。在PyTorch中，这样做的首选方式是使用<code class="fe ms mt mu me b">nn.Embedding</code>。像我们见过的其他<code class="fe ms mt mu me b">nn</code>层(例如<code class="fe ms mt mu me b">nn.Linear</code>)一样，<code class="fe ms mt mu me b">nn.Embedding</code>必须首先被实例化。实例化有两个必需的参数，即嵌入的数量(即词汇大小𝑉)和单词嵌入的维度(在前面的例子中是300)。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="bb74" class="mi ke iq me b gy mj mk l ml mm">VOCAB_SIZE = 5000<br/>EMBED_DIM = 300</span><span id="dd86" class="mi ke iq me b gy mw mk l ml mm">embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)</span><span id="05da" class="mi ke iq me b gy mw mk l ml mm">embedding <strong class="me ir">=</strong> nn.Embedding(VOCAB_SIZE, EMBED_DIM)</span></pre><p id="aad7" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在引擎盖下，这创建了一个5000×300的单词嵌入矩阵。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="5ebf" class="mi ke iq me b gy mj mk l ml mm">embedding.weight.size()</span></pre><p id="6cd1" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">请注意，这个矩阵基本上是一个300维的单词嵌入5000个单词中的每一个，堆叠在彼此的顶部。在这个嵌入矩阵中查找一个单词嵌入，就是简单地选择这个矩阵的特定行，对应于这个单词。</p><p id="f24f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">当学习单词嵌入时，<code class="fe ms mt mu me b">nn.Embedding</code>查找通常是模型模块中的第一个操作。例如，如果我们要为之前的SWEM模型学习单词embeddings，该模型可能看起来像这样:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4eac" class="mi ke iq me b gy mj mk l ml mm">class SWEMWithEmbeddings(nn.Module):<br/>    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):<br/>        super().__init__()<br/>        self.embedding = nn.Embedding(vocab_size, embedding_size)<br/>        self.fc1 = nn.Linear(embedding_size, hidden_dim)<br/>        self.fc2 = nn.Linear(hidden_dim, num_outputs)</span><span id="5d24" class="mi ke iq me b gy mw mk l ml mm">def forward(self, x):<br/>        x = self.embedding(x)<br/>        x = torch.mean(x, dim=0)<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.fc2(x)<br/>        return x</span></pre><p id="c115" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">这里，我们将模型各层的大小抽象为构造函数参数，因此我们需要在初始化时指定这些超参数。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="eac0" class="mi ke iq me b gy mj mk l ml mm">model = SWEMWithEmbeddings(<br/>    vocab_size = 5000,<br/>    embedding_size = 300, <br/>    hidden_dim = 64, <br/>    num_outputs = 1,<br/>)<br/>print(model)</span></pre><p id="25ca" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">注意，通过使嵌入成为我们模型的一部分，对<code class="fe ms mt mu me b">forward()</code>函数的预期输入现在是输入句子的单词标记，所以我们也必须修改我们的数据输入管道。我们将在下一个笔记本(4B)中看到如何做到这一点。</p><h1 id="2170" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">递归神经网络</h1><p id="6292" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在深度学习的背景下，序列数据通常用递归神经网络(RNNs)建模。由于自然语言可以被看作是一个单词序列，自然神经网络通常用于自然语言处理。正如我们之前看到的全连接和卷积网络一样，rnn使用线性和非线性变换的组合来将输入投影到更高级别的表示中，这些表示可以与其他层堆叠在一起。</p><h2 id="5854" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">作为序列的句子</h2><p id="f41c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">顺序模型与我们之前看到的模型之间的关键区别在于“时间”维度的存在:句子(或段落、文档)中的单词有一个传达意义的顺序:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9efa74e720d958748f13114271dc1d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*iZDuQcxeIUQtvJx4Syz_9A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</figcaption></figure><p id="d514" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在上面的示例序列中，单词“Recurrent”是𝑡=1单词，我们用𝑤1表示；同样，“神经”是𝑤2，等等。正如前面几节所希望给你留下的印象，将单词建模为嵌入向量𝑥1,…,𝑥𝑇通常比一键向量(𝑤1,…𝑤𝑇对应的令牌)更有利，所以我们的第一步通常是为每个输入单词做一个嵌入表查找。让我们假设300维的单词嵌入，并且为了简单起见，假设一个大小为1的小批量。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="db16" class="mi ke iq me b gy mj mk l ml mm">mb = 1<br/>x_dim = 300 <br/>sentence = ["recurrent", "neural", "networks", "are", "great"]</span><span id="c0a6" class="mi ke iq me b gy mw mk l ml mm">xs = []<br/>for word in sentence:<br/>    xs.append(torch.tensor(normalized_embeddings[index[word]]).view(1, x_dim))<br/>    <br/>xs = torch.stack(xs, dim=0)<br/>print("xs shape: {}".format(xs.shape))</span></pre><p id="4151" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">请注意，我们将输入格式化为(单词×小批×嵌入尺寸单词×小批×嵌入尺寸)。这是PyTorch RNNs的首选输入顺序。</p><p id="f9fa" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">假设我们想要处理这个例子。在我们之前的情感分析例子中，我们只是取了一段时间内的平均嵌入，将输入视为一个“单词包”对于简单的问题，这可以出奇地好，但是正如您可能想象的那样，句子中单词的顺序通常很重要，有时，我们也希望能够对这种时间意义进行建模。输入RNNs。</p><h2 id="460d" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">回顾:全连接层</h2><p id="4361" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们介绍RNN之前，让我们再次回顾一下我们在逻辑回归和多层感知器示例中使用的全连接层，在符号上有一些变化:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a508948bc8fe8eed1f6b267e05f9283b.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*0KyK1A_OEBG1MbIqcxf0Jg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</figcaption></figure><p id="1e1a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">对于隐藏状态，我们将把全连接图层的结果称为ℎ，而不是𝑦。变量𝑦通常保留给神经网络的最后一层；因为逻辑回归是单层的，所以使用𝑦就可以了。然而，如果我们假设有一个以上的层，更常见的是将中间表示称为ℎ.注意，我们也使用𝑓(来表示非线性激活函数。过去，我们将𝑓()视为ReLU，但这也可能是𝜎()或tanh()非线性。可视化:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/ec7bf025c4d6c71a01bdc6d4e4493ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*C9wZXSrH5nzmv1AWAZaXiQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</figcaption></figure><p id="d997" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">这里要注意的关键是，我们用线性变换(用𝑊W和𝑏b)投射输入𝑥x，然后对输出应用非线性，在训练期间给我们ℎh.，我们的目标是学习𝑊W和𝑏b.</p><h2 id="5a19" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">基本的RNN</h2><p id="ab83" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">与我们之前看到的使用全连接图层的示例不同，顺序数据具有多个输入𝑥1,…,𝑥𝑇，而不是单个𝑥.我们需要根据RNN的情况调整我们的模型。虽然有几种变体，但RNN的一种常见基本配方是埃尔曼RNN，如下所示*:</p><p id="b3e8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ℎ𝑡=tanh((𝑥𝑡𝑊𝑥+𝑏𝑥)+(ℎ𝑡−1𝑊ℎ+𝑏ℎ))</p><p id="b9f2" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">其中tanh()t是双曲正切，一个非线性激活函数。RNNs按顺序一次处理一个单词(𝑥𝑡)，在每个时间步产生一个隐藏状态ℎ𝑡ht。上面等式的前半部分应该看起来很熟悉；与全连接层一样，我们对每个输入𝑥𝑡进行线性变换，然后应用非线性。请注意，我们在每个时间步都应用了相同的线性变换(𝑊𝑥，𝑏𝑥)。不同之处在于，我们还对之前隐藏的状态ℎ𝑡−1应用了单独的线性变换(𝑊ℎ，𝑏ℎ),并将其添加到我们的投影输入中。这种反馈被称为<em class="mv">循环</em>连接。</p><p id="83e7" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">RNN架构中的这些有向循环赋予了它们对时间动态进行建模的能力，使它们特别适合对序列(例如文本)进行建模。我们可以将RNN图层可视化如下:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6b42b1781a910811c4ba8985c02e3da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*uZVDIHTeizzvyEgaMTCwFg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</figcaption></figure><p id="d7cb" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们可以在时间中展开一个RNN，让它的时序性更加明显:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/3ede5950ff7ad08d39b33fb52065c7bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uE9luJS3xLAgMfjY07593w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</figcaption></figure><p id="f737" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">您可以将这些循环连接视为允许模型在计算当前输入的隐藏状态时考虑序列的先前隐藏状态。</p><p id="9476" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">*注意:我们实际上不需要两个单独的偏差𝑏𝑥和𝑏ℎ，因为你可以将两个偏差合并成一个单一的可学习参数𝑏.然而，单独编写它有助于清楚地表明我们正在对𝑥𝑡和ℎ𝑡−1.执行线性转换说到组合变量，我们也可以通过将𝑥𝑡xt和ℎ𝑡−1串联成单个向量𝑧𝑡，然后执行单个矩阵乘𝑧𝑡𝑊𝑧+𝑏来表达上述操作，其中𝑊𝑧本质上是𝑊𝑥和𝑊ℎ串联。事实上，这就是实现的“官方”RNNs模块的数量，因为单独矩阵乘法运算数量的减少使得计算效率更高。这些都是实现细节。</p><h2 id="f6a9" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">PyTorch的RNNs</h2><p id="5a13" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们如何在PyTorch中实现RNN？有相当多的方法，但让我们首先从零开始建立埃尔曼RNN，使用输入序列“递归神经网络是伟大的”。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="df49" class="mi ke iq me b gy mj mk l ml mm"># As always, import PyTorch first<br/>import numpy as np<br/>import torch</span></pre><p id="85c1" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在RNN中，我们将输入𝑥𝑡和之前隐藏的状态ℎ𝑡−1都投影到某个隐藏的维度，我们将选择该维度为128。为了执行这些操作，我们要定义一些我们将要学习的变量。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="d698" class="mi ke iq me b gy mj mk l ml mm">h_dim = 128</span><span id="4fee" class="mi ke iq me b gy mw mk l ml mm"># For projecting the input<br/>Wx = torch.randn(x_dim, h_dim)/np.sqrt(x_dim)<br/>Wx.requires_grad_()<br/>bx = torch.zeros(h_dim, requires_grad=True)</span><span id="5d8e" class="mi ke iq me b gy mw mk l ml mm"># For projecting the previous state<br/>Wh = torch.randn(h_dim, h_dim)/np.sqrt(h_dim)<br/>Wh.requires_grad_()<br/>bh = torch.zeros(h_dim, requires_grad=True)</span><span id="84a3" class="mi ke iq me b gy mw mk l ml mm">print(Wx.shape, bx.shape, Wh.shape, bh.shape)</span></pre><p id="fb21" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了方便起见，我们为RNN的一个时间步长定义一个函数。该函数采用当前输入𝑥𝑡和先前隐藏的状态ℎ𝑡−1，执行线性变换𝑥𝑊𝑥+𝑏𝑥和ℎ𝑊ℎ+𝑏ℎ，然后是双曲正切非线性。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="673c" class="mi ke iq me b gy mj mk l ml mm">def RNN_step(x, h):<br/>    h_next = torch.tanh((torch.matmul(x, Wx) + bx) + (torch.matmul(h, Wh) + bh))</span><span id="ea97" class="mi ke iq me b gy mw mk l ml mm">return h_next</span></pre><p id="f702" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们的RNN的每一步都需要输入(即单词表示)和之前的隐藏状态(之前序列的总结)。注意，在句子的开头，我们没有先前的隐藏状态，所以我们将其初始化为某个值，例如，全零:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="3e91" class="mi ke iq me b gy mj mk l ml mm"># Word embedding for first word<br/>x1 = xs[0, :, :]</span><span id="4495" class="mi ke iq me b gy mw mk l ml mm"># Initialize hidden state to 0<br/>h0 = torch.zeros([mb, h_dim])</span></pre><p id="f5a8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了采取RNN的一次性步骤，我们调用我们编写的函数，在𝑥1和ℎ0.传递在这种情况下，</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="c428" class="mi ke iq me b gy mj mk l ml mm"># Forward pass of one RNN step for time step t=1<br/>h1 = RNN_step(x1, h0)</span><span id="d887" class="mi ke iq me b gy mw mk l ml mm">print("Hidden state h1 dimensions: {0}".format(h1.shape))</span></pre><p id="1fae" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们可以再次调用<code class="fe ms mt mu me b">RNN_step</code>函数，从我们的RNN中获得下一个时间步长输出。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="656a" class="mi ke iq me b gy mj mk l ml mm"># Word embedding for second word<br/>x2 = xs[1, :, :]</span><span id="e201" class="mi ke iq me b gy mw mk l ml mm"># Forward pass of one RNN step for time step t=2<br/>h2 = RNN_step(x2, h1)</span><span id="2788" class="mi ke iq me b gy mw mk l ml mm">print("Hidden state h2 dimensions: {0}".format(h2.shape))</span></pre><p id="52e7" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们可以根据需要继续展开RNN。对于每一步，我们馈入当前输入(𝑥𝑡)和先前的隐藏状态(ℎ𝑡−1)以获得新的输出。</p><h2 id="d786" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">使用<code class="fe ms mt mu me b">torch.nn</code></h2><p id="0d66" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在实践中，很像全连接和卷积层，我们通常不像上面那样从头实现RNNs，而是依赖于更高级别的API。PyTorch在<code class="fe ms mt mu me b">torch.nn</code>库中实现了RNNs。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4793" class="mi ke iq me b gy mj mk l ml mm">import torch.nn</span><span id="969e" class="mi ke iq me b gy mw mk l ml mm">rnn = nn.RNN(x_dim, h_dim)<br/>print("RNN parameter shapes: {}".format([p.shape for p in rnn.parameters()]))</span></pre><p id="2af0" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">请注意，由<code class="fe ms mt mu me b">torch.nn</code>创建的RNN产生的参数与我们从上面的例子中得到的参数具有相同的维度。</p><p id="263c" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了使用RNN执行向前传递，我们将整个输入序列传递给<code class="fe ms mt mu me b">forward()</code>函数，该函数返回每个时间步长的隐藏状态(<code class="fe ms mt mu me b">hs</code>)和最终的隐藏状态(<code class="fe ms mt mu me b">h_T</code>)。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="45e3" class="mi ke iq me b gy mj mk l ml mm">hs, h_T = rnn(xs)</span><span id="54dd" class="mi ke iq me b gy mw mk l ml mm">print("Hidden states shape: {}".format(hs.shape))<br/>print("Final hidden state shape: {}".format(h_T.shape))</span></pre><p id="f057" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们如何处理这些隐藏状态呢？这取决于型号和任务。就像多层感知器和卷积神经网络一样，rnn也可以堆叠在多个层中。在这种情况下，输出ℎ1,…,ℎ𝑇是下一层的顺序输入。如果RNN层是最终层，则ℎ𝑇或ℎ1,…,ℎ𝑇的平均值/最大值可以用作数据序列的汇总编码。预测的结果也会对RNN产出的最终用途产生影响。</p><h2 id="cb5d" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">门控RNNs</h2><p id="202b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">虽然我们刚刚探索的rnn可以成功地模拟简单的序列数据，但它们往往难以处理较长的序列，其中<a class="ae kc" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度</a>是一个特别大的问题。多年来已经提出了许多RNN变体来缓解这个问题，并且经验表明这些变体更加有效。特别是，长短期记忆(LSTM)和门控循环单元(GRU)最近在深度学习中得到了广泛的应用。我们不打算在这里详细讨论它们与普通rnn在结构上有什么不同；一个奇妙的总结可以在这里找到<a class="ae kc" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">。注意,“RNN”作为一个名字有点超载:它既可以指我们之前讨论过的基本递归模型，也可以指一般的递归模型(包括LSTMs和GRUs)。</a></p><p id="605b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">创建LSTMs和GRUs图层的方法与创建基本RNN图层的方法大致相同。同样，不要自己实现它，建议使用<code class="fe ms mt mu me b">torch.nn</code>实现，尽管我们强烈建议您查看源代码，这样您就能理解幕后发生了什么。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7471" class="mi ke iq me b gy mj mk l ml mm">lstm = nn.LSTM(x_dim, h_dim)<br/>print("LSTM parameters: {}".format([p.shape for p in lstm.parameters()]))</span><span id="0814" class="mi ke iq me b gy mw mk l ml mm">gru = nn.GRU(x_dim, h_dim)<br/>print("GRU parameters: {}".format([p.shape for p in gru.parameters()]))</span></pre><h1 id="3f9b" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">火炬报</h1><p id="bf43" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">就像PyTorch拥有用于计算机视觉的<a class="ae kc" href="https://pytorch.org/docs/stable/torchvision/index.html" rel="noopener ugc nofollow" target="_blank"> Torchvision </a>一样，PyTorch也拥有用于自然语言处理的<a class="ae kc" href="https://torchtext.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Torchtext </a>。与Torchvision一样，Torchtext拥有大量流行的NLP基准数据集，涵盖广泛的任务(如情感分析、语言建模、机器翻译)。它也有一些预先训练好的单词嵌入，包括流行的单词表示全局向量(GloVe)。如果您需要加载自己的数据集，Torchtext有许多有用的容器，可以使数据管道更容易。</p><p id="cf73" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">您需要安装TorchText来使用它:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="0121" class="mi ke iq me b gy mj mk l ml mm"><em class="mv"># If you environment isn't currently active, activate it:</em><br/><em class="mv"># conda activate pytorch</em></span><span id="e79e" class="mi ke iq me b gy mw mk l ml mm">pip install torchtext</span></pre><p id="eedd" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一旦你理解了这句话，你就已经完成了PyTorch中自然语言处理(NLP)入门的所有步骤</p><p id="1342" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">以下是您今天的成就总结:</p><ul class=""><li id="06e9" class="my mz iq ld b le mn li mo lm na lq nb lu nc ly oc ne nf ng bi translated"><strong class="ld ir">单词嵌入</strong></li><li id="d2a8" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir">在深度模型中使用单词嵌入</strong></li><li id="cee6" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir">学习单词嵌入</strong></li><li id="d5dd" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir"> <em class="mv">递归神经网络(RNNs):句子作为序列，复习:全连接层，基本RNN，PyTorch中的RNNs，使用torch.nn，门控RNNs，</em>T5】</strong></li><li id="8074" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir">火炬文本</strong></li></ul></div></div>    
</body>
</html>