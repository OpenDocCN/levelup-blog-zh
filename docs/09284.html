<html>
<head>
<title>Introduction to Natural Language Processing (NLP) in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorchä¸­çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»‹ç»</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://levelup.gitconnected.com/introduction-to-natural-language-processing-nlp-in-pytorch-8b7344c9dfec?source=collection_archive---------6-----------------------#2021-07-23">https://levelup.gitconnected.com/introduction-to-natural-language-processing-nlp-in-pytorch-8b7344c9dfec?source=collection_archive---------6-----------------------#2021-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6452c6a59f75fabb7944ab1cf7e1213b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5Vf07_3j_IfELV2mhiDRg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">ç…§ç‰‡ç”±<a class="ae kc" href="https://unsplash.com/@pietrozj" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>åœ¨<a class="ae kc" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>ä¸Šæ‹æ‘„</figcaption></figure><h1 id="44a5" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">å•è¯åµŒå…¥</h1><p id="71b3" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">å•è¯åµŒå…¥æˆ–å•è¯å‘é‡æä¾›äº†ä¸€ç§å°†å•è¯ä»è¯æ±‡è¡¨æ˜ å°„åˆ°ä½ç»´ç©ºé—´çš„æ–¹æ³•ï¼Œåœ¨ä½ç»´ç©ºé—´ä¸­ï¼Œå…·æœ‰ç›¸ä¼¼å«ä¹‰çš„å•è¯é å¾—å¾ˆè¿‘ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ç»„é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯å‘é‡æ¥ç†Ÿæ‚‰å®ƒä»¬çš„å±æ€§ã€‚å­˜åœ¨å¤šç»„é¢„è®­ç»ƒçš„å•è¯åµŒå…¥ï¼›è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ConceptNet Numberbatchï¼Œå®ƒä»¥æ˜“äºä½¿ç”¨çš„æ ¼å¼(h5)æä¾›äº†ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ä¸‹è½½ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="56e1" class="mi ke iq me b gy mj mk l ml mm"># Download word vectors<br/>from urllib.request import urlretrieve<br/>import os<br/>if not os.path.isfile('datasets/mini.h5'):<br/>    print("Downloading Conceptnet Numberbatch word embeddings...")<br/>    conceptnet_url = '<a class="ae kc" href="http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'" rel="noopener ugc nofollow" target="_blank">http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'</a><br/>    urlretrieve(conceptnet_url, 'datasets/mini.h5')</span></pre><p id="9a10" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¦è¯»å–h5æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨<code class="fe ms mt mu me b">h5py</code>åŒ…ã€‚å¦‚æœæ‚¨éµå¾ªäº†PyTorchåœ¨1Açš„å®‰è£…è¯´æ˜ï¼Œæ‚¨åº”è¯¥å·²ç»ä¸‹è½½äº†å®ƒã€‚å¦åˆ™ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="8569" class="mi ke iq me b gy mj mk l ml mm"><em class="mv"># If you environment isn't currently active, activate it:</em><br/><em class="mv"># conda activate pytorch</em></span><span id="906b" class="mi ke iq me b gy mw mk l ml mm">pip install h5py</span></pre><p id="c68f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æ‚¨å¯èƒ½éœ€è¦é‡æ–°æ‰“å¼€æ­¤ç¬”è®°æœ¬ï¼Œå®‰è£…æ‰èƒ½ç”Ÿæ•ˆã€‚</p><p id="1caa" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä¸‹é¢ï¼Œæˆ‘ä»¬ç”¨åŒ…æ‰“å¼€åˆšåˆšä¸‹è½½çš„<code class="fe ms mt mu me b">mini.h5</code>æ–‡ä»¶ã€‚æˆ‘ä»¬ä»æ–‡ä»¶ä¸­æå–ä¸€ä¸ªutf-8ç¼–ç çš„å•è¯åˆ—è¡¨ï¼Œä»¥åŠå®ƒä»¬çš„300ç»´å‘é‡ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="cd77" class="mi ke iq me b gy mj mk l ml mm"># Load the file and pull out words and embeddings<br/>import h5py</span><span id="2a6f" class="mi ke iq me b gy mw mk l ml mm">with h5py.File('datasets/mini.h5', 'r') as f:<br/>    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]<br/>    all_embeddings = f['mat']['block0_values'][:]<br/>    <br/>print("all_words dimensions: {}".format(len(all_words)))<br/>print("all_embeddings dimensions: {}".format(all_embeddings.shape))</span><span id="fe8f" class="mi ke iq me b gy mw mk l ml mm">print("Random example word: {}".format(all_words[1337]))</span></pre><p id="5381" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ç°åœ¨ï¼Œ<code class="fe ms mt mu me b">all_words</code>æ˜¯ä¸€åˆ—ğ‘‰å­—ç¬¦ä¸²(æˆ‘ä»¬ç§°ä¹‹ä¸º<em class="mv">è¯æ±‡è¡¨</em>)ï¼Œè€Œ<code class="fe ms mt mu me b">all_embeddings</code>æ˜¯ä¸€ä¸ªğ‘‰Ã—300çŸ©é˜µã€‚ç´å¼¦çš„å½¢å¼æ˜¯<code class="fe ms mt mu me b">/c/language_code/word</code>â€”â€”ä¾‹å¦‚<code class="fe ms mt mu me b">/c/en/cat</code>å’Œ<code class="fe ms mt mu me b">/c/es/gato</code>ã€‚</p><p id="f40d" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬åªå¯¹è‹±è¯­å•è¯æ„Ÿå…´è¶£ã€‚æˆ‘ä»¬ä½¿ç”¨Python list comprehensionsæ¥æå–è‹±è¯­å•è¯çš„ç´¢å¼•ï¼Œç„¶ååªæå–è‹±è¯­å•è¯(å»æ‰å…­ä¸ªå­—ç¬¦çš„<code class="fe ms mt mu me b">/c/en/</code>å‰ç¼€)åŠå…¶åµŒå…¥ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="3339" class="mi ke iq me b gy mj mk l ml mm"># Restrict our vocabulary to just the English words<br/>english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]<br/>english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]<br/>english_embeddings = all_embeddings[english_word_indices]</span><span id="ea05" class="mi ke iq me b gy mw mk l ml mm">print("Number of English words in all_words: {0}".format(len(english_words)))<br/>print("english_embeddings dimensions: {0}".format(english_embeddings.shape))</span><span id="b4f4" class="mi ke iq me b gy mw mk l ml mm">print(english_words[1337])</span></pre><p id="7628" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">å•è¯å‘é‡çš„å¤§å°ä¸å¦‚å®ƒçš„æ–¹å‘é‡è¦ï¼›æ•°é‡å¯ä»¥è¢«è®¤ä¸ºä»£è¡¨ä½¿ç”¨çš„é¢‘ç‡ï¼Œä¸å•è¯çš„è¯­ä¹‰æ— å…³ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å¯¹è¯­ä¹‰æ„Ÿå…´è¶£ï¼Œæ‰€ä»¥æˆ‘ä»¬<em class="mv">å½’ä¸€åŒ–</em>æˆ‘ä»¬çš„å‘é‡ï¼Œå°†æ¯ä¸ªå‘é‡é™¤ä»¥å®ƒçš„é•¿åº¦ã€‚ç»“æœæ˜¯ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„å•è¯å‘é‡çš„é•¿åº¦éƒ½æ˜¯1ï¼Œå› æ­¤ï¼Œä½äºå•ä½åœ†ä¸Šã€‚ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ä¸å®ƒä»¬ä¹‹é—´è§’åº¦çš„ä½™å¼¦æˆæ­£æ¯”ï¼Œå¹¶æä¾›äº†ç›¸ä¼¼æ€§çš„åº¦é‡(ä½™å¼¦è¶Šå¤§ï¼Œè§’åº¦è¶Šå°)ã€‚</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/594e93f8496fd204ae3fe7c018e1e351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*VBY52u4xClPcZ3_cZGfN3Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">å›¾ç‰‡æ¥è‡ªä½œè€…</figcaption></figure><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="ebe7" class="mi ke iq me b gy mj mk l ml mm">import numpy as np</span><span id="d9ef" class="mi ke iq me b gy mw mk l ml mm">norms = np.linalg.norm(english_embeddings, axis=1)<br/>normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])</span></pre><p id="2143" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬æƒ³æ–¹ä¾¿åœ°æŸ¥æ‰¾å•è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå­—å…¸ï¼Œå®ƒå°†æˆ‘ä»¬ä»ä¸€ä¸ªå•è¯æ˜ å°„åˆ°å®ƒåœ¨å•è¯åµŒå…¥çŸ©é˜µä¸­çš„ç´¢å¼•ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="d598" class="mi ke iq me b gy mj mk l ml mm">index = {word: i for i, word in enumerate(english_words)}</span></pre><p id="cdf8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½æµ‹é‡å•è¯å¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬ç”¨NumPyå–ç‚¹ç§¯ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="b419" class="mi ke iq me b gy mj mk l ml mm">def similarity_score(w1, w2):<br/>    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])<br/>    return score</span><span id="1ec2" class="mi ke iq me b gy mw mk l ml mm"># A word is as similar with itself as possible:<br/>print('cat\tcat\t', similarity_score('cat', 'cat'))</span><span id="9cbf" class="mi ke iq me b gy mw mk l ml mm"># Closely related words still get high scores:<br/>print('cat\tfeline\t', similarity_score('cat', 'feline'))<br/>print('cat\tdog\t', similarity_score('cat', 'dog'))</span><span id="8e21" class="mi ke iq me b gy mw mk l ml mm"># Unrelated words, not so much<br/>print('cat\tmoo\t', similarity_score('cat', 'moo'))<br/>print('cat\tfreeze\t', similarity_score('cat', 'freeze'))</span><span id="7d4d" class="mi ke iq me b gy mw mk l ml mm"># Antonyms are still considered related, sometimes more so than synonyms<br/>print('antonym\topposite\t', similarity_score('antonym', 'opposite'))<br/>print('antonym\tsynonym\t', similarity_score('antonym', 'synonym'))</span><span id="1d6c" class="mi ke iq me b gy mw mk l ml mm">int('antonym\tsynonym\t', similarity_score('antonym', 'synonym'))</span></pre><p id="a21c" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä¾‹å¦‚ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ‰¾åˆ°ä¸ç»™å®šå•è¯æœ€ç›¸ä¼¼çš„å•è¯</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="42a3" class="mi ke iq me b gy mj mk l ml mm">def closest_to_vector(v, n):<br/>    all_scores = np.dot(normalized_embeddings, v)<br/>    best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))<br/>    return best_words[:n]</span><span id="fcf2" class="mi ke iq me b gy mw mk l ml mm">def most_similar(w, n):<br/>    return closest_to_vector(normalized_embeddings[index[w], :], n)</span><span id="c728" class="mi ke iq me b gy mw mk l ml mm">print(most_similar('cat', 10))<br/>print(most_similar('dog', 10))<br/>print(most_similar('duke', 10))</span></pre><p id="0f73" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨<code class="fe ms mt mu me b">closest_to_vector</code>æ¥å¯»æ‰¾æˆ‘ä»¬è‡ªå·±åˆ›é€ çš„â€œé™„è¿‘â€çš„å•è¯å‘é‡ã€‚è¿™è®©æˆ‘ä»¬èƒ½å¤Ÿè§£å†³ç±»æ¯”ã€‚æ¯”å¦‚ä¸ºäº†è§£å†³ç±»æ¯”â€œç”·:å¼Ÿ::å¥³:ï¼Ÿâ€ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸€ä¸ªæ–°çš„å‘é‡<code class="fe ms mt mu me b">brother - man + woman</code>:å…„å¼Ÿçš„æ„æ€ï¼Œå‡å»ç”·äººçš„æ„æ€ï¼ŒåŠ ä¸Šå¥³äººçš„æ„æ€ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®å“ªäº›å•è¯åœ¨åµŒå…¥ç©ºé—´ä¸­æœ€æ¥è¿‘è¿™ä¸ªæ–°å‘é‡ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="e7a4" class="mi ke iq me b gy mj mk l ml mm">def solve_analogy(a1, b1, a2):<br/>    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]<br/>    return closest_to_vector(b2, 1)</span><span id="dc38" class="mi ke iq me b gy mw mk l ml mm">print(solve_analogy("man", "brother", "woman"))<br/>print(solve_analogy("man", "husband", "woman"))<br/>print(solve_analogy("spain", "madrid", "france"))</span></pre><p id="77c8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¿™ä¸‰ä¸ªç»“æœéƒ½æŒºå¥½çš„ï¼Œä½†æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç±»æ¯”çš„ç»“æœå¯èƒ½ä¼šä»¤äººå¤±æœ›ã€‚å°è¯•å…¶ä»–ç±»æ¯”ï¼Œçœ‹çœ‹ä½ æ˜¯å¦èƒ½æƒ³å‡ºåŠæ³•æ¥è§£å†³ä½ æ³¨æ„åˆ°çš„é—®é¢˜(ä¾‹å¦‚ï¼Œå¯¹<code class="fe ms mt mu me b">solve_analogy()</code>ç®—æ³•çš„ä¿®æ”¹)ã€‚</p><h1 id="beac" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">åœ¨æ·±åº¦æ¨¡å‹ä¸­ä½¿ç”¨å•è¯åµŒå…¥</h1><p id="229a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">å•è¯åµŒå…¥å¾ˆæœ‰è¶£ï¼Œä½†å®ƒä»¬çš„ä¸»è¦ç”¨é€”æ˜¯è®©æˆ‘ä»¬è®¤ä¸ºå•è¯å­˜åœ¨äºè¿ç»­çš„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­ï¼›ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°æœ‰çš„å…·æœ‰è¿ç»­æ•°å­—æ•°æ®çš„æœºå™¨å­¦ä¹ æŠ€æœ¯(å¦‚é€»è¾‘å›å½’æˆ–ç¥ç»ç½‘ç»œ)æ¥å¤„ç†æ–‡æœ¬ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªç‰¹åˆ«ç®€å•çš„ç‰ˆæœ¬ã€‚æˆ‘ä»¬å°†å¯¹ä¸€ç»„ç”µå½±è¯„è®ºè¿›è¡Œ<em class="mv">æƒ…æ„Ÿåˆ†æ</em>:ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†å°è¯•æ ¹æ®æ–‡æœ¬å°†ç”µå½±è¯„è®ºåˆ†ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚</p><p id="3efb" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å•è¯åµŒå…¥æ¨¡å‹æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å°†ä»£è¡¨ä¸€ç¯‡è¯„è®ºï¼Œå› ä¸º<em class="mv">è¡¨ç¤ºå•è¯åœ¨è¯„è®ºä¸­çš„åµŒå…¥</em>ã€‚ç„¶åæˆ‘ä»¬å°†è®­ç»ƒä¸€ä¸ªä¸¤å±‚MLP(ä¸€ä¸ªç¥ç»ç½‘ç»œ)æ¥å°†è¯„è®ºåˆ†ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚æ­£å¦‚ä½ å¯èƒ½çŒœåˆ°çš„é‚£æ ·ï¼Œä»…ä»…ä½¿ç”¨åµŒå…¥çš„å¹³å‡å€¼ä¼šä¸¢å¼ƒå¥å­ä¸­çš„å¤§é‡ä¿¡æ¯ï¼Œä½†æ˜¯å¯¹äºæƒ…æ„Ÿåˆ†æè¿™æ ·çš„ä»»åŠ¡ï¼Œå®ƒä¼šæœ‰æƒŠäººçš„æ•ˆæœã€‚</p><p id="889c" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">å¦‚æœæ‚¨è¿˜æ²¡æœ‰ï¼Œè¯·ä¸‹è½½<code class="fe ms mt mu me b">movie-simple.txt</code>æ–‡ä»¶ã€‚è¯¥æ–‡ä»¶çš„æ¯ä¸€è¡ŒåŒ…å«</p><ol class=""><li id="74a4" class="my mz iq ld b le mn li mo lm na lq nb lu nc ly nd ne nf ng bi translated">æ•°å­—0(è¡¨ç¤ºè´Ÿæ•°)æˆ–æ•°å­—1(è¡¨ç¤ºæ­£æ•°)ï¼Œåé¢è·Ÿç€</li><li id="f100" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly nd ne nf ng bi translated">ä¸€ä¸ªåˆ¶è¡¨ç¬¦(ç©ºç™½å­—ç¬¦)ï¼Œç„¶å</li><li id="e88d" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly nd ne nf ng bi translated">è¯„è®ºæœ¬èº«ã€‚</li></ol><p id="8f04" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è®©æˆ‘ä»¬é¦–å…ˆè¯»å–æ•°æ®æ–‡ä»¶ï¼Œå°†æ¯ä¸€è¡Œè§£ææˆä¸€ä¸ªè¾“å…¥è¡¨ç¤ºåŠå…¶å¯¹åº”çš„æ ‡ç­¾ã€‚åŒæ ·ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨SWEMï¼Œæˆ‘ä»¬å°†æŠŠæ‰€æœ‰å•è¯çš„å•è¯åµŒå…¥çš„å¹³å‡å€¼ä½œä¸ºæˆ‘ä»¬çš„è¾“å…¥ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7559" class="mi ke iq me b gy mj mk l ml mm">import string<br/>remove_punct=str.maketrans('','',string.punctuation)</span><span id="aaa3" class="mi ke iq me b gy mw mk l ml mm"># This function converts a line of our data file into<br/># a tuple (x, y), where x is 300-dimensional representation<br/># of the words in a review, and y is its label.<br/>def convert_line_to_example(line):<br/>    # Pull out the first character: that's our label (0 or 1)<br/>    y = int(line[0])<br/>    <br/>    # Split the line into words using Python's split() function<br/>    words = line[2:].translate(remove_punct).lower().split()<br/>    <br/>    # Look up the embeddings of each word, ignoring words not<br/>    # in our pretrained vocabulary.<br/>    embeddings = [normalized_embeddings[index[w]] for w in words<br/>                  if w in index]<br/>    <br/>    # Take the mean of the embeddings<br/>    x = np.mean(np.vstack(embeddings), axis=0)<br/>    return x, y</span><span id="b5a1" class="mi ke iq me b gy mw mk l ml mm"># Apply the function to each line in the file.<br/>xs = []<br/>ys = []<br/>with open("datasets/movie-simple.txt", "r", encoding='utf-8', errors='ignore') as f:<br/>    for l in f.readlines():<br/>        x, y = convert_line_to_example(l)<br/>        xs.append(x)<br/>        ys.append(y)</span><span id="7ebf" class="mi ke iq me b gy mw mk l ml mm"># Concatenate all examples into a numpy array<br/>xs = np.vstack(xs)<br/>ys = np.vstack(ys)</span><span id="829a" class="mi ke iq me b gy mw mk l ml mm">print("Shape of inputs: {}".format(xs.shape))<br/>print("Shape of labels: {}".format(ys.shape))</span><span id="10a8" class="mi ke iq me b gy mw mk l ml mm">num_examples = xs.shape[0]</span></pre><p id="8e05" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æ³¨æ„ï¼Œåœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œä½œä¸ºé¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„è¾“å…¥å•è¯å·²ç»è¢«è½¬æ¢ä¸ºå‘é‡ã€‚ä¸å­¦ä¹ å•è¯åµŒå…¥ç›¸åï¼Œè¿™å®è´¨ä¸Šæ˜¯åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å°†æˆ‘ä»¬çš„å•è¯åµŒå…¥é”å®šåœ¨é€‚å½“çš„ä½ç½®ã€‚å­¦ä¹ å•è¯åµŒå…¥ï¼Œæ— è®ºæ˜¯ä»é›¶å¼€å§‹è¿˜æ˜¯ä»ä¸€äº›é¢„å…ˆè®­ç»ƒçš„åˆå§‹åŒ–ä¸­è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸éƒ½æ˜¯å¯å–çš„ï¼Œå› ä¸ºå®ƒä½¿å®ƒä»¬ä¸“é—¨ç”¨äºç‰¹å®šçš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ç›¸å¯¹è¾ƒå°ï¼Œè€Œä¸”æˆ‘ä»¬çš„è®¡ç®—é¢„ç®—ä¹Ÿæœ‰é™ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æ”¾å¼ƒå­¦ä¹ è¿™ä¸ªæ¨¡å‹çš„å•è¯åµŒå…¥ã€‚æˆ‘ä»¬ç¨åå°†å†æ¬¡è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚</p><p id="a373" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ç°åœ¨æˆ‘ä»¬å·²ç»è§£æäº†æ•°æ®ï¼Œè®©æˆ‘ä»¬ä¿å­˜20%çš„æ•°æ®(å››èˆäº”å…¥ä¸ºæ•´æ•°)ç”¨äºæµ‹è¯•ï¼Œå…¶ä½™çš„ç”¨äºè®­ç»ƒã€‚æˆ‘ä»¬åŠ è½½çš„æ–‡ä»¶é¦–å…ˆæœ‰æ‰€æœ‰çš„è´Ÿé¢è¯„ä»·ï¼Œç„¶åæ˜¯æ‰€æœ‰çš„æ­£é¢è¯„ä»·ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨å°†å®ƒæ‹†åˆ†æˆè®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†ä¹‹å‰å¯¹å…¶è¿›è¡Œæ´—ç‰Œã€‚ç„¶åæˆ‘ä»¬å°†æ•°æ®è½¬æ¢æˆPyTorchå¼ é‡ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬è¾“å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="cb8b" class="mi ke iq me b gy mj mk l ml mm">print("First 20 labels before shuffling: {0}".format(ys[:20, 0]))</span><span id="2be8" class="mi ke iq me b gy mw mk l ml mm">shuffle_idx = np.random.permutation(num_examples)<br/>xs = xs[shuffle_idx, :]<br/>ys = ys[shuffle_idx, :]</span><span id="e3a3" class="mi ke iq me b gy mw mk l ml mm">print("First 20 labels after shuffling: {0}".format(ys[:20, 0]))</span><span id="71de" class="mi ke iq me b gy mw mk l ml mm">import torch</span><span id="a673" class="mi ke iq me b gy mw mk l ml mm">num_train = 4*num_examples // 5</span><span id="6065" class="mi ke iq me b gy mw mk l ml mm">x_train = torch.tensor(xs[:num_train])<br/>y_train = torch.tensor(ys[:num_train], dtype=torch.float32)</span><span id="94b8" class="mi ke iq me b gy mw mk l ml mm">x_test = torch.tensor(xs[num_train:])<br/>y_test = torch.tensor(ys[num_train:], dtype=torch.float32)</span></pre><p id="0c66" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬å¯ä»¥åœ¨å°†æ¯ä¸€æ‰¹æ•°æ®è¾“å…¥åˆ°æ¨¡å‹ä¸­æ—¶åˆ†åˆ«å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–ï¼Œä½†æ˜¯ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªTensorDatasetå’ŒDataLoaderï¼Œå°±åƒæˆ‘ä»¬è¿‡å»å¯¹MNISTä½¿ç”¨çš„é‚£æ ·ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="ff82" class="mi ke iq me b gy mj mk l ml mm">reviews_train = torch.utils.data.TensorDataset(x_train, y_train)<br/>reviews_test = torch.utils.data.TensorDataset(x_test, y_test)</span><span id="a188" class="mi ke iq me b gy mw mk l ml mm">train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)<br/>test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)</span></pre><p id="adcf" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æ˜¯æ—¶å€™åœ¨PyTorchä¸­æ„å»ºæ¨¡å‹äº†ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="63e6" class="mi ke iq me b gy mj mk l ml mm">import torch.nn as nn<br/>import torch.nn.functional as F</span></pre><p id="279b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºæ¨¡å‹ï¼Œç»„ç»‡æˆä¸€ä¸ª<code class="fe ms mt mu me b">nn.Module</code>ã€‚æˆ‘ä»¬å¯ä»¥å°†MLPçš„è¾“å‡ºæ•°è®¾ä¸ºè¯¥æ•°æ®é›†çš„ç±»æ•°(å³2)ã€‚ç„¶è€Œï¼Œç”±äºæˆ‘ä»¬è¿™é‡Œåªæœ‰ä¸¤ä¸ªè¾“å‡ºç±»(â€œæ­£â€ä¸â€œè´Ÿâ€)ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªè¾“å‡ºå€¼ï¼Œå°†æ‰€æœ‰å¤§äº00çš„ç§°ä¸ºâ€œæ­£â€ï¼Œå°†æ‰€æœ‰å°äº00çš„ç§°ä¸ºâ€œè´Ÿâ€ã€‚å¦‚æœæˆ‘ä»¬é€šè¿‡sigmoidè¿ç®—ä¼ é€’æ­¤è¾“å‡ºï¼Œåˆ™å€¼è¢«æ˜ å°„åˆ°[0ï¼Œ1][0ï¼Œ1]ï¼Œ0.50.5æ˜¯åˆ†ç±»é˜ˆå€¼ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4e79" class="mi ke iq me b gy mj mk l ml mm">class SWEM(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.fc1 = nn.Linear(300, 64)<br/>        self.fc2 = nn.Linear(64, 1)</span><span id="94fa" class="mi ke iq me b gy mw mk l ml mm">def forward(self, x):<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.fc2(x)<br/>        return x</span></pre><p id="c04a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå› ä¸ºæˆ‘ä»¬åªè¿›è¡ŒäºŒè¿›åˆ¶åˆ†ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨äºŒè¿›åˆ¶äº¤å‰ç†µ(BCE)æŸå¤±ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„äº¤å‰ç†µæŸå¤±ã€‚ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œå¸¦é€»è¾‘â€çš„ç‰ˆæœ¬ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="06b9" class="mi ke iq me b gy mj mk l ml mm">## Training<br/># Instantiate model<br/>model = SWEM()</span><span id="c7aa" class="mi ke iq me b gy mw mk l ml mm"># Binary cross-entropy (BCE) Loss and Adam Optimizer<br/>criterion = nn.BCEWithLogitsLoss()<br/>optimizer = torch.optim.Adam(model.parameters(), lr=0.001)</span><span id="ad05" class="mi ke iq me b gy mw mk l ml mm"># Iterate through train set minibatchs <br/>for epoch in range(250):<br/>    correct = 0<br/>    num_examples = 0<br/>    for inputs, labels in train_loader:<br/>        # Zero out the gradients<br/>        optimizer.zero_grad()<br/>        <br/>        # Forward pass<br/>        y = model(inputs)<br/>        loss = criterion(y, labels)<br/>        <br/>        # Backward pass<br/>        loss.backward()<br/>        optimizer.step()<br/>        <br/>        predictions = torch.round(torch.sigmoid(y))<br/>        correct += torch.sum((predictions == labels).float())<br/>        num_examples += len(inputs)<br/>    <br/>    # Print training progress<br/>    if epoch % 25 == 0:<br/>        acc = correct/num_examples<br/>        print("Epoch: {0} \t Train Loss: {1} \t Train Acc: {2}".format(epoch, loss, acc))</span><span id="edbf" class="mi ke iq me b gy mw mk l ml mm">## Testing<br/>correct = 0<br/>num_test = 0</span><span id="2ae9" class="mi ke iq me b gy mw mk l ml mm">with torch.no_grad():<br/>    # Iterate through test set minibatchs <br/>    for inputs, labels in test_loader:<br/>        # Forward pass<br/>        y = model(inputs)<br/>        <br/>        predictions = torch.round(torch.sigmoid(y))<br/>        correct += torch.sum((predictions == labels).float())<br/>        num_test += len(inputs)<br/>    <br/>print('Test accuracy: {}'.format(correct/num_test))</span></pre><p id="d052" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬ç°åœ¨å¯ä»¥æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å­¦ä¹ äº†ä»€ä¹ˆï¼Œçœ‹çœ‹å®ƒå¦‚ä½•å“åº”ä¸åŒå•è¯çš„å•è¯å‘é‡:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7060" class="mi ke iq me b gy mj mk l ml mm"># Check some words<br/>words_to_test = ["exciting", "hated", "boring", "loved"]</span><span id="eeb8" class="mi ke iq me b gy mw mk l ml mm">for word in words_to_test:<br/>    x = torch.tensor(normalized_embeddings[index[word]].reshape(1, 300))<br/>    print("Sentiment of the word '{0}': {1}".format(word, torch.sigmoid(model(x))))</span></pre><p id="4b2b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¯•è¯•è‡ªå·±çš„ä¸€äº›è¯å§ï¼ä½ ä¹Ÿå¯ä»¥å°è¯•æ”¹å˜æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒå®ƒï¼Œçœ‹çœ‹ç»“æœå¦‚ä½•å˜åŒ–ã€‚èƒ½å¦ä¿®æ”¹æ¶æ„ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Ÿæˆ–è€…ï¼Œä½ èƒ½åœ¨ä¸ç‰ºç‰²å¤ªå¤šå‡†ç¡®æ€§çš„æƒ…å†µä¸‹ç®€åŒ–æ¨¡å‹å—ï¼Ÿå¦‚æœä½ å°è¯•ç›´æ¥å¯¹å‡å€¼åµŒå…¥è¿›è¡Œåˆ†ç±»å‘¢ï¼Ÿ</p><h1 id="7332" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">å­¦ä¹ å•è¯åµŒå…¥</h1><p id="f8c8" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥ï¼Œä½†æ²¡æœ‰å­¦ä¹ å®ƒä»¬ã€‚å•è¯åµŒå…¥æ˜¯é¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªè®­ç»ƒä¸­ä¿æŒä¸å˜ã€‚å¦‚æœæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å–œæ¬¢å­¦ä¹ å•è¯embeddingså’Œæˆ‘ä»¬çš„æ¨¡å‹ã€‚é¢„è®­ç»ƒå•è¯åµŒå…¥é€šå¸¸æ˜¯åœ¨å…·æœ‰æ— ç›‘ç£ç›®æ ‡çš„å¤§å‹è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œå¹¶ä¸”é€šå¸¸æ˜¯éç‰¹å®šçš„ã€‚å¦‚æœæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å–œæ¬¢å­¦ä¹ å•è¯åµŒå…¥ï¼Œè¦ä¹ˆä»é›¶å¼€å§‹ï¼Œè¦ä¹ˆé€šè¿‡å¾®è°ƒï¼Œå› ä¸ºä½¿å®ƒä»¬ç‰¹å®šäºä»»åŠ¡å¯èƒ½ä¼šæé«˜æ€§èƒ½ã€‚</p><p id="6a24" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬å¦‚ä½•å­¦ä¹ å•è¯åµŒå…¥ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©å®ƒä»¬æˆä¸ºæˆ‘ä»¬æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯åŠ è½½æ•°æ®çš„ä¸€éƒ¨åˆ†ã€‚åœ¨PyTorchä¸­ï¼Œè¿™æ ·åšçš„é¦–é€‰æ–¹å¼æ˜¯ä½¿ç”¨<code class="fe ms mt mu me b">nn.Embedding</code>ã€‚åƒæˆ‘ä»¬è§è¿‡çš„å…¶ä»–<code class="fe ms mt mu me b">nn</code>å±‚(ä¾‹å¦‚<code class="fe ms mt mu me b">nn.Linear</code>)ä¸€æ ·ï¼Œ<code class="fe ms mt mu me b">nn.Embedding</code>å¿…é¡»é¦–å…ˆè¢«å®ä¾‹åŒ–ã€‚å®ä¾‹åŒ–æœ‰ä¸¤ä¸ªå¿…éœ€çš„å‚æ•°ï¼Œå³åµŒå…¥çš„æ•°é‡(å³è¯æ±‡å¤§å°ğ‘‰)å’Œå•è¯åµŒå…¥çš„ç»´åº¦(åœ¨å‰é¢çš„ä¾‹å­ä¸­æ˜¯300)ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="bb74" class="mi ke iq me b gy mj mk l ml mm">VOCAB_SIZE = 5000<br/>EMBED_DIM = 300</span><span id="dd86" class="mi ke iq me b gy mw mk l ml mm">embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)</span><span id="05da" class="mi ke iq me b gy mw mk l ml mm">embedding <strong class="me ir">=</strong> nn.Embedding(VOCAB_SIZE, EMBED_DIM)</span></pre><p id="aad7" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">åœ¨å¼•æ“ç›–ä¸‹ï¼Œè¿™åˆ›å»ºäº†ä¸€ä¸ª5000Ã—300çš„å•è¯åµŒå…¥çŸ©é˜µã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="5ebf" class="mi ke iq me b gy mj mk l ml mm">embedding.weight.size()</span></pre><p id="6cd1" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¯·æ³¨æ„ï¼Œè¿™ä¸ªçŸ©é˜µåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ª300ç»´çš„å•è¯åµŒå…¥5000ä¸ªå•è¯ä¸­çš„æ¯ä¸€ä¸ªï¼Œå †å åœ¨å½¼æ­¤çš„é¡¶éƒ¨ã€‚åœ¨è¿™ä¸ªåµŒå…¥çŸ©é˜µä¸­æŸ¥æ‰¾ä¸€ä¸ªå•è¯åµŒå…¥ï¼Œå°±æ˜¯ç®€å•åœ°é€‰æ‹©è¿™ä¸ªçŸ©é˜µçš„ç‰¹å®šè¡Œï¼Œå¯¹åº”äºè¿™ä¸ªå•è¯ã€‚</p><p id="f24f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">å½“å­¦ä¹ å•è¯åµŒå…¥æ—¶ï¼Œ<code class="fe ms mt mu me b">nn.Embedding</code>æŸ¥æ‰¾é€šå¸¸æ˜¯æ¨¡å‹æ¨¡å—ä¸­çš„ç¬¬ä¸€ä¸ªæ“ä½œã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¦ä¸ºä¹‹å‰çš„SWEMæ¨¡å‹å­¦ä¹ å•è¯embeddingsï¼Œè¯¥æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4eac" class="mi ke iq me b gy mj mk l ml mm">class SWEMWithEmbeddings(nn.Module):<br/>    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):<br/>        super().__init__()<br/>        self.embedding = nn.Embedding(vocab_size, embedding_size)<br/>        self.fc1 = nn.Linear(embedding_size, hidden_dim)<br/>        self.fc2 = nn.Linear(hidden_dim, num_outputs)</span><span id="5d24" class="mi ke iq me b gy mw mk l ml mm">def forward(self, x):<br/>        x = self.embedding(x)<br/>        x = torch.mean(x, dim=0)<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.fc2(x)<br/>        return x</span></pre><p id="c115" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¨¡å‹å„å±‚çš„å¤§å°æŠ½è±¡ä¸ºæ„é€ å‡½æ•°å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šè¿™äº›è¶…å‚æ•°ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="eac0" class="mi ke iq me b gy mj mk l ml mm">model = SWEMWithEmbeddings(<br/>    vocab_size = 5000,<br/>    embedding_size = 300, <br/>    hidden_dim = 64, <br/>    num_outputs = 1,<br/>)<br/>print(model)</span></pre><p id="25ca" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æ³¨æ„ï¼Œé€šè¿‡ä½¿åµŒå…¥æˆä¸ºæˆ‘ä»¬æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå¯¹<code class="fe ms mt mu me b">forward()</code>å‡½æ•°çš„é¢„æœŸè¾“å…¥ç°åœ¨æ˜¯è¾“å…¥å¥å­çš„å•è¯æ ‡è®°ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿå¿…é¡»ä¿®æ”¹æˆ‘ä»¬çš„æ•°æ®è¾“å…¥ç®¡é“ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç¬”è®°æœ¬(4B)ä¸­çœ‹åˆ°å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚</p><h1 id="2170" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">é€’å½’ç¥ç»ç½‘ç»œ</h1><p id="6292" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">åœ¨æ·±åº¦å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œåºåˆ—æ•°æ®é€šå¸¸ç”¨é€’å½’ç¥ç»ç½‘ç»œ(RNNs)å»ºæ¨¡ã€‚ç”±äºè‡ªç„¶è¯­è¨€å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ä¸ªå•è¯åºåˆ—ï¼Œè‡ªç„¶ç¥ç»ç½‘ç»œé€šå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„å…¨è¿æ¥å’Œå·ç§¯ç½‘ç»œä¸€æ ·ï¼Œrnnä½¿ç”¨çº¿æ€§å’Œéçº¿æ€§å˜æ¢çš„ç»„åˆæ¥å°†è¾“å…¥æŠ•å½±åˆ°æ›´é«˜çº§åˆ«çš„è¡¨ç¤ºä¸­ï¼Œè¿™äº›è¡¨ç¤ºå¯ä»¥ä¸å…¶ä»–å±‚å †å åœ¨ä¸€èµ·ã€‚</p><h2 id="5854" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">ä½œä¸ºåºåˆ—çš„å¥å­</h2><p id="f41c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">é¡ºåºæ¨¡å‹ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ¨¡å‹ä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºâ€œæ—¶é—´â€ç»´åº¦çš„å­˜åœ¨:å¥å­(æˆ–æ®µè½ã€æ–‡æ¡£)ä¸­çš„å•è¯æœ‰ä¸€ä¸ªä¼ è¾¾æ„ä¹‰çš„é¡ºåº:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9efa74e720d958748f13114271dc1d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*iZDuQcxeIUQtvJx4Syz_9A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">ä½œè€…å›¾ç‰‡</figcaption></figure><p id="d514" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">åœ¨ä¸Šé¢çš„ç¤ºä¾‹åºåˆ—ä¸­ï¼Œå•è¯â€œRecurrentâ€æ˜¯ğ‘¡=1å•è¯ï¼Œæˆ‘ä»¬ç”¨ğ‘¤1è¡¨ç¤ºï¼›åŒæ ·ï¼Œâ€œç¥ç»â€æ˜¯ğ‘¤2ï¼Œç­‰ç­‰ã€‚æ­£å¦‚å‰é¢å‡ èŠ‚æ‰€å¸Œæœ›ç»™ä½ ç•™ä¸‹çš„å°è±¡ï¼Œå°†å•è¯å»ºæ¨¡ä¸ºåµŒå…¥å‘é‡ğ‘¥1,â€¦,ğ‘¥ğ‘‡é€šå¸¸æ¯”ä¸€é”®å‘é‡(ğ‘¤1,â€¦ğ‘¤ğ‘‡å¯¹åº”çš„ä»¤ç‰Œ)æ›´æœ‰åˆ©ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥é€šå¸¸æ˜¯ä¸ºæ¯ä¸ªè¾“å…¥å•è¯åšä¸€ä¸ªåµŒå…¥è¡¨æŸ¥æ‰¾ã€‚è®©æˆ‘ä»¬å‡è®¾300ç»´çš„å•è¯åµŒå…¥ï¼Œå¹¶ä¸”ä¸ºäº†ç®€å•èµ·è§ï¼Œå‡è®¾ä¸€ä¸ªå¤§å°ä¸º1çš„å°æ‰¹é‡ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="db16" class="mi ke iq me b gy mj mk l ml mm">mb = 1<br/>x_dim = 300 <br/>sentence = ["recurrent", "neural", "networks", "are", "great"]</span><span id="c0a6" class="mi ke iq me b gy mw mk l ml mm">xs = []<br/>for word in sentence:<br/>    xs.append(torch.tensor(normalized_embeddings[index[word]]).view(1, x_dim))<br/>    <br/>xs = torch.stack(xs, dim=0)<br/>print("xs shape: {}".format(xs.shape))</span></pre><p id="4151" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†è¾“å…¥æ ¼å¼åŒ–ä¸º(å•è¯Ã—å°æ‰¹Ã—åµŒå…¥å°ºå¯¸å•è¯Ã—å°æ‰¹Ã—åµŒå…¥å°ºå¯¸)ã€‚è¿™æ˜¯PyTorch RNNsçš„é¦–é€‰è¾“å…¥é¡ºåºã€‚</p><p id="f9fa" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">å‡è®¾æˆ‘ä»¬æƒ³è¦å¤„ç†è¿™ä¸ªä¾‹å­ã€‚åœ¨æˆ‘ä»¬ä¹‹å‰çš„æƒ…æ„Ÿåˆ†æä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªæ˜¯å–äº†ä¸€æ®µæ—¶é—´å†…çš„å¹³å‡åµŒå…¥ï¼Œå°†è¾“å…¥è§†ä¸ºä¸€ä¸ªâ€œå•è¯åŒ…â€å¯¹äºç®€å•çš„é—®é¢˜ï¼Œè¿™å¯ä»¥å‡ºå¥‡åœ°å¥½ï¼Œä½†æ˜¯æ­£å¦‚æ‚¨å¯èƒ½æƒ³è±¡çš„é‚£æ ·ï¼Œå¥å­ä¸­å•è¯çš„é¡ºåºé€šå¸¸å¾ˆé‡è¦ï¼Œæœ‰æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¯¹è¿™ç§æ—¶é—´æ„ä¹‰è¿›è¡Œå»ºæ¨¡ã€‚è¾“å…¥RNNsã€‚</p><h2 id="460d" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">å›é¡¾:å…¨è¿æ¥å±‚</h2><p id="4361" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">åœ¨æˆ‘ä»¬ä»‹ç»RNNä¹‹å‰ï¼Œè®©æˆ‘ä»¬å†æ¬¡å›é¡¾ä¸€ä¸‹æˆ‘ä»¬åœ¨é€»è¾‘å›å½’å’Œå¤šå±‚æ„ŸçŸ¥å™¨ç¤ºä¾‹ä¸­ä½¿ç”¨çš„å…¨è¿æ¥å±‚ï¼Œåœ¨ç¬¦å·ä¸Šæœ‰ä¸€äº›å˜åŒ–:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a508948bc8fe8eed1f6b267e05f9283b.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*0KyK1A_OEBG1MbIqcxf0Jg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">ä½œè€…å›¾ç‰‡</figcaption></figure><p id="1e1a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">å¯¹äºéšè—çŠ¶æ€ï¼Œæˆ‘ä»¬å°†æŠŠå…¨è¿æ¥å›¾å±‚çš„ç»“æœç§°ä¸ºâ„ï¼Œè€Œä¸æ˜¯ğ‘¦ã€‚å˜é‡ğ‘¦é€šå¸¸ä¿ç•™ç»™ç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚ï¼›å› ä¸ºé€»è¾‘å›å½’æ˜¯å•å±‚çš„ï¼Œæ‰€ä»¥ä½¿ç”¨ğ‘¦å°±å¯ä»¥äº†ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾æœ‰ä¸€ä¸ªä»¥ä¸Šçš„å±‚ï¼Œæ›´å¸¸è§çš„æ˜¯å°†ä¸­é—´è¡¨ç¤ºç§°ä¸ºâ„.æ³¨æ„ï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨ğ‘“(æ¥è¡¨ç¤ºéçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚è¿‡å»ï¼Œæˆ‘ä»¬å°†ğ‘“()è§†ä¸ºReLUï¼Œä½†è¿™ä¹Ÿå¯èƒ½æ˜¯ğœ()æˆ–tanh()éçº¿æ€§ã€‚å¯è§†åŒ–:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/ec7bf025c4d6c71a01bdc6d4e4493ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*C9wZXSrH5nzmv1AWAZaXiQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">ä½œè€…å›¾ç‰‡</figcaption></figure><p id="d997" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¿™é‡Œè¦æ³¨æ„çš„å…³é”®æ˜¯ï¼Œæˆ‘ä»¬ç”¨çº¿æ€§å˜æ¢(ç”¨ğ‘ŠWå’Œğ‘b)æŠ•å°„è¾“å…¥ğ‘¥xï¼Œç„¶åå¯¹è¾“å‡ºåº”ç”¨éçº¿æ€§ï¼Œåœ¨è®­ç»ƒæœŸé—´ç»™æˆ‘ä»¬â„h.ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å­¦ä¹ ğ‘ŠWå’Œğ‘b.</p><h2 id="5a19" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">åŸºæœ¬çš„RNN</h2><p id="ab83" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ä½¿ç”¨å…¨è¿æ¥å›¾å±‚çš„ç¤ºä¾‹ä¸åŒï¼Œé¡ºåºæ•°æ®å…·æœ‰å¤šä¸ªè¾“å…¥ğ‘¥1,â€¦,ğ‘¥ğ‘‡ï¼Œè€Œä¸æ˜¯å•ä¸ªğ‘¥.æˆ‘ä»¬éœ€è¦æ ¹æ®RNNçš„æƒ…å†µè°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹ã€‚è™½ç„¶æœ‰å‡ ç§å˜ä½“ï¼Œä½†RNNçš„ä¸€ç§å¸¸è§åŸºæœ¬é…æ–¹æ˜¯åŸƒå°”æ›¼RNNï¼Œå¦‚ä¸‹æ‰€ç¤º*:</p><p id="b3e8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">â„ğ‘¡=tanh((ğ‘¥ğ‘¡ğ‘Šğ‘¥+ğ‘ğ‘¥)+(â„ğ‘¡âˆ’1ğ‘Šâ„+ğ‘â„))</p><p id="b9f2" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">å…¶ä¸­tanh()tæ˜¯åŒæ›²æ­£åˆ‡ï¼Œä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚RNNsæŒ‰é¡ºåºä¸€æ¬¡å¤„ç†ä¸€ä¸ªå•è¯(ğ‘¥ğ‘¡)ï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥äº§ç”Ÿä¸€ä¸ªéšè—çŠ¶æ€â„ğ‘¡htã€‚ä¸Šé¢ç­‰å¼çš„å‰åŠéƒ¨åˆ†åº”è¯¥çœ‹èµ·æ¥å¾ˆç†Ÿæ‚‰ï¼›ä¸å…¨è¿æ¥å±‚ä¸€æ ·ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªè¾“å…¥ğ‘¥ğ‘¡è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œç„¶ååº”ç”¨éçº¿æ€§ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½åº”ç”¨äº†ç›¸åŒçš„çº¿æ€§å˜æ¢(ğ‘Šğ‘¥ï¼Œğ‘ğ‘¥)ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬è¿˜å¯¹ä¹‹å‰éšè—çš„çŠ¶æ€â„ğ‘¡âˆ’1åº”ç”¨äº†å•ç‹¬çš„çº¿æ€§å˜æ¢(ğ‘Šâ„ï¼Œğ‘â„),å¹¶å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„æŠ•å½±è¾“å…¥ä¸­ã€‚è¿™ç§åé¦ˆè¢«ç§°ä¸º<em class="mv">å¾ªç¯</em>è¿æ¥ã€‚</p><p id="83e7" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">RNNæ¶æ„ä¸­çš„è¿™äº›æœ‰å‘å¾ªç¯èµ‹äºˆäº†å®ƒä»¬å¯¹æ—¶é—´åŠ¨æ€è¿›è¡Œå»ºæ¨¡çš„èƒ½åŠ›ï¼Œä½¿å®ƒä»¬ç‰¹åˆ«é€‚åˆå¯¹åºåˆ—(ä¾‹å¦‚æ–‡æœ¬)è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬å¯ä»¥å°†RNNå›¾å±‚å¯è§†åŒ–å¦‚ä¸‹:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6b42b1781a910811c4ba8985c02e3da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*uZVDIHTeizzvyEgaMTCwFg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">ä½œè€…å›¾ç‰‡</figcaption></figure><p id="d7cb" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬å¯ä»¥åœ¨æ—¶é—´ä¸­å±•å¼€ä¸€ä¸ªRNNï¼Œè®©å®ƒçš„æ—¶åºæ€§æ›´åŠ æ˜æ˜¾:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/3ede5950ff7ad08d39b33fb52065c7bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uE9luJS3xLAgMfjY07593w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">ä½œè€…å›¾ç‰‡</figcaption></figure><p id="f737" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æ‚¨å¯ä»¥å°†è¿™äº›å¾ªç¯è¿æ¥è§†ä¸ºå…è®¸æ¨¡å‹åœ¨è®¡ç®—å½“å‰è¾“å…¥çš„éšè—çŠ¶æ€æ—¶è€ƒè™‘åºåˆ—çš„å…ˆå‰éšè—çŠ¶æ€ã€‚</p><p id="9476" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">*æ³¨æ„:æˆ‘ä»¬å®é™…ä¸Šä¸éœ€è¦ä¸¤ä¸ªå•ç‹¬çš„åå·®ğ‘ğ‘¥å’Œğ‘â„ï¼Œå› ä¸ºä½ å¯ä»¥å°†ä¸¤ä¸ªåå·®åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„å¯å­¦ä¹ å‚æ•°ğ‘.ç„¶è€Œï¼Œå•ç‹¬ç¼–å†™å®ƒæœ‰åŠ©äºæ¸…æ¥šåœ°è¡¨æ˜æˆ‘ä»¬æ­£åœ¨å¯¹ğ‘¥ğ‘¡å’Œâ„ğ‘¡âˆ’1.æ‰§è¡Œçº¿æ€§è½¬æ¢è¯´åˆ°ç»„åˆå˜é‡ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡å°†ğ‘¥ğ‘¡xtå’Œâ„ğ‘¡âˆ’1ä¸²è”æˆå•ä¸ªå‘é‡ğ‘§ğ‘¡ï¼Œç„¶åæ‰§è¡Œå•ä¸ªçŸ©é˜µä¹˜ğ‘§ğ‘¡ğ‘Šğ‘§+ğ‘æ¥è¡¨è¾¾ä¸Šè¿°æ“ä½œï¼Œå…¶ä¸­ğ‘Šğ‘§æœ¬è´¨ä¸Šæ˜¯ğ‘Šğ‘¥å’Œğ‘Šâ„ä¸²è”ã€‚äº‹å®ä¸Šï¼Œè¿™å°±æ˜¯å®ç°çš„â€œå®˜æ–¹â€RNNsæ¨¡å—çš„æ•°é‡ï¼Œå› ä¸ºå•ç‹¬çŸ©é˜µä¹˜æ³•è¿ç®—æ•°é‡çš„å‡å°‘ä½¿å¾—è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚è¿™äº›éƒ½æ˜¯å®ç°ç»†èŠ‚ã€‚</p><h2 id="f6a9" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">PyTorchçš„RNNs</h2><p id="5a13" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">æˆ‘ä»¬å¦‚ä½•åœ¨PyTorchä¸­å®ç°RNNï¼Ÿæœ‰ç›¸å½“å¤šçš„æ–¹æ³•ï¼Œä½†è®©æˆ‘ä»¬é¦–å…ˆä»é›¶å¼€å§‹å»ºç«‹åŸƒå°”æ›¼RNNï¼Œä½¿ç”¨è¾“å…¥åºåˆ—â€œé€’å½’ç¥ç»ç½‘ç»œæ˜¯ä¼Ÿå¤§çš„â€ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="df49" class="mi ke iq me b gy mj mk l ml mm"># As always, import PyTorch first<br/>import numpy as np<br/>import torch</span></pre><p id="85c1" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">åœ¨RNNä¸­ï¼Œæˆ‘ä»¬å°†è¾“å…¥ğ‘¥ğ‘¡å’Œä¹‹å‰éšè—çš„çŠ¶æ€â„ğ‘¡âˆ’1éƒ½æŠ•å½±åˆ°æŸä¸ªéšè—çš„ç»´åº¦ï¼Œæˆ‘ä»¬å°†é€‰æ‹©è¯¥ç»´åº¦ä¸º128ã€‚ä¸ºäº†æ‰§è¡Œè¿™äº›æ“ä½œï¼Œæˆ‘ä»¬è¦å®šä¹‰ä¸€äº›æˆ‘ä»¬å°†è¦å­¦ä¹ çš„å˜é‡ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="d698" class="mi ke iq me b gy mj mk l ml mm">h_dim = 128</span><span id="4fee" class="mi ke iq me b gy mw mk l ml mm"># For projecting the input<br/>Wx = torch.randn(x_dim, h_dim)/np.sqrt(x_dim)<br/>Wx.requires_grad_()<br/>bx = torch.zeros(h_dim, requires_grad=True)</span><span id="5d8e" class="mi ke iq me b gy mw mk l ml mm"># For projecting the previous state<br/>Wh = torch.randn(h_dim, h_dim)/np.sqrt(h_dim)<br/>Wh.requires_grad_()<br/>bh = torch.zeros(h_dim, requires_grad=True)</span><span id="84a3" class="mi ke iq me b gy mw mk l ml mm">print(Wx.shape, bx.shape, Wh.shape, bh.shape)</span></pre><p id="fb21" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ä¸ºRNNçš„ä¸€ä¸ªæ—¶é—´æ­¥é•¿å®šä¹‰ä¸€ä¸ªå‡½æ•°ã€‚è¯¥å‡½æ•°é‡‡ç”¨å½“å‰è¾“å…¥ğ‘¥ğ‘¡å’Œå…ˆå‰éšè—çš„çŠ¶æ€â„ğ‘¡âˆ’1ï¼Œæ‰§è¡Œçº¿æ€§å˜æ¢ğ‘¥ğ‘Šğ‘¥+ğ‘ğ‘¥å’Œâ„ğ‘Šâ„+ğ‘â„ï¼Œç„¶åæ˜¯åŒæ›²æ­£åˆ‡éçº¿æ€§ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="673c" class="mi ke iq me b gy mj mk l ml mm">def RNN_step(x, h):<br/>    h_next = torch.tanh((torch.matmul(x, Wx) + bx) + (torch.matmul(h, Wh) + bh))</span><span id="ea97" class="mi ke iq me b gy mw mk l ml mm">return h_next</span></pre><p id="f702" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬çš„RNNçš„æ¯ä¸€æ­¥éƒ½éœ€è¦è¾“å…¥(å³å•è¯è¡¨ç¤º)å’Œä¹‹å‰çš„éšè—çŠ¶æ€(ä¹‹å‰åºåˆ—çš„æ€»ç»“)ã€‚æ³¨æ„ï¼Œåœ¨å¥å­çš„å¼€å¤´ï¼Œæˆ‘ä»¬æ²¡æœ‰å…ˆå‰çš„éšè—çŠ¶æ€ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å…¶åˆå§‹åŒ–ä¸ºæŸä¸ªå€¼ï¼Œä¾‹å¦‚ï¼Œå…¨é›¶:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="3e91" class="mi ke iq me b gy mj mk l ml mm"># Word embedding for first word<br/>x1 = xs[0, :, :]</span><span id="4495" class="mi ke iq me b gy mw mk l ml mm"># Initialize hidden state to 0<br/>h0 = torch.zeros([mb, h_dim])</span></pre><p id="f5a8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä¸ºäº†é‡‡å–RNNçš„ä¸€æ¬¡æ€§æ­¥éª¤ï¼Œæˆ‘ä»¬è°ƒç”¨æˆ‘ä»¬ç¼–å†™çš„å‡½æ•°ï¼Œåœ¨ğ‘¥1å’Œâ„0.ä¼ é€’åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="c428" class="mi ke iq me b gy mj mk l ml mm"># Forward pass of one RNN step for time step t=1<br/>h1 = RNN_step(x1, h0)</span><span id="d887" class="mi ke iq me b gy mw mk l ml mm">print("Hidden state h1 dimensions: {0}".format(h1.shape))</span></pre><p id="1fae" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬å¯ä»¥å†æ¬¡è°ƒç”¨<code class="fe ms mt mu me b">RNN_step</code>å‡½æ•°ï¼Œä»æˆ‘ä»¬çš„RNNä¸­è·å¾—ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥é•¿è¾“å‡ºã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="656a" class="mi ke iq me b gy mj mk l ml mm"># Word embedding for second word<br/>x2 = xs[1, :, :]</span><span id="e201" class="mi ke iq me b gy mw mk l ml mm"># Forward pass of one RNN step for time step t=2<br/>h2 = RNN_step(x2, h1)</span><span id="2788" class="mi ke iq me b gy mw mk l ml mm">print("Hidden state h2 dimensions: {0}".format(h2.shape))</span></pre><p id="52e7" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦ç»§ç»­å±•å¼€RNNã€‚å¯¹äºæ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬é¦ˆå…¥å½“å‰è¾“å…¥(ğ‘¥ğ‘¡)å’Œå…ˆå‰çš„éšè—çŠ¶æ€(â„ğ‘¡âˆ’1)ä»¥è·å¾—æ–°çš„è¾“å‡ºã€‚</p><h2 id="d786" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">ä½¿ç”¨<code class="fe ms mt mu me b">torch.nn</code></h2><p id="0d66" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">åœ¨å®è·µä¸­ï¼Œå¾ˆåƒå…¨è¿æ¥å’Œå·ç§¯å±‚ï¼Œæˆ‘ä»¬é€šå¸¸ä¸åƒä¸Šé¢é‚£æ ·ä»å¤´å®ç°RNNsï¼Œè€Œæ˜¯ä¾èµ–äºæ›´é«˜çº§åˆ«çš„APIã€‚PyTorchåœ¨<code class="fe ms mt mu me b">torch.nn</code>åº“ä¸­å®ç°äº†RNNsã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4793" class="mi ke iq me b gy mj mk l ml mm">import torch.nn</span><span id="969e" class="mi ke iq me b gy mw mk l ml mm">rnn = nn.RNN(x_dim, h_dim)<br/>print("RNN parameter shapes: {}".format([p.shape for p in rnn.parameters()]))</span></pre><p id="2af0" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">è¯·æ³¨æ„ï¼Œç”±<code class="fe ms mt mu me b">torch.nn</code>åˆ›å»ºçš„RNNäº§ç”Ÿçš„å‚æ•°ä¸æˆ‘ä»¬ä»ä¸Šé¢çš„ä¾‹å­ä¸­å¾—åˆ°çš„å‚æ•°å…·æœ‰ç›¸åŒçš„ç»´åº¦ã€‚</p><p id="263c" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä¸ºäº†ä½¿ç”¨RNNæ‰§è¡Œå‘å‰ä¼ é€’ï¼Œæˆ‘ä»¬å°†æ•´ä¸ªè¾“å…¥åºåˆ—ä¼ é€’ç»™<code class="fe ms mt mu me b">forward()</code>å‡½æ•°ï¼Œè¯¥å‡½æ•°è¿”å›æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„éšè—çŠ¶æ€(<code class="fe ms mt mu me b">hs</code>)å’Œæœ€ç»ˆçš„éšè—çŠ¶æ€(<code class="fe ms mt mu me b">h_T</code>)ã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="45e3" class="mi ke iq me b gy mj mk l ml mm">hs, h_T = rnn(xs)</span><span id="54dd" class="mi ke iq me b gy mw mk l ml mm">print("Hidden states shape: {}".format(hs.shape))<br/>print("Final hidden state shape: {}".format(h_T.shape))</span></pre><p id="f057" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æˆ‘ä»¬å¦‚ä½•å¤„ç†è¿™äº›éšè—çŠ¶æ€å‘¢ï¼Ÿè¿™å–å†³äºå‹å·å’Œä»»åŠ¡ã€‚å°±åƒå¤šå±‚æ„ŸçŸ¥å™¨å’Œå·ç§¯ç¥ç»ç½‘ç»œä¸€æ ·ï¼Œrnnä¹Ÿå¯ä»¥å †å åœ¨å¤šä¸ªå±‚ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å‡ºâ„1,â€¦,â„ğ‘‡æ˜¯ä¸‹ä¸€å±‚çš„é¡ºåºè¾“å…¥ã€‚å¦‚æœRNNå±‚æ˜¯æœ€ç»ˆå±‚ï¼Œåˆ™â„ğ‘‡æˆ–â„1,â€¦,â„ğ‘‡çš„å¹³å‡å€¼/æœ€å¤§å€¼å¯ä»¥ç”¨ä½œæ•°æ®åºåˆ—çš„æ±‡æ€»ç¼–ç ã€‚é¢„æµ‹çš„ç»“æœä¹Ÿä¼šå¯¹RNNäº§å‡ºçš„æœ€ç»ˆç”¨é€”äº§ç”Ÿå½±å“ã€‚</p><h2 id="cb5d" class="mi ke iq bd kf nm nn dn kj no np dp kn lm nq nr kr lq ns nt kv lu nu nv kz nw bi translated">é—¨æ§RNNs</h2><p id="202b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">è™½ç„¶æˆ‘ä»¬åˆšåˆšæ¢ç´¢çš„rnnå¯ä»¥æˆåŠŸåœ°æ¨¡æ‹Ÿç®€å•çš„åºåˆ—æ•°æ®ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥å¤„ç†è¾ƒé•¿çš„åºåˆ—ï¼Œå…¶ä¸­<a class="ae kc" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">æ¶ˆå¤±æ¢¯åº¦</a>æ˜¯ä¸€ä¸ªç‰¹åˆ«å¤§çš„é—®é¢˜ã€‚å¤šå¹´æ¥å·²ç»æå‡ºäº†è®¸å¤šRNNå˜ä½“æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”ç»éªŒè¡¨æ˜è¿™äº›å˜ä½“æ›´åŠ æœ‰æ•ˆã€‚ç‰¹åˆ«æ˜¯ï¼Œé•¿çŸ­æœŸè®°å¿†(LSTM)å’Œé—¨æ§å¾ªç¯å•å…ƒ(GRU)æœ€è¿‘åœ¨æ·±åº¦å­¦ä¹ ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚æˆ‘ä»¬ä¸æ‰“ç®—åœ¨è¿™é‡Œè¯¦ç»†è®¨è®ºå®ƒä»¬ä¸æ™®é€šrnnåœ¨ç»“æ„ä¸Šæœ‰ä»€ä¹ˆä¸åŒï¼›ä¸€ä¸ªå¥‡å¦™çš„æ€»ç»“å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°<a class="ae kc" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">ã€‚æ³¨æ„,â€œRNNâ€ä½œä¸ºä¸€ä¸ªåå­—æœ‰ç‚¹è¶…è½½:å®ƒæ—¢å¯ä»¥æŒ‡æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„åŸºæœ¬é€’å½’æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æŒ‡ä¸€èˆ¬çš„é€’å½’æ¨¡å‹(åŒ…æ‹¬LSTMså’ŒGRUs)ã€‚</a></p><p id="605b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">åˆ›å»ºLSTMså’ŒGRUså›¾å±‚çš„æ–¹æ³•ä¸åˆ›å»ºåŸºæœ¬RNNå›¾å±‚çš„æ–¹æ³•å¤§è‡´ç›¸åŒã€‚åŒæ ·ï¼Œä¸è¦è‡ªå·±å®ç°å®ƒï¼Œå»ºè®®ä½¿ç”¨<code class="fe ms mt mu me b">torch.nn</code>å®ç°ï¼Œå°½ç®¡æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨æŸ¥çœ‹æºä»£ç ï¼Œè¿™æ ·æ‚¨å°±èƒ½ç†è§£å¹•åå‘ç”Ÿäº†ä»€ä¹ˆã€‚</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7471" class="mi ke iq me b gy mj mk l ml mm">lstm = nn.LSTM(x_dim, h_dim)<br/>print("LSTM parameters: {}".format([p.shape for p in lstm.parameters()]))</span><span id="0814" class="mi ke iq me b gy mw mk l ml mm">gru = nn.GRU(x_dim, h_dim)<br/>print("GRU parameters: {}".format([p.shape for p in gru.parameters()]))</span></pre><h1 id="3f9b" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">ç«ç‚¬æŠ¥</h1><p id="bf43" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">å°±åƒPyTorchæ‹¥æœ‰ç”¨äºè®¡ç®—æœºè§†è§‰çš„<a class="ae kc" href="https://pytorch.org/docs/stable/torchvision/index.html" rel="noopener ugc nofollow" target="_blank"> Torchvision </a>ä¸€æ ·ï¼ŒPyTorchä¹Ÿæ‹¥æœ‰ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„<a class="ae kc" href="https://torchtext.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Torchtext </a>ã€‚ä¸Torchvisionä¸€æ ·ï¼ŒTorchtextæ‹¥æœ‰å¤§é‡æµè¡Œçš„NLPåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡(å¦‚æƒ…æ„Ÿåˆ†æã€è¯­è¨€å»ºæ¨¡ã€æœºå™¨ç¿»è¯‘)ã€‚å®ƒä¹Ÿæœ‰ä¸€äº›é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯åµŒå…¥ï¼ŒåŒ…æ‹¬æµè¡Œçš„å•è¯è¡¨ç¤ºå…¨å±€å‘é‡(GloVe)ã€‚å¦‚æœæ‚¨éœ€è¦åŠ è½½è‡ªå·±çš„æ•°æ®é›†ï¼ŒTorchtextæœ‰è®¸å¤šæœ‰ç”¨çš„å®¹å™¨ï¼Œå¯ä»¥ä½¿æ•°æ®ç®¡é“æ›´å®¹æ˜“ã€‚</p><p id="cf73" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">æ‚¨éœ€è¦å®‰è£…TorchTextæ¥ä½¿ç”¨å®ƒ:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="0121" class="mi ke iq me b gy mj mk l ml mm"><em class="mv"># If you environment isn't currently active, activate it:</em><br/><em class="mv"># conda activate pytorch</em></span><span id="e79e" class="mi ke iq me b gy mw mk l ml mm">pip install torchtext</span></pre><p id="eedd" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä¸€æ—¦ä½ ç†è§£äº†è¿™å¥è¯ï¼Œä½ å°±å·²ç»å®Œæˆäº†PyTorchä¸­è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨çš„æ‰€æœ‰æ­¥éª¤</p><p id="1342" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">ä»¥ä¸‹æ˜¯æ‚¨ä»Šå¤©çš„æˆå°±æ€»ç»“:</p><ul class=""><li id="06e9" class="my mz iq ld b le mn li mo lm na lq nb lu nc ly oc ne nf ng bi translated"><strong class="ld ir">å•è¯åµŒå…¥</strong></li><li id="d2a8" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir">åœ¨æ·±åº¦æ¨¡å‹ä¸­ä½¿ç”¨å•è¯åµŒå…¥</strong></li><li id="cee6" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir">å­¦ä¹ å•è¯åµŒå…¥</strong></li><li id="d5dd" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir"> <em class="mv">é€’å½’ç¥ç»ç½‘ç»œ(RNNs):å¥å­ä½œä¸ºåºåˆ—ï¼Œå¤ä¹ :å…¨è¿æ¥å±‚ï¼ŒåŸºæœ¬RNNï¼ŒPyTorchä¸­çš„RNNsï¼Œä½¿ç”¨torch.nnï¼Œé—¨æ§RNNsï¼Œ</em>T5ã€‘</strong></li><li id="8074" class="my mz iq ld b le nh li ni lm nj lq nk lu nl ly oc ne nf ng bi translated"><strong class="ld ir">ç«ç‚¬æ–‡æœ¬</strong></li></ul></div></div>    
</body>
</html>