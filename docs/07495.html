<html>
<head>
<title>Writing a Python web scraper for academic papers with RapidAPI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用RapidAPI编写学术论文的Python web scraper</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/writing-a-python-reference-web-scraper-with-rapidapi-5e810e0e1d88?source=collection_archive---------9-----------------------#2021-02-22">https://levelup.gitconnected.com/writing-a-python-reference-web-scraper-with-rapidapi-5e810e0e1d88?source=collection_archive---------9-----------------------#2021-02-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/c15560494043a1abd057a62fe8712564.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*ykQEd_yU9JOWpopT-8AU5w.png"/></div></figure><p id="432d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在学术界工作时，一个常见的问题是有一个很大的参考文献列表，你需要收集一些元数据(作者、期刊、出版日期、DOI、<em class="ks">等</em>)。).我最近在为我的研究创建数据库时遇到了这个问题，我需要获得大约300个独立参考文献的URL链接。我有一个包含大部分信息的Word文档，包括第一作者和文章标题，但是没有简单的方法来获得它们的链接。我决定创建一个简单的Python脚本来完成这项工作，而不是浏览每一篇参考文献，复制标题，在Google搜索中输入标题，单击适当的链接，然后保存URL。</p><p id="8261" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这个工具使用Pandas和<a class="ae kt" href="https://rapidapi.com/apigeek/api/google-search3" rel="noopener ugc nofollow" target="_blank"> RapidAPI </a>创建一个CSV文件，其中包含所有输入到脚本的引用的链接。</p><p id="9dcc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">故事是这样的——代码的Github库的链接包含在结尾！</p><h1 id="dceb" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">环境设置</h1><p id="10dd" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">与所有编码项目一样，在虚拟环境中工作是一个很好的实践。对于我的特定用例，我一直在使用用于Linux 2的<a class="ae kt" href="https://docs.microsoft.com/en-us/windows/wsl/install-win10" rel="noopener ugc nofollow" target="_blank"> Windows子系统</a>(运行Ubuntu 20.04)、<a class="ae kt" href="https://code.visualstudio.com/" rel="noopener ugc nofollow" target="_blank"> Visual Studio代码</a>和<a class="ae kt" href="https://pipenv.pypa.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Pipenv </a>(用于Python包和环境管理)。你可以在这里阅读关于这个设置<a class="ae kt" href="https://medium.com/swlh/setting-up-a-secure-django-project-repository-with-docker-and-django-environ-4af72ce037f0" rel="noopener">的更多信息。</a></p><p id="a9ed" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我使用Jupyter笔记本完成了大部分原型制作，但是代码非常简单，可以包含在一个简单的python文件中。</p><h2 id="8def" class="lx kv iq bd kw ly lz dn la ma mb dp le kf mc md li kj me mf lm kn mg mh lq mi bi translated">创建虚拟环境</h2><p id="a209" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">在终端窗口中(我使用的是<a class="ae kt" href="https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?activetab=pivot:overviewtab" rel="noopener ugc nofollow" target="_blank"> Windows终端</a>，在我看来，它比cmd.exe或Powershell漂亮得多)，运行以下命令来创建并激活一个新的工作环境:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="1160" class="lx kv iq mo b gy ms mt l mu mv">$ mkdir refscraper &amp;&amp; cd refscraper<br/>$ pipenv --python 3.9.1</span></pre><p id="c76b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我在这个项目中使用的是相对较新的Python版本，但是这个项目也可以使用旧版本，只是要注意Python 2 <em class="ks">和</em>的语法差异。3.</p><p id="859e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">接下来，我们将安装我们需要的唯一的第三方包，<a class="ae kt" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> Pandas </a>。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="21e0" class="lx kv iq mo b gy ms mt l mu mv">(refscraper) $ pipenv install pandas</span></pre><p id="c830" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，我们可以创建我们将需要的其他文件:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="2f78" class="lx kv iq mo b gy ms mt l mu mv">(refscraper) $ touch ref_scraper.py refs.txt</span></pre><p id="63c0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这将创建空文件，我们稍后将填充这些文件。我们的目录现在应该是这样的:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="4b7d" class="lx kv iq mo b gy ms mt l mu mv">/refscraper<br/>  |--Pipfile<br/>  |--Pipfile.lock<br/>  |--ref_scraper.py<br/>  |--refs.txt</span></pre><h2 id="76d6" class="lx kv iq bd kw ly lz dn la ma mb dp le kf mc md li kj me mf lm kn mg mh lq mi bi translated"><code class="fe mw mx my mo b">refs.txt</code>文件</h2><p id="e25a" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">数据文件的格式(我们将其命名为<code class="fe mw mx my mo b">refs.txt</code>)对于我们如何编写python代码非常重要。对于这个项目，我得到了一个Word文档，其中列出了参考资料，如图1所示。</p><figure class="mj mk ml mm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/3285e343a30af4f0eb2c0c9bdbe50c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjQvKX5J-MVovAsG3FtPEQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk translated">图一。参考文件格式示例</figcaption></figure><p id="5c6b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以编写一个脚本直接从文档中读取这些数据，因为Word文档本质上只是臃肿的XML文件。然而，将引用直接复制到一个纯文本文件是很容易的，使用Python可以更容易地解析该文件。复制到<code class="fe mw mx my mo b">refs.txt</code>后，文件如下所示:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="9ce0" class="lx kv iq mo b gy ms mt l mu mv">1 Sekiya, M. Sakaino, &amp; Toshiaki, S. T. Linear Logistic Regression<br/>  for Estimation of Lower Limb Muscle Activations, IEEE Trans.<br/>  Neural Syst. Rehabil Eng. 27, 523-532 (2019).<br/>2 Malouf, R. A comparison of algorithms for maximum entropy<br/>  parameter estimation, In proceedings of the 6th conference on<br/>  Natural language learning, Association for Computational<br/>  Linguistics, 20, 1-7 (2002).<br/>3 Andrew, G. &amp; Gao, J. Scalable training of &lt;i&gt;L&lt;/i&gt;&lt;sup&gt;1&lt;/sup&gt;<br/>  regularized log-linear models. In Proceedings of the 24th<br/>  international conference on Machine learning, ACM: Corvalis,<br/>  Oregon, USA, 33-40 (2007).<br/>4 Elith, J. Leathwick, &amp; J. Hastie, R. T. A working guide to boosted<br/>  regression trees. J. Anim. Ecol., 77 , 802-813 (2008).</span></pre><p id="930f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中每个引用前面的数字只是行号。现在，我们可以开始用Python解析一些链接了！</p><h1 id="56ff" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">Python脚本</h1><p id="f6de" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">我们需要做的第一件事是读入我们的<code class="fe mw mx my mo b">refs.txt</code>文件，这样我们就可以处理数据。为此，我们将使用熊猫数据帧。在文本编辑器中打开<code class="fe mw mx my mo b">ref_scraper.py</code>文件。要使用VS代码，你可以在文件所在的目录中键入<code class="fe mw mx my mo b">code .</code>,它应该会打开一个新的编辑器窗口。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="c397" class="lx kv iq mo b gy ms mt l mu mv"># ref_scraper.py<br/>import pandas as pd</span><span id="7a70" class="lx kv iq mo b gy ni mt l mu mv"># declare a variable that points to the refs.txt file<br/>REF_TEXT = "./refs.txt"</span><span id="83c1" class="lx kv iq mo b gy ni mt l mu mv"># read the data into a DataFrame<br/>df = pd.read_csv(<br/>        REF_TEXT, <br/>        delimiter='\n',<br/>        header=None,<br/>        names=['ref_full']<br/>    )<br/>df.index += 1</span></pre><p id="5491" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这段代码做了两件事:创建一个带有一列<code class="fe mw mx my mo b">ref_full</code>的DataFrame，其中包含每个引用的完整文本；将索引从1而不是0开始移动(这是为了更容易将链接映射到论文中的实际参考编号)。接下来，我们需要从参考文献的全文中解析标题，这样我们就可以在以后使用它来形成我们的搜索查询。让我们看一个来自我们的<code class="fe mw mx my mo b">refs.txt </code>文件的例子来决定如何实现这一点。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="5556" class="lx kv iq mo b gy ms mt l mu mv">Sekiya, M. Sakaino, &amp; Toshiaki, S. T. Linear Logistic Regression for Estimation of Lower Limb Muscle Activations, IEEE Trans. Neural Syst. Rehabil Eng. 27, 523-532 (2019).</span></pre><p id="cdca" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">看起来我们可以在<code class="fe mw mx my mo b">.</code>字符上拆分完整的字符串，这将给我们一个列表。这不会是完美的，但它将服务于我们的目的。让我们在Python shell中摆弄一下(在终端中，运行<code class="fe mw mx my mo b">$ python</code>)。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="b1cd" class="lx kv iq mo b gy ms mt l mu mv">&gt;&gt;&gt; import pandas as pd<br/>&gt;&gt;&gt; df = pd.read_csv(<br/>...    './refs.txt',<br/>...    delimiter='\n',<br/>...    header=None,<br/>...    names=['ref_full'])<br/>&gt;&gt;&gt; df.index += 1<br/>&gt;&gt;&gt; ref1 = df['ref_full'][1]<br/>&gt;&gt;&gt; print(ref1)<br/>'Sekiya, M. Sakaino, &amp; Toshiaki, S. T. Linear Logistic Reg...'<br/>&gt;&gt;&gt; ref1.split('.')<br/>['Sekiya, M', ' Sakaino, &amp; Toshiaki, S', ..., ]</span></pre><p id="e4f8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我已经截断了字符串和列表，使它们适合一行，但是需要注意的重要一点是，通过在完整的字符串上调用<code class="fe mw mx my mo b">.split('.')</code>，返回的列表元素之一包含了标题(可能还有一些其他文本，比如介绍该作品的会议)。如果我们假设包含标题的列表元素将是整个列表中最长的一个，我们可以很容易地获取它并返回它。我们还会做更多的清洁工作。回到Python控制台！</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="2a8c" class="lx kv iq mo b gy ms mt l mu mv">&gt;&gt;&gt; # removing quote characters<br/>&gt;&gt;&gt; ref_1.replace('"', '').replace("'", '')<br/>&gt;&gt;&gt; split_ref = ref1.split('.')<br/>&gt;&gt;&gt; print(max(split_ref, key=len))<br/>Linear Logistic Regression for Estimation of Lower Limb...</span></pre><p id="7de2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">太好了！现在让我们将我们的临时工作合并到一个好的助手函数中，我们可以将它应用到数据帧的每一行。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="cacf" class="lx kv iq mo b gy ms mt l mu mv"># ref_scraper.py</span><span id="f990" class="lx kv iq mo b gy ni mt l mu mv">def get_title_from_ref(ref):<br/>    return(<br/>        max(<br/>            ref.split('.'), key=len)<br/>            .strip().replace('"', '').replace("'", '')<br/>    )</span><span id="2a67" class="lx kv iq mo b gy ni mt l mu mv"># create a new column and apply our new function<br/>df['title'] = df['ref_full'].apply(lambda x: get_title_from_ref(x))</span></pre><p id="dbb8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在我们有了一个DataFrame列，希望它只包含参考文献的标题(同样，根据格式还有一些额外的内容，但这对我们的搜索查询来说应该不成问题)。说到…</p><h2 id="4a28" class="lx kv iq bd kw ly lz dn la ma mb dp le kf mc md li kj me mf lm kn mg mh lq mi bi translated">构建搜索查询</h2><p id="57fb" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">在我们可以使用我们的数据从互联网上获取东西之前，我们需要对我们将使用的工具做出一些选择。对于这个项目，我发现Python的内置<code class="fe mw mx my mo b">requests</code>库和RapidAPI的一个Google搜索API很符合我的需求(如果看到这篇文章的人有更好的选择，请告诉我！).</p><p id="6338" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了开始使用这个API，我们需要<a class="ae kt" href="https://rapidapi.com/signup" rel="noopener ugc nofollow" target="_blank">用RapidAPI </a>创建一个免费帐户。一旦你创建了你的账户，从APIGeek 导航到<a class="ae kt" href="https://rapidapi.com/apigeek/api/google-search3" rel="noopener ugc nofollow" target="_blank">谷歌搜索API。为了使用这个API，您需要通过点击“Subscribe to Test”按钮来订阅它。这将把你带到一个页面，在这里你可以选择你想要的层。我选择了免费层，它每月提供600个免费API调用。选择订阅后，您应该被重定向回API页面，在这里您应该会在右侧看到一个“代码片段”窗口。单击下拉选择器(目前可能显示为“(Node.js) Unirest”)，并选择Python &gt; Requests。这将给我们一些提示，告诉我们如何构建自己的代码来使用API。</a></p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="3b78" class="lx kv iq mo b gy ms mt l mu mv"># Code snippet from RapidAPI.com<br/>import requests</span><span id="dd9b" class="lx kv iq mo b gy ni mt l mu mv">url="https://google-search3.p.rapidapi.com/api/v1/search/q=elon+musk&amp;num=100"</span><span id="fda0" class="lx kv iq mo b gy ni mt l mu mv">headers={<br/>  'x-rapid-api-key': "your-key-will-be-here",<br/>  'x-rapidapi-host': "google-search3.p.rapidapi.com"<br/>}</span><span id="66f4" class="lx kv iq mo b gy ni mt l mu mv">response=requests.request("GET", url, headers=headers)</span><span id="dae1" class="lx kv iq mo b gy ni mt l mu mv">print(response.text)</span></pre><p id="b52a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里有一些重要的事情需要注意:</p><ol class=""><li id="f322" class="nj nk iq jw b jx jy kb kc kf nl kj nm kn nn kr no np nq nr bi translated"><code class="fe mw mx my mo b">url</code>结构，它表明我们的查询字符串必须有<code class="fe mw mx my mo b">+</code>符号，而不是空格。</li><li id="fc53" class="nj nk iq jw b jx ns kb nt kf nu kj nv kn nw kr no np nq nr bi translated">这个<code class="fe mw mx my mo b">headers</code>，其中必须包括你个人的<code class="fe mw mx my mo b">x-rapid-api-key</code>。</li><li id="d716" class="nj nk iq jw b jx ns kb nt kf nu kj nv kn nw kr no np nq nr bi translated">向端点发出<code class="fe mw mx my mo b">GET</code>请求时将返回给我们的响应对象。</li></ol><p id="4a1e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们回到我们的代码，创建一个函数，将我们的<code class="fe mw mx my mo b">title</code>转换成一个合适的查询字符串，并将该字符串放入适当的url。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="2c6d" class="lx kv iq mo b gy ms mt l mu mv"># ref_scraper.py</span><span id="98e4" class="lx kv iq mo b gy ni mt l mu mv">def create_request_url(title):<br/>    q_string = title.replace(' ', '+')<br/>    return f"https://google-search3.p.rapidapi.com/api/v1/search/q={q_string}num=2"</span></pre><p id="84fb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这个函数将用<code class="fe mw mx my mo b">+</code>符号替换<code class="fe mw mx my mo b">title</code>中的所有空格，然后将那个字符串插入到我们需要进行API调用的<code class="fe mw mx my mo b">url</code>中。我还任意用<code class="fe mw mx my mo b">2</code>替换了<code class="fe mw mx my mo b">num</code>值，因为我们不需要每个引用都有<code class="fe mw mx my mo b">100</code>链接。</p><p id="712c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这里，我们还应该设置一个包含API键字符串的变量:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="941f" class="lx kv iq mo b gy ms mt l mu mv"># ref_scraper.py</span><span id="4586" class="lx kv iq mo b gy ni mt l mu mv">RAPID_API_KEY = "your-key-will-be-here"</span></pre><p id="bafa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在让我们创建一个函数，它将从我们的数据帧中获取一个<code class="fe mw mx my mo b">title</code>,并获取从API端点返回的第一个链接:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="e6ad" class="lx kv iq mo b gy ms mt l mu mv"># ref_scraper.py</span><span id="eae9" class="lx kv iq mo b gy ni mt l mu mv"># import requests and json libraries<br/>import requests<br/>import json</span><span id="e43f" class="lx kv iq mo b gy ni mt l mu mv">def get_ref_link(i, title):<br/>    <br/>    # build the headers dictionary<br/>    headers = {<br/>        'x-rapid-api-key': RAPID_API_KEY,<br/>        'x-rapid-api-host': "google-search3.p.rapidapi.com"<br/>    }</span><span id="c255" class="lx kv iq mo b gy ni mt l mu mv">    # get the query url for the given title<br/>    query_url = create_request_url(title)</span><span id="785f" class="lx kv iq mo b gy ni mt l mu mv">    # create an empty string for the link<br/>    link = ""</span><span id="0e6c" class="lx kv iq mo b gy ni mt l mu mv">    # optional print statement to track progress<br/>    print(f"Getting link for ref[{i}]...")</span><span id="8dd7" class="lx kv iq mo b gy ni mt l mu mv">    # get the response from the API<br/>    try:<br/>        r = requests.request("GET", query_url, headers=headers)<br/>    except ConnectionError:<br/>        pass</span><span id="466f" class="lx kv iq mo b gy ni mt l mu mv">    # get the link from the response.text<br/>    if r.status_code == 200:<br/>        j = json.loads(r.text)<br/>        try:<br/>            link += j['results'][0]['link']<br/>        except:<br/>            link += "no link found"<br/>    else:<br/>        link += "request failed"<br/>    <br/>    # another optional print to track progress<br/>    print(f"Done. Link: [{link}]")</span><span id="ab4f" class="lx kv iq mo b gy ni mt l mu mv">    return link</span></pre><p id="3d39" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">该函数将返回在API响应中找到的第一个链接的url。它还有一些基本的错误处理来处理可能的超时或找不到链接的情况。我们还为函数包含了一个参数<code class="fe mw mx my mo b">i </code>;这样做的原因将在下一步中更加清楚。现在的最后一步是为我们的数据帧中的每个<code class="fe mw mx my mo b">title</code>使用这个函数！让我们在脚本中添加一些功能代码:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="0b8e" class="lx kv iq mo b gy ms mt l mu mv"># ref_scraper.py</span><span id="96e7" class="lx kv iq mo b gy ni mt l mu mv"># import the time library so we can time the script<br/>import time</span><span id="ed4d" class="lx kv iq mo b gy ni mt l mu mv">start = time.time()</span><span id="4add" class="lx kv iq mo b gy ni mt l mu mv"># create an empty list for all the links<br/>links = []</span><span id="deb2" class="lx kv iq mo b gy ni mt l mu mv"># `iterrows()` returns the index `i` and the content `j` for each<br/># row of the DataFrame<br/>for i, j in df.iterrows():<br/>    link = get_ref_link(i, j['title'])<br/>    links.append(link)</span><span id="f2c9" class="lx kv iq mo b gy ni mt l mu mv">delta = time.time() - start</span></pre><p id="43cd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果我们现在运行这个脚本，我们将得到一个列表<code class="fe mw mx my mo b">links</code>,其中包含使用API找到的所有URL。最后，<em class="ks">最后的</em>步骤是将这个列表添加到我们的数据帧中，然后将数据保存到一个新文件中。我们还将打印脚本运行的时间，只是为了好玩。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="3fe6" class="lx kv iq mo b gy ms mt l mu mv"># ref_scraper.py</span><span id="7480" class="lx kv iq mo b gy ni mt l mu mv">df['link'] = links</span><span id="30e0" class="lx kv iq mo b gy ni mt l mu mv"># set a variable for the output path of our new file<br/>OUTPUT_FILE_PATH = './refs-links.csv'</span><span id="94f4" class="lx kv iq mo b gy ni mt l mu mv">df.to_csv(OUTPUT_FILE_PATH)</span><span id="aa6e" class="lx kv iq mo b gy ni mt l mu mv">print(f"Finished in {delta:.2f seconds}")</span></pre><p id="beda" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们完事了。如果我们在终端中使用<code class="fe mw mx my mo b">$ python ref_scraper.py</code>运行脚本，我们现在在目录中有一个. csv文件，它有3列:<code class="fe mw mx my mo b">ref_full</code>、<code class="fe mw mx my mo b">title</code>和<code class="fe mw mx my mo b">link</code>。现在，在电子表格应用程序中打开这个文件并把<code class="fe mw mx my mo b">link</code>列复制到你需要的任何地方都非常容易。</p><p id="c2fe" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">以下是完整的脚本:</p><figure class="mj mk ml mm gt jr"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl nz oa hu ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="ij ik il im in"><h1 id="45d1" class="ku kv iq bd kw kx og kz la lb oh ld le lf oi lh li lj oj ll lm ln ok lp lq lr bi translated">包扎</h1><p id="3560" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">这个项目的Github库可以在<a class="ae kt" href="https://github.com/bluginbuhl/ref-scraper" rel="noopener ugc nofollow" target="_blank">这里</a>找到。请随意投稿！</p><h2 id="04c1" class="lx kv iq bd kw ly lz dn la ma mb dp le kf mc md li kj me mf lm kn mg mh lq mi bi translated">未来特征</h2><p id="4694" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">显然这个脚本有一些限制。以下是我将来想添加到这个项目中的一些东西:</p><ol class=""><li id="d0da" class="nj nk iq jw b jx jy kb kc kf nl kj nm kn nn kr no np nq nr bi translated">更好地处理参考列表中的标题解析</li><li id="7d21" class="nj nk iq jw b jx ns kb nt kf nu kj nv kn nw kr no np nq nr bi translated">返回其他元数据，包括doi</li><li id="1be2" class="nj nk iq jw b jx ns kb nt kf nu kj nv kn nw kr no np nq nr bi translated">添加对Word文档、Google文档和pdf的解析</li><li id="b34b" class="nj nk iq jw b jx ns kb nt kf nu kj nv kn nw kr no np nq nr bi translated">使用一个更快、更自由的API</li><li id="5f5d" class="nj nk iq jw b jx ns kb nt kf nu kj nv kn nw kr no np nq nr bi translated">或者，使用<code class="fe mw mx my mo b">Selenium</code>和<code class="fe mw mx my mo b">BeautifulSoup</code>代替API来抓取网页</li></ol></div></div>    
</body>
</html>