<html>
<head>
<title>Exploiting Schema Inference in Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用Apache Spark中的模式推理</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/exploiting-schema-inference-in-apache-spark-c10f7fe5fa9f?source=collection_archive---------10-----------------------#2020-08-10">https://levelup.gitconnected.com/exploiting-schema-inference-in-apache-spark-c10f7fe5fa9f?source=collection_archive---------10-----------------------#2020-08-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/aef41b1ee28d939ecfc3e7be7e6157f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hznYsSo_EtfqWNJu.jpg"/></div></div></figure><p id="e304" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Apache Spark最大的特性之一是能够动态推断模式。读取数据并生成模式虽然很容易使用，但会使数据读取速度变慢。但是，有一个技巧可以一次性生成模式，然后从磁盘加载它。让我们开始吧！</p><p id="3c67" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">就本文而言，让我们假设我们正在使用具有复杂数据结构的数据，其中创建任何种类的case类或struct类型都将有一百行长。</p><h1 id="4e84" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">保存模式</h1><p id="599b" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">让我们从读取一些样本数据并对其实施模式推理开始。使用Apache Spark和Scala语言，可以像下面这样完成:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="317f" class="ml la it mh b gy mm mn l mo mp">val carsDf = spark.read<br/>    .option("inferSchema", "true")<br/>    .json("src/main/resources/data/cars.json")</span></pre><p id="d939" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将一个样本JSON文件(<code class="fe mq mr ms mh b">cars.json</code>)读入一个名为<code class="fe mq mr ms mh b">carsDf</code>的数据帧。该数据集的示例记录如下所示:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="0d72" class="ml la it mh b gy mm mn l mo mp">{<br/>   "Name":"chevrolet chevelle malibu",<br/>   "Miles_per_Gallon":18,<br/>   "Cylinders":8,<br/>   "Displacement":307,<br/>   "Horsepower":130,<br/>   "Weight_in_lbs":3504,<br/>   "Acceleration":12,<br/>   "Year":"1970-01-01",<br/>   "Origin":"USA"<br/>}</span></pre><p id="aa91" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正如我们在上面看到的，这个模式非常容易理解。现在，让我们看看Spark从中推断出了什么:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="7141" class="ml la it mh b gy mm mn l mo mp">val schemaJson = carsDf.schema.json<br/>println(schemaJson)</span></pre><p id="ad62" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此操作的输出可能是这样的:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="824f" class="ml la it mh b gy mm mn l mo mp">{<br/>   "type":"struct",<br/>   "fields":[<br/>      {<br/>         "name":"Acceleration",<br/>         "type":"double",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="e779" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Cylinders",<br/>         "type":"long",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="76ff" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Displacement",<br/>         "type":"double",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="8c40" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Horsepower",<br/>         "type":"long",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="5415" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Miles_per_Gallon",<br/>         "type":"double",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="f22a" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Name",<br/>         "type":"string",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="653a" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Origin",<br/>         "type":"string",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="45e8" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Weight_in_lbs",<br/>         "type":"long",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="bef9" class="ml la it mh b gy mt mn l mo mp">         }<br/>      },<br/>      {<br/>         "name":"Year",<br/>         "type":"string",<br/>         "nullable":true,<br/>         "metadata":{</span><span id="c057" class="ml la it mh b gy mt mn l mo mp">         }<br/>      }<br/>   ]<br/>}</span></pre><p id="cb44" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这就是Spark如何定义它设法推断的模式。现在，每次我们启动Spark作业时，都必须重新创建这些信息。当进行开发时，会有很多这样的事情，所以我们在模式的重建上浪费了很多时间。让我们尝试生成一次，并在连续读取中重用它。</p><p id="8435" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我使用的Scala中，模式的JSON文件可以保存到本地文件系统，代码如下:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="8bb2" class="ml la it mh b gy mm mn l mo mp">val file = new File("src/resources/schema.json")<br/>val bw = new BufferedWriter(new FileWriter(file))<br/>bw.write(schemaJson)<br/>bw.close()</span></pre><p id="56bd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这样，我们推断出的模式将作为名为<code class="fe mq mr ms mh b">schema.json</code>的JSON文件保存到本地文件系统中。下一步，我们将看到如何阅读它，并用它来加速火花。</p><h1 id="976f" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">加载模式</h1><p id="085c" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">现在，让我们看看如何使用Spark和Scala环境读取JSON文件。这可以使用以下代码来完成:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="bed1" class="ml la it mh b gy mm mn l mo mp">import org.apache.spark.sql.types.{DataType, StructType}</span><span id="86a4" class="ml la it mh b gy mt mn l mo mp">val schemaJson = Source<br/>    .fromFile("src/resources/schema.json")<br/>    .getLines<br/>    .mkString</span><span id="294c" class="ml la it mh b gy mt mn l mo mp">val schemaStructType = Try(DataType.fromJson(schemaJson))<br/>        .getOrElse(LegacyTypeStringParser.parse(schemaJson)) <br/>    match {<br/>      case t: StructType =&gt; t<br/>      case _ =&gt; throw new RuntimeException(s"Failed parsing JSON Schema")<br/>    }</span></pre><p id="ea16" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">由于Spark SQL包，上面的代码部分从JSON文件中读取模式，并将其解析为一个<code class="fe mq mr ms mh b">StructType</code>实例。我在这里使用了模式匹配，以适应所使用的不正确路径。</p><p id="1ddc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有了预生成的模式，在Spark中读取数据将会更快。使用我们的带有<code class="fe mq mr ms mh b">cars.json</code>文件的例子，我们可以这样读取这个数据:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="4687" class="ml la it mh b gy mm mn l mo mp">val carsDf = spark.read<br/>    .schema(schemaStructType)<br/>    .json("src/main/resources/data/cars.json")</span></pre><h1 id="6ff0" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">摘要</h1><p id="607b" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">我希望这篇文章对你有用。如果有，不要犹豫，喜欢或分享这个帖子。此外，如果你愿意，你可以在我的社交媒体上关注我🙂</p></div></div>    
</body>
</html>