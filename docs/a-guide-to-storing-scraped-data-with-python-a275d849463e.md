# 使用 Python 存储抓取数据的指南

> 原文：<https://levelup.gitconnected.com/a-guide-to-storing-scraped-data-with-python-a275d849463e>

Web 抓取是为数据科学项目收集大量数据的有效工具，根据您的目标，您需要以某种方式存储抓取的数据。

在本文中，我们将介绍一些常见的、有效的存储抓取数据的方法，比如将数据存储在文件或数据库中，以及一个快速奖励。但是，让我们从数据的结构和一致性的一些考虑开始。

![](img/6fc8126e368f641801aae9dabd8c87ca.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由 [Ruchindra Gunasekara](https://unsplash.com/@ruchindra?utm_source=medium&utm_medium=referral) 拍摄的照片

# 结构和一致性

在你开始收集甚至思考存储数据的最佳方式之前，考虑数据的结构以及如何保持数据的一致性是很重要的。这里的问题是:当代码停止运行时，您的数据会是什么样子？

如果您从同一个网站上的几个相似页面收集数据，那么您可能能够将所有内容存储在同一个表中，但是如果您从非常不同的来源收集数据，那么一开始将它们分开存储可能更容易。

知道你存储的变量的类型也很重要。是文字还是数字？如果某个特定页面上的某个字段丢失了，该怎么办？如果是数字，也许用零代替更有意义。如果是文本，你可以用任何字符串来表示一个空字段，比如“-”。

但是如果这没有意义，就用空字符串替换它，比如" "。无论您做什么，做点什么，否则您的数据可能会变得不一致，因为有些行将比其他行长，您将无法信任您刚刚收集的数据。

# CSV 文件

## 关于一致性的最后一点说明

为了在 CSV 文件中正确存储数据，您需要考虑分隔符。顾名思义，逗号分隔值— CSV 是一个文件，其中的值用逗号分隔。但是如果你的数据也有很多逗号，那么你的文件将会变得一团糟。

然后，您可以删除数据中的每个逗号，或者使用不同的分隔符。如果逗号是像“1，000，000”这样的数字，那么可能更容易删除它们，因为它们不会有用。但是，如果您正在抓取的是文本，那么逗号可能是相关的，例如，最好将分隔符改为分号。

## 数据帧到 CSV

使用 web 抓取创建 CSV 文件的第一种方法是使用 DataFrame.to_csv()方法。这非常简单，只需将数据帧导出为 CSV 文件。

但是，为了导出数据帧，您首先需要将数据作为数据帧。实现这一点的一个简单方法是创建一个包含所有数据的大列表。列表列表中的每个列表代表数据帧中的一行，包含单个页面中的数据。

当您收集完数据后，您可以将列表转换为 DataFrame，然后将其导出为 CSV 文件。

请注意，当创建 DataFrame 时，您可以直接传递列名，这也将在 CSV 文件中。

此外，建议在`to_csv()`方法中将 index 设置为`False`,因为在文件中包含索引通常没有意义。最后，请注意 sep 参数用于用分号分隔值。

不过，这种方法有一个很大的缺点。注意，代码完成后，您将只有一个包含一些信息的 CSV 文件。这种情况意味着，如果发生任何问题，您的代码崩溃，比方说，只抓取了一半的页面，您将无法访问这些数据，将不得不重新开始。

但是，如果您选择使用这种方法，那么使用机制来避免代码引发异常就变得更加重要了。因此，请确保在需要时使用 try 和 except 子句，在代码中插入尽可能多的暂停以避免服务器过载，并利用代理提供商，如 [Infatica](https://infatica.io/) ，因为他们能够为您提供更好的 IP 地址基础设施，这样您就可以确保您的代码将继续运行。

## 在文件中写入

另一种选择是在抓取数据时将所有内容写入 CSV 文件。这种方法减少了代码停止运行时造成的损害，因为您已经收集的所有内容都将保存在 CSV 文件中，您不需要从头开始。

实现起来也很简单。你可以选择使用 Python [CSV 库](https://docs.python.org/3/library/csv.html)进行更复杂的操作，或者只使用内置函数`open()`。

关于此代码的两点注意事项:

1.  `open` 函数中的第二个参数是模式。有几种打开文件的模式。以下是其中的一些:

*   r:打开阅读(默认)；
*   a:打开以供写入，追加到文件的末尾(如果存在的话)。如果文件不存在，它将被创建；
*   w:打开进行写入，首先截断文件。如果该文件不存在，将会创建它。

对于其他选项，查看[文档](https://docs.python.org/3/library/functions.html#open)。

2.关闭文件总是很重要的。这就是为什么你需要`try` 和`finally` 子句。如果在写入时发生错误，文件将被关闭。

执行相同操作的另一种方法是使用一个`with`子句。它很有用，因为它保证了文件无论如何都会被关闭，这使得您的代码更加简单。

使用这种方法，您的 CSV 文件将没有列的名称，除非您手动添加它们，或者使用一个`if`子句仅在您抓取的第一页之前将其写入文件。这是它看起来的样子:

## 比较

虽然看起来第二种方法会消耗更多的时间，因为代码每次抓取新页面时都会打开文件，但事实并非如此。

我用每种方法抓取相同的网页一百次。刮刀完全一样，唯一的区别是我存储数据的方式。这是每次接近所用的平均时间。

```
# First approach
0.6287886786460877 seconds# Second approach
0.6286333727836609 seconds
```

我边刮边写的速度甚至快了一点点。

# 存储在数据库中

如果你有一个更复杂的系统，那么你可能想把数据放在一个数据库里。对于这种情况，很容易将 Python 代码与 SQL 数据库集成在一起。

本节假设您已经掌握了一些基本的 SQL 知识，它并不打算成为 SQL 速成课程。这个想法只是为了展示如何快速地将一个 scraper 集成到一个数据库中。

Python 有一些有用的库，允许用户连接数据库，比如 PyMySQL(如果你使用 MySQL)和 sqlite3(如果你使用 sqlite)。

这两个库的用法非常相似，因此我们仅以 sqlite3 为例。为此，我假设您已经创建了数据库和表。如果是这样，那么我们有一些步骤可以遵循:

1.  创建与数据库的连接；
2.  实例化一个光标对象；
3.  确保游标使用的是正确的数据库，尤其是当您有多个数据库时。

```
import sqlite3conn = sqlite3.connect('my_database.db')
cur = conn.cursor()
cur.execute('USE my_database')
```

请注意，SQL 命令是作为字符串传递的。

完成后，我们需要做的就是使用光标将抓取的数据插入到我们抓取的每个页面的数据库中，然后使用连接进行提交。

注意，这种方法类似于编写 CSV 文件，除了我们提交到数据库。此外，关闭光标和连接也很重要，因此您可能希望使用 try 和 finally 子句来确保无论发生什么情况，它们都将被关闭。

# 奖励:电子邮件

有时您可能不需要存储数据，或者至少不仅仅需要存储数据。你可能希望立即得到通知，也许是因为这意味着有问题，也许是因为这是一个你不能错过的好机会。

所以作为奖励，让我们看看如何用 Python 发送电子邮件。使用 SMTPlib 模块，很容易编写一个自动发送电子邮件的函数。

该函数接收电子邮件的主题、消息和收件人，但您也可以更改为接收发件人的信息。此外，这个特殊的功能被设置为从实时服务器发送电子邮件，但也可以使用 Gmail 和其他功能。检查[文档](https://docs.python.org/3/library/smtplib.html)。

最后，如果在创建与电子邮件服务器的连接时出现问题，try 和 except 子句对于防止代码崩溃非常重要。

我希望这篇文章能有所帮助。如果你有问题，有建议，或者只是想保持联系，请随时通过 [Twitter](https://twitter.com/_otavioss) 、 [GitHub](https://github.com/otavio-s-s) ，或者 [Linkedin](https://www.linkedin.com/in/otavioss28/) 联系我。