<html>
<head>
<title>Deploy your Machine Learning model as a REST API on AWS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AWS上将您的机器学习模型部署为REST API</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/deploy-your-machine-learning-model-as-a-rest-api-on-aws-english-dcb1a0db3110?source=collection_archive---------2-----------------------#2020-04-23">https://levelup.gitconnected.com/deploy-your-machine-learning-model-as-a-rest-api-on-aws-english-dcb1a0db3110?source=collection_archive---------2-----------------------#2020-04-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6ec7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以你已经花了几天、几周甚至几个月的时间研究你的尖端机器学习模型；清理数据、工程特性、调整模型参数和无休止的测试。现在它终于可以生产了，您希望以一种简单可靠的方式提供它。实现这一点的一种方法是将其部署为REST API。</p><p id="ceff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在机器学习公司，我们依靠亚马逊网络服务来托管我们的机器学习模型。例如，我们的自动产品分类模型就托管在他们的平台上。与几年前相比，亚马逊提供的服务和功能使部署大规模机器学习模型变得相对容易。但是对于没有AWS平台或模型部署经验的人来说，这仍然是一项艰巨的任务。</p><p id="c7be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将向您展示如何使用Docker和AWS服务(如ECR、Sagemaker和Lambda)将您的机器学习模型部署为REST API。我们将从保存经过训练的机器学习模型的状态开始，创建推理代码和可以在Docker容器中运行的轻量级服务器。然后，我们将把容器化的模型部署到ECR，并在Sagemaker中创建一个机器学习端点。然后，我们将通过创建REST API端点来结束。本文中使用的模型是使用Scikit Learn制作的，但这里详细介绍的方法将适用于任何ML框架，其中可以序列化、冻结或保存估计器或转换器的状态。</p><p id="1f04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文中使用的所有代码都可以在下面的资源库中找到:【https://github.com/leongn/model_to_api<a class="ae kl" href="https://github.com/leongn/model_to_api" rel="noopener ugc nofollow" target="_blank"/></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/77579e33dff527624d41a011285c51dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*6Fn3vPtHzoryJqg5nU74Dw.jpeg"/></div></figure><h1 id="1116" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">模型</h1><p id="1269" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">对于这篇文章，我们将使用一个情绪分析模型，该模型对100000条带标签的推文进行了训练(积极或消极情绪)。该模型将一个句子或一系列句子作为输入，并输出相应文本的预测情感。它由两部分组成，第一部分是Scikit Learn的TFIDF矢量器，用于处理文本，第二部分是逻辑回归分类器(也来自Sklearn)，负责预测句子的情感。</p><h1 id="bb98" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">保存模型</h1><p id="39b6" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">部署过程的第一步是准备和存储您的模型，以便可以在其他地方轻松地重新打开它。这可以通过序列化来实现，序列化会冻结经过训练的分类器的状态并保存它。为此，我们将使用Scikit Learn的<em class="lx"> Joblib </em>，这是一个专门为存储大型numpy数组而优化的序列化库，因此特别适合Scikit Learn模型。如果您的模型中有不止一个Scikit Learn估计器或转换器(例如，像我们这样的TFIDF预处理器)，您也可以使用<em class="lx"> Joblib </em>保存它们。情感分析模型包括两个我们必须保存/冻结的组件:文本预处理器<em class="lx">和分类器。下面的代码将分类器和tfidf矢量器转储到文件夹<em class="lx"> Model_Artifacts </em>中。</em></p><pre class="kn ko kp kq gt ly lz ma mb aw mc bi"><span id="b25e" class="md kv iq lz b gy me mf l mg mh">from sklearn.externals import joblib joblib.dump(classifier, 'Model_Artifacts/classifier.pkl') joblib.dump(tfidf_vectorizer, 'Model_Artifacts/tfidf_vectorizer.pkl')</span></pre><p id="2823" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型组件的大小和数量没有限制。如果模型比通常的内存占用量更大，或者需要更多的处理能力，那么您可以选择一个更强大的AWS实例。</p><h1 id="20e3" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">创建Dockerfile文件</h1><p id="bcfc" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">一旦估算器和转换器被序列化，我们就可以创建一个Docker映像来保存我们的推理和服务器环境。Docker使得打包您的本地环境并在任何其他服务器/环境/计算机上使用它成为可能，而不必担心技术细节。你可以在这里了解更多关于Docker图像和容器以及如何定义它们:<a class="ae kl" href="https://docs.docker.com/get-started/" rel="noopener ugc nofollow" target="_blank">https://docs.docker.com/get-started/</a>。Docker映像将包含所有必要的组件，这些组件将使您的模型能够执行预测并与外界通信。我们可以用指定环境内容的Docker文件定义一个Docker映像:我们希望为webserver安装Python 3、Nginx和各种Python包，如Scikit-Learn、Flask和Pandas。</p><p id="6086" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">dockerfile可以在<em class="lx">容器</em>文件夹中找到:<a class="ae kl" href="https://github.com/leongn/model_to_api/blob/master/container/Dockerfile" rel="noopener ugc nofollow" target="_blank"> <em class="lx"> Dockerfile </em> </a></p><p id="08d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">docker文件的第一部分将安装运行服务器和执行我们的Python代码所需的一组Linux包，第二部分定义我们需要使用的一组Python包(如Pandas和Flask ),第三部分定义环境变量。文件的最后一部分告诉Docker哪个文件夹(<a class="ae kl" href="https://github.com/leongn/model_to_api/tree/master/container/sentiment_analysis" rel="noopener ugc nofollow" target="_blank"> <em class="lx">)包含我们的推理和必须添加到图像中的服务器代码。</em></a></p><h1 id="8804" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">定义服务和推理代码</h1><p id="f566" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">我们现在可以开始从Docker容器内部创建服务于您的机器学习模型的代码。我们将使用Flask微框架来处理来自外部的请求，并返回您的模型所做的预测。这是位于文件夹<a class="ae kl" href="https://github.com/leongn/model_to_api/tree/master/container/sentiment_analysis" rel="noopener ugc nofollow" target="_blank"> <em class="lx">容器/人气_分析</em> </a>中的<a class="ae kl" href="https://github.com/leongn/model_to_api/blob/master/container/sentiment_analysis/predictor.py" rel="noopener ugc nofollow" target="_blank"> <em class="lx"> predictor.py </em> </a> <em class="lx">文件。</em></p><p id="1602" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文件的第一部分导入所有必需的依赖项(这里使用的任何依赖项也应该添加到Dockerfile文件的第二部分)。我们用Joblib加载序列化的模型组件。然后我们创建Flask应用程序来服务我们的预测。第一个路由(Ping)通过检查分类器变量是否存在来检查容器的健康状况。如果没有，它将返回一个错误。Sagemaker随后使用这个“ping”来检查服务器是否正在运行和健康。</p><p id="4f86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来是预测部分:服务器接受带有JSON数据的POST请求，格式如下:<code class="fe mi mj mk lz b"><br/> {"input":<br/> [{"text" : "Input text 1"},<br/> {"text" : "Input text 2"},<br/> {"text": "Input text 3"}]}</code></p><p id="2e82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们将JSON数据转换为Pandas数据帧，然后我们使用TFIDF矢量器转换DF中的句子，并使用我们的分类器进行预测。分类器输出0(负面情绪)和1(正面情绪)。为了可读性，我们将把0和1分别改为负数和正数。最后，我们将结果转换回JSON，JSON可以作为请求的应答发送回来。</p><h1 id="8bf3" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">配置web服务器</h1><p id="f935" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">大多数人可以跳过这一部分，因为他们不必修改服务器使用的工作线程数量。但是如果您的模型很大(比如至少2GB)，您可能需要修改文件中的model_server_workers参数。该参数决定并行启动多少个Gunicorn服务器实例。默认值将使用系统中CPU的总数作为工作线程的数量。每一个都将保存模型和推理代码的副本，所以如果您有一个大型模型，那么您的机器/实例/服务器上的内存可能会很快填满。因此，您可能希望手动设置工作线程的数量。对于大型模型，将其设置为1并执行一些测试，看看您可以达到多高，该参数可以通过环境变量来定义。</p><h1 id="9b6a" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">建立码头工人形象</h1><p id="3cea" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">现在我们已经完成了环境、推理代码和服务器的定义，我们可以开始构建Docker映像进行测试了。</p><p id="4101" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将您的终端移动到docker文件所在的文件夹中。然后执行下面的命令来构建名为prediction _ docker _ image<em class="lx">:</em>的映像</p><p id="9871" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lx"> docker build -t预测_docker_image。</em></p><p id="d26b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意命令末尾的点，它告诉Docker在当前目录中查找Docker文件。如果出现权限错误，请在命令前面添加sudo。Docker现在将开始构建映像，并将它添加到您系统上的当前映像库中。</p><h1 id="58ac" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">启动docker容器</h1><p id="45dc" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">现在docker映像已经构建好了，我们将运行并测试它。但是首先，我们必须将所有序列化的模型元素和您可能使用的其他额外文件移动到该文件夹中。该文件夹与Sagemaker将在AWS实例上使用的文件夹设置相同。</p><p id="8803" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们将使用<a class="ae kl" href="https://github.com/leongn/model_to_api/blob/master/local_test/serve_local.sh" rel="noopener ugc nofollow" target="_blank"> serve_local.sh </a>脚本启动映像和服务器，该脚本告诉docker映像测试数据位于何处()，在哪些端口上运行(8080)以及包含推理代码的Docker映像的名称是什么。移动到您的终端所在的文件夹，执行以下命令来启动Docker容器和服务器:</p><p id="7ca8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lx">。/serve _ local . sh prediction _ docker _ image</em></p><p id="eaaa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">容器将被启动，服务器启动，您将看到以下消息(工作进程的数量取决于model_server_workers参数):</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/78cc61d00050944d4901cd2ce6a3c5b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ezM2Kla4Q14oWRKt.png"/></div></div></figure><p id="ec14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您想检查Docker容器的内存和CPU使用情况，打开一个新的终端并执行命令:<em class="lx"> docker stats </em></p><h1 id="f160" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">执行测试推理</h1><p id="61eb" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">现在服务器正在运行，我们可以给它发送一些数据来测试它。这可以通过<a class="ae kl" href="https://github.com/leongn/model_to_api/blob/master/local_test/predict.sh" rel="noopener ugc nofollow" target="_blank"> predict.sh </a>脚本来完成。这个脚本向服务器发送一个JSON文件。</p><p id="6f86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在终端所在的文件夹中，键入以下命令:</p><p id="da8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lx">。/predict.sh input.json </em></p><p id="c02f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果一切顺利，你应该会收到一些预测。恭喜，您的Docker映像已经可以部署了！</p><h1 id="3e14" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">部署Docker映像</h1><p id="7d29" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">docker图像和机器学习模型工件在Sagemaker中保持分离。这样，您可以用您的推理代码创建一个Docker图像文件，但是插入您的模型的不同版本。我们首先将刚刚创建的Docker图像推送到Amazon Elastic Container Registry(ECR ),它将存储我们的图像。为此，您必须安装AWS CLI，并在系统上配置一个现有的AWS帐户。如果要检查AWS配置，请在终端中执行以下命令:</p><p id="07c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lx"> aws配置列表</em></p><p id="dfc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果还没有配置，您可以使用<em class="lx"> aws configure </em>命令创建一个，并输入您的aws凭证。需要注意的是，您应该仔细选择您的区域，所有的模型组件都必须位于同一个区域，以便能够进行通信。</p><p id="485e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以使用<em class="lx">容器</em>文件夹中的<a class="ae kl" href="https://github.com/leongn/model_to_api/blob/master/container/build_and_push.sh" rel="noopener ugc nofollow" target="_blank"> build_and_push.sh </a>脚本来执行推送过程。这个脚本将自动创建一个新的ECR存储库(如果它还不存在的话),并将您的映像推送到这个存储库中。将您的终端移动到文件夹并执行:</p><p id="d1fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lx">。/build _ and _ push . sh prediction _ docker _ image</em></p><p id="7132" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦图像上传，打开位于<a class="ae kl" href="https://console.aws.amazon.com/ecs/" rel="noopener ugc nofollow" target="_blank">https://console.aws.amazon.com</a>的AWS控制台，进入ECR并点击左侧窗格中的<em class="lx">存储库</em>。然后选择您刚刚上传的图像，并复制页面顶部的存储库URI(创建一个临时文本文件来保存这些信息可能会很有用，因为我们稍后会用到它)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mq"><img src="../Images/de54b8cf968c0ea313dbd256bf3c7935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5F_DBOYIfReyBjkN.png"/></div></div></figure><h1 id="4705" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">上传模型工件到S3</h1><p id="b89b" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">如前所述，Docker映像只包含推理环境和代码，不包含经过训练的序列化模型。序列化的训练模型文件，在Sagemaker上称为模型工件，将被存储在一个单独的S3存储桶中。我们首先将所有必要的(序列化的)模型组件放在一个start压缩文件中(在存储库中找到<a class="ae kl" href="https://github.com/leongn/model_to_api/tree/master/local_test/test_dir/model" rel="noopener ugc nofollow" target="_blank">)。然后，您可以将它们上传到一个现有的S3存储桶(与ECR映像位于同一区域)，或者创建一个新的存储桶。这些步骤可以使用AWS CLI或通过在线界面来执行。</a></p><p id="7d33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要使用AWS在线界面创建一个新的铲斗，转到<a class="ae kl" href="https://console.aws.amazon.com" rel="noopener ugc nofollow" target="_blank"> AWS控制台</a>并选择S3。接下来，点击Create bucket，为Bucket选择一个名称(例如:perspective-analysis-artifacts)和您希望Bucket所在的区域。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/64b24cbf19b8d33e43b7e91c833bfc71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/0*BawOmk1EvLrvoqHe.png"/></div></figure><p id="8e17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该地区应该与您用于ECR的地区相同。在下一个<em class="lx">配置选项</em>页面，您可以更改与您的铲斗相关的不同设置，我建议您选择自动加密。第三个屏幕与设置访问权限相关。缺省值应该适合大多数用户。最后，最后一页允许您查看所有设置。</p><p id="938a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建存储桶后，通过单击打开存储桶，然后选择Upload。继续选择包含您的模型工件的压缩文件。对于接下来的步骤，默认设置是合适的。等待上传完成，点击新上传的文件，复制页面底部的网址以备后用。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/84d8229fc026332447369d4b3ec0e8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/0*pAtzs5jGgnJcviUU.png"/></div></figure><h1 id="aa70" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">在Sagemaker中设置图像</h1><h2 id="0e4b" class="md kv iq bd kw ms mt dn la mu mv dp le jy mw mx li kc my mz lm kg na nb lq nc bi translated">模型创建</h2><p id="2719" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">现在，映像已部署到ECR，机器学习模型工件已上传到S3，我们可以开始配置Sagemaker预测端点。我们必须从创建Sagemaker模型资源开始。通过您的AWS控制台转到Sagemaker，然后在左侧面板中的“推理”下，单击“模型”，然后单击屏幕右侧的“创建模型”(确保您仍然在正确的区域中)。首先，我们必须给我们的模型起一个名字，并给它分配一个IAM角色。如果您已经有了Sagemaker的IAM角色，请选择该角色。否则，从下拉菜单中选择创建角色。我们想授予我们的Sagemaker模型访问我们的S3存储桶的权限，或者在特定的S3存储桶下给出您的S3存储桶的名称(情感-分析-工件),或者选择任何S3存储桶。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nd"><img src="../Images/5bfbf54577d43c4cf84e0048d4cf38c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L5lSB5n2B0KLSPbB.png"/></div></div></figure><p id="9a23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们必须告诉Sagemaker Docker图像和模型工件的位置。现在，您可以将我们在前面的步骤中获得的ECR URI和S3 URL粘贴到相应的字段中。在您的图像URI(例如:* * * * * * * * * . ecr . eu-central-1 . amazonaws . com/perspective _ analysis _ image:latest)的末尾添加标签<em class="lx"> latest </em>以确保Sagemaker始终从ECR中选择您的模型的最新版本。容器主机名是可选的，添加像Version-1.0这样的标记可能对版本控制有用。单击create model结束。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ne"><img src="../Images/6c0a789a2a2054b4f5fce21abcea20ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fOFPOMVM8gMb_dhX.png"/></div></div></figure><h2 id="ebcb" class="md kv iq bd kw ms mt dn la mu mv dp le jy mw mx li kc my mz lm kg na nb lq nc bi translated">端点配置</h2><p id="bc85" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">下一步是在Sagemaker中创建端点配置(左窗格-&gt;端点配置-&gt;创建端点配置)。这让我们可以指定将哪个模型添加到端点，以及在哪个AWS实例上运行它。首先给它起一个名字(例如:情感分析端点配置)。然后单击Add model并选择之前创建的模型。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nf"><img src="../Images/68ecc7d435a4377c2e9294484f4b947d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7Wt6Y4Bg83SODrgo.png"/></div></div></figure><p id="90ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后我们可以通过点击模型旁边的<em class="lx">编辑</em>按钮来编辑模型，选择在哪个AWS实例上运行它。默认的ml.m4.large对于大多数负载应该足够了，但是如果您的模型相对较小或较大，计算密集型或轻量级，您可以选择另一个(参见可用实例的类型:【https://aws.amazon.com/sagemaker/pricing/instance-types/】的和每个地区的价格:<a class="ae kl" href="https://aws.amazon.com/sagemaker/pricing/" rel="noopener ugc nofollow" target="_blank">的https://aws.amazon.com/sagemaker/pricing/</a>)。点击<em class="lx">创建端点配置</em>完成此步骤。</p><h2 id="1f7f" class="md kv iq bd kw ms mt dn la mu mv dp le jy mw mx li kc my mz lm kg na nb lq nc bi translated">端点创建</h2><p id="aba9" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">最后一步是创建Sagemaker端点(左窗格-&gt;端点-&gt;创建端点)。首先给端点一个名称，这个名称将在以后被API网关用来调用端点。然后选择在上一步中创建的端点配置，并单击选择端点配置。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ng"><img src="../Images/77ff431d9c82ca34f8896b6118aa4d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DrGqaycQ5WPux3KN.png"/></div></div></figure><p id="bb7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过单击创建端点完成该过程。Sagemaker将开始部署您的模型，这可能需要一点时间。</p><p id="adf2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果一切按计划进行，您将会在状态栏中看到InService。</p><h1 id="302c" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">创建REST API</h1><p id="da90" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">一旦创建了Sagemaker端点，您就可以使用AWS CLI或AWS Python SDK (Boto3)从您的AWS帐户中访问该模型。如果您想执行一些内部测试，这是没问题的，但是我们想让它对外部世界可用，所以我们必须创建一个API。</p><p id="a8e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可以通过亚马逊的圣杯图书馆(<a class="ae kl" href="https://github.com/aws/chalice" rel="noopener ugc nofollow" target="_blank">https://github.com/aws/chalice</a>)轻松实现。这是一个微框架，允许在AWS lambda上快速创建和部署应用程序，并简化API的创建。</p><p id="76e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，您需要使用以下命令安装Chalice和用于Python的AWS SDK</p><p id="5d8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lx"> sudo pip安装圣杯boto3 </em></p><p id="5a1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">API网关需要部署在与模型端点相同的区域中。为了确保它是正确的，您可以通过在终端中输入以下命令再次检查您的区域配置:<em class="lx"> aws配置列表</em></p><p id="8e55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果区域不正确，使用<em class="lx"> aws configure </em>进行正确配置</p><p id="77bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这部分的代码可以在<a class="ae kl" href="https://github.com/leongn/model_to_api/tree/master/api_creation" rel="noopener ugc nofollow" target="_blank"> <em class="lx"> api_creation </em> </a>文件夹中找到。</p><p id="24fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">主文件夹中的<a class="ae kl" href="https://github.com/leongn/model_to_api/blob/master/api_creation/app.py" rel="noopener ugc nofollow" target="_blank"> app.py </a>文件包含了API的路由逻辑:当接收到某些请求(比如POST)时会执行什么动作。在这里，您最终可以更改传入数据的格式，或者进行一些额外的检查，如果Docker容器中的推理代码没有执行这些检查的话。这段代码的主要功能是当一个带有数据的POST请求被发送到您的API时调用您的Sagemaker端点，并返回响应。</p><p id="cdec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当一切都配置好了，您可以通过移动到您的终端的<a class="ae kl" href="https://github.com/leongn/model_to_api/tree/master/api_creation" rel="noopener ugc nofollow" target="_blank"> api_creation </a>文件夹并执行下面的命令来部署API网关:<em class="lx"> chalice deploy </em></p><p id="a1ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，Chalice将返回REST API URL。复制并保存它。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/21800e8b7a63c8e10fd9d6394466fd07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/0*_76vogVNOOyXIuNO.png"/></div></figure><p id="9440" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，您可以使用该端点URL来执行请求。您可以使用以下Python代码来测试它:</p><pre class="kn ko kp kq gt ly lz ma mb aw mc bi"><span id="827e" class="md kv iq lz b gy me mf l mg mh">import requests # Define test JSON input_sentiment = {'input': [{'text' : 'Today was a great day!'}, {'text' : 'Happy with the end result!'}, {'text': 'Terrible service and crowded. Would not ]} input_json = json.dumps(input_sentiment) # Define your api URL here api_url = 'https://*******.execute-api.eu-central-1.amazonaws.com/api/' res = requests.post(api_url, json=input_json) output_api = res.text print(output_api)</span></pre><p id="7429" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的代码产生了以下输出:</p><p id="b8fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">{ "输出":<br/> [{"label ":"正" }，<br/> {"label ":"正" }，<br/> {"label ":"负" }]}</p><p id="b04b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看起来与输入一致，您的机器学习API现在完全可以运行了！</p><h1 id="25dd" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">关闭端点</h1><p id="abe8" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">一旦完成并且不再需要API，不要忘记删除Sagemaker中的端点，因为实例成本会不断增加。</p><h1 id="86e1" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">结论</h1><p id="c2cc" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">在这篇文章中，你看到了如何:</p><ul class=""><li id="e6b9" class="ni nj iq jp b jq jr ju jv jy nk kc nl kg nm kk nn no np nq bi translated">序列化您的机器学习模型组件以进行部署</li><li id="083f" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">使用推理和服务器代码创建您自己的Docker映像</li><li id="383c" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">将您的Docker图像推送到ECR</li><li id="6a8f" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">将您的模型工件保存到S3</li><li id="5edc" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">配置并创建一个Sagemaker端点</li><li id="86ae" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">用Chalice创建API端点</li></ul></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="dac5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lx">原发布于</em><a class="ae kl" href="https://machine-learning-company.nl/deploy-machine-learning-model-rest-api-using-aws/" rel="noopener ugc nofollow" target="_blank"><em class="lx">https://machine-learning-company . nl</em></a><em class="lx">。<br/>作者:</em> <a class="ae kl" href="https://www.linkedin.com/in/leon-gerritsen/" rel="noopener ugc nofollow" target="_blank"> <em class="lx">莱昂</em> </a></p></div></div>    
</body>
</html>