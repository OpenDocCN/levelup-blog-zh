<html>
<head>
<title>Make Your Python Web Scraper Smarter</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让您的Python Web Scraper更智能</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/make-your-python-web-scraper-smarter-6233f2d10c3f?source=collection_archive---------0-----------------------#2019-06-20">https://levelup.gitconnected.com/make-your-python-web-scraper-smarter-6233f2d10c3f?source=collection_archive---------0-----------------------#2019-06-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/2492674a3e5d5d5650c5e61b1acf1a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*c-oO0pMZ2HoAHoxDP-m6JQ.jpeg"/></div></figure><p id="4746" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我最近开始学习使用Python进行web抓取。到目前为止，我认为很有趣的一件事是如何通过增加更多的功能来使你的网络抓取器变得“更聪明”。例如，我想让它能够翻页，并在完成当前页面上所有需要的数据后继续工作。</p><h1 id="37e4" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">样板工程</strong></h1><p id="c35e" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">我想知道每一个中国名人之间的关系。为了获得这些信息，我需要从<a class="ae lv" href="http://www.1905.com/mdb/relation/list/s0t1p1.html" rel="noopener ugc nofollow" target="_blank">http://www.1905.com/mdb/relation/list/s0t1p1.html</a>提取数据，这是一个包含所有中国名人关系的网站。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/d07d06ccb862109f07ce9e6c80a1008a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*uQsGfB3VKcjlTS-h8GN74w.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">名人关系</figcaption></figure><p id="9d81" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">注意URL末尾的“s0t1p1 ”:它表示类别号、关系类型(婚姻、家庭成员、夫妇等)和页码。因此，s0t1p1指示类别1、关系类型1(婚姻)和页面1。</p><p id="90ec" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里我用Python中的<a class="ae lv" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">美汤</strong> </a>读取我的HTML文件并从中提取数据。我的这篇<a class="ae lv" href="https://medium.com/@yunhanh/quick-web-scraping-with-python-beautiful-soup-4dde18468f1f" rel="noopener"> <em class="mf">文章</em> </a>解释了什么是美丽的汤，以及我们如何使用它来轻松地从web上抓取数据。</p><p id="69ac" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">导入必要的模块和包之后，让我们开始吧。</p><h1 id="f3aa" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">创建漂亮的汤对象</strong></h1><p id="00ee" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">使用<code class="fe mg mh mi mj b">urllib</code>读取HTML后，我们需要创建一个漂亮的Soup对象。将html传递给BeautifulSoup()函数，Beautiful Soup包将解析html(它获取html文本并将其解析为Python对象。)</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="ada7" class="mo kt iq mj b gy mp mq l mr ms">html = urlopen(url)<br/>soup = BeautifulSoup(html, “html.parser”)</span></pre><p id="e7a8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">或者，你可以换一种方式来做，这种方法可以避免直接做<code class="fe mg mh mi mj b">html = urlopen(url)</code>时的错误</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="2c0c" class="mo kt iq mj b gy mp mq l mr ms">url_i = "YOUR_URL"</span><span id="3b74" class="mo kt iq mj b gy mt mq l mr ms">referer = url_i</span><span id="4e0b" class="mo kt iq mj b gy mt mq l mr ms">user_agent = “YOUR_USER_AGENT”</span><span id="7de8" class="mo kt iq mj b gy mt mq l mr ms">headers = {‘User-Agent’: user_agent, ‘Referer’: referer, ‘Connection’: ‘keep-alive’}</span><span id="c97a" class="mo kt iq mj b gy mt mq l mr ms">try:</span><span id="7a56" class="mo kt iq mj b gy mt mq l mr ms">  req = urllib.request.Request(url = url_i, headers = headers)</span><span id="a0a2" class="mo kt iq mj b gy mt mq l mr ms">  response = urlopen(req)</span><span id="19ff" class="mo kt iq mj b gy mt mq l mr ms">  html = response.read()</span><span id="e148" class="mo kt iq mj b gy mt mq l mr ms"># throw exceptions</span><span id="7162" class="mo kt iq mj b gy mt mq l mr ms">except error.HTTPError as e:</span><span id="417b" class="mo kt iq mj b gy mt mq l mr ms">  print(e.reason)</span><span id="2172" class="mo kt iq mj b gy mt mq l mr ms">soup = BeautifulSoup(html, “html.parser”)</span></pre><h1 id="c6cf" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">提取数据</strong></h1><p id="6b52" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">在获得漂亮的Soup对象后，我们可以用它来提取我们想要的数据。在这个示例项目中，我们使用了我在之前的<a class="ae lv" href="https://medium.com/@yunhanh/quick-web-scraping-with-python-beautiful-soup-4dde18468f1f" rel="noopener">帖子</a>中提到的相同方法，即基于标签和类抓取数据。除了名人的名字和他们的关系，我还想要他们对应的ID，也就是<code class="fe mg mh mi mj b">href</code>。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/d93e6702de4193b11b7740e916e602e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6nVSSj3Yjxe3WFJmGIdc7A.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">A single relation between celebrity “文章” and “马伊琍”</figcaption></figure><p id="2db1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过构建一个嵌套循环，你的web scraper会自动抓取所有需要的数据。</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="ba91" class="mo kt iq mj b gy mp mq l mr ms">def relations(soup):<br/>  k = soup.find_all(‘div’, class_=’gx_List mt10')<br/>  for x in k:<br/>    allP = x.find_all(‘ul’)<br/>    for person in allP:<br/>      result = “”<br/>      for el in person.find_all(‘li’):<br/>        iD = “”<br/>        for a in el.find_all(‘a’, href = True):<br/>          p = el.get_text(“|”, strip = True)<br/>          iD += a[‘href’]<br/>          iD += “|”<br/>        res = p + iD <br/>        res += “\n” <br/>        result += res<br/>      return result</span></pre><p id="be17" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">上面的函数将在第一页上生成数据，如下所示:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mz"><img src="../Images/c1cd51e1fec753d0961ce3cd5fae4192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4G8n5zzcEfvvNPi5n2VzQ.png"/></div></div></figure><h1 id="4b29" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">翻页</strong></h1><p id="47c3" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">现在来说说翻页。<strong class="jw ir"> </strong>很多时候，数据分布在一个特定网站的各个地方，尤其是当它有多个页面包含您想要一次提取的相同类型的数据时。因此，在开发你的网页抓取工具时，翻页器是必要的。</p><p id="18d1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了让它运行多次，我们需要构建另一个函数来跟踪当前页面、页码以及scraper是否到达页面列表的末尾。在这个示例网站中，我们无法找出存在的页面总数，所以我们不能使用一个特定的数字来设置我们的循环的限制。</p><h2 id="2e03" class="mo kt iq bd ku na nb dn ky nc nd dp lc kf ne nf lg kj ng nh lk kn ni nj lo nk bi translated">1.一个柜台</h2><p id="fa19" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">让我们首先将计数器I初始化为零。它跟踪当前页面，并在每次迭代中递增1。</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="a2ee" class="mo kt iq mj b gy mp mq l mr ms"># NOT A COMPLETE CODE FOR THE WHILE LOOP </span><span id="a364" class="mo kt iq mj b gy mt mq l mr ms">i = 0</span><span id="1b1e" class="mo kt iq mj b gy mt mq l mr ms">While SOME CONDITION :<br/>  <br/>  i +=1</span><span id="de54" class="mo kt iq mj b gy mt mq l mr ms">  url_i = “http://www.1905.com/mdb/relation/list/" + “s0t1p” + str(i) + “.html”</span><span id="8d68" class="mo kt iq mj b gy mt mq l mr ms">  referer = url_i</span><span id="3394" class="mo kt iq mj b gy mt mq l mr ms">  user_agent = “YOUR USER AGENT”</span><span id="c0bc" class="mo kt iq mj b gy mt mq l mr ms">  headers = {‘User-Agent’: user_agent, ‘Referer’: referer, ‘Connection’: ‘keep-alive’}</span><span id="fe93" class="mo kt iq mj b gy mt mq l mr ms">  try:</span><span id="bf65" class="mo kt iq mj b gy mt mq l mr ms">    req = urllib.request.Request(url = url_i, headers = headers)</span><span id="0155" class="mo kt iq mj b gy mt mq l mr ms">    response = urlopen(req)</span><span id="1d48" class="mo kt iq mj b gy mt mq l mr ms">    html = response.read()</span><span id="9d28" class="mo kt iq mj b gy mt mq l mr ms"># throw exceptions<br/>  except error.HTTPError as e:<br/>    print(e.reason)</span><span id="68f8" class="mo kt iq mj b gy mt mq l mr ms">  soup = BeautifulSoup(html, “html.parser”)</span></pre><h2 id="dea3" class="mo kt iq bd ku na nb dn ky nc nd dp lc kf ne nf lg kj ng nh lk kn ni nj lo nk bi translated">2.页面</h2><p id="6400" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">下一步是找出网站上指示页面的元素。右击你的网站，选择“检查”并进入“元素”。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b202254cc256d98fb8a6378c8997c09d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*_SGz5TErxUpBXsq2Mc0RdA.png"/></div></figure><p id="caf2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">(The text “下一页“ means “next page” in Chinese.)</p><p id="9799" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在仔细观察html文本的这一部分。为了找到页面的信息，我们需要提取整个div标签</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="261d" class="mo kt iq mj b gy mp mq l mr ms">page = soup.find_all (id = “new_page”)</span></pre><p id="bc5e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">要检查scraper是否到达了页面列表的末尾，我们需要检查html文本的这一行在该页面上时是否存在:</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="d988" class="mo kt iq mj b gy mp mq l mr ms">&lt;a href=”/mdb/relation/list/s0t1p2.html”&gt;下一页&lt;/a&gt;&lt;/div&gt;</span></pre><p id="b14b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此，我们可以构建这样一个循环:</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="8fa0" class="mo kt iq mj b gy mp mq l mr ms">for p in page:<br/>  allp = p.find_all(‘a’, href = True)<br/>  for x in allp:<br/>    if x.get_text() == “下一页”:<br/>      print(“More pages ahead, keep going!”)<br/>      if x[‘href’] == “/mdb/relation/list/s1t0p” + str(i+1)+ “.html”:<br/>        break<br/>      else:</span><span id="7fd1" class="mo kt iq mj b gy mt mq l mr ms">       # MORE CODE HERE<br/>     <br/>f.write(relations(soup))<br/>print(relations(soup))</span></pre><h2 id="b6d5" class="mo kt iq bd ku na nb dn ky nc nd dp lc kf ne nf lg kj ng nh lk kn ni nj lo nk bi translated">3.结束While循环</h2><p id="ba37" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">现在让我们考虑如何结束while循环，并确保web scraper已经抓取了每一页。我们还没有设置while条件，所以我们可以使用计数器来进行限制。</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="5314" class="mo kt iq mj b gy mp mq l mr ms">while <strong class="mj ir">i &gt;= 0</strong>:</span><span id="bbb3" class="mo kt iq mj b gy mt mq l mr ms">(content of the while loop)</span><span id="ff19" class="mo kt iq mj b gy mt mq l mr ms">…</span><span id="8715" class="mo kt iq mj b gy mt mq l mr ms">else:</span><span id="02f9" class="mo kt iq mj b gy mt mq l mr ms"><strong class="mj ir">i = -1</strong> <br/></span></pre><p id="75dc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里，I不再大于等于0，这会使while循环中断。</p><h2 id="e496" class="mo kt iq bd ku na nb dn ky nc nd dp lc kf ne nf lg kj ng nh lk kn ni nj lo nk bi translated">4.把所有东西放在一起</h2><p id="8c64" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">让我们将所有内容放在一个函数中，让Python将结果写入一个名为“celebrity.txt”的文本文件</p><p id="5d22" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">翻页功能的完整代码:</p><pre class="lx ly lz ma gt mk mj ml mm aw mn bi"><span id="4c7a" class="mo kt iq mj b gy mp mq l mr ms">def pageturning():<br/>  f = open(“celebrity.txt”, “w+”)<br/>  i = 0<br/>  <br/>  while i&gt;=0 :<br/>    i += 1<br/>    url_i = “<a class="ae lv" href="http://www.1905.com/mdb/relation/list/" rel="noopener ugc nofollow" target="_blank">http://www.1905.com/mdb/relation/list/</a>" + “s1t0p” + str(i) + “.html”<br/>    referer = url_i<br/>    user_agent = ‘YOUR_USER_AGENT’<br/>    headers = {‘User-Agent’: user_agent, ‘Referer’: referer, ‘Connection’: ‘keep-alive’}<br/>    <br/>    try:<br/>      req = urllib.request.Request(url = url_i, headers = headers)<br/>      response = urlopen(req)<br/>      html = response.read()<br/>    except error.HTTPError as e:<br/>      break<br/>    <br/>    soup = BeautifulSoup(html, “lxml”)<br/>    <br/>    page = soup.find_all(id = “new_page”)</span><span id="a4bb" class="mo kt iq mj b gy mt mq l mr ms">    for p in page:<br/>      allp = p.find_all(‘a’, href = True)<br/>      <br/>      for x in allp:<br/>        if x.get_text() == “下一页”:<br/>          print(“More pages ahead, keep going!”)<br/>          # inner loop ends</span><span id="bb1d" class="mo kt iq mj b gy mt mq l mr ms">      if x[‘href’] == “/mdb/relation/list/s1t0p” + str(i+1)+ “.html”:<br/>        break</span><span id="5c9d" class="mo kt iq mj b gy mt mq l mr ms">      else:<br/>        i = -1<br/>    f.write(relations(soup))<br/>    print(relations(soup)) <br/> print(“You have reached the end”)<br/> <br/> return</span></pre><p id="69f5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">下面是结果的样子:我们的web scraper的修改版本在到达页面末尾时会自动翻页，总共生成1387行结果。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/30d0182e89c0a424086761884169382b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*GP3LmhFeXCiNpBdKebwjtw.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">结果文本文件“celebrity.txt”</figcaption></figure><h1 id="ca96" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">总结</strong></h1><p id="c2b7" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">希望这篇文章能帮助你建立一些开发更好的Python web scraper的想法。实现翻页功能肯定还有很多其他方法。如果您确定想要抓取的总页数，您可以简单地将这个限制作为您的循环条件。但是，当不同的网站有不同的页码时，这种实现方式适用于更一般的情况。只要你知道如何跟踪页码，并且能够在你的网页抓取器收集数据时设置限制，你就都是好的。</p></div></div>    
</body>
</html>