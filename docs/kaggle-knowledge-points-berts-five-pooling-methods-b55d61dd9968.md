# Kaggle çŸ¥è¯†ç‚¹:ä¼¯ç‰¹çš„äº”ç§æ±‡é›†æ³•

> åŸæ–‡ï¼š<https://levelup.gitconnected.com/kaggle-knowledge-points-berts-five-pooling-methods-b55d61dd9968>

![](img/45d92b12a51a100c689ba4433eebd147.png)

BERT æ¨¡å‹å¯ä»¥ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œä¹Ÿæ˜¯å½“å‰ NLP æ¨¡å‹çš„å¿…è¦æ–¹æ³•ã€‚åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨`[CLS]`çš„ç›¸åº”è¾“å‡ºæ¥å®Œæˆæ–‡æœ¬åˆ†ç±»ï¼Œå½“ç„¶è¿˜æœ‰å…¶ä»–æ–¹æ³•ã€‚

è¿™å…è®¸æ¯ä¸ª`token`å¯¹åº”çš„è¾“å‡ºè¢«ä½¿ç”¨`pooling`ï¼Œç„¶ååœ¨é€šè¿‡åè¢«åˆ†ç±»ã€‚æœ¬æ–‡å°†ä»‹ç»å‡ ç§æ„å»ºå’Œä½¿ç”¨ BERT çš„å¸¸ç”¨æ–¹æ³•ã€‚

# æ–¹æ³• 1:å¹³å‡æ±‡é›†

`token`è®¡ç®—æ¯ä¸ªå¯¹åº”è¾“å‡ºçš„å¹³å‡å€¼ï¼Œè¿™é‡Œéœ€è¦è€ƒè™‘`attention_mask`ï¼Œå³éœ€è¦è€ƒè™‘æœ‰æ•ˆè¾“å…¥`token`ã€‚

```
class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()

    def forward(self, last_hidden_state, attention_mask):
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
        sum_mask = input_mask_expanded.sum(1)
        sum_mask = torch.clamp(sum_mask, min = 1e-9)
        mean_embeddings = sum_embeddings/sum_mask
        return mean_embeddings
```

# æ–¹æ³• 2:æœ€å¤§æ± åŒ–

è®¡ç®—æ¯ä¸ª`token`å¯¹åº”è¾“å‡ºçš„æœ€å¤§å€¼ï¼Œè¿™é‡Œéœ€è¦è€ƒè™‘`attention_mask`ï¼Œä¹Ÿå°±æ˜¯æœ‰æ•ˆè¾“å…¥éœ€è¦è€ƒè™‘`token`ã€‚

```
class MaxPooling(nn.Module):
    def __init__(self):
        super(MaxPooling, self).__init__()

    def forward(self, last_hidden_state, attention_mask):
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        embeddings = last_hidden_state.clone()
        embeddings[input_mask_expanded == 0] = -1e4
        max_embeddings, _ = torch.max(embeddings, dim = 1)
        return max_embeddings
```

# æ–¹æ³• 3:æœ€å°å…¬æ‘Š

è®¡ç®—æ¯ä¸ª`token`å¯¹åº”è¾“å‡ºçš„æœ€å°å€¼ï¼Œè¿™é‡Œéœ€è¦è€ƒè™‘`attention_mask`ï¼Œå³éœ€è¦è€ƒè™‘æœ‰æ•ˆè¾“å…¥`token`ã€‚

```
class MinPooling(nn.Module):
    def __init__(self):
        super(MinPooling, self).__init__()

    def forward(self, last_hidden_state, attention_mask):
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        embeddings = last_hidden_state.clone()
        embeddings[input_mask_expanded == 0] = 1e-4
        min_embeddings, _ = torch.min(embeddings, dim = 1)
        return min_embeddings
```

# æ–¹æ³• 4:åŠ æƒæ± 

è®¡ç®—æ¯ä¸ª`token`å¯¹åº”è¾“å‡ºçš„é‡é‡ã€‚è¿™é‡Œçš„æƒé‡å¯ä»¥é€šè¿‡ç‰¹å¾è®¡ç®—ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ IDF è®¡ç®—ã€‚

```
class WeightedLayerPooling(nn.Module):
    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):
        super(WeightedLayerPooling, self).__init__()
        self.layer_start = layer_start
        self.num_hidden_layers = num_hidden_layers
        self.layer_weights = layer_weights if layer_weights is not None \
            else nn.Parameter(
                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)
            )
```

```
 def forward(self, ft_all_layers):
        all_layer_embedding = torch.stack(ft_all_layers)
        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :] weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())
        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum() return weighted_average
```

# æ–¹æ³• 5:é›†ä¸­æ³¨æ„åŠ›

å°†æ¯ä¸ª`token`ç‰¹å¾å•ç‹¬æ·»åŠ åˆ°ä¸€ä¸ªå›¾å±‚ä¸­è¿›è¡Œå…³æ³¨åº¦è®¡ç®—ï¼Œå¢åŠ æ¨¡å‹çš„å»ºæ¨¡èƒ½åŠ›ã€‚

```
class AttentionPooling(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.attention = nn.Sequential(
        nn.Linear(in_dim, in_dim),
        nn.LayerNorm(in_dim),
        nn.GELU(),
        nn.Linear(in_dim, 1),
        )
```

```
 def forward(self, last_hidden_state, attention_mask):
        w = self.attention(last_hidden_state).float()
        w[attention_mask==0]=float('-inf')
        w = torch.softmax(w,1)
        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)
        return attention_embeddings
```

# æ€»ç»“

ä»æ¨¡å‹å¤æ‚åº¦æ¥çœ‹:attention pooling > weighted layer pooling > mean pooling/min pooling/max pooling

ä»æ¨¡å‹ç²¾åº¦æ¥çœ‹:æ³¨æ„æ± >åŠ æƒå±‚æ± >å¹³å‡æ± >æœ€å¤§æ± >æœ€å°æ± 

ä½¿ç”¨å¤šç§æ± çš„ç›®çš„æ˜¯å¢åŠ  BERT æ¨¡å‹çš„å¤šæ ·æ€§ï¼Œå¹¶è€ƒè™‘åœ¨æ¨¡å‹é›†æˆä¸­ä½¿ç”¨å®ƒã€‚

å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿæˆä¸ºä¸€ä¸ªåª’ä»‹æˆå‘˜ï¼Œé€šè¿‡æ— é™åˆ¶çš„é˜…è¯»ç»§ç»­å­¦ä¹ ã€‚å¦‚æœä½ ä½¿ç”¨[è¿™ä¸ªé“¾æ¥](https://machinelearningabc.medium.com/membership)æˆä¸ºä¼šå‘˜ï¼Œä½ å°†æ”¯æŒæˆ‘ï¼Œä¸éœ€è¦ä½ é¢å¤–ä»˜è´¹ã€‚æå‰æ„Ÿè°¢ï¼Œå†è§ï¼

# åˆ†çº§ç¼–ç 

æ„Ÿè°¢æ‚¨æˆä¸ºæˆ‘ä»¬ç¤¾åŒºçš„ä¸€å‘˜ï¼åœ¨ä½ ç¦»å¼€ä¹‹å‰:

*   ğŸ‘ä¸ºæ•…äº‹é¼“æŒï¼Œè·Ÿç€ä½œè€…èµ°ğŸ‘‰
*   ğŸ“°æŸ¥çœ‹[å‡çº§ç¼–ç å‡ºç‰ˆç‰©](https://levelup.gitconnected.com/?utm_source=pub&utm_medium=post)ä¸­çš„æ›´å¤šå†…å®¹
*   ğŸ””å…³æ³¨æˆ‘ä»¬:[Twitter](https://twitter.com/gitconnected)|[LinkedIn](https://www.linkedin.com/company/gitconnected)|[æ—¶äº‹é€šè®¯](https://newsletter.levelup.dev)

ğŸš€ğŸ‘‰ [**åŠ å…¥å‡çº§è¾¾äººé›†ä½“ï¼Œæ‰¾åˆ°ä¸€ä»½æƒŠè‰³çš„å·¥ä½œ**](https://jobs.levelup.dev/talent/welcome?referral=true)