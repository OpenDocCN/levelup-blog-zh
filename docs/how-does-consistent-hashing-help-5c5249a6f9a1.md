# 一致性散列如何帮助负载平衡和服务器

> 原文：<https://levelup.gitconnected.com/how-does-consistent-hashing-help-5c5249a6f9a1>

## 一致散列法指南

![](img/4d2c7d8fa9756232a493f7689155179d.png)

科恩·斯佩尔曼在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

正如在[的上一篇文章](https://blog.devgenius.io/basics-of-load-balancing-f8a1abd97ece)中所讨论的，负载平衡器非常有助于在服务器之间有效地分配负载。

LB 通过始终将流量路由到健康的节点来帮助确保高可用性，从而为客户端确保高可用性和高效的系统。

对于不在服务器节点上持久存储数据的服务来说，这很好。如果服务保留数据并基于该数据返回响应(或者该数据本身就是响应)，那么它就变得复杂了。

让我们用另一个故事来理解这一点。

# 简单的服务

我们正在建立一个数据服务。用例很简单。系统应该能够存储键和值对。存储的数据应该是持久的。系统应该能够返回给定键的数据。

为了简单起见，我们将保持设计简单，不进入系统的内部数据结构等。

有一个服务(DataService)，它公开 API 来存储和获取数据。服务被公开为 rest 服务。

![](img/6ac6072964dc0b4e9d0afe1010915051.png)

服务运行良好，直到有一天流量超过了服务器的容量。那天服务器崩溃了。

# 扩展—使用负载平衡器的服务

正如我们在 LB 基础文章中所讨论的，显而易见的解决方案是在多台服务器上托管数据服务，并通过 LB 公开它。它有助于服务层很好地扩展。

![](img/c15d0eb298d5b19e177f68907c41ffd8.png)

然而，存储实际 key <>值数据的数据存储仍然是相同的，因此它逐渐成为瓶颈。

# 扩展—具有读取副本的数据库

解决方案是制作更多的数据库副本。

由于数据不一致和数据冲突的复杂用例，拥有整个数据库的主动-主动副本并不容易。因此，创建了多个副本来读取数据，而写操作将始终进行到同一个主节点。

任何数据更新都将从主节点复制到其他读取节点。

![](img/45cba7320bf0463153b2eb556fda8874.png)

这种方法有两个问题:

*   读取操作伸缩性良好。但是，写操作仍然只绑定到一个写节点。它对于写入是不可伸缩。
*   读取副本将始终位于写入节点之后。数据写入主节点，然后异步复制到读取节点**。读数可能不一致(陈旧数据)。**

# **缩放—使用分片**

**其中一个解决方案是使用[分片](https://en.wikipedia.org/wiki/Shard_(database_architecture))来拥有多个节点，这些节点可以服务于读写操作。整个数据将跨碎片分布，每个节点包含一个数据子集，不会与其他服务节点发生任何冲突。**

**每个节点都是一个功能齐全的读写数据存储，但只存储一部分数据。**

**这将在多个节点上分配负载。它将很好地解决伸缩问题，因为客户端请求由多个节点来处理。**

**然而，它还需要一种机制，通过这种机制，对特定数据(键)的请求总是到达同一个服务器。**

**这是使用[散列](https://en.wikipedia.org/wiki/Hash_function)和 mod 技术实现的。数据的唯一键(在这种情况下是键/值对中的键)被散列，并对可用服务器的数量取一个模。该算法将总是给出结果，根据输入的键数据引用相同的服务器(索引),因此将确保请求总是到达相同的服务器进行读/写。**

**![](img/3431c59d80ee765360b747e0956fda70.png)**

**每个节点还可以有自己的备份备用节点，或者在另一个 shard 节点上有自己的备份。这将确保有一个数据副本，在主节点停机时可以使用。**

**这看起来棒极了，解决了大部分伸缩性、冗余和可用性问题。**

## **再散列问题**

**只要我们有固定数量的服务器，这种方法就可以很好地工作。这在分布式系统中是不实际的。**

**真正的扩展意味着能够根据需要动态扩展和缩小。这意味着服务器的数量可以随时改变。**

**只要我们添加或删除一个服务器，它就需要在服务器之间重新散列和重新分布数据，以保持数据分布平衡。**

**这也意味着每次添加或删除节点时，都必须在服务器节点之间移动大量数据。这将需要大量的处理时间和带宽。在此期间，系统可能不可用，这很糟糕。**

**例如，我们有从散列 1 到 1000 分布的键。我们有 5 台服务器，因此有了碎片。每个碎片映射到 200 个键。**

**![](img/908b018b9d163107c666f589c65d1765.png)**

**现在，如果我们再添加 5 个节点，就需要重新调整分片和键映射。现在每个碎片将有 100 条记录。因此，许多数据需要基于重散列的键跨碎片移动。**

**![](img/25d2f4102e09ad326c582f4cf17dbf2d.png)**

**在动态可伸缩系统中，这可能是一个巨大的挑战，因为动态可伸缩系统可能需要频繁地伸缩。**

> **在现实生活中，这可能会发生在万亿级的数据上。因此，这将产生巨大的重新散列成本和数据移动成本。**

# **扩展—环上的服务器(一致哈希)**

**那么解决办法是什么呢？**

**我们需要一种解决方案，其中请求分发方案(基于密钥和 mod 的散列)不依赖于物理服务器的数量。因此，如果我们删除或添加更多的服务器，数据移动和影响可以最小化。**

**这意味着一种分布式散列技术，它不依赖于服务器的数量，而是给出一个位置，这个位置可以在以后用某种逻辑映射到服务器。**

**这就是一致性哈希方案的用处。1997 年，卡尔格等人在麻省理工学院首次描述了这种现象。这是一个惊人的简单，但非常有效的技术。**

**到目前为止，我们正在使用一组服务器，这些服务器被映射到 hashing + mod 的密钥，以便找到正确的服务器。如果我们假设一个服务器环，而不是一个线性阵列，会怎么样？**

**这个环可以有多个位置。其中一些位置会有对应的服务器。密钥上的散列算法将给出环上的位置，而不是服务器的直接索引。**

**当请求到来时，它是如何工作的:**

*   **散列+ Mod 将给出环上的位置(而不是服务器索引)**
*   **系统检查环上是否有任何服务器映射到给定的位置。**
*   **如果映射了服务器，请求将被路由到该服务器。**
*   **如果没有服务器映射到该位置，系统将在环上该位置的右侧进行检查(顺时针)。在环上找到的第一个服务器将用于路由请求。**
*   **如果系统到达环的末端，则从环的起点开始搜索下一个服务器，即从零索引开始。该请求将被路由到该搜索中的第一个服务器。**

**![](img/0772aa2b8623c28755aa13335873d1be.png)**

**在上图中，我们在环上绘制了 600 个位置。6 台服务器映射到 6 个不同的位置。散列到这些给定范围之一的任何请求键都将被路由到相应的服务器，如上表所示。**

**例如，值为 1 时，系统将找不到任何直接映射到环上该位置的服务器。它将开始顺时针移动，寻找环上的下一个服务器。下一个服务器是 S2。系统会将请求发送到 S2。**

## **有什么好处**

**密钥被动态映射到环上的一个位置，而不是任何特定的服务器。**

**服务器也被动态地映射到环上的一个位置(可能通过使用另一种散列技术)。**

**因此，系统只需要使用一种机制来映射这两个动态计算的位置，这是由服务器环解决的。**

**假设，为了管理一段时间后增加的负载，我们想在位置 525 再添加一个服务器 S7。到目前为止，451-525 的范围被映射到 S1。现在，这将被映射到 S7。这需要将该键范围内的数据从 S1 移动到 S7。**

**仅此而已。不需要其他数据移动。其他节点都不会受到影响。其余的整个系统将保持原样运行。**

**对 451–525 范围内的任何键的下一个请求将自动路由到服务器 7，因为它是环上的下一个可用服务器。**

**![](img/8e40c7d29d1e6aeb06015e3cfdf806b2.png)**

**这节省了大量开销。**

**数据移动仅影响 2 个服务器节点(上例中的 S7 和 S1)，而不会影响所有服务器和所有数据。**

**这意味着需要移动的数据量要少得多。因此，它将在相对少得多的时间内完成，并将大大减少对客户的影响。**

**它解决了以前由于动态扩展需求而导致的批量数据移动问题。**

*****不过，还有一个问题。*****

**在上面的例子中，假设服务器 S6 和 S7 由于某种原因而停机。**

**376 到 600 范围内的所有数据都将被移动到服务器 S1。服务器 S1 现在包含大量数据，它必须满足更多的请求(几乎是以前负载的 3 倍)。**

**尽管环上还有 5 台服务器，但是 S6 和 S7 的故障将由环上的下一台服务器(S1)来管理。**

**这是一个简单的例子。在现实生活中，这一数据可能高达数百万甚至更多。这意味着，我们仍然面临一个挑战，一个节点可能会因大量请求和数据而过载。在这种情况下，这是服务器 S1。**

# **扩展—环上的虚拟服务器**

**给出的解决方案是创建许多现有服务器的虚拟副本，并将它们映射到环上的随机位置(或基于权重)。**

**我们仍然有相同数量的物理服务器(在上面的第一个例子中是 6 台)。但是，我们正在为这 6 台服务器创建多个虚拟副本。让我们假设，我们为这些服务器中的每一个再创建 5 个虚拟副本，因此现在总共有 30 个服务器(虚拟)要映射到环上。**

**一个服务器将被映射以满足环上 20 个键的范围。**

**仍然可以使用某种散列技术(或任何其他最佳算法)来驱动映射，以保持其随机性，但却是高效分布的。**

**![](img/d25817247ec2b0783614a2b421c3d526.png)**

**为了简单起见，上面的例子假设环上所有 30 个服务器的范围分布相等，并且到每个物理服务器的映射也是相等加权的，但是随机的。在现实生活中，任何定制策略都可以用于这两种映射，具体取决于服务器容量等各种因素。**

**这提供了另一层灵活性，可以根据容量和流量预测来调整给定服务器上的可能负载。**

**在上面的策略中，如果一个物理服务器宕机，环上的 6 个虚拟节点也会宕机。然而，这 6 个节点随机分散在环上的不同位置。我们可能需要移动 120 个键的范围(在本例中，每个虚拟服务器 6*20 个键)。但是，它们将位于不同的位置，不同的物理服务器将位于这些虚拟服务器的旁边。**

**从上面的例子看，如果 S3 倒下了。负载将在 S6、S5 等之间分配。参考上图中的表格。**

**因此，负载将被分配到随机的物理服务器上。没有一台物理服务器会承担所有的负载。**

**这意味着，即使一个或两个节点出现故障，受影响的密钥也会分布在许多服务器上，同时仍能控制数据移动量。**

**这解决了两个问题，即控制批量数据移动和保持每个服务器节点上的负载公平分布。**

**这不是很棒吗？**

# **总结**

**将服务器和密钥映射分布在一个环上而不相互依赖的散列概念称为一致散列。**

**总结一下一致性哈希的好处:**

*   **它非常有效地在节点上分配负载**
*   **一个节点的故障不会影响所有其他节点的数据。重新散列的影响仅限于少数几个节点，数据移动量极小。**
*   **如果任何一个节点出现故障，它的负载将被分配给许多其他节点。这有助于避免一个节点负担过重。**
*   **它使得向上和向下扩展非常有效，这是分布式系统的核心要求。**

**这使得一致性哈希成为分布式系统、缓存和数据存储的首选之一。**

**但是，在任何节点发生故障的情况下，节点如何确保耐用性呢？系统将来自每个节点的数据的多个副本保存在环中的其他节点上。它基于大多数系统中可配置的复制因子。这确保了在任何严重故障的情况下数据不会丢失。**

**数据复制有其自身的复杂性、更深入的用例以及有趣的设计方面。我们将在以后的文章中讨论它。**

**在那之前，敬请期待，快乐学习…**

# **参考**

*   **第[篇关于一致性哈希的论文](https://dl.acm.org/doi/10.1145/258533.258660)**
*   **[关于 DynamoDB](http://www.cs.cornell.edu/courses/cs5414/2017fa/papers/dynamo.pdf) 的论文，描述了一致性哈希的使用**
*   **[哈希函数](https://en.wikipedia.org/wiki/Hash_function)**
*   **对更多系统设计文章感兴趣，[参考这里](https://blog.devgenius.io/evolution-of-system-design-from-micro-functions-to-micro-services-53cdf8e276ac)**

**如果你喜欢看这篇文章，请分享、鼓掌，并关注类似的故事！**

***如有任何建议，请随时通过****Linkedin****:*[*Mohit Gupta*](https://www.linkedin.com/in/mohitkgupta/)联系我**