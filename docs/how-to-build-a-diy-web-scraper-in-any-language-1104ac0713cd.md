# 如何用任何语言建立一个自己动手的网页抓取器

> 原文：<https://levelup.gitconnected.com/how-to-build-a-diy-web-scraper-in-any-language-1104ac0713cd>

![](img/5727068ea1ee00b34630f460caee2980.png)

图片来源:[Franz 12](https://www.shutterstock.com/g/franz12)/[shutterstock.com](http://shutterstock.com)

制作自己的网页抓取器(或爬虫)出奇的简单。它也可能非常有用。让我告诉你为什么:

我写的第一个 scraper 是大量下载我在播客时代之前曾经听过的一个老广播节目的存档片段。是的，我就是那么老。然后我就可以装上我的第一个 mp3 播放器(如下图),坐公交车去学校。

![](img/e5bec3a5d60c9f8164aad7cfeae5695d.png)

图片来源: [Brendinooo](https://en.wikipedia.org/wiki/User:Brendinooo) 在[英文维基百科](https://en.wikipedia.org/wiki/)

我说过我老了吗？iPods 当时还不存在。

我写的第二个 scraper 是一个线程 Python 交易。给它一个网站列表和一个文件或目录列表，它会告诉你哪一个是后者的主机。这可以用于——咳咳——各种目的。

最近，我的任务是开发一些基本的东西来抓取一个领域，并提供搜索引擎优化分析。我没有让我的工具硬适应这个问题，而是首先创建了一个易于扩展的通用基础。这个过程激发了这篇文章。

现在我知道有很多现有的工具可以帮助你抓取和抓取网页。然而，自己编写一个允许你创建一些轻量级的东西，或者容易适应或者为你的特定用例定制。

自己动手构建有助于提高您的开发技能和对技术工作原理的理解。另一种方法是必须通读文档，找出如何实现一个现有的解决方案，这个解决方案通常被嵌套菜单或配置标志弄得臃肿不堪。

所以我选择了 DIY 方法。鉴于我对 PHP + cURL 技术的熟悉，我在最近的尝试中使用了 PHP+cURL，但是我在这里分享的原理和(伪)代码是通用的，可以应用于任何语言。

# 考虑以下情况

想想 HTML 中使用`<a>`标签的不同方式。你很快就会列出你需要考虑的事情。以下是一些显而易见的例子:

1.  标签通常包含链接，但是它们也可以包含其他东西:锚、内联 JavaScript、`mailto:`地址、替代协议，或者它们可以是空白的。你的 scraper 需要排除这些情况，只关注超链接。
2.  超链接可以采用不同的格式。它们可以是绝对的，并以`[http://](/,)` [、](/,) `https://`或`//`开始(保持当前协议)。它们也可以是相对的。

既然您已经考虑了从`<a>`标签中提取链接所涉及的内容，那么就花些时间考虑组成一个典型 URL 的不同部分。看一看:

`https://sub.domain.tld/folder/file.ext?param1=a&param2=b#fragment`

分解如下:

1.  **协议:** https://
2.  **主机:**子域. tld
3.  **路径:**/文件夹/文件. ext
4.  **查询参数:**？param1=a & param2=b
5.  **片段:**#片段

当你抓取相对链接时，你会想把它们转换成绝对链接，这意味着添加协议、主机和文件夹——它们被获取的路径的一部分。同时掉落片段:`page.ext#heading1`和`page.ext#heading2`是等价的，应该只访问一次。

其他一些值得思考的警告:

1.  `/`和`/index.ext`以及`/path`和`/path/`的区别
2.  您是否希望将`/page.ext`和`/page.ext?param=1`视为相同或不同的页面
3.  如果你想让你的刮刀停留在子域内或穿越子域
4.  应该避免无限循环。不要重新访问同一个页面，因为它在多个地方被引用
5.  构建一个参数来指定要抓取的最大页数——这可以防止事情失去控制，尤其是在开发和测试期间
6.  注意超时、HTTP 状态码、重定向、你的 scraper 的用户代理和“referer”参数、cookies、认证信息等。

考虑到所有这些，是时候开始开发了。

# 测试驱动开发

我喜欢 TDD，你也应该喜欢。它非常适合这种类型的项目。不要重复访问 web 上的某个网站并引起怀疑(尤其是如果你陷入了一个无限循环)，而是在开发期间制作一组本地托管的测试页面来指导你的工作。

从广义上来说，您的测试页面应该涵盖我们在上一节中讨论的所有内容。您所需要的只是两个 HTML 页面，它们具有不同格式的指向内部和外部位置的链接。这让您可以确保成功地提取和抓取它们。还应该有一些包含锚之类的东西的标签，以确保它们不会绊倒你的刮刀。还包括你正在构建的任何特定内容的例子，比如媒体文件或从中提取数据的特定 HTML 标签。

当你开发的时候，用你的刮刀刮这些页面。首先，让它从索引页面聚合和输出链接。然后让它爬行到第二页并停止，收集更多的信息。然后在执行增量测试的同时，继续将其构建到最终形式。标准开发流程，真的。

如果你不想自己建立这组页面，我已经为你做了[这里](https://github.com/kld87/php-curl-scraper/tree/master/test)。快速浏览每一页，了解其目的，并根据需要进行调整。

# 准备 3 个文件

因为这是一个与语言无关的教程，我将分享 3 个核心类的伪代码，你可以用它们作为基本结构和行为分段。简而言之，它们是:

1.  一个“**页面**”对象，解析并存储一个 web 请求的结果，该请求具有帮助提取数据的功能。
2.  一个" **Scraper** "对象，它处理抓取操作的逻辑和从检索到的数据创建页面。
3.  一个"**控制器**"作为程序的入口点，该程序接收参数，实例化并调用一个 scraper，并以您想要的格式输出它提取的数据。

显然有很多方法可以解决这个问题，这将作为一个引导你思考的向导。适应您认为合适的用例、编码风格和语言选择。我还将进一步包含我实际实现这种方法的链接。

## 页面对象

编写一个简单的 Page 对象，程序的其余部分将使用它来存储和提取抓取的信息。它的构造函数应该接受对 web 请求的响应，包括 HTML 内容和相关的元数据。这个类应该有一些方便的函数，可以返回相关的信息，比如页面上的链接、元数据，比如状态代码、url 和加载时间，以及您想要提取的任何附加信息——下面我将使用页面的`<title>`标签的内容作为例子。

**注意:** *大多数语言都会有一些内置的调用来解析和处理 XML/HTML 之类的标记(我在这里称之为* `MLPARSER` *)。那些没有的将会有图书馆为你做它。找到适合你的给定语言的方法，而不是重新发明轮子，记住:* [*你不能用正则表达式来解析 HTML*](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags) *(看看这个问题和它的第一个回答就知道了)。*

## 刮刀物体

您的 scraper 对象应该使用一个参数来构造，以便从目标 URL 开始抓取。它应该有一个函数来控制给定数量的页面的抓取，存储抓取的页面，然后返回它们。它将需要记住哪些网页已经访问过，并有一个功能，以确定哪个网页应该访问下一步。它还需要一些辅助函数来排序和跟踪内部和外部 URL，并将相对链接变成绝对链接。

这样写允许重复调用`scrape()`函数而不会引起任何问题。您的控制器逻辑可以根据目前返回的内容选择是否继续抓取目标，而无需重新初始化或重新实例化任何内容。

**注 1:** *我在这个文件中把 URL 分解成内部/外部类别，并使它们成为绝对的。这也可以在 Page 类中完成，这是一个偏好问题。因为在调试之后，我基本上放弃了聚集的 URL，只使用它们来控制，所以我在这里做了。*

**注 2:** *与标记解析器注一样，我们调用一个泛型* `WEBLIBRARY` *来代替您的语言或所选库发起并返回 HTTP 请求的结果，并将相对链接转换为绝对链接的能力。*

## 控制器

控制器文件应该相当简单明了，主要取决于语言和情况。首先解析用户输入的目标 URL、抓取限制和您的用例的任何其他选项。然后实例化一个 Scraper 对象，调用它的`scrape()`函数，然后根据需要格式化输出你的结果。

**注意:** *回复:上面的 TDD 部分，我已经在* `scrapeTarget()` *里面的最明智(对我来说)的地方输出了一些开发和调试期间的数据。*

## 其他文件

根据你使用的语言和你想要达到的目标，你可能会得到一些 core 3 的补充文件。一个通用的工具文件可以帮助清理和组织一些代码和函数，这些代码和函数可以在你的 scraper 中共享。您可能还拥有一些外部库，用于处理 HTML 解析和执行 web 请求，以及您选择的语言进行引导所需的任何其他东西。

对于错误处理，我做得非常一般，基本上是让我的程序死亡，如果它的一个 web 请求失败，就抛出一个错误，因为我正在监督它的操作。您可以根据需要选择添加更健壮的错误处理。

## 我的实现

如前所述，我是用 PHP 和 cURL 做的。你可以看看我的 3 个文件，分别是[页面](https://github.com/kld87/php-curl-scraper/blob/master/Util/Page.php)、[刮刀](https://github.com/kld87/php-curl-scraper/blob/master/Util/Scraper.php)和[控制器](https://github.com/kld87/php-curl-scraper/blob/master/Controllers/ScrapeController.php)——外加一个上面讨论过的[实用程序文件](https://github.com/kld87/php-curl-scraper/blob/master/Util/CurlUtil.php)。或者直接挖通[完成回购](https://github.com/kld87/php-curl-scraper)。

# 前进并编码

这应该给你一个在编写自己的 scraper 时需要考虑的事情的想法，以及如何完成这个壮举的大致想法。有许多不同的方法可以解决这个问题，但是我建议创建一个通用库，让您可以管理页面、处理抓取，然后控制程序的整体流程——可以在此基础上进行调整。

祝好运，请负责任地刮！