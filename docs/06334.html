<html>
<head>
<title>How To Crawl A Web Page with Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Scrapy抓取网页</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/how-to-crawl-a-web-page-with-scrapy-3b9d12797813?source=collection_archive---------11-----------------------#2020-11-16">https://levelup.gitconnected.com/how-to-crawl-a-web-page-with-scrapy-3b9d12797813?source=collection_archive---------11-----------------------#2020-11-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/5cb5881188cd6c6eb68e27fa78854203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4wpxQZccZxufT-Up"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">由<a class="ae kf" href="https://unsplash.com/@arianismmm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Arian Darvishi </a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="5de9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Scrapy是最流行和最强大的Pythons刮库之一；它采用一种“包含电池”的方法来进行刮擦，这意味着它处理所有刮擦器都需要的许多常见功能，因此开发人员不必每次都重新发明轮子。它使刮削成为一个快速而有趣的过程。Scrapy和大多数Python包一样，位于PyPI(也称为pip)上。PyPI，Python包索引，是一个社区拥有的所有已发布Python软件的存储库。如果您安装了Python，就像本教程的先决条件中概述的那样，那么您的机器上已经安装了pip，因此您可以使用以下命令安装Scrapy:</p><p id="2d01" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe le lf lg lh b">pip install scrapy</code></p><h1 id="3031" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">编码:</h1><p id="a1c2" class="pw-post-body-paragraph kg kh it ki b kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld im bi translated">我们将创建一个蜘蛛类，并将<code class="fe le lf lg lh b">scrapy</code>传递给它，然后我们将为name和start_url提供两个属性</p><p id="febf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe le lf lg lh b">name</code> —蜘蛛的名字</p><p id="6416" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe le lf lg lh b">start_url</code> —蜘蛛将抓取的URL列表，我们将从一个URL开始</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="ba5f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们一行一行地分解代码。首先，我们已经导入了<code class="fe le lf lg lh b">scrapy</code>模块，因此我们可以使用它的类和包。接下来，我们获取Scrapy提供的Spider类，并在它的基础上创建一个子类，名为BrickSetSpider。把子类想象成它的父类的更专门化的形式。Spider子类有一些方法和行为，这些方法和行为定义了如何跟踪URL并从它找到的页面中提取数据，但是它不知道去哪里寻找或者寻找什么数据。通过对它进行子类化，我们可以给它这些信息。然后我们给这个蜘蛛起名叫<code class="fe le lf lg lh b">brickset_spider</code>。</p><p id="f374" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们提供的目标网站的网址是2016年数据<a class="ae kf" href="http://brickset.com/sets/year-2016" rel="noopener ugc nofollow" target="_blank">http://brickset.com/sets/year-2016</a>，如果你在浏览器中打开这个网址，它将重定向到向你显示许多乐高套装的网页。现在，您可以使用命令提示符来执行代码。但是，<code class="fe le lf lg lh b">scrapy</code>有自己的命令行运行界面<code class="fe le lf lg lh b">scrapy runspider scraper.py</code></p><h2 id="1e7d" class="mr lj it bd lk ms mt dn lo mu mv dp ls kr mw mx lw kv my mz ma kz na nb me nc bi translated">输出:</h2><pre class="ml mm mn mo gt nd lh ne nf aw ng bi"><span id="259d" class="mr lj it lh b gy nh ni l nj nk">2016-09-22 23:37:45 [scrapy] INFO: Scrapy 1.1.2 started (bot: scrapybot)<br/>2016-09-22 23:37:45 [scrapy] INFO: Overridden settings: {}<br/>2016-09-22 23:37:45 [scrapy] INFO: Enabled extensions:<br/>['scrapy.extensions.logstats.LogStats',<br/> 'scrapy.extensions.telnet.TelnetConsole',<br/> 'scrapy.extensions.corestats.CoreStats']<br/>2016-09-22 23:37:45 [scrapy] INFO: Enabled downloader middlewares:<br/>['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',<br/> ...<br/> 'scrapy.downloadermiddlewares.stats.DownloaderStats']<br/>2016-09-22 23:37:45 [scrapy] INFO: Enabled spider middlewares:<br/>['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',<br/> ...<br/> 'scrapy.spidermiddlewares.depth.DepthMiddleware']<br/>2016-09-22 23:37:45 [scrapy] INFO: Enabled item pipelines:<br/>[]<br/>2016-09-22 23:37:45 [scrapy] INFO: Spider opened<br/>2016-09-22 23:37:45 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)<br/>2016-09-22 23:37:45 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023<br/>2016-09-22 23:37:47 [scrapy] DEBUG: Crawled (200) &lt;GET http://brickset.com/sets/year-2016&gt; (referer: None)<br/>2016-09-22 23:37:47 [scrapy] INFO: Closing spider (finished)<br/>2016-09-22 23:37:47 [scrapy] INFO: Dumping Scrapy stats:<br/>{'downloader/request_bytes': 224,<br/> 'downloader/request_count': 1,<br/> ...<br/> 'scheduler/enqueued/memory': 1,<br/> 'start_time': datetime.datetime(2016, 9, 23, 6, 37, 45, 995167)}<br/>2016-09-22 23:37:47 [scrapy] INFO: Spider closed (finished)</span></pre><p id="a46b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出太长。让我们来分析一下这个蜘蛛的工作原理。</p><p id="54e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">scraper初始化并加载了处理从URL读取数据所需的附加组件和扩展。</p><p id="ac43" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2:是我们给了<code class="fe le lf lg lh b">start_url</code>的URL，抓取了网站的<code class="fe le lf lg lh b">HTML</code>代码。</p><p id="5092" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3:接下来，他们将HTML代码传递给蜘蛛默认解析器，它什么也不做，因为我们从未为此编写过自己的解析器。</p><h1 id="6f88" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">提取数据:</strong></h1><p id="88c1" class="pw-post-body-paragraph kg kh it ki b kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld im bi translated">好的，我们刚刚创建了一个非常基本的程序<code class="fe le lf lg lh b">Scrapy</code>，它只是提取页面的<code class="fe le lf lg lh b">HTML</code>代码，什么也不做。它不做任何爬行或抓取，所以我们将扩展代码并添加抓取功能。一个基本的网页有这样的结构。</p><p id="df77" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1:出现在每个网页上的标题。</p><p id="e717" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2:不同的HTML标签和元素以及我们需要搜索和提取的其他内容。</p><p id="8c48" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3:然后是集合本身，显示在看起来像表格或有序列表的地方。每组都有相似的格式:</p><pre class="ml mm mn mo gt nd lh ne nf aw ng bi"><span id="e208" class="mr lj it lh b gy nh ni l nj nk">&lt;body&gt;<br/>  &lt;section class="setlist"&gt;<br/>    &lt;article class='set'&gt;<br/>      &lt;a href="https://images.brickset.com/sets/large/10251-1.jpg?201510121127" <br/>      class="highslide plain mainimg" onclick="return hs.expand(this)"&gt;&lt;img <br/>      src="https://images.brickset.com/sets/small/10251-1.jpg?201510121127" title="10251-1: <br/>      Brick Bank" onError="this.src='/assets/images/spacer.png'" /&gt;&lt;/a&gt;<br/>      &lt;div class="highslide-caption"&gt;<br/>        &lt;h1&gt;Brick Bank&lt;/h1&gt;&lt;div class='tags floatleft'&gt;&lt;a href='/sets/10251-1/Brick- <br/>        Bank'&gt;10251-1&lt;/a&gt; &lt;a href='/sets/theme-Creator-Expert'&gt;Creator Expert&lt;/a&gt; &lt;a <br/>        class='subtheme' href='/sets/theme-Creator-Expert/subtheme-Modular- <br/>        Buildings'&gt;Modular Buildings&lt;/a&gt; &lt;a class='year' href='/sets/theme-Creator- <br/>        Expert/year-2016'&gt;2016&lt;/a&gt; &lt;/div&gt;&lt;div class='floatright'&gt;&amp;copy;2016 LEGO <br/>        Group&lt;/div&gt;<br/>          &lt;div class="pn"&gt;<br/>            &lt;a href="#" onclick="return hs.previous(this)" title="Previous (left arrow <br/>            key)"&gt;&amp;#171; Previous&lt;/a&gt;<br/>            &lt;a href="#" onclick="return hs.next(this)" title="Next (right arrow key)"&gt;Next <br/>            &amp;#187;&lt;/a&gt;<br/>          &lt;/div&gt;<br/>      &lt;/div&gt;<br/><br/>...<br/><br/>    &lt;/article&gt;<br/>  &lt;/section&gt;<br/>&lt;/body&gt;</span></pre><p id="0d30" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们将传递给scraper并从中提取信息的<code class="fe le lf lg lh b">HTML</code>代码的外观。<code class="fe le lf lg lh b">scrapy</code>使用选择器抓取数据，选择器是我们用来在网页上寻找一个或多个元素的模式，<code class="fe le lf lg lh b">Scrapy</code>支持<code class="fe le lf lg lh b">CSS selectors</code>或<code class="fe le lf lg lh b">XPATH selectors</code>。在这篇教程文章中，我们将使用CSS选择器，因为使用CSS选择器比<code class="fe le lf lg lh b">XPATH</code>更容易定位元素。如果你看了一下上面的HTML代码<code class="fe le lf lg lh b">&lt;article class=’set’&gt;</code>，保存每个集合数据的文章在设置了类的文章标签中，所以我们可以简单地将我们的选择器设置为. set。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="be89" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代码将获取带有集合的类，并在它们上面循环以提取数据，现在让我们编写代码来提取数据。如果你再看一遍源代码，你会注意到乐高套装的每个标题都在<code class="fe le lf lg lh b">h1 tag</code>里面。</p><pre class="ml mm mn mo gt nd lh ne nf aw ng bi"><span id="82d9" class="mr lj it lh b gy nh ni l nj nk">&lt;h1&gt;Brick Bank&lt;/h1&gt;&lt;div class='tags floatleft'&gt;&lt;a href='/sets/10251-1/Brick-Bank'&gt;10251-1&lt;/a&gt;</span></pre><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="5f8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你会注意到代码中的两件事，首先我们用<code class="fe le lf lg lh b">append ::text</code>来命名我们的选择器。这是一个CSS伪选择器，它获取标签内部的文本，而不是标签本身。然后我们在由<code class="fe le lf lg lh b">brickset.css(NAME_SELECTOR)</code>返回的对象上调用<code class="fe le lf lg lh b">extract_first()the </code>方法，因为我们只想要匹配选择器的第一个元素。这给了我们一个字符串，而不是一个元素列表，现在保存文件并再次运行scraper，这一次您将在命令提示符下看到集合名称。</p><h2 id="3acd" class="mr lj it bd lk ms mt dn lo mu mv dp ls kr mw mx lw kv my mz ma kz na nb me nc bi translated">输出:</h2><pre class="ml mm mn mo gt nd lh ne nf aw ng bi"><span id="6233" class="mr lj it lh b gy nh ni l nj nk">...<br/>[scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'name': 'Brick Bank'}<br/>[scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'name': 'Volkswagen Beetle'}<br/>[scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'name': 'Big Ben'}<br/>[scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'name': 'Winter Holiday Train'}<br/>...</span></pre><p id="394d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们扩展我们的刮刀刮图像，碎片，和微型数字，或一套来的微型数字。再次查看HTML代码，您会看到图像在<code class="fe le lf lg lh b">&lt;a&gt; tag</code>中。棋子放在<code class="fe le lf lg lh b">&lt;dd&gt; tag</code>里，微型人物放在<code class="fe le lf lg lh b">&lt;dt&gt; tag</code>里。现在，为了获取数据，我们将使用Xpath，因为用CSS选择器从标签中获取数据非常复杂。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="9432" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出:</p><pre class="ml mm mn mo gt nd lh ne nf aw ng bi"><span id="0717" class="mr lj it lh b gy nh ni l nj nk">2016-09-22 23:52:37 [scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'minifigs': '5', 'pieces': '2380', 'name': 'Brick Bank', 'image': 'http://images.brickset.com/sets/small/10251-1.jpg?201510121127'}<br/>2016-09-22 23:52:37 [scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'minifigs': None, 'pieces': '1167', 'name': 'Volkswagen Beetle', 'image': 'http://images.brickset.com/sets/small/10252-1.jpg?201606140214'}<br/>2016-09-22 23:52:37 [scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'minifigs': None, 'pieces': '4163', 'name': 'Big Ben', 'image': 'http://images.brickset.com/sets/small/10253-1.jpg?201605190256'}<br/>2016-09-22 23:52:37 [scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'minifigs': None, 'pieces': None, 'name': 'Winter Holiday Train', 'image': 'http://images.brickset.com/sets/small/10254-1.jpg?201608110306'}<br/>2016-09-22 23:52:37 [scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'minifigs': None, 'pieces': None, 'name': 'XL Creative Brick Box', 'image': '/assets/images/misc/blankbox.gif'}<br/>2016-09-22 23:52:37 [scrapy] DEBUG: Scraped from &lt;200 http://brickset.com/sets/year-2016&gt;<br/>{'minifigs': None, 'pieces': '583', 'name': 'Creative Building Set', 'image': 'http://images.brickset.com/sets/small/10702-1.jpg?201511230710'}</span></pre><p id="4702" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，我们定义了一个图像，片段，微型选择器，并把每个的XPath存储到其中，在接下来的行中，我们使用XPath函数。extract_first函数从标签中删除我们的数据。最后，我们完成了页面抓取，我们的新挑战是让我们的抓取器将页面移动到网站上的另一个页面。看看下面的源代码。</p><pre class="ml mm mn mo gt nd lh ne nf aw ng bi"><span id="4b5c" class="mr lj it lh b gy nh ni l nj nk">&lt;ul class="pagelength"&gt;<br/><br/>  ...<br/><br/>  &lt;li class="next"&gt;<br/>    &lt;a href="http://brickset.com/sets/year-2017/page-2"&gt;&amp;#8250;&lt;/a&gt;<br/>  &lt;/li&gt;<br/>  &lt;li class="last"&gt;<br/>    &lt;a href="http://brickset.com/sets/year-2016/page-32"&gt;&amp;#187;&lt;/a&gt;<br/>  &lt;/li&gt;<br/>&lt;/ul&gt;</span></pre><p id="e0fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你所看到的，李瑟娥标签有一个名为next的类，它有一个包含页面URL的<a>标签，我们只需要从那个标签中删除那些URL。</a></p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="61a4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们为“下一页”链接定义一个选择器，提取第一个匹配，并检查它是否存在。<code class="fe le lf lg lh b">scrapy.Request</code>是我们返回的一个值，表示“嘿，抓取这个页面”，而<code class="fe le lf lg lh b">callback=self.parse</code>表示“一旦你从这个页面获得了HTML，就把它传递回这个方法，这样我们就可以解析它，提取数据，并找到下一个页面。T</p><p id="e413" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他的意思是，一旦我们转到下一页，我们将在那里寻找下一页的链接，在那一页上，我们将寻找下一页的链接，以此类推，直到我们找不到下一页的链接。这是网络抓取的关键部分:找到并跟踪链接。</p><p id="459c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个例子中，它是非常线性的；一个页面有一个到下一个页面的链接，直到我们点击最后一个页面，但是你可以跟随链接到标签，或其他搜索结果，或任何其他你喜欢的URL。现在，如果你保存你的代码并再次运行蜘蛛程序，你会发现它不仅仅是在遍历第一页集合时停止。它继续在23页上检查所有779个匹配项！从大的方面来看，这并不是一大块数据，但是现在您知道了自动寻找新页面的过程。</p><h1 id="059d" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">结论:</h1><p id="3122" class="pw-post-body-paragraph kg kh it ki b kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld im bi translated">我们学习了如何用python创建一个<code class="fe le lf lg lh b">scrapy</code>铲运机机器人。我们学习了scraper如何工作的基础知识，如何使用CSS选择器和Xpath选择器，如何从标签中获取每个数据，等等。但是事情并没有就此结束。在刮痧世界里，还有很多东西有待发现。希望这篇文章对你以后有所帮助。请随意分享您对此的回应。</p></div></div>    
</body>
</html>