<html>
<head>
<title>Tabular Q-Learning Agent vs. Irrational Agent in the Game of Tic-Tac-Toe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">井字游戏中的表格Q学习代理与非理性代理</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/tabular-q-learning-agent-vs-irrational-agent-in-the-game-of-tic-tac-toe-6de6c85f0c42?source=collection_archive---------6-----------------------#2022-06-02">https://levelup.gitconnected.com/tabular-q-learning-agent-vs-irrational-agent-in-the-game-of-tic-tac-toe-6de6c85f0c42?source=collection_archive---------6-----------------------#2022-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1e19844fdad3a02a49dbc40e93f58f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T6_exi8C7LUuj6V6a178bQ.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Pixabay在Pexels上拍摄的照片</figcaption></figure><p id="7b3a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在之前的教程中，我描述了如何将井字游戏建模为强化学习(RL)问题，以及如何用Python实现该游戏。有关这些主题的更多信息，请参见下文:</p><div class="ld le gp gr lf lg"><a rel="noopener  ugc nofollow" target="_blank" href="/framing-tic-tac-toe-as-a-reinforcement-learning-problem-eb76b6ece4de"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd iu gy z fp ll fr fs lm fu fw is bi translated">将井字游戏框架化为强化学习问题</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">介绍</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu jz lg"/></div></div></a></div><div class="ld le gp gr lf lg"><a rel="noopener  ugc nofollow" target="_blank" href="/setting-up-tic-tac-toe-for-reinforcement-learning-in-python-43e2f42cfce8"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd iu gy z fp ll fr fs lm fu fw is bi translated">在Python中为强化学习设置井字游戏</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">在我之前的媒体文章中…</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="lp l"><div class="lv l lr ls lt lp lu jz lg"/></div></div></a></div><p id="63ea" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，我解释我如何应用表格Q-learning来帮助参与人1(代理1)使用称为表格Q-learning的RL技术学习一个智能的接近最优的策略。这种Q学习技术对于少于10，000到100，000个状态的小问题是有用的；这真的取决于你拥有的计算资源，因为你系统中的状态越多，你就需要训练更长的时间来获得更好的性能。就个人而言，我认为对于超过50，000个状态，我永远不会使用表格方法，而是会选择一些函数近似方法，例如深度Q网络(DQN)。</p><p id="abe9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">与上一篇关于实现井字游戏的文章一样，设置是相同的，代码也没有变化:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lw"><img src="../Images/db70c6e8b74fe9e8554959ddd3759f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3cfTdXUP1UilKmAzZST7A.png"/></div></div></figure><p id="2348" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们可以开始设置模拟参数。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mb"><img src="../Images/1aa9bba1dad812a0b72dedc2066e9666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MLCZqwuI7W1Eff7VqgspZA.png"/></div></div></figure><p id="f34d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们可以在上面看到，如果我们愿意，我们有一个随时可视化棋盘状态的字典，但我不会在本教程中使用太多。下面我们可以看到我们的Q表的初始化，我们将在本教程中学习。这是一个10维的Numpy数组，因为我们为状态保留了9维，还有一个控制维。</p><p id="a662" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们为表格Q学习算法设置了一些学习常数。α是学习率，γ是折扣因子，ε是确定代理1以何种概率进行随机控制或利用其Q表获得最佳可能控制的参数。简而言之，我解释了ε是如何在离散的时间间隔中衰减的。</p><p id="1f33" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此外，我在超过100，000集的训练中训练了代理1的Q表策略。我使用数组来记录玩家1、玩家2的获胜记录，以及结果是平局时的记录。稍后我将使用这些数组来可视化智能代理1、非理性代理2的学习性能，以及抽取率如何随时间变化。</p><p id="16bf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">关于所使用的ε贪婪随机训练策略的更多信息:我使用20个离散的衰减间隔。STAGE_LENGTH是我用来确定在哪一集衰减epsilon的常数，衰减因子的量。对于这个模拟，epsilon将每5000集衰减一次。然后，在所有用于模拟的常量和变量设置下方的单元格中，我打印出我们的Q表的形状。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mc"><img src="../Images/96cd3679c467d87dc60bf392d90d57e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aD-lHNioiEiCXOp3W7An2A.png"/></div></div></figure><p id="63da" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我展示了一些方便的函数，我用它们来使模拟的主体更加整洁，并使用更少的代码。我可能会将更多的模拟代码打包到一些额外的函数中，但是为了训练我们的智能代理1，当前的代码结构工作得很好，并且不太臃肿。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/012708275182caf1a64aceebce8269a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWYs5qkFjf70Cfc0hQQYiA.png"/></div></div></figure><p id="bab6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的功能是代理1的培训策略。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/226f460bd5cd66e5517a437008a93453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BO2W0_hsMFfXqQgMDR3-HA.png"/></div></div></figure><p id="ccc8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的函数是代理1的测试策略，使用经过训练的Q表。注意这里没有使用greedy。在测试过程中，代理只会利用它学到的知识。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mf"><img src="../Images/91da5a405f563c17f3c968fbf6dfdab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzoXEUXxqaI-GUk_V-IfGA.png"/></div></div></figure><p id="3bc9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的函数是如上所述在离散周期衰减ε的辅助函数，在条件中使用STAGE_LENGTH来测试是时候进一步衰减ε了。然后，我只是做一个健全性检查，确保epsilon有规律地衰减，正如您将在培训模拟的底部看到的那样。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/ee8cde079dd2f886b584c1381eccec2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*AA0jwUeDNvveXXxZLzapbg.png"/></div></figure><p id="3083" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的函数是另一个帮助器函数，它在每一集开始时重置控制空间。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/d7a0e8510e26668a7fc5fcdbc630beee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yvbt86BUZIxBZECAc5mmRg.png"/></div></div></figure><p id="5b21" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的方法是在导致代理1获胜的一集期间，对所收集的经验执行代理1的Q表更新规则。我注意到，在最后使用终端奖励只更新状态控制对的Q表导致学习非常慢，因此我为Q表提出了这种批量学习规则。缓慢和糟糕的学习是因为当代理人只在最后更新时，它没有办法将早期的游戏状态和控制关联为有效的移动，因此学习的进展被延缓。简而言之，我沿着导致代理1赢得这一集的完整状态控制轨迹训练Q表。以这种方式执行Q表更新会在短时间内带来出色的培训，您很快就会看到这一点。</p><p id="f30a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">while循环之前模拟的开始如下所示。为了训练，我们循环播放剧集:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mi"><img src="../Images/396904958fcb221c008f8fc6ef06bbea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6-KhjtN9NwWeCxR4jkSt_g.png"/></div></div></figure><p id="82b7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，在模拟的内部，代理人轮流玩游戏，直到游戏结束:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/2cefdc552b1b833fd6463faeee562630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oYw5mwtvHuJO8QjJlmPtAA.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mk"><img src="../Images/563dd21493596c6f0ea3372b0ec0aa6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fwVMosW_NRIa-qDfYuC4JA.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ml"><img src="../Images/09d71d92c582ab4c67d654c01d3895c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZfB6y3rWwCrD1pu00-iGQ.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/d31e0c97cba244a814e0f20a93cff673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OYx7h32-k0TQ7LwZfKfng.png"/></div></div></figure><p id="7657" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面你可以看到衰减ε从1.0开始，然后是0.7，…20个衰减阶段的打印结果。</p><p id="9a4d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，让我们想象一下代理1的学习表现，代理2的表现，以及游戏以平局结束的频率。我把这些数据称为玩家1的胜率，玩家2的胜率，和局率。从频率主义概率的角度来看，我们可以将这些解释为每个结果的概率:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mn"><img src="../Images/456e03104c4c3e20b9dc24fbf5576a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kj9kz8ugbgtYEncy-QAaWQ.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/1b6aa3af8e78af8146d07e1cd56a2cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RybF1ou3Hx0h_1lH80hgFg.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/b9ae68ead1f4f7c27a5edecf10ade3c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xq_26sBAg0fYngIzSGRJg.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/f12733b1c521735f3be14600256672aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9HD2NHUK3AxkJinOrdT5g.png"/></div></div></figure><p id="d9e0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从训练中，我们可以看到一号玩家赢的概率大约是86.4%。2号玩家赢的概率大约是9.13%，而剧集结束的概率大约是4.52%。从Matplotlib图中我们可以看到，随着时间的推移，代理1越来越频繁地获胜，它确实在学习一个有效的策略。此外，代理2赢得越来越少。此外，比赛以平局收场的情况也越来越少。</p><p id="4f7f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在让我们看看代理1在测试中表现如何:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/316d5cfb836dc8c4ba52ffd83933709f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZEbXS3_H5fzEWqP-u-noQ.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ms"><img src="../Images/37c4bfedbe1c456e949b96da3c067030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DyuFxyB_aoCY-_kgQsXpww.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mt"><img src="../Images/906b36526ec10cdc769bffb609379f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dyXzZGcZS3GV6tKiXOADw.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mu"><img src="../Images/94b9ac9cd69f8374a768e2e2de59569b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgXoAIljD8iM9dN4WHFwYQ.png"/></div></div></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/db6f04f21ca9caa1790f1f918dd19588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ot_TpCjDbvHc8s91E3qtg.png"/></div></div></figure><p id="c3fd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在测试中，我们可以清楚地看到，代理1已经学会了一个有效的策略，赢得了近91.1%的剧集。我敢打赌，如果多加训练，我们会做得更好。如果训练一百万集，然后考一万集会怎么样？你能改进这里显示的结果吗？下面评论和讨论！</p></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><p id="e540" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">直到下一次，</p><p id="02ef" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">凯勒。</p><p id="0b78" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">考虑成为一个媒体成员，永远不会错过我的故事。无限制地访问我的作品和其他作者的作品:</p><div class="ld le gp gr lf lg"><a href="https://medium.com/@CalebMBowyer/membership" rel="noopener follow" target="_blank"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd iu gy z fp ll fr fs lm fu fw is bi translated">加入我的介绍链接媒体-迦勒鲍耶</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">medium.com</p></div></div><div class="lp l"><div class="nd l lr ls lt lp lu jz lg"/></div></div></a></div></div></div>    
</body>
</html>