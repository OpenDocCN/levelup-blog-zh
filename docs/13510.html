<html>
<head>
<title>GloVe and fastText Clearly Explained: Extracting Features from Text Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GloVe和fastText解释清楚:从文本数据中提取特征</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/glove-and-fasttext-clearly-explained-extracting-features-from-text-data-1d227ab017b2?source=collection_archive---------1-----------------------#2022-09-12">https://levelup.gitconnected.com/glove-and-fasttext-clearly-explained-extracting-features-from-text-data-1d227ab017b2?source=collection_archive---------1-----------------------#2022-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="dace" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学</h2><div class=""/><div class=""><h2 id="d292" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">他们怎么比Word2Vec好？</h2></div><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="a5d2" class="la lb it kw b gy lc ld l le lf"><strong class="kw jd">Table of Contents</strong></span><span id="9667" class="la lb it kw b gy lg ld l le lf"><a class="ae lh" href="#1229" rel="noopener ugc nofollow"><strong class="kw jd">🧤 GloVe</strong></a><br/>  <a class="ae lh" href="#429c" rel="noopener ugc nofollow">⚙️ The Basics</a><br/>  <a class="ae lh" href="#9f25" rel="noopener ugc nofollow">🧮 Cost Function Derivation</a><br/>  <a class="ae lh" href="#6f74" rel="noopener ugc nofollow">🔮 Final Prediction</a><br/>  <a class="ae lh" href="#e987" rel="noopener ugc nofollow">🪙 Advantages &amp; Limitations</a></span><span id="0697" class="la lb it kw b gy lg ld l le lf"><a class="ae lh" href="#3a13" rel="noopener ugc nofollow"><strong class="kw jd">⏩ fastText</strong></a><br/>  <a class="ae lh" href="#60c8" rel="noopener ugc nofollow">📚 Skip-gram reviewed</a><br/>  <a class="ae lh" href="#3b6e" rel="noopener ugc nofollow">📈 Improving Skip-gram</a><br/>  <a class="ae lh" href="#8378" rel="noopener ugc nofollow">🆚 fastText vs Word2Vec</a></span><span id="0f1c" class="la lb it kw b gy lg ld l le lf"><a class="ae lh" href="#18fc" rel="noopener ugc nofollow"><strong class="kw jd">🚀 Summary</strong></a></span></pre><p id="4928" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">在</span>大多数情况下，<a class="ae lh" href="https://towardsdatascience.com/all-you-need-to-know-about-bag-of-words-and-word2vec-text-feature-extraction-e386d9ed84aa" rel="noopener" target="_blank"> <strong class="lk jd"> Word2Vec </strong> </a>嵌入比文本的单词袋表示更好，它允许您自定义特征向量的长度，并使用<strong class="lk jd">(中心，上下文)</strong>单词对来捕捉记号之间的关系。</p><p id="895c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，Word2Vec仍然有一些限制，其中四个是:</p><ol class=""><li id="0f53" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated">Word2Vec依赖于关于单词的本地信息，即单词的上下文仅依赖于其邻居。</li><li id="d0eb" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated">获得的单词嵌入是训练神经网络的副产品，因此特征向量之间的线性关系是一个黑盒(某种)。</li><li id="6fde" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated">Word2Vec无法理解词汇外(OOV)单词，即训练数据中不存在的单词。您可以分配一个用于所有OOV单词的UNK令牌，或者您可以使用对OOV单词稳定的其他模型。</li><li id="5c15" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated">通过给每个单词分配不同的向量，Word2Vec忽略单词的<a class="ae lh" href="https://en.wikipedia.org/wiki/Morphology_(linguistics)" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">形态</strong> </a>。例如，<em class="nb">吃</em>、<em class="nb">吃</em>、吃<em class="nb">吃</em>被Word2Vec认为是独立的不同单词，但它们来自同一个词根:<em class="nb">吃</em>，其中可能包含有用的信息。</li></ol><p id="d453" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个故事中，我们将介绍理论上可以解决这些限制的嵌入模型:<strong class="lk jd"> GloVe </strong>和<strong class="lk jd"> fastText </strong>。从现在开始，我们将通过<strong class="lk jd">文档</strong>对文本进行单个观察，通过<strong class="lk jd">语料库</strong>对文档进行集合。</p><h1 id="1229" class="nc lb it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">🧤手套</h1><h2 id="429c" class="la lb it bd nd nt nu dn nh nv nw dp nl lr nx ny nn lv nz oa np lz ob oc nr iz bi translated">⚙️基础</h2><p id="28c8" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">与Word2Vec不同，GloVe (Global Vectors)的创建是为了直接找到特征向量的线性结构<em class="nb"/>。在这样做的时候，它直接捕获全局语料库统计数据，这利用了数据中的大量重复。GloVe解决了Word2Vec的限制1和2。</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/47d0d3fc854890f8cf73cce0fd10fbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*t018O_LOSIRG9U7dqeztbA.gif"/></div><figcaption class="om on gj gh gi oo op bd b be z dk translated">GIF由<a class="ae lh" href="https://giphy.com/spongebob/" rel="noopener ugc nofollow" target="_blank">海绵宝宝</a>在<a class="ae lh" href="http://giphy.com" rel="noopener ugc nofollow" target="_blank"> giphy </a>上</figcaption></figure><p id="79a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，一些符号！</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi oq"><img src="../Images/4fd49aa80ec33c3a6236b52650423b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gra74Kt3qppvuLnoiwn_DA.png"/></div></div></figure><p id="6ae5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">举个例子，假设你有一个文档集，上面写着“敏捷的棕色狐狸跳过了懒惰的狗”。那么在同现窗口大小为1的情况下，<em class="nb"> X </em>将如下。很容易看出<em class="nb"> X </em>是对称的。</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi ov"><img src="../Images/99ed503bd186a98a4b20da85530b38e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vc4xE_FLFjBAUSQLEXaZiA.png"/></div></div><figcaption class="om on gj gh gi oo op bd b be z dk translated">窗口大小为1的单词-单词共现计数矩阵示例|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></figcaption></figure><p id="e598" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GloVe的思路是将<em class="nb"> X </em>转换成特征矩阵<em class="nb"> W </em>(其行由<em class="nb"> wᵢ </em>或<em class="nb"> wⱼ </em>填充)和<em class="nb"> V </em>(其行由<em class="nb"> vₖ </em>填充)。最普通的手套模型采用以下形式</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/98e1a35a61a9c646ee40a18cb94a8c31.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*dCFy4_ziDWOXfEV-4hpABw.png"/></div></figure><p id="b784" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="nb"> F </em>关联<em class="nb"> W </em>和<em class="nb"> V </em>到<em class="nb"> X </em>。我们来看看<em class="nb"> F </em>如何区分相关词和无关词:</p><ol class=""><li id="88f1" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated">如果字<em class="nb"> k </em>与字<em class="nb"> i </em>和<em class="nb"> j </em>都相关(<em class="nb"> Pᵢₖ </em>和<em class="nb"> Pⱼₖ </em>大)，或者与字<em class="nb"> i </em>和<em class="nb"> j </em>都不相关(<em class="nb"> Pᵢₖ </em>和<em class="nb"> Pⱼₖ </em>小)，那么<em class="nb"> F </em>的值将接近1。</li><li id="37ed" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated">如果单词<em class="nb"> k </em>与<em class="nb">恰好是单词<em class="nb"> i </em>和<em class="nb"> j </em>中的一个</em>(pᵢₖ和<em class="nb"> Pⱼₖ </em>中的一个小，另一个大)，那么<em class="nb"> F </em>的值将远离1。</li></ol><h2 id="9f25" class="la lb it bd nd nt nu dn nh nv nw dp nl lr nx ny nn lv nz oa np lz ob oc nr iz bi translated">🧮成本函数推导</h2><p id="f04e" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">满足<em class="nb"> F </em>的特征向量有很多代数运算。为了找到一个合适的成本函数，让我们缩小<em class="nb"> F </em>的可能性数量，以便它是可计算的，但仍然有意义。这是一个8步流程:</p><p id="f3ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">①。</strong>由于矢量空间本质上是线性结构，对<em class="nb"> Pᵢₖ </em> / <em class="nb"> Pⱼₖ </em>中出现的信息进行编码的最自然的方式是矢量差分。因此，你可以限制<em class="nb"> F </em>为</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi ox"><img src="../Images/17ab74252a30893e96771c81fb187ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*xRqJ7rvLaqEhPCRDgM5ERw.png"/></div></div></figure><p id="2c9b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">②。</strong>注意<em class="nb"> F </em>将矢量转换成标量。为了保持向量的线性结构，再一次限制<em class="nb"> F </em>，以便它现在接收其参数的点积，</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/8b521b8aac4c6340bedfb518499058f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*fBDOnNtx3Cs_OSNcDJw-Ig.png"/></div></figure><p id="a62b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">③。</strong>没有什么特别的<em class="nb"> i </em>和<em class="nb"> j </em>，它们指向的是语料库中的任意词。因此，你可以将等式(2)中的<em class="nb"> i </em>和<em class="nb"> j </em>的角色对调，得到</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/37287dce68efd5bfca18f4b3c05b3ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*tggU_XLadPu52S-EwONgWA.png"/></div></figure><p id="8c0b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">定义<em class="nb">u</em>=<em class="nb">wᵢ</em>wⱼ，你有</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/36d4fdb4688c6afc3cc335353053717e.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*NGpZ6Mr8Mvk6ERRsrB89zg.png"/></div></figure><p id="c82c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">④。在<em class="nb"> X </em>中，中心词和上下文词之间的区别是任意的，你可以自由交换这两个角色。为了始终如一地做到这一点，你不仅要交换<em class="nb"> W </em> ↔ <em class="nb"> V </em>，还要交换<em class="nb"> X </em> ↔ <em class="nb"> Xᵀ </em>。</strong></p><p id="804c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该模型在这种重新标记下应该是不变的，但是等式(2)不是，因为RHS由于<em class="nb"> X </em>的对称性而不变，但是LHS不是。为达到不变性，<em class="nb"> F </em>假设为从(ℝ，+)到(ℝ⁺，×)的一个<a class="ae lh" href="https://en.wikipedia.org/wiki/Group_homomorphism" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">群同态</strong>。是的，我们再一次限制<em class="nb"> F </em>。因此</a></p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/5dd7e78f06fb8f431ca5eeba55553859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*AEsb1QOpGbv9smqDWsJCaw.png"/></div></figure><p id="ab3c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> (5)。</strong>方程(4)的一个解是</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/96e89c62468e483453448d5e8518391b.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*j-JEPQUuJz-z8wZGb_UCkA.png"/></div></figure><p id="c17a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb"> F </em>的群同态由<em class="nb"> F </em> = exp满足，所以</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/d3bd7ba94fabf812d78b963729f8fd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*Feez0xkPjay5dwHz2SeqYw.png"/></div></figure><p id="8a42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> (6)。</strong>由于对数(<em class="nb"> Xᵢ </em>)这一术语并不依赖于<em class="nb"> k </em>，所以它可以被看作是<em class="nb"> bᵢ </em>对<em class="nb"> wᵢ </em>的一个偏误术语。为了恢复交换对称性，还要为vₖ 加上一个偏置<em class="nb"> aₖ </em>。最终模型变成了</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/106603499b543c6b9542c20388938640.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*Q2hFdNR6fRSY_uNtqVGE7Q.png"/></div></figure><p id="89dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">(七)。</strong>第(6)部分的模型称为对数双线性回归模型。参数<em class="nb"> wᵢ </em>、<em class="nb"> vₖ </em>、<em class="nb"> bᵢ </em>和<em class="nb"> aₖ </em>的最佳值可以使用最小二乘法找到。</p><p id="7120" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">引入一个加权函数<em class="nb"> f </em> ( <em class="nb"> Xᵢₖ </em>)来补偿任何时候<em class="nb"> Xᵢₖ </em> = 0，因为此时log( <em class="nb"> Xᵢₖ </em>)是未定义的，并且还平衡频繁和非频繁单词对模型的贡献。模型的成本函数变成</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/733a45b791aca7b552576ead9db3f61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*Y3uWyxRRPWyyKN8yEcTquw.png"/></div></figure><p id="58cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<code class="fe pg ph pi kw b">vocab_size</code>是语料库的大小。</p><p id="2c96" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> (8)。</strong>什么是好的选择<em class="nb"> f </em>？首先，出于第(7)部分解释的原因，<em class="nb"> f </em>要满足<em class="nb"> f </em> (0) = 0。同样，如果它是连续的，它应该比log <em class="nb"> x </em>更快地接近零。其次，<em class="nb"> f </em> ( <em class="nb"> x </em>)应该是非递减的，这样罕见的同现(小的<em class="nb"> x </em>)就不会过重(有相对大的<em class="nb"> f </em>)。第三，<em class="nb"> f </em> ( <em class="nb"> x </em>)对于<em class="nb"> x </em>的大值应该相对较小，这样才不会对频繁的同现进行过加权。一个好的简单选择是</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/4784d18e09aed20969f47f907bad71f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*oCQvvbg7SofxmW_t3om-iA.png"/></div></figure><p id="aa57" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="nb"> x </em> ₘₐₓ和<em class="nb"> α </em>是超参数。下面是<em class="nb"> f </em> ( <em class="nb"> x </em>)的剧情。</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi pk"><img src="../Images/893a30a37d55831584546e1ea9caa402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ys7NCaCKFAZIHtsssO5bA.png"/></div></div><figcaption class="om on gj gh gi oo op bd b be z dk translated">f(x)与<em class="pl"> x </em> ₘₐₓ = 2，α = 0.75。实际上，<em class="pl"> x </em> ₘₐₓ应该大得多(比如100) |图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></figcaption></figure><h2 id="6f74" class="la lb it bd nd nt nu dn nh nv nw dp nl lr nx ny nn lv nz oa np lz ob oc nr iz bi translated">🔮最终预测</h2><p id="42e6" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">所以，你已经:</p><ol class=""><li id="6e09" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated">计算单词-单词共现计数矩阵<em class="nb"> X </em>，以及</li><li id="f87b" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated">随机初始化参数<em class="nb"> wᵢ </em>、<em class="nb"> vₖ </em>、<em class="nb"> bᵢ </em>、<em class="nb"> aₖ </em>。</li></ol><p id="7d6b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">经过一些迭代，你得到了优化的wᵢ和vₖ，它们分别构建了单词嵌入。现在怎么办？你选择哪一个作为你最终的单词嵌入？</p><p id="61c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于<em class="nb"> X </em>是对称的，因此<em class="nb"> W </em>和<em class="nb"> V </em>是等价的，不同之处仅在于它们的随机初始化。为了减少过拟合，最终的字嵌入被设置为<em class="nb"> W </em> + <em class="nb"> V </em>。</p><h2 id="e987" class="la lb it bd nd nt nu dn nh nv nw dp nl lr nx ny nn lv nz oa np lz ob oc nr iz bi translated">🪙的优势和局限性</h2><p id="2df9" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">优势:</p><ol class=""><li id="90bd" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated">因为GloVe试图直接捕获特征向量的线性结构，所以对于<strong class="lk jd">单词类比</strong>任务，它比Word2Vec表现得更好。在<strong class="lk jd">相似度</strong>任务和<strong class="lk jd">命名实体识别</strong>上也优于相关模型。</li><li id="3674" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated">GloVe给予单词对适当的权重，使得没有单词支配训练过程。</li></ol><p id="821c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">局限性:</p><ol class=""><li id="e1b8" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated">GloVe是在单词-单词共现计数矩阵上训练的，这需要大量的内存来存储，特别是如果你正在处理一个非常大的语料库。</li><li id="3a17" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated">就像Word2Vec一样，GloVe不识别OOV单词，并且忽略单词的形态学。</li></ol><h1 id="3a13" class="nc lb it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">⏩快速文本</h1><p id="b4ed" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">顾名思义，fastText是一种基于Word2Vec skip-gram模型的快速训练单词表示，使用标准的多核CPU，可以在不到10分钟的时间内对超过10亿个单词进行训练。</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi pm"><img src="../Images/e21c9efc48260c37e874cbc94a47541b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2d37wuG4Fw9qWbIG"/></div></div><figcaption class="om on gj gh gi oo op bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@chrisliverani?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯·利维拉尼</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="0f21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">fastText可以解决本文开头提到的普通Word2Vec的局限性3和4。该模型在学习单词表示的同时还考虑了词法，这是通过考虑子单词单元(character <em class="nb"> n </em> -grams)来捕获的。</p><h2 id="60c8" class="la lb it bd nd nt nu dn nh nv nw dp nl lr nx ny nn lv nz oa np lz ob oc nr iz bi translated">📚跳过已审核的图表</h2><p id="deaa" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">skip-gram的目标是找到对预测文档中的周围/上下文单词有用的单词表示。更正式地说，给定表示为单词序列<em class="nb"> w₁ </em>，…，<em class="nb"> wₘ </em>的训练语料库，skip-gram的目标是最大化以下对数似然性:</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/e9ef16f7a0f34d125f21aa5efd277795.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*x8FSTYzdHsw9P4D3JHLxXQ.png"/></div></figure><p id="7791" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中𝒞 <em class="nb"> ᵢ </em>是围绕中心词<em class="nb"> wᵢ </em>的上下文词的索引集。定义概率<em class="nb"> p </em>的一个可能选择是密集神经网络的输出层的softmax。</p><p id="5f53" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下图解释了skip-gram的训练过程。这里，∑表示网络上一层输出的加权和，S表示softmax。</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi po"><img src="../Images/b15bf83060549927f6439f8505f2a853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OEr4EnPr2WZB0ksY.png"/></div></div><figcaption class="om on gj gh gi oo op bd b be z dk translated">skip-gram如何学习|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></figcaption></figure><blockquote class="pp pq pr"><p id="da2c" class="li lj nb lk b ll lm kd ln lo lp kg lq ps ls lt lu pt lw lx ly pu ma mb mc md im bi translated">关于skip-gram和Word2Vec的更多信息:</p></blockquote><div class="pv pw gp gr px py"><a href="https://towardsdatascience.com/all-you-need-to-know-about-bag-of-words-and-word2vec-text-feature-extraction-e386d9ed84aa" rel="noopener follow" target="_blank"><div class="pz ab fo"><div class="qa ab qb cl cj qc"><h2 class="bd jd gy z fp qd fr fs qe fu fw jc bi translated">关于单词包和Word2Vec —文本特征提取，您需要知道的一切</h2><div class="qf l"><h3 class="bd b gy z fp qd fr fs qe fu fw dk translated">Word2Vec为什么更好，为什么不够好</h3></div><div class="qg l"><p class="bd b dl z fp qd fr fs qe fu fw dk translated">towardsdatascience.com</p></div></div><div class="qh l"><div class="qi l qj qk ql qh qm ok py"/></div></div></a></div><p id="3050" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于计算量大，这个公式是不切实际的。然而，预测上下文单词的问题可以被构造为一组独立的二元分类任务，其中目标是独立地预测上下文单词的存在(或不存在)。</p><p id="47c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于语料库中索引为<em class="nb"> i </em>的中心词，所有对应的上下文词都被认为是正例，并且从语料库中随机选择负例(除上下文词之外的词)。使用二元逻辑损失，skip-gram最小化以下目标函数:</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/23ff228b79ab00a279e0f361033c1e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*h7s4d4W7fLqbeBf34lvV4w.png"/></div></figure><p id="f004" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中𝒩 <em class="nb"> ᵢ </em>，<em class="nb"> ⱼ </em>是从语料库中采样的负面例子的索引集合，而<em class="nb"> s </em>是将对<strong class="lk jd">(中心，上下文)</strong>映射到ℝ.的评分函数对于skip-gram，选择的<em class="nb"> s </em>将是一个密集的神经网络。</p><h2 id="3b6e" class="la lb it bd nd nt nu dn nh nv nw dp nl lr nx ny nn lv nz oa np lz ob oc nr iz bi translated">📈改进跳跃图</h2><p id="6b0d" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">因为skip-gram对每个单词使用不同的特征向量，所以它忽略了单词的内部结构。fastText试图通过将每个单词视为其字符的集合来解决这个问题。一个单词的向量被简单地认为是其字符的所有向量的总和。</p><p id="a3a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是T21这个人物是如何被创造出来的呢？假设你把单词<em class="nb">写成</em>和<em class="nb"> n </em> = 3。那么字符3-grams的<em class="nb">写</em>就会是</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="a0bf" class="la lb it kw b gy lc ld l le lf">&lt;wr, wri, rit, ite, te&gt;, &lt;write&gt;</span></pre><p id="2c4d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有五个字符3-grams和一个特殊序列<code class="fe pg ph pi kw b">&lt;write&gt;</code>。单词开头和结尾的符号<code class="fe pg ph pi kw b">&lt;</code>和<code class="fe pg ph pi kw b">&gt;</code>可以让我们将前缀和后缀与其他字符序列区分开来。</p><p id="1b9b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在创建了角色<em class="nb">n</em>-克数和你选择的<em class="nb"> n </em>之后，你就有了一个主体<em class="nb">n</em>-克数大小<em class="nb"> G </em>。让</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/4f54bbb39eab06741e687884c1ad0420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*qhxK0pQsnGevu_jBfqN5dA.png"/></div></figure><p id="dc8c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将快速文本的<strong class="lk jd">(中心，上下文)</strong>对的评分函数定义为中心词的<em class="nb"> n </em> -grams和上下文词的矢量表示的点积之和，即:</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/3e61e8ff95f18ee8d305d3931da2ba94.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*G7-aXLE09UIMwVUWTgqLzw.png"/></div></figure><p id="cbc5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个简单的模型允许跨单词共享表示，从而允许它学习罕见单词的可靠表示。因此，快速文本建模实际上只是解决</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/26b1b654cda8f327ad378a27997a4714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*ozVx5mGUFY3Ie7iuixN7EQ.png"/></div></figure><p id="9692" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为使用<em class="nb"> n </em> -grams会增加语料库的大小，所以您需要解决内存问题。您可以使用一个散列函数，将<em class="nb">n</em>g映射为1到<em class="nb"> K </em>的整数，其中<em class="nb"> K </em>是您选择的一个大数。</p><h2 id="8378" class="la lb it bd nd nt nu dn nh nv nw dp nl lr nx ny nn lv nz oa np lz ob oc nr iz bi translated">🆚fastText vs Word2Vec</h2><p id="0506" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">与普通的Word2Vec相比，fastText在<a class="ae lh" href="https://en.wikipedia.org/wiki/Syntax" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">句法</strong> </a>任务上表现明显更好，尤其是当训练语料库的规模很小时。尽管在语义任务上，Word2Vec比fastText略胜一筹。随着训练语料库规模的增加，差异变得越来越小。</p><p id="fc83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与Word2Vec不同，fastText甚至可以通过对其组成字符<em class="nb">n</em>grams的向量求和来获得OOV单词的向量，前提是在训练数据中至少存在一个字符<em class="nb">n</em>grams。</p><h1 id="18fc" class="nc lb it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">🚀摘要</h1><p id="a142" class="pw-post-body-paragraph li lj it lk b ll od kd ln lo oe kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">祝贺你到达终点！</p><p id="1b76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个故事中，向您介绍了两种有可能提高Word2Vec性能的方法:</p><ol class=""><li id="9122" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated"><strong class="lk jd"> GloVe </strong>也通过使用关于单词的全局信息直接找到特征向量的线性结构。</li><li id="b5f8" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><strong class="lk jd"> fastText </strong>通过character<em class="nb">n</em>-grams of words学习形态学，并能估计OOV词的特征向量。</li></ol><p id="78f7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于<strong class="lk jd">单词类比</strong>任务，GloVe的表现优于Word2Vec。在<strong class="lk jd">相似度</strong>任务和<strong class="lk jd">命名实体识别</strong>上也优于相关模型。fastText在<strong class="lk jd">句法</strong>任务上明显优于Word2Vec，而Word2Vec在<strong class="lk jd">语义</strong>任务上略胜fastText。</p><figure class="kr ks kt ku gt oj gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi qr"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qs qt hx qu" role="separator"><span class="qv bw bk qw qx qy"/><span class="qv bw bk qw qx qy"/><span class="qv bw bk qw qx"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="nb">成为会员</em> </strong> </a> <em class="nb">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔖<em class="nb">想了解更多关于经典机器学习模型的工作原理，以及它们如何优化参数？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="pv pw gp gr px"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb ok ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wi mn fp wj fr fs qe fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wk au wl wm wn ta wo an eh ei wp wq wr el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ws l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="pv pw gp gr px"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb ok ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wi mn fp wj fr fs qe fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wk au wl wm wn ta wo an eh ei wp wq wr el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ws l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="pv pw gp gr px"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb ok ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wi mn fp wj fr fs qe fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wk au wl wm wn ta wo an eh ei wp wq wr el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ws l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="pv pw gp gr px"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb ok ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wi mn fp wj fr fs qe fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wk au wl wm wn ta wo an eh ei wp wq wr el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ws l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="pv pw gp gr px"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb ok ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wi mn fp wj fr fs qe fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wk au wl wm wn ta wo an eh ei wp wq wr el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----1d227ab017b2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ws l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div></div><div class="ab cl qs qt hx qu" role="separator"><span class="qv bw bk qw qx qy"/><span class="qv bw bk qw qx qy"/><span class="qv bw bk qw qx"/></div><div class="im in io ip iq"><p id="616e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[1] Jeffrey Pennington，Richard Socher，Christopher d . Manning(2014):<em class="nb">GloVe:单词表征的全局向量</em>。[ <a class="ae lh" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> pdf </a></p><p id="ac66" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] Armand Joulin，Edouard Grave，Piotr Bojanowski，Tomas Mikolov (2016): <em class="nb">高效文本分类的锦囊妙计</em>。[ <a class="ae lh" href="https://arxiv.org/pdf/1607.01759v3.pdf" rel="noopener ugc nofollow" target="_blank"> pdf </a></p><p id="6c05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] Piotr Bojanowski，Edouard Grave，Armand Joulin，Tomas Mikolov (2017): <em class="nb">用子词信息丰富词向量</em>。[ <a class="ae lh" href="https://arxiv.org/pdf/1607.04606v2.pdf" rel="noopener ugc nofollow" target="_blank"> pdf </a></p><p id="9f17" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4]拉迪姆·řehůřek(2022):<em class="nb">教程:面向学习的课程</em>。<a class="ae lh" href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html" rel="noopener ugc nofollow" target="_blank">快速文本模型</a></p></div></div>    
</body>
</html>