<html>
<head>
<title>How to Fine-Tune an NLP Classification Model with Transformers and HuggingFace</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Transformers和HuggingFace微调NLP分类模型</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/how-to-fine-tune-an-nlp-classification-model-with-transformers-and-huggingface-1a2c0ea79c2?source=collection_archive---------2-----------------------#2022-06-07">https://levelup.gitconnected.com/how-to-fine-tune-an-nlp-classification-model-with-transformers-and-huggingface-1a2c0ea79c2?source=collection_archive---------2-----------------------#2022-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="88da" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用转换器构建自定义NLP分类模型的终极指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/f888b49ca91eb145fb9095f5dba13da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*l6tGpQlpzvLg3Z-p.png"/></div></figure><p id="9dc9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">本教程是关于如何使用transformers训练您的自定义NLP分类模型的终极指南，从预训练的模型开始，然后使用迁移学习对其进行微调。我们将与名为“变形金刚”的HuggingFace库合作。</p><h1 id="707d" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">分类模型</h1><p id="05b5" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">出于展示的目的，我们将建立一个分类模型，试图预测一封电子邮件是“火腿”还是“垃圾邮件”。在另一个教程中，我们使用Scikit-Learn和TF-IDF构建了一个<a class="ae mj" href="https://predictivehacks.com/example-of-a-machine-learning-algorithm-to-predict-spam-emails-in-pyth" rel="noopener ugc nofollow" target="_blank">垃圾邮件检测器</a>。为了获得数据并比较这两种不同的方法，请随意查看教程。</p><p id="9414" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对于本教程，我们将与免费的<strong class="ks iu">亚马逊SageMaker工作室实验室</strong>合作。或者，你可以和<strong class="ks iu"> Colab </strong>或者当地合作。本教程是可重复的，因此您可以跟着编写代码。</p><h2 id="1cf0" class="mk ln it bd lo ml mm dn ls mn mo dp lw kz mp mq ly ld mr ms ma lh mt mu mc mv bi translated">安装所需的库</h2><p id="9a07" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">对于本教程，您可以下载以下库:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="cf4d" class="mk ln it mx b gy nb nc l nd ne">!pip install transformers<br/>!pip install datasets<br/>!pip install numpy<br/>!pip install pandas</span></pre><h2 id="2370" class="mk ln it bd lo ml mm dn ls mn mo dp lw kz mp mq ly ld mr ms ma lh mt mu mc mv bi translated">加载数据</h2><p id="c67a" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">假设您将训练和测试数据集存储为CSV文件。让我们看看如何将它们作为数据集加载。请注意，HuggingFace要求数据作为数据集字典</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="9007" class="mk ln it mx b gy nb nc l nd ne">import datasets<br/>from datasets import load_dataset, load_from_disk</span><span id="edaa" class="mk ln it mx b gy nf nc l nd ne">dataset = load_dataset('csv', data_files={'train': 'train_spam.csv', 'test': 'test_spam.csv'})</span><span id="ffba" class="mk ln it mx b gy nf nc l nd ne">dataset</span></pre><p id="e510" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">输出:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="69d8" class="mk ln it mx b gy nb nc l nd ne">DatasetDict({<br/>    train: Dataset({<br/>        features: ['text', 'label'],<br/>        num_rows: 3900<br/>    })<br/>    test: Dataset({<br/>        features: ['text', 'label'],<br/>        num_rows: 1672<br/>    })<br/>})</span></pre><h2 id="5887" class="mk ln it bd lo ml mm dn ls mn mo dp lw kz mp mq ly ld mr ms ma lh mt mu mc mv bi translated">微调模型</h2><p id="c310" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">请记住"<strong class="ks iu">目标</strong>"变量应该称为"<strong class="ks iu">标签</strong>"并且应该是数字。在这个数据集中，我们处理的是一个二元问题，0 (Ham)或者1 (Spam)。因此，我们将从“<strong class="ks iu">基于蒸馏的</strong>”开始，然后对其进行微调。首先，我们将加载分词器。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="35fe" class="mk ln it mx b gy nb nc l nd ne">from transformers import AutoTokenizer</span><span id="6a28" class="mk ln it mx b gy nf nc l nd ne">tokenizer = AutoTokenizer.from_pretrained("distilbert-base-cased")</span><span id="53b1" class="mk ln it mx b gy nf nc l nd ne">def tokenize_function(examples):<br/>    return tokenizer(examples["text"], padding="max_length", truncation=True)</span><span id="0784" class="mk ln it mx b gy nf nc l nd ne">tokenized_datasets = dataset.map(tokenize_function, batched=True)</span></pre><p id="b17f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">输出:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="6993" class="mk ln it mx b gy nb nc l nd ne">Loading cached processed dataset at ham_spam_dataset/train/cache-3a436b86c79a53fe.arrow<br/>Loading cached processed dataset at ham_spam_dataset/test/cache-9524e6b19881902e.arrow</span></pre><p id="c745" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后，我们将加载序列分类的模型。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="731c" class="mk ln it mx b gy nb nc l nd ne">from transformers import AutoModelForSequenceClassification<br/>checkpoint = "distilbert-base-cased"<br/>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)</span></pre><p id="a0ab" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">注意，我们设置了<strong class="ks iu"> "num_labels=2" </strong>。如果你正在处理更多的类，你必须相应地调整数量。</p><p id="562f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">由于我们想要报告模型的准确性，我们可以添加以下函数。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="ab7a" class="mk ln it mx b gy nb nc l nd ne">import numpy as np<br/>from datasets import load_metric</span><span id="2c74" class="mk ln it mx b gy nf nc l nd ne">metric = load_metric("accuracy")</span><span id="7f54" class="mk ln it mx b gy nf nc l nd ne">def compute_metrics(eval_pred):<br/>    logits, labels = eval_pred<br/>    predictions = np.argmax(logits, axis=-1)<br/>    return metric.compute(predictions=predictions, references=labels)</span></pre><h2 id="ecdc" class="mk ln it bd lo ml mm dn ls mn mo dp lw kz mp mq ly ld mr ms ma lh mt mu mc mv bi translated">训练模型</h2><p id="567c" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">现在，我们准备训练模型。我们将只训练一个纪元，但可以随意添加更多。我建议三到五个。在培训师那里，你有很多争论的选择。我们保留默认值，但是我鼓励你看一下文档，因为很多时候实验像批量大小、学习速率等参数是很重要的。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="85d4" class="mk ln it mx b gy nb nc l nd ne">from transformers import TrainingArguments, Trainer</span><span id="b53e" class="mk ln it mx b gy nf nc l nd ne">training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch", num_train_epochs=1)<br/></span><span id="74f7" class="mk ln it mx b gy nf nc l nd ne">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=tokenized_datasets["train"],<br/>    eval_dataset=tokenized_datasets["test"],<br/>    compute_metrics=compute_metrics,<br/>)</span><span id="268d" class="mk ln it mx b gy nf nc l nd ne">trainer.train()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/9648ffea309780da295562b11702e8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1-YJPwb0NDo2LgPG.png"/></div></div></figure><p id="16bb" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们可以看到，模型运行了一个历元，准确率为<strong class="ks iu"> 98.3%！</strong></p><h1 id="8150" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">保存模型</h1><p id="1755" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">我建议将模型和标记器保存在同一个路径下，以便同时加载它们。请记住，在我们的例子中，我们没有微调标记器。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="17a6" class="mk ln it mx b gy nb nc l nd ne">model.save_pretrained("CustomModels/CustomHamSpam")</span><span id="d952" class="mk ln it mx b gy nf nc l nd ne"># alternatively save the trainer<br/># trainer.save_model("CustomModels/CustomHamSpam")</span><span id="b7fb" class="mk ln it mx b gy nf nc l nd ne">tokenizer.save_pretrained("CustomModels/CustomHamSpam")</span></pre><p id="477a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这是我们的模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/850660335ce4638d333079630ab677fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/0*TgFwsqaR4o4Zb1HP.png"/></div></figure><h1 id="a941" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">加载模型</h1><p id="7c87" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">我们可以如下加载模型和标记器。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="dc06" class="mk ln it mx b gy nb nc l nd ne"># load the model<br/>from transformers import AutoModelForSequenceClassification</span><span id="43ab" class="mk ln it mx b gy nf nc l nd ne">load_model = AutoModelForSequenceClassification.from_pretrained("CustomModels/CustomHamSpam")</span><span id="a841" class="mk ln it mx b gy nf nc l nd ne">load_tokenizer = AutoTokenizer.from_pretrained("CustomModels/CustomHamSpam")</span></pre><h1 id="97e3" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">做预测</h1><p id="e4cf" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">我们可以使用<code class="fe nm nn no mx b">TextClassificationPipeline</code>进行预测。让我们看看这封电子邮件是一个火腿或垃圾邮件:</p><blockquote class="np nq nr"><p id="9e6b" class="kq kr ns ks b kt ku ju kv kw kx jx ky nt la lb lc nu le lf lg nv li lj lk ll im bi translated">XXXMobileMovieClub:要使用您的点数，请单击下一条txt消息中的WAP链接或单击此处&gt; &gt;<a class="ae mj" href="http://wap." rel="noopener ugc nofollow" target="_blank"> http://wap。【xxxmobilemovieclub.com】T2？n=QJKGIGHJJGCBL</a></p></blockquote><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="28d7" class="mk ln it mx b gy nb nc l nd ne">model = load_model<br/>tokenizer = load_tokenizer<br/>pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)<br/># outputs a list of dicts <br/>pipe("XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; <a class="ae mj" href="http://wap." rel="noopener ugc nofollow" target="_blank">http://wap.</a> xxxmobilemovieclub.com?n=QJKGIGHJJGCBL")</span></pre><p id="2714" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">输出:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="3831" class="mk ln it mx b gy nb nc l nd ne">[[{'label': 'LABEL_0', 'score': 0.009705818258225918},<br/>  {'label': 'LABEL_1', 'score': 0.9902942180633545}]]</span></pre><p id="96a2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们也可以使用管道，如下所示:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="8198" class="mk ln it mx b gy nb nc l nd ne">from transformers import pipeline</span><span id="ab3b" class="mk ln it mx b gy nf nc l nd ne">my_pipeline  = pipeline("text-classification", model=load_model, tokenizer=load_tokenizer)</span><span id="6ece" class="mk ln it mx b gy nf nc l nd ne">data = ["I love you", "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; <a class="ae mj" href="http://wap." rel="noopener ugc nofollow" target="_blank">http://wap.</a> xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"]</span><span id="8230" class="mk ln it mx b gy nf nc l nd ne">my_pipeline(data)</span></pre><p id="5eb2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">输出:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="d10c" class="mk ln it mx b gy nb nc l nd ne">[{'label': 'LABEL_0', 'score': 0.9980890154838562},<br/> {'label': 'LABEL_1', 'score': 0.9902942180633545}]</span></pre><p id="f23e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们可以看到，电子邮件“我爱你”被标记为0(即火腿)，我们之前看到的第二封被标记为1(即垃圾邮件)。</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><div class="kj kk kl km gt od"><a href="https://jorgepit-14189.medium.com/membership" rel="noopener follow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">用我的推荐链接加入媒体-乔治皮皮斯</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">阅读乔治·皮皮斯(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">jorgepit-14189.medium.com</p></div></div><div class="om l"><div class="on l oo op oq om or ko od"/></div></div></a></div></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="440e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最初由<a class="ae mj" href="https://predictivehacks.com/how-to-fine-tuned-an-nlp-classification-model-with-transformers-and-huggingface/" rel="noopener ugc nofollow" target="_blank">预测黑客</a>发布</p></div></div>    
</body>
</html>