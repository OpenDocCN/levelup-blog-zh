# PySpark 窗口函数简介

> 原文：<https://levelup.gitconnected.com/an-short-introduction-to-pyspark-window-functions-a60bcb28cc3b>

## 避免缓慢而复杂的代码

![](img/3e61b20f76a745c752e6ca622778cdd3.png)

照片由[零点拍摄](https://unsplash.com/@zerotake?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

根据 Python 的[禅](https://peps.python.org/pep-0020/)

> 应该有一种——最好只有一种——显而易见的方法来做这件事。虽然这种方式一开始可能并不明显，除非你是荷兰人。

我不是荷兰人，这可能解释了为什么我花了一段时间才发现 [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) 中的窗口函数，即 Apache Spark 的 Python API，当 pandas 的内存单节点处理开始显示其局限性时使用。在我发现之前，我有很多方法可以实现想要的功能，但是代码复杂，速度慢，可读性差。

这篇文章的目的是提供一个关于窗口函数的简单介绍。我并不认为我可以在介绍中涵盖所有可能的窗口函数的使用，这也不是我的目的。相反，本文的目的是展示实现相同结果的几种替代方法，旨在说明窗口功能的有用性和表现力。

**导入并启动数据集**

我们从必要的进口开始

然后，我们创建一个包含学生成绩的合成数据集。数据集有 200 行，这并不证明使用 PySpark 是正确的，但它对于快速实验来说很方便。我更喜欢创建一个合成数据集，而不是提供数据链接，因为读者更容易试验代码片段。

起始数据集可通过以下方式创建

假设`spark`是 spark 会话。细节并不太重要。我们有三列，分别是学生姓名，年份(三年，即 2020，2021，2022)和范围在[0.0，10.0]内的随机成绩。如果希望有意义地测试性能，可以通过增加学生人数和年数来创建更大的数据集，但在一定程度上，PySpark 数据框是通过 pandas 数据框创建的。

**不明显的方式**

我们展示了两种不使用窗口函数计算每年哪个学生分数最高的方法。

第一种方法依赖于聚合连接模式

我们首先计算每年的最高分数，并将其存储在名为`max_grade_in_year`的中间数据帧中。然后，我们使用这个数据框将原始数据框与学生成绩连接起来。请注意左侧半连接的使用，它保留了左侧数据框中与第二个匹配的行，其中连接条件同时使用了年份和年级。尽管它是可行的，但是将数据框与其自身(或其自身的衍生物)连接起来是一个信号，表明可能存在更好的方法。

另一种可能是通过创建熊猫 UDF 来使用分离-聚集-组合模式。pandas UDFs 的使用在 Spark 3.x 中有了显著的改进，这里显示的代码是在使用 Spark 3.2.1 中测试的

结果与之前相同，连接已被消除。然而，这种解决方案并不理想，因为 UDF 应该只在必要时使用，即使它们是受益于矢量化执行的熊猫 UDF。

**显而易见的方式**

用窗口函数解决问题要简洁得多

结果是一样的，代码可读性大大提高了。该解决方案需要创建一个使用年份指定分区的窗口。通过使用直观的名称，窗口的应用程序读起来像英语。应用窗口函数不会改变行数，而只是增加了一个具有每年最高等级的列。剩下的都是简单的 PySpark 操作。

对于更熟悉 SQL 的读者来说，使用

在 SQL 查询执行之前，我们创建了一个名为`grades`的本地临时视图。就我个人而言，我总是更喜欢 PySpark API，但是具有长期 SQL 经验的数据分析师可能不会同意。火花迎合所有人！

窗口函数对数据框的一部分进行操作，可用于聚合(如本文中所示)、排序或分析操作。它们返回具有相同行数的数据框，可以认为它们是一种创建新列的方法，然后可以对新列进行过滤或在其他操作中使用。窗口函数常常使代码更加简洁，但常常被数据分析师和数据科学家所忽视。一旦你掌握了它们，它们会成为你武器库中的有力工具。