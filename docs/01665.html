<html>
<head>
<title>High Volume Incoming Connections</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高容量传入连接</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/linux-kernel-tuning-for-high-performance-networking-high-volume-incoming-connections-196e863d458a?source=collection_archive---------0-----------------------#2020-01-21">https://levelup.gitconnected.com/linux-kernel-tuning-for-high-performance-networking-high-volume-incoming-connections-196e863d458a?source=collection_archive---------0-----------------------#2020-01-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c2a2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">高性能网络系列的Linux内核调优</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9bca4cf80816424bf23f02291adb7381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qGXIA1S5lQk5i8BN"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@aligns?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔丹·哈里森</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="5e12" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">最大化接收连接数</h1><p id="9eb7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">需要理解的一些更令人困惑的事情是linux网络栈的不断变化的世界。不仅仅是因为一些术语在不完全理解这些概念的工程师的博客帖子中被误用，还因为这些设置的含义已经演变了多年。在撰写本文时，这些细节从linux内核v2.2开始就是准确的，并在可能的情况下强调了从v5.5开始的内核之间的设置差异。</p><blockquote class="mk ml mm"><p id="3836" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated">本文中的示例命令依赖于linux命令<code class="fe mw mx my mz b"><strong class="lq ir">netstat</strong></code> ( <code class="fe mw mx my mz b">net-tools</code>包)和<code class="fe mw mx my mz b"><strong class="lq ir">ss</strong></code> ( <code class="fe mw mx my mz b">iproute2</code>包)。</p></blockquote><p id="2b86" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">本文中概述的内核设置可以按照下面的配置内核设置入门进行调整:</p><div class="na nb gp gr nc nd"><a rel="noopener  ugc nofollow" target="_blank" href="/linux-kernel-tuning-for-high-performance-networking-configuring-kernel-settings-96b519a3305f"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd ir gy z fp ni fr fs nj fu fw ip bi translated">面向高性能网络的Linux内核调优:配置内核设置</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">Linux内核配置</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr kp nd"/></div></div></a></div><h1 id="2946" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak"> TCP接收队列和netdev_max_backlog </strong></h1><p id="9e4c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在网络堆栈能够处理数据包之前，每个CPU内核可以在环形缓冲区中保存大量数据包。如果缓冲区的填充速度快于TCP堆栈处理数据包的速度，则丢弃数据包计数器将递增，数据包将被丢弃。应该增加<code class="fe mw mx my mz b"><strong class="lq ir">net.core.netdev_max_backlog</strong></code>设置，以便在高突发流量的服务器上最大化排队等待处理的数据包数量。</p><blockquote class="mk ml mm"><p id="9242" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated"><code class="fe mw mx my mz b"><strong class="lq ir">net.core.netdev_max_backlog</strong></code>是每个CPU内核的设置。</p></blockquote><h1 id="4e59" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak"> TCP Backlog队列和tcp_max_syn_backlog </strong></h1><p id="7900" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">TCP Backlog队列包含等待完成的不完整连接。</p><p id="4d46" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">为从接收队列中拾取并被移动到SYN Backlog队列中的任何<strong class="lq ir"> SYN </strong>数据包创建一个连接。该连接被标记为“SYN_RECV ”,并且一个<strong class="lq ir"> SYN+ACK </strong>被发送回客户端。</p><blockquote class="mk ml mm"><p id="5715" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated">这些连接在相应的<strong class="lq ir"> ACK </strong>被接收和处理之前<strong class="lq ir">不会被移动</strong>到接受队列。</p></blockquote><p id="9bcb" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">队列中的最大连接数在<code class="fe mw mx my mz b"><strong class="lq ir">net.ipv4.tcp_max_syn_backlog</strong></code>内核设置中设置。</p><p id="0c6b" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">在正常负载下，SYN backlog条目的数量在正常负载下应该是<strong class="lq ir">不大于1</strong>，在高负载下应该保持在<code class="fe mw mx my mz b">tcp_max_syn_backlog</code>限制以下。要检查TCP端口的SYN积压的当前大小，运行以下命令<em class="mn">(示例使用TCP端口80) </em>:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="3f4a" class="nw kx iq mz b be nx ny l nz oa">ss -n state syn-recv sport = :80 | wc -l</span></pre><p id="6631" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">如果有大量的连接处于“SYN_RECV”状态，这可能会导致服务器在处理大量流量时出现问题。在增加这个限制之前，可以通过调整相关的TCP设置来减少SYN数据包在这个队列中的时间。</p><h2 id="37a7" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on"> SYN Cookies </em></h2><p id="76ba" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">对此进行调整可以减少SYN数据包在接收队列中的停留时间。如果SYN cookies没有启用，客户端将简单地重试发送一个<strong class="lq ir"> SYN </strong>数据包。如果启用了SYN cookies(<code class="fe mw mx my mz b"><strong class="lq ir">net.ipv4.tcp_syncookies</strong></code>)，则不会创建连接，也不会将其放入SYN backlog中，但是会向客户端发送一个<strong class="lq ir"> SYN+ACK </strong>数据包，就好像它是。在正常流量下，SYN cookies可能是有益的，但是在大量突发流量期间，一些连接细节将会丢失，并且在建立连接时客户端将会遇到问题。除了SYN cookies之外，还有其他一些东西，但是这里有一篇名为“<a class="ae kv" href="https://kognitio.com/blog/syn-cookies-ate-my-dog-breaking-tcp-on-linux/" rel="noopener ugc nofollow" target="_blank"> SYN cookies吃了我的狗</a>”的文章，作者Graeme Cole详细解释了为什么在高性能服务器上启用SYN cookies会导致问题。</p><h2 id="b0bb" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on"> SYN+ACK重试次数</em></h2><p id="62e9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">对此进行调整可以显著减少SYN数据包在接收队列中的停留时间。当发送了一个<strong class="lq ir"> SYN+ACK </strong>但从未收到响应ACK包时会发生什么？在这种情况下，服务器上的网络堆栈将重试发送<strong class="lq ir"> SYN+ACK </strong>。计算尝试之间的延迟是为了允许服务器恢复。</p><blockquote class="mk ml mm"><p id="d2a6" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated">如果服务器接收到一个<strong class="lq ir"> SYN </strong>，发送一个<strong class="lq ir"> SYN+ACK </strong>，并且没有接收到<strong class="lq ir"> ACK </strong>，重试所花费的时间长度遵循<a class="ae kv" href="https://en.wikipedia.org/wiki/Exponential_backoff" rel="noopener ugc nofollow" target="_blank">指数补偿</a>算法，因此重试次数取决于重试计数器。</p></blockquote><p id="a201" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">定义<strong class="lq ir"> SYN+ACK </strong>重试次数的内核设置是<code class="fe mw mx my mz b"><strong class="lq ir">net.ipv4.tcp_synack_retries</strong></code>，默认设置为5。这将在第一次尝试后按以下时间间隔重试:1秒、3秒、7秒、15秒、31秒。第一次尝试后大约63秒后，最后一次重试将超时，这相当于重试次数为6次时进行下一次尝试的时间。仅这一项就可以在包超时之前将一个<strong class="lq ir"> SYN </strong>包保留在SYN backlog中超过60秒。如果SYN backlog队列很小，不需要大量的连接就可以在网络堆栈中导致放大事件，在这种情况下，半开连接永远不会完成，也不会建立任何连接。将SYN+ACK重试次数设置为0或1，以避免在高性能服务器上出现这种情况。</p><h2 id="bd2c" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">同步重试次数</em></h2><p id="9147" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">对此进行调整可以显著减少SYN数据包在接收队列中的停留时间。虽然SYN重试指的是客户端在等待<strong class="lq ir"> SYN+ACK </strong>时重试发送<strong class="lq ir"> SYN </strong>的次数，但它也会影响进行代理连接的高性能服务器。由于流量高峰，nginx服务器与后端服务器建立几十个代理连接，这可能会使后端服务器的网络堆栈在短时间内过载，重试可能会在后端的接收队列和SYN积压队列上造成放大。这反过来会影响所服务的客户端连接。SYN重试的内核设置是<code class="fe mw mx my mz b"><strong class="lq ir">net.ipv4.tcp_syn_retries</strong></code>，默认为5或6，具体取决于发行版。将SYN重试次数限制为0或1，而不是重试63-130秒以上(指数回退)。</p><p id="129b" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">有关解决反向代理服务器上的客户端连接问题的更多信息，请参见以下内容:</p><div class="na nb gp gr nc nd"><a rel="noopener  ugc nofollow" target="_blank" href="/linux-kernel-tuning-for-high-performance-networking-f3256ffecf98"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd ir gy z fp ni fr fs nj fu fw ip bi translated">针对高性能网络的Linux内核调优:临时端口</h2><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="nm l"><div class="oo l no np nq nm nr kp nd"/></div></div></a></div><h1 id="2add" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak"> TCP接受队列和somaxconn </strong></h1><p id="8f81" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">当通过指定一个“backlog”参数调用<code class="fe mw mx my mz b">listen()</code>时，应用程序负责在打开一个监听器端口时创建它们的接受队列。从linux kernel v2.2开始，这个参数从设置一个套接字可以容纳的最大未完成连接数改为等待接受的最大已完成连接数。如上所述，现在用内核设置<code class="fe mw mx my mz b">net.ipv4.tcp_max_syn_backlog</code>来设置未完成连接的最大数量。</p><h2 id="8f30" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">TCP listen()积压</em></h2><p id="fc03" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">虽然应用程序对它打开的每个侦听器上的接受队列大小负责，但是侦听器的接受队列中的连接数是有限制的。有两种设置可以控制队列的大小:</p><ol class=""><li id="9106" class="op oq iq lq b lr mo lu mp lx or mb os mf ot mj ou ov ow ox bi translated">应用程序发出的TCP listen()调用上的backlog参数</li><li id="bd83" class="op oq iq lq b lr oy lu oz lx pa mb pb mf pc mj ou ov ow ox bi translated">来自内核sysctl: <code class="fe mw mx my mz b"><strong class="lq ir">net.core.somaxconn</strong></code>的内核限制最大值</li></ol><h2 id="3e62" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">接受队列默认值</em></h2><p id="77e1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><code class="fe mw mx my mz b">net.core.somaxconn</code>的默认值来自于<code class="fe mw mx my mz b"><strong class="lq ir">SOMAXCONN</strong></code>常量，在v5.3之前的linux内核中该常量被设置为<a class="ae kv" href="https://github.com/torvalds/linux/blob/v5.3/include/linux/socket.h#L266" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> 128 </strong> </a>，而在v5.4中<code class="fe mw mx my mz b">SOMAXCONN</code>被提升为<a class="ae kv" href="https://github.com/torvalds/linux/blob/v5.4/include/linux/socket.h#L266" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> 4096 </strong> </a>。然而，在撰写本文时，v5.4是最新版本，尚未被广泛采用，因此在许多尚未修改<code class="fe mw mx my mz b">net.core.somaxconn</code>的生产系统中，接受队列将被截断为128。</p><p id="6990" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">如果没有在应用程序配置中设置，或者有时只是在服务器软件中硬编码，应用程序在为侦听器配置默认backlog时通常会使用<code class="fe mw mx my mz b">SOMAXCONN</code>常量的值。一些应用程序设置了自己的默认值，比如nginx将它设置为511——在linux内核上，通过v5.3，它被无声地截断为128。查看应用程序文档以配置监听器，了解使用了什么。</p><p id="89af" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">要检查为开放TCP侦听器端口配置的accept()队列大小，请运行以下命令(示例端口80):</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="f974" class="nw kx iq mz b be nx ny l nz oa">ss -plnt sport = :80|cat</span></pre><h2 id="c5d7" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">接受队列最大值</em></h2><p id="5975" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">net.core.somaxconn的最大值在内核v2.2到v4.0.x中是<strong class="lq ir"> 65535 </strong>，在内核v4.1.0+中是<strong class="lq ir"> 4294967295 </strong>。</p><h2 id="31cf" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">接受队列覆盖</em></h2><p id="7080" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">许多应用程序允许在配置中指定接受队列的大小，方法是在监听器指令上提供一个“backlog”值，或者在调用<code class="fe mw mx my mz b">listen()</code>时使用一个配置。例如，nginx有一个backlog参数，可以添加到listen指令中，用于调整侦听器端口的接受队列的大小:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="f2a2" class="nw kx iq mz b be nx ny l nz oa">listen 80 backlog=65535;</span></pre><blockquote class="mk ml mm"><p id="9b22" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated">如果应用程序使用大于<code class="fe mw mx my mz b"><strong class="lq ir">net.core.somaxconn</strong></code>的backlog值调用<code class="fe mw mx my mz b">listen()</code>，那么该侦听器的backlog将被静默地截断为somaxconn值。</p></blockquote><h2 id="aaea" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">申请工人</em></h2><p id="d09a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如果接受队列很大，还要考虑增加应用程序中可以处理队列接受请求的线程数量。例如，为高容量nginx服务器在HTTP侦听器上设置20480的backlog，而不允许足够的worker_connections来管理队列，这将导致来自服务器的连接拒绝响应。</p><h1 id="3a38" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">文件描述符(文件句柄、连接)</h1><p id="929c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在linux系统上，一切都是文件。这包括实际的文件和文件夹、符号链接、管道和套接字等。因此，配置进程的最大连接数也需要配置进程可以打开的文件数。</p><blockquote class="mk ml mm"><p id="855e" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated">连接中的每个套接字也使用一个文件描述符。</p></blockquote><h2 id="6d06" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">打开文件系统限制</em></h2><p id="55a3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">可以分配给系统的所有文件句柄的最大数量由内核设置<code class="fe mw mx my mz b"><strong class="lq ir">fs.file-max</strong></code>设定。</p><blockquote class="mk ml mm"><p id="822b" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated"><code class="fe mw mx my mz b"><strong class="lq ir">fs.file-max</strong></code>设置是系统上可以分配和使用的文件句柄的最大总数。</p></blockquote><p id="f38c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">要查看当前分配的文件描述符数量和允许的最大数量，请查看以下文件:</p><pre class="kg kh ki kj gt ns mz pd pe aw pf bi"><span id="32c9" class="ob kx iq mz b gy pg ph l pi oa"># cat /proc/sys/fs/file-nr<br/>1976      0       2048</span></pre><p id="1c77" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">输出显示正在使用的文件描述符的数量是1976，已分配但空闲的文件描述符的数量是0(在内核v2.6+上这将总是显示“0”，意味着已使用和已分配的总是匹配)，最大值是2048。在高性能系统上，这个值应该设置得足够高，以处理系统上所有进程的最大连接数和任何其他文件描述符需求。2048对于这类系统来说是非常低的，1976已经非常接近最大值了。</p><h2 id="edb4" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">打开文件过程限制</em></h2><p id="c909" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">单个进程可以打开的最大文件数量由内核设置<code class="fe mw mx my mz b"><strong class="lq ir">fs.nr_open</strong></code>决定。该设置应不大于<code class="fe mw mx my mz b">fs.file-max</code>的三分之一。默认情况下，<code class="fe mw mx my mz b">fs.nr_open</code>应该足够大，可以容纳系统上运行的任何单个进程，而不需要调整它。</p><blockquote class="mk ml mm"><p id="9a43" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated"><code class="fe mw mx my mz b"><strong class="lq ir">fs.nr_open</strong></code>设置是可以为“打开文件数”或无文件用户限制设置的最大值。</p></blockquote><h2 id="d6fd" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">打开文件用户限制</em></h2><p id="b48e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">除了文件描述符系统和进程限制之外，每个用户还被限制打开文件描述符的最大数量。这是用系统的limits.conf (nofile)设置的，或者如果在systemd (LimitNOFILE)下运行进程，则在进程systemd单元文件中设置。要查看默认情况下用户可以打开的文件描述符的最大数量:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="02b1" class="nw kx iq mz b be nx ny l nz oa">$ ulimit -n<br/>1024</span></pre><p id="da69" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">而在systemd下，以nginx为例:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="2513" class="nw kx iq mz b be nx ny l nz oa">$ systemctl show nginx | grep LimitNOFILE<br/>4096</span></pre><h1 id="a902" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">将打开文件设置更新为所需值</h1><p id="cf95" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有许多指南可以解释如何使这些设置适用于文件系统需要的进程。这是一个详细的方法，在大容量系统上有效，应该对任何系统都有效。</p><h2 id="fc28" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on"> 1。配置打开文件系统限制</em></h2><p id="93fc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">选择一个<strong class="lq ir"> <em class="mn">系统限制</em> </strong>，它将容纳系统上所需的打开文件总数。将单个工作负载进程所需的打开文件数乘以预期运行的进程数。将<code class="fe mw mx my mz b"><strong class="lq ir">fs.max-file</strong></code>内核设置设为这个值，加上一些缓冲。例如，一个系统正在运行4个进程，需要800，000个打开的文件，如果该设置还不够高，可以使用值3200000。</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="6d31" class="nw kx iq mz b be nx ny l nz oa">fs.file-max = 3400000 # (800000 * 4) + 200000</span></pre><h2 id="d127" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on"> 2。配置</em>打开文件<em class="on">过程限制</em></h2><p id="3f5c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">选择一个<strong class="lq ir"> <em class="mn">进程限制</em> </strong>来容纳单个工作负载进程所需的最大打开文件数。例如，工作负载流程最多需要800，000个打开的文件:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="a0c0" class="nw kx iq mz b be nx ny l nz oa">fs.nr_open = 801000</span></pre><h2 id="d735" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on"> 3。配置</em>打开文件<em class="on">用户限制</em></h2><p id="fb5c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">要调整<strong class="lq ir"> <em class="mn">用户限制</em> </strong>以利用系统限制，请将<code class="fe mw mx my mz b"><strong class="lq ir">nofile</strong></code>值设置为所有侦听器的连接套接字所需的打开文件的最大数量加上工作进程所需的任何其他文件描述符，并包括一些缓冲区。用户限制设置在/etc/security/limits.conf下，或/etc/security/limits.d/下的conf文件中，或服务的systemd单元文件中。示例:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="3573" class="nw kx iq mz b be nx ny l nz oa"># cat /etc/security/limits.d/nginx.conf<br/>nginx soft nofile 800000<br/>nginx hard nofile 800000<br/><br/># cat /lib/systemd/system/nginx.service <br/>[Unit]<br/>Description=OpenResty Nginx - high performance web server<br/>Documentation=https://www.nginx.org/en/docs/<br/>After=network-online.target remote-fs.target nss-lookup.target<br/>Wants=network-online.target<br/>[Service]<br/>Type=forking<br/>LimitNOFILE=800000<br/>PIDFile=/var/run/nginx.pid<br/>ExecStart=/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf<br/>ExecReload=/bin/kill -s HUP $MAINPID<br/>ExecStop=/bin/kill -s TERM $MAINPID<br/>[Install]<br/>WantedBy=multi-user.target</span></pre><h1 id="56bf" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">工作线程限制(线程/执行器)</h1><p id="7309" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">与文件描述符限制一样，一个进程可以创建的工作线程数也受到内核设置和用户限制的限制。</p><h2 id="075a" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated"><em class="on">线程系统限制</em></h2><p id="e6b0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">进程可以加速工作线程。可以创建的所有线程的最大数量由内核设置<code class="fe mw mx my mz b"><strong class="lq ir">kernel.threads-max</strong></code>设置。要查看系统上执行的最大线程数和当前线程数，请运行以下命令:</p><p id="0d98" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">获取当前最大线程数:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="1d5e" class="nw kx iq mz b be nx ny l nz oa">cat /proc/sys/kernel/threads-max</span></pre><p id="592b" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">默认值是内存页数除以4。</p><p id="a1fc" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">运行的线程总数:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="4b09" class="nw kx iq mz b be nx ny l nz oa">$ ps -eo nlwp | awk '$1 ~ /^[0-9]+$/ { n += $1 } END { print n }'</span></pre><p id="d2c9" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">只要线程总数低于最大值，服务器就能够为进程创建新的线程，只要它们在用户限制之内。</p><h2 id="9166" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated">线程进程限制</h2><p id="43ae" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">与打开文件限制的内核设置不同，线程没有直接的进程限制设置。这由内核间接处理。</p><p id="c3e0" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">影响线程分叉数量的一个设置是<code class="fe mw mx my mz b"><strong class="lq ir">kernel.pid_max</strong></code>。这将通过限制可用进程id的数量来设置可以同时执行的最大线程数量。增加这个值将允许系统同时执行更多的线程。</p><p id="f668" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">另一个设定是<code class="fe mw mx my mz b"><strong class="lq ir">vm.max_map_count</strong></code>。这控制了每个线程的映射内存区域的数量。一般的经验法则是增加这个值，使系统中预期的并发线程数加倍。</p><h2 id="f2e5" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated">线程<em class="on">用户限制</em></h2><p id="5718" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">除了max threads系统限制之外，每个用户进程都有一个最大线程数限制。这再次用系统的limits.conf ( <code class="fe mw mx my mz b"><strong class="lq ir">nproc</strong></code>)设置，或者如果在systemd ( <code class="fe mw mx my mz b"><strong class="lq ir">LimitNPROC</strong></code>)下运行一个进程，则在进程systemd单元文件中设置。要查看一个进程可以派生的最大线程数()，请执行以下操作:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="d543" class="nw kx iq mz b be nx ny l nz oa">$ ulimit -u<br/>4096</span></pre><p id="7c84" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">而在systemd下，以nginx为例:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="ffb9" class="nw kx iq mz b be nx ny l nz oa">$ systemctl show nginx | grep LimitNPROC<br/>4096</span></pre><h2 id="60df" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated">将线程设置更新为所需值</h2><p id="02ba" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在大多数系统中，<strong class="lq ir"> <em class="mn">系统限制</em> </strong>已经设置得足够高，可以处理高性能服务器所需的线程数量。然而，为了调整系统限制，将<code class="fe mw mx my mz b"><strong class="lq ir">kernel.threads-max</strong></code>内核设置为系统需要的最大线程数，加上一些缓冲。示例:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="2c37" class="nw kx iq mz b be nx ny l nz oa">kernel.threads-max = 3261780</span></pre><p id="356b" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">要调整<strong class="lq ir"> <em class="mn">用户限制</em> </strong>，请将该值设置得足够高，以满足处理流量(包括一些缓冲区)所需的工作线程数量。与<code class="fe mw mx my mz b">nofile</code>一样，<code class="fe mw mx my mz b"><strong class="lq ir">nproc</strong></code>用户限制设置在/etc/security/limits.conf下，或/etc/security/limits.d/下的一个conf文件中，或服务的systemd单元文件中。例如，使用nproc和nofile:</p><pre class="kg kh ki kj gt ns mz pd pe aw pf bi"><span id="ff86" class="ob kx iq mz b gy pg ph l pi oa"># cat /etc/security/limits.d/nginx.conf<br/>nginx soft nofile 800000<br/>nginx hard nofile 800000<br/>nginx soft nproc 800000<br/>nginx hard nproc 800000</span><span id="c64c" class="ob kx iq mz b gy pj ph l pi oa"># cat /lib/systemd/system/nginx.service <br/>[Unit]<br/>Description=OpenResty Nginx - high performance web server<br/>Documentation=<a class="ae kv" href="https://nginx.org/en/docs/" rel="noopener ugc nofollow" target="_blank">https://www.nginx.org/e</a>n/docs/<br/>After=network-online.target remote-fs.target nss-lookup.target<br/>Wants=network-online.target</span><span id="89cf" class="ob kx iq mz b gy pj ph l pi oa">[Service]<br/>Type=forking<br/>LimitNOFILE=800000<br/>LimitNPROC=800000<br/>PIDFile=/var/run/nginx.pid<br/>ExecStart=/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf<br/>ExecReload=/bin/kill -s HUP $MAINPID<br/>ExecStop=/bin/kill -s TERM $MAINPID</span><span id="46db" class="ob kx iq mz b gy pj ph l pi oa">[Install]<br/>WantedBy=multi-user.target</span></pre><h2 id="9270" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated">TIME_WAIT中的TCP反向代理连接</h2><p id="b2df" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在高容量突发流量下，在关闭连接握手期间，陷入“TIME_WAIT”的代理连接可能会占用许多资源。这种状态表示客户端已经从服务器(或上游工作器)接收到最后的<strong class="lq ir"> FIN </strong>包，并且正被保留给任何延迟的传输中的包以进行适当的处理。默认情况下，连接在“TIME_WAIT”中存在的时间是2 x MSL(最大分段长度)，即2x 60秒。在许多情况下，这是正常的预期行为，默认值120秒是可以接受的。但是，当处于“TIME_WAIT”状态的连接数量很大时，这可能会导致应用程序用完临时端口来连接客户端套接字。在这种情况下，通过减少<strong class="lq ir"> FIN </strong>超时，让这些超时更快。</p><p id="6a38" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">控制这个超时的内核设置是<code class="fe mw mx my mz b"><strong class="lq ir">net.ipv4.tcp_fin_timeout</strong></code>，对于高性能服务器来说，一个好的设置是在5到7秒之间。</p><h1 id="32dd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">将这一切结合在一起</h1><p id="d999" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">接收队列</strong>的大小应该能够处理尽可能多的数据包，因为linux可以在不导致丢包的情况下处理网卡，包括一些小的缓冲区，以防峰值比预期的高一点。应该监视softnet_stat文件中丢失的数据包，以发现正确的值。一个好的经验法则是使用为tcp_max_syn_backlog设置的值，至少允许尽可能多的<strong class="lq ir"> SYN </strong>数据包可以被处理以创建半开连接。请记住，这是每个CPU在其接收缓冲区中可以拥有的包的数量，所以用期望的总数除以CPU的数量是保守的。</p><p id="887f" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated"><strong class="lq ir"> SYN backlog队列</strong>的大小应该考虑到高性能服务器上的大量半开连接，以处理突发的偶然峰值流量。一个好的经验法则是，至少将它设置为侦听器在接受队列中可以拥有的最大已建立连接数，但不要高于侦听器可以拥有的已建立连接数的两倍。还建议在这些系统上关闭SYN cookie保护，以避免来自合法客户端的高突发初始连接的数据丢失。</p><p id="a3aa" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated"><strong class="lq ir">接受队列</strong>的大小应允许在高突发流量期间作为临时缓冲区保存大量等待处理的已建立连接。一个好的经验法则是将其设置为工作线程数的20–25%。</p><h2 id="a713" class="ob kx iq bd ky oc od dn lc oe of dp lg lx og oh li mb oi oj lk mf ok ol lm om bi translated">配置</h2><p id="ae4b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">本文使用nginx讨论了以下内核设置。</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="acfd" class="nw kx iq mz b be nx ny l nz oa"># /etc/sysctl.d/99-nginx.conf<br/><br/><br/># /proc/sys/fs/file-max<br/># Maximum number of file handles that can be allocated.<br/>#  aka: open files.<br/># NOTES<br/># - This should be sized to accommodate the number of connections<br/>#    (aka: file handles or open files) needed by all processes.<br/># RECOMMENDATION<br/># - Increase this setting if more high connection processes are<br/>#    started.<br/># SEE ALSO<br/># - /proc/sys/fs/file-nr<br/>fs.file-max = 3400000<br/><br/># /proc/sys/fs/nr_open<br/># Maximum number of file handles that a single process can<br/>#  allocate, aka: open files or connections.<br/># NOTES<br/># - Each process requires a high number of connections to operate.<br/># RECOMMENDATION<br/># - None<br/># SEE ALSO<br/># - net.core.somaxconn<br/># - user limits: nofile<br/>fs.nr_open = 801000<br/><br/><br/># /proc/sys/net/core/somaxconn<br/># Accept Queue Limit, maximum number of established connections<br/>#  waiting for accept() per listener.<br/># NOTES<br/># - Maximum size of accept() for each listener.<br/># - Do not size this less than net.ipv4.tcp_max_syn_backlog<br/># SEE ALSO<br/># net.ipv4.tcp_max_syn_backlog<br/>net.core.somaxconn = 65535<br/><br/># /proc/sys/net/ipv4/tcp_max_syn_backlog<br/># SYN Backlog Queue, number of half-open connections<br/># NOTES<br/># - Example server: 8 cores, can handle over 65535 total half-open<br/>#    connections.<br/># - Do not size this more than net.core.somaxconn<br/># SEE ALSO<br/># - net.core.netdev_max_backlog<br/># - net.core.somaxconn<br/>net.ipv4.tcp_max_syn_backlog = 65535<br/><br/># /proc/sys/net/core/netdev_max_backlog<br/># Receive Queue Size per CPU Core, number of packets.<br/># NOTES<br/># - Example server: 8 cores, each core should at least be able to<br/>#    receive 1/8 of the tcp_max_syn_backlog.<br/># RECOMMENDATION<br/># - Size this to be double the number needed; in the example, 1/4.<br/># SEE ALSO<br/># - net.ipv4.tcp_max_syn_backlog<br/>net.core.netdev_max_backlog = 16386<br/><br/><br/># /proc/sys/net/ipv4/syn_retries<br/># /proc/sys/net/ipv4/synack_retries<br/># Maximum number of SYN and SYN+ACK retries before packet<br/>#  expires.<br/># NOTES<br/># - Reduces connection time to fail<br/>net.ipv4.tcp_syn_retries = 1<br/>net.ipv4.tcp_synack_retries = 1<br/><br/># /proc/sys/net/ipv4/tcp_fin_timeout<br/># Timeout in seconds to close client connections in TIME_WAIT<br/>#  after receiving FIN packet.<br/># NOTES<br/># - Improves socket availability performance, allows for closed<br/>#    connections to be resused more quickly.<br/>net.ipv4.tcp_fin_timeout = 5<br/><br/># /proc/sys/net/ipv4/tcp_syncookies<br/># Disable SYN cookie flood protection.<br/># NOTES<br/># - Only disable this on systems that require a high volume of<br/>#    legal connections in a short amount of time, ie: bursts.<br/>net.ipv4.tcp_syncookies = 0<br/><br/># /proc/sys/kernel/threadsmax<br/># Maximum number of threads system can have, total.<br/># NOTES<br/># - Commented, may not be needed; check system.<br/># SEE ALSO<br/># - user limits.<br/>#kernel.threads-max = 3261780<br/></span></pre><p id="3b89" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">本文讨论了以下用户限制设置:</p><pre class="kg kh ki kj gt ns mz nt bn nu nv bi"><span id="ef39" class="nw kx iq mz b be nx ny l nz oa"># /etc/security/limits.d/nginx.conf<br/>nginx soft nofile 800000<br/>nginx hard nofile 800000<br/>nginx soft nproc 800000<br/>nginx hard nproc 800000</span></pre><h1 id="f1bb" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="12c7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">本文中的设置只是示例，不应该未经测试就直接复制到您的生产服务器配置中。还有一些额外的内核设置会影响网络堆栈的性能。总的来说，这些是我在为高性能连接调优内核时使用的最重要的设置。</p><p id="695a" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">如果这篇文章中的任何信息不准确，请发表评论，我会更新文章以纠正信息。</p></div></div>    
</body>
</html>