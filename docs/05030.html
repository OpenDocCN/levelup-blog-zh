<html>
<head>
<title>GPT-3 A Powerful New Beginning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT-3强有力的新开端</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/gpt-3-a-powerful-new-beginning-d809d21586?source=collection_archive---------8-----------------------#2020-07-30">https://levelup.gitconnected.com/gpt-3-a-powerful-new-beginning-d809d21586?source=collection_archive---------8-----------------------#2020-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e33a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个文本生成神经网络，具有迄今为止最大的训练模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b0a2ced7b3cb54e037f8fc41ca82f766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kFVcrhKY221d-Zlmj5i8RQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">李萨如神经网络，作者图片</figcaption></figure><p id="302d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">OpenAI的GPT-3是一个强大的文本生成神经网络，在迄今为止最大的文本语料库上进行了预训练，能够基于其输入做出不可思议的预测文本响应，是目前为止最强大的语言模型。</p><p id="0b56" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">GPT是“再生”、“再培训”、“再培训”、“再培训”、“再培训”、“再培训”的缩写。OpenAI于2019年2月宣布的GPT-2在<a class="ae lu" href="https://skylion007.github.io/OpenWebTextCorpus/" rel="noopener ugc nofollow" target="_blank"> WebText </a>数据集上进行训练，该数据集包含从Reddit提交的内容中提取的超过800万个文档或38GB的文本数据。2019年11月，GPT-2的最终版本发布，包含15亿个参数的预训练。</p><p id="6014" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">客观地说，GPT-3已经在1750亿个参数和不到1万亿个单词上进行了训练，使其具有15亿个参数的前任GPT-2看起来只有百分之一大小。请注意，GPT-2在不到一年前的2019年11月正式发布。下一个最大的模型是谷歌的T5 T9，它只有110亿个参数。</p><h1 id="3568" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">它擅长什么？</h1><p id="5033" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">GPT-3预测文本和语言的能力是不可思议的。它能够编写功能性的<a class="ae lu" href="https://gpt-tailwind.com/" rel="noopener ugc nofollow" target="_blank">代码</a>，能够以人类声音对话做出响应，生成<a class="ae lu" href="https://openai.com/blog/image-gpt/" rel="noopener ugc nofollow" target="_blank">图像</a>，撰写文章、虚构故事、书籍，甚至是写电子邮件这样的平凡任务。</p><p id="6aa4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">预测并不总是完美的，因为一个GPT-3实际上并不理解这些话的意思。阅读— <strong class="la iu">任何限制？</strong></p><h1 id="0f87" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">简单地说</h1><p id="6a70" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">最简单的，GPT-3采用输入文本的短语，并预测下一个文本输出应该是什么。这种类型的机器学习不会“思考”，它会根据之前训练的数据和运行时翻译器来处理文本输入。</p><p id="40a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">预训练在大规模数据集上进行，包括公共互联网、图书语料库和维基百科。通过大量增加训练样本，它提高了反应的质量和表现。由于体积如此之大，GPT-3的训练成本估计高达500万美元，这给未来GPT-3的下一个版本带来了成本可伸缩性的问题。</p><p id="05b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用于训练GPT-3的主要训练数据集之一来自<a class="ae lu" href="https://commoncrawl.org/" rel="noopener ugc nofollow" target="_blank"> CommonCrawl </a> <a class="ae lu" href="https://commoncrawl.org/," rel="noopener ugc nofollow" target="_blank">，</a>，这是一个免费可用的公共数据集，由包含近万亿个单词的公共网络的抓取组成。CommonCrawl在训练中占了60%的权重，输入了超过4000亿个令牌。</p><h2 id="1c75" class="mt lx it bd ly mu mv dn mc mw mx dp mg lh my mz mi ll na nb mk lp nc nd mm ne bi translated">数据集细分和训练分布</h2><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="bbc4" class="mt lx it ng b gy nk nl l nm nn">dataset       tokens        weight in training<br/>-----------   -----------   ------------------<br/>CommonCrawl   410 billion   60% <br/>WebText2      19 billion    22%<br/>Books1        12 billion    8%<br/>Books2        55 billion    8%<br/>Wikipedia     3 billion     3%</span></pre><h1 id="1b5e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">为什么数据集越大越好？</h1><blockquote class="no np nq"><p id="92ad" class="ky kz lv la b lb lc ju ld le lf jx lg nr li lj lk ns lm ln lo nt lq lr ls lt im bi translated">最近的工作表明，通过对大量文本进行预训练，然后对特定任务进行微调，在许多NLP任务和基准上取得了实质性的进展。虽然这种方法在体系结构上通常是任务不可知的，但它仍然需要特定于任务的数千或数万个示例的微调数据集。<strong class="la iu">相比之下，人类通常可以通过几个例子或简单的指令来完成一项新的语言任务——这是当前的自然语言处理系统仍然很难做到的。在这里，我们表明，扩大语言模型大大提高了任务无关的，少数镜头的性能，有时甚至达到了与现有的最先进的微调方法的竞争力。</strong>具体来说，我们训练GPT-3，这是一个具有1750亿个参数的自回归语言模型，比任何先前的非稀疏语言模型多10倍，并在少数镜头设置中测试其性能。</p></blockquote><p id="1c5c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">来源<a class="ae lu" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> arvix 2005.14165v4 </a>。</p><h1 id="2f20" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">有什么限制吗？</h1><p id="8cbc" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">是的，GPT 3号的<a class="ae lu" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank">创造者</a>认识到了局限性。在文本合成领域:</p><blockquote class="no np nq"><p id="5b10" class="ky kz lv la b lb lc ju ld le lf jx lg nr li lj lk ns lm ln lo nt lq lr ls lt im bi translated">在文本合成方面，虽然总体质量较高，但GPT-3样本有时仍会在文档级别重复语义，在足够长的段落中开始失去连贯性，自相矛盾，偶尔包含不符合逻辑的句子或段落。</p></blockquote><p id="7a84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相比之下，人类能够保持一种持久的精神观点，而GPT-3可能会在较长的段落中失去焦点并“忘记”。</p><p id="11c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在离散的语言任务中，如“常识物理学”:</p><blockquote class="no np nq"><p id="9fd2" class="ky kz lv la b lb lc ju ld le lf jx lg nr li lj lk ns lm ln lo nt lq lr ls lt im bi translated">在离散语言任务领域，我们非正式地注意到，GPT-3似乎在“常识物理学”方面有特殊困难，尽管在测试该领域的一些数据集(如PIQA [BZB+19])上表现良好。<strong class="la iu">具体来说，GPT-3对“如果我把奶酪放进冰箱，它会融化吗？”这类问题有困难。</strong></p></blockquote><p id="fc62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从这篇文章中不清楚“常识物理学”是否可以在未来通过物理数据集的训练来减轻。</p><p id="fe6d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于大多数深度学习系统中的偏见，这是一个普遍而重要的问题:</p><blockquote class="no np nq"><p id="562e" class="ky kz lv la b lb lc ju ld le lf jx lg nr li lj lk ns lm ln lo nt lq lr ls lt im bi translated">最后，GPT-3具有大多数深度学习系统共有的一些限制——它的决定不容易解释，它对新输入的预测不一定校准良好，这可以通过在标准基准上比人类高得多的表现方差来观察，并且它保留了它已经训练过的数据的偏差。</p></blockquote></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="4eb6" class="lw lx it bd ly lz ob mb mc md oc mf mg jz od ka mi kc oe kd mk kf of kg mm mn bi translated">结论</h1><p id="67a2" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">GPT-3是一个重大的飞跃，但更多的是在规模上，因为它与GPT-2有许多相同的架构。它真正要做的，是测试语言模型的缩放假设，以大幅提高性能。</p><p id="bccb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一些更有前景的领域是作为增强创造力工具的GPT 3，一般写作，代码生成，或代码合作试点。这不是将接管世界的人工智能，但应该作为一个重要时刻被记住。</p></div></div>    
</body>
</html>