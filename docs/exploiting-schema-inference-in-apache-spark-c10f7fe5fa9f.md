# 利用 Apache Spark 中的模式推理

> 原文：<https://levelup.gitconnected.com/exploiting-schema-inference-in-apache-spark-c10f7fe5fa9f>

![](img/aef41b1ee28d939ecfc3e7be7e6157f2.png)

Apache Spark 最大的特性之一是能够动态推断模式。读取数据并生成模式虽然很容易使用，但会使数据读取速度变慢。但是，有一个技巧可以一次性生成模式，然后从磁盘加载它。让我们开始吧！

就本文而言，让我们假设我们正在使用具有复杂数据结构的数据，其中创建任何种类的 case 类或 struct 类型都将有一百行长。

# 保存模式

让我们从读取一些样本数据并对其实施模式推理开始。使用 Apache Spark 和 Scala 语言，可以像下面这样完成:

```
val carsDf = spark.read
    .option("inferSchema", "true")
    .json("src/main/resources/data/cars.json")
```

我将一个样本 JSON 文件(`cars.json`)读入一个名为`carsDf`的数据帧。该数据集的示例记录如下所示:

```
{
   "Name":"chevrolet chevelle malibu",
   "Miles_per_Gallon":18,
   "Cylinders":8,
   "Displacement":307,
   "Horsepower":130,
   "Weight_in_lbs":3504,
   "Acceleration":12,
   "Year":"1970-01-01",
   "Origin":"USA"
}
```

正如我们在上面看到的，这个模式非常容易理解。现在，让我们看看 Spark 从中推断出了什么:

```
val schemaJson = carsDf.schema.json
println(schemaJson)
```

此操作的输出可能是这样的:

```
{
   "type":"struct",
   "fields":[
      {
         "name":"Acceleration",
         "type":"double",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Cylinders",
         "type":"long",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Displacement",
         "type":"double",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Horsepower",
         "type":"long",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Miles_per_Gallon",
         "type":"double",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Name",
         "type":"string",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Origin",
         "type":"string",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Weight_in_lbs",
         "type":"long",
         "nullable":true,
         "metadata":{ }
      },
      {
         "name":"Year",
         "type":"string",
         "nullable":true,
         "metadata":{ }
      }
   ]
}
```

这就是 Spark 如何定义它设法推断的模式。现在，每次我们启动 Spark 作业时，都必须重新创建这些信息。当进行开发时，会有很多这样的事情，所以我们在模式的重建上浪费了很多时间。让我们尝试生成一次，并在连续读取中重用它。

在我使用的 Scala 中，模式的 JSON 文件可以保存到本地文件系统，代码如下:

```
val file = new File("src/resources/schema.json")
val bw = new BufferedWriter(new FileWriter(file))
bw.write(schemaJson)
bw.close()
```

这样，我们推断出的模式将作为名为`schema.json`的 JSON 文件保存到本地文件系统中。下一步，我们将看到如何阅读它，并用它来加速火花。

# 加载模式

现在，让我们看看如何使用 Spark 和 Scala 环境读取 JSON 文件。这可以使用以下代码来完成:

```
import org.apache.spark.sql.types.{DataType, StructType}val schemaJson = Source
    .fromFile("src/resources/schema.json")
    .getLines
    .mkStringval schemaStructType = Try(DataType.fromJson(schemaJson))
        .getOrElse(LegacyTypeStringParser.parse(schemaJson)) 
    match {
      case t: StructType => t
      case _ => throw new RuntimeException(s"Failed parsing JSON Schema")
    }
```

由于 Spark SQL 包，上面的代码部分从 JSON 文件中读取模式，并将其解析为一个`StructType`实例。我在这里使用了模式匹配，以适应所使用的不正确路径。

有了预生成的模式，在 Spark 中读取数据将会更快。使用我们的带有`cars.json`文件的例子，我们可以这样读取这个数据:

```
val carsDf = spark.read
    .schema(schemaStructType)
    .json("src/main/resources/data/cars.json")
```

# 摘要

我希望这篇文章对你有用。如果有，不要犹豫，喜欢或分享这个帖子。此外，如果你愿意，你可以在我的社交媒体上关注我🙂