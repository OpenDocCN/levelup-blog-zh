<html>
<head>
<title>Quick Primer on Distributed Training with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch分布式培训快速入门</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/quick-primer-on-distributed-training-with-pytorch-ad362d8aa032?source=collection_archive---------11-----------------------#2020-01-08">https://levelup.gitconnected.com/quick-primer-on-distributed-training-with-pytorch-ad362d8aa032?source=collection_archive---------11-----------------------#2020-01-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6f73be5ba6b071ae74604c2768c9e6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXETcUSVOrmVGnMsw0zRJw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">由Unsplash网站上的Joey Kyber </figcaption></figure><div class=""/><h1 id="4f1f" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">简介</strong></h1><p id="76e5" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">深度学习算法是众所周知的数据饥渴。对于一个相当大的模型来说，大型数据集需要几天甚至几周的时间才能收敛到一个好的解决方案，这是很常见的。它们的功能和灵活性是有代价的，因为通常有几个超参数旋钮——网络结构、学习速率以及许多其他模型特性——需要仔细调整。由于它们的高度经验性，就像大多数学习算法一样，这种设置可以很容易地组合爆炸，导致在有限或共享资源上进行数百或数千次长期运行的实验。</p><p id="9279" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">运行大量的实验无疑增加了偶然发现一个优秀模型的机会，但要使它切实可行，还需要更快更可扩展的训练。深度神经网络通常使用反向传播算法进行训练，该算法本质上是经典的迭代梯度下降(GD)函数优化方法，通常使用<a class="ae jg" href="https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms" rel="noopener ugc nofollow" target="_blank">操作风格</a>进行丰富，使其在处理更大的数据集和复杂的损失表面时更加鲁棒和高效。</p><p id="15dd" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在优化训练时间时，有几个方面需要考虑，它们围绕着高效的数据处理、计算和通信。训练通常以批处理模式进行——在单次迭代中一次处理一批训练数据，以进行模型更新(因此，也称为批处理随机梯度下降或批处理SGD)。在这种情况下，许多模型计算可以表示为矩阵代数运算，这可以利用GPU设备的大规模并行化能力。虽然这种功能比基于CPU的执行有显著的提高，但GPU卡上的可用内存会限制批处理的大小。显而易见的下一步，一个相当容易实现的结果，是利用多个GPU，并简单地在数据维度上分配计算。当然，只有在处理速度没有被在多个GPU上管理训练所增加的通信开销抵消的情况下，这才有意义。</p><p id="835c" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">分布式深度学习的实现很棘手，有许多陷阱，可能会导致难以检测的细微错误。幸运的是，大多数流行的框架都提供了经过良好测试和基准测试的工具和实用程序来扩展培训工作。在这篇文章中，我将讨论由<a class="ae jg" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>提供的关键策略，特别关注数据并行性(最流行的方法)，即数据集在多个worker/GPU上进行分区和处理。一个假设是模型足够小，可以放在单个GPU的内存中。这里不讨论的其他策略还有<a class="ae jg" href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html" rel="noopener ugc nofollow" target="_blank">模型并行</a>和<a class="ae jg" href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255" rel="noopener">梯度累加</a>，更适合于特别大的模型，或者甚至几个训练数据样本都装不进GPU内存的时候。</p><p id="4de9" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这篇文章的源代码可以在<a class="ae jg" href="https://github.com/hgrover/pytorchdistr.git" rel="noopener ugc nofollow" target="_blank">https://github.com/hgrover/pytorchdistr.git</a>获得。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><p id="d3ee" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi mo translated"><span class="l mp mq mr bm ms mt mu mv mw di"> P </span> yTorch提供了两种并行化训练作业的主要方法——<strong class="lg jk">数据并行</strong>和<strong class="lg jk">分布式数据并行</strong>。每种方法都包含一个包装器，它:a)封装所有较低层次的通信细节，以协调和同步分布式培训；以及b)为最终用户展示了一个干净的API。除非对代码进行一些小的修改，否则使用包装模型的体验与使用本地的非分布式模型几乎是一样的。</p><h2 id="7e3e" class="mx kh jj bd ki my mz dn km na nb dp kq lp nc nd ku lt ne nf ky lx ng nh lc ni bi translated"><strong class="ak">数据并行</strong></h2><p id="7ed5" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae jg" href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" rel="noopener ugc nofollow" target="_blank"> DataParallel (DP) </a>是更简单、更直接的方法，只需要最少的努力和对训练代码的修改。它的工作方式是将一个训练数据批次分割成更小的、大小相似的子批次，子批次的数量与可用的GPU数量相等。然后，这些子批次在各自的GPU上并行处理。</p><p id="53d8" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一个GPU充当主GPU(默认为GPU:0)，它协调整个流程。在SGD的每次迭代中，主:a)在每个可用的GPU上复制并广播模型权重(<strong class="lg jk"> <em class="nj">复制</em> </strong>步骤)；b)拆分训练数据批次，并将作业命令发送到各个GPU(<strong class="lg jk"><em class="nj">分散</em> </strong>步骤)。这使得每个GPU能够处理其自己的数据子批次，以计算关于模型参数的局部损失和梯度(<strong class="lg jk"> <em class="nj"> parallel_apply </em> </strong>步骤)。一旦完成，主GPU: c)从每个GPU收集局部梯度(<strong class="lg jk"> <em class="nj">收集</em> </strong>步骤)；d)、聚集梯度并执行模型更新(<strong class="lg jk"> <em class="nj">更新</em> </strong>步骤)。因此，在每次迭代开始时，所有GPU都被提供完全相同的模型参数。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/dfc8851fdda38c16ac3d7b31c1c99b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*_7vBLdKpLaZ1XTcIGoshXQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd ki"> DataParallel: </strong>单节点，多GPU设置；作为具有多个线程的单个进程来实现，以管理工作线程</figcaption></figure><p id="cedc" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">所有这些步骤都是由PyTorch库执行的。作为一个最终用户，对代码的唯一修改是将模型对象包装在DataParallel中，如下面的要点所示。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="3970" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">虽然当试图从多个可用的GPU中挤出性能时，DP是有效的第一线攻击，但至少有一些限制:a)它遵循单进程、多线程设计，因此仅在单个节点上工作；b)由于在主设备和工作设备之间的每次迭代中的数据传输，它需要大量的通信开销；c)主GPU显然有更多的职责，因此通常比混合中的其他GPU工作更努力(不对称的工作负载)。事实上，在更新步骤中，主设备独自工作来更新模型参数，而集群中的其他设备则等待更新，因此未得到充分利用。</p><h2 id="d27e" class="mx kh jj bd ki my mz dn km na nb dp kq lp nc nd ku lt ne nf ky lx ng nh lc ni bi translated"><strong class="ak">分布式数据并行</strong></h2><p id="5233" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">与DP一样，<a class="ae jg" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributeddataparallel" rel="noopener ugc nofollow" target="_blank">distributed data parallel(DDP)</a>也在数据维度上实现了并行化。它解决了DP的一些基本限制；特别是，DDP在单节点和多节点环境中都可以工作。DDP的实现涉及多个python流程，这些流程必须在代码中的适当位置进行协调和同步。这在工作人员之间提供了更大的工作量平衡，并确保了数据传输带来的更低的通信开销。由于这些优点，DDP也是单节点环境中的推荐方法。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ca044d51098894dc7067eb83b455fbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*XCr_BL4deLtUkLcb8yZzBw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd ki">分布式数据并行:</strong>多节点、多GPU设置；每个worker都由自己的python进程管理。</figcaption></figure><p id="668e" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">与DP的一个关键区别是，在更新模型参数时，它遵循不同的协议。与DP不同，它不是在主节点上收集本地梯度以执行聚合和模型更新，然后将这些更新广播回工作器，而是使用高效的<a class="ae jg" href="https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da" rel="noopener" target="_blank"> all-reduce操作</a>在所有GPU上聚合梯度，该操作使平均梯度对所有GPU工作器可用，然后每个GPU工作器可以并行更新其自己的模型副本。</p><p id="a9d6" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在幕后，PyTorch在代码中的关键点执行同步，因此不会遗漏任何GPU。结果，并且因为模型在训练开始之前被预先复制在每个GPU上，所以在每次迭代结束时，所有模型副本都具有相同的更新参数。</p><p id="53b4" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">根据<a class="ae jg" href="https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel" rel="noopener ugc nofollow" target="_blank"> PyTorch docs </a>的说法，DDP最有效的方法是分配一个进程来管理每个GPU设备。在培训开始之前，必须启动并初始化所有流程，以便进行通信和同步。有多种<a class="ae jg" href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods" rel="noopener ugc nofollow" target="_blank">方法可以实现这个目标</a>——共享文件系统、环境变量和TCP。在本文中，我使用了由PyTorch提供的<a class="ae jg" href="https://pytorch.org/docs/stable/distributed.html#launch-utility" rel="noopener ugc nofollow" target="_blank">启动器实用程序</a>填充的环境变量。这为所有进程的协同操作设置了所需的环境，包括一个用于所有进程协调的<em class="nj">主地址</em>和<em class="nj">端口</em>，表示同步时要等待的进程总数的<em class="nj">世界大小</em>，以及每个进程的<em class="nj">等级</em>(每个节点上的<em class="nj">本地</em>和跨节点的<em class="nj">全局</em>)。下面的要点显示了单节点和多节点启动的关键步骤和相关命令。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="2e36" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">启动后，每个进程的第一步是调用阻塞的<em class="nj"> init_process_group </em>函数，使进程之间建立通信，并建立初始化(此处为env vars)和后端选择的过程(有<a class="ae jg" href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends" rel="noopener ugc nofollow" target="_blank">多个可用的</a> — GLOO、NCCL、MPI。在这个例子中，我使用了NCCL)。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="2960" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一旦初始化了所有进程，下一步就是将每个进程映射到它将控制的GPU设备。这必须在客户端代码级别进行管理。<em class="nj">本地等级</em>(由上述启动器工具设置)指特定节点上进程的等级，从0到该特定节点上启动的进程数量。然后，这可以用于索引由该特定进程管理的GPU设备。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="2d1d" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这也确保了每个进程/ GPU，结合一个<a class="ae jg" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler" rel="noopener ugc nofollow" target="_blank">分布式数据采样器对象</a>，处理它们自己的数据子集。每个节点上的每个GPU并行处理自己的一批训练数据。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="16ed" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如前所述，一旦执行了上述初始化和设置步骤，DDP包装器就会编排训练。从最终用户的角度来看，这是一个展开模型的替代。请注意，由于已经使用<em class="nj">本地等级</em>为每个进程适当地设置了GPU设备，因此通过<em class="nj"> cuda() </em>方法调用，每个工人的模型副本会自动移动到正确的设备。类似地，device_ids和output_device在DDP包装构造函数调用中被正确地实例化。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="d477" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是利用PyTorch的本地分布式训练包装器所需要做的全部工作。</p><p id="75a5" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在多个GPU设备上扩展SGD的并行实现策略暴露了训练效率和模型准确性之间的<a class="ae jg" href="https://arxiv.org/pdf/1709.05011.pdf" rel="noopener ugc nofollow" target="_blank">权衡</a>，尤其是在使用大量设备时。这是一个活跃的研究领域，已经提出了各种方法和技巧来根据经验管理这种权衡。虽然本文中讨论的PyTorch特性的基本用法足以满足许多实际用例以及适度的扩展，但是PyTorch还公开了<a class="ae jg" href="https://arxiv.org/pdf/1711.00705.pdf" rel="noopener ugc nofollow" target="_blank">低级通信原语</a>来实现更高级或新颖的策略。</p><p id="75d0" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">此外，最近的发展——如<a class="ae jg" href="https://rapids.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg jk"> RAPIDS数据科学平台</strong></a><strong class="lg jk">——</strong>可能会进一步显著提升，因为它直接解决了使用大型实例集群时通信开销的核心问题，否则这可能会显著降低分布式培训可实现的收益。在后续的文章中会有更多的介绍。</p><p id="9df6" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg jk">附加说明</strong></p><p id="711f" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">出于本文演示的目的，我使用并改编了PyTorch文档 <strong class="lg jk">中一个<a class="ae jg" href="https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html" rel="noopener ugc nofollow" target="_blank">示例的大部分代码和数据集。</a></strong></p><p id="d01d" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">所有测试都是在高性能计算(HPC)环境中完成的，其中我们的计算节点配备了4或8个NVIDIA Tesla V100 GPU卡，每个卡都有16GB的设备上内存。每个节点还可以访问可以从中加载数据集的共享卷。在RedHat Enterprise v7.4平台上，使用PyTorch v1.3.1、cuda toolkit v10.1和cuDNN v7.6运行测试。在单个节点(最多8个GPU)上成功执行了DP脚本，在多个节点(最多4个，每个节点4个GPU)上成功执行了DDP脚本。python 3 . 6 . 5版用于所有测试。</p></div></div>    
</body>
</html>