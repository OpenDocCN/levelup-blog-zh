<html>
<head>
<title>ETL and Machine Learning using Pyspark on AWS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AWS上使用Pyspark进行ETL和机器学习</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/etl-and-machine-learning-using-pyspark-on-aws-d4e0ed35bfdd?source=collection_archive---------21-----------------------#2020-06-29">https://levelup.gitconnected.com/etl-and-machine-learning-using-pyspark-on-aws-d4e0ed35bfdd?source=collection_archive---------21-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d270" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我已经使用AWS EMR服务为数据集开发了ETL、EDA和ML管道。我正在分享我的管道来帮助其他开发者，同时也收到一些建设性的反馈(非常感谢)。关于Jupyter笔记本和建议，请查看我的GitHub repo:</p><p id="740e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/armand-hoxha25/DSND_Capstone" rel="noopener ugc nofollow" target="_blank">https://github.com/armand-hoxha25/DSND_Capstone</a></p><h1 id="5c64" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">AWS设置</h1><p id="ce82" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">对于这个数据集，我使用以下配置创建了一个AWS EMR:</p><ul class=""><li id="699c" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn lx ly lz ma bi translated"><strong class="js iu"> 3个m5.xlarge，</strong> 4个vCore，16 GiB内存，仅EBS存储，EBS存储:64 GiB</li><li id="7b77" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated">软件包括:Hive 2.3.6，Pig 0.17.0，Hue 4.4.0，Spark 2.4.4，Livy 0.6.0</li></ul><p id="47df" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我还建议使用以下配置JSON:</p><p id="3b85" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[{"classification":"livy-conf "，" properties ":{ " livy . server . session . time out ":" 8h " }，" configurations":[]}，{"classification":"spark "，" properties ":{ " spark . executor . memory ":" 8g "，" spark.driver.memory":"24g "，" spark . py spark . virtualenv . enabled ":" True " }，" configurations":[]}]</p><p id="8856" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的配置将允许您运行超过一个小时的执行计算，允许Spark使用24GB的内存，并允许虚拟环境从PyPi安装包。</p><h1 id="85e9" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">导入库</h1><p id="4ca0" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我将使用的主要库是Pyspark，目的是在AWS EMR集群上运行它，因为数据大约为16GB，无法一次加载到大多数计算机的内存中(有分块加载数据的步骤，但这超出了本项目的范围)。我将使用PySpark加载数据，清理，特征提取以及机器学习。对于一些EDA，我将使用Pandas、Numpy、Matplotlib和Seaborn在特征提取之后，我期望数据大小被显著减小，以便它可以被加载到存储器中。一旦数据在内存中变得更容易读取，使用pandas和其他图形库来显示结果就更容易了。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="709d" class="mp kq it ml b gy mq mr l ms mt">from pyspark.sql import SparkSession<br/>from pyspark.sql import functions as F<br/>from pyspark.sql.types import *<br/>from pyspark.ml.feature import MinMaxScaler<br/>from pyspark.sql import types as T<br/>from pyspark.ml.linalg import Vectors, VectorUDT<br/>from pyspark.sql.functions import lit, udf, struct, countDistinct, collect_list, avg, count, col<br/>from pyspark.sql.types import ArrayType, BooleanType, LongType, FloatType<br/>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.ml.classification import GBTClassifier<br/>from pyspark.sql.functions import *<br/><br/># Create spark session<br/>spark = SparkSession \<br/>    .builder \<br/>    .appName("Sparkify") \<br/>    .getOrCreate()<br/>spark.sparkContext.install_pypi_package("pandas")<br/>spark.sparkContext.install_pypi_package("sklearn")<br/>spark.sparkContext.install_pypi_package("matplotlib")<br/>spark.sparkContext.install_pypi_package("seaborn")<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.metrics import roc_curve<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span></pre><h1 id="cbc9" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">输入数据</h1><p id="db66" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我最初导入了名为<code class="fe mu mv mw ml b"><strong class="js iu">mini_sparkify_event_data.json</strong></code>的小数据集，我使用这个小数据集是为了在我的本地计算机上开发我的初始工作流。当我接近完成我的项目时，我在AWS EMR集群上使用了完整的数据集。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="e25b" class="mp kq it ml b gy mq mr l ms mt">event_data = "s3n://udacity-dsnd/sparkify/sparkify_event_data.json"<br/>df = spark.read.json(event_data)<br/>df.createOrReplaceTempView("data")<br/>print('shape of data is {}, {}'.format(df.count(),len(df.columns)))</span><span id="065c" class="mp kq it ml b gy mx mr l ms mt">shape of data is 26259199, 18</span></pre><p id="af63" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据是什么样的？查看模式将有助于理解列。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="ebbb" class="mp kq it ml b gy mq mr l ms mt">df.printSchema()<br/>root<br/> |-- artist: string (nullable = true)<br/> |-- auth: string (nullable = true)<br/> |-- firstName: string (nullable = true)<br/> |-- gender: string (nullable = true)<br/> |-- itemInSession: long (nullable = true)<br/> |-- lastName: string (nullable = true)<br/> |-- length: double (nullable = true)<br/> |-- level: string (nullable = true)<br/> |-- location: string (nullable = true)<br/> |-- method: string (nullable = true)<br/> |-- page: string (nullable = true)<br/> |-- registration: long (nullable = true)<br/> |-- sessionId: long (nullable = true)<br/> |-- song: string (nullable = true)<br/> |-- status: long (nullable = true)<br/> |-- ts: long (nullable = true)<br/> |-- userAgent: string (nullable = true)<br/> |-- userId: string (nullable = true)</span></pre><p id="a3ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个条目看起来像什么？</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="b797" class="mp kq it ml b gy mq mr l ms mt">df.head(1)</span><span id="d4cc" class="mp kq it ml b gy mx mr l ms mt">[Row(artist='Popol Vuh', auth='Logged In', firstName='Shlok', gender='M', itemInSession=278, lastName='Johnson', length=524.32934, level='paid', location='Dallas-Fort Worth-Arlington, TX', method='PUT', page='NextSong', registration=1533734541000, sessionId=22683, song='Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent='"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36"', userId='1749042')]</span></pre><p id="c16b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到，每一行都包含非常有用的信息，比如级别、页面，以及一些我们不知道有多大用处的信息，比如性别、ts和itemInSession。还有一些我不会使用的列，比如名字、姓氏、位置、方法、注册、状态、用户代理。<br/> <strong class="js iu"> userId </strong>将是用于为每个用户提取特性的密钥。</p><h1 id="ab92" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">缺少值</h1><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="b93d" class="mp kq it ml b gy mq mr l ms mt">predrop=df.count()<br/>df=df.dropna()<br/>postdrop = df.count()<br/>print('{} rows were dropped, NaNs account for {} % of the data'\<br/>      .format(predrop-postdrop,100*(predrop-postdrop)/predrop))<br/># there are no NaNs</span><span id="d3ed" class="mp kq it ml b gy mx mr l ms mt">5408927 rows were dropped, NaNs account for 20.59821779026847 % of the data</span></pre><h1 id="2f3b" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">删除列</h1><p id="3593" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我不打算使用这些数据中的所有栏目，其中一些栏目无助于得出各组之间的统计差异，只会给数据增加不必要的信息。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="429b" class="mp kq it ml b gy mq mr l ms mt">drop_columns =['artist', 'firstName','lastName','song', 'useAgent','location','method',\<br/>               'registration']<br/>for col in drop_columns:<br/>    df=df.drop(col)</span></pre><h1 id="e08e" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">特征工程</h1><p id="ef82" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在这个数据集中，用户的动作被描绘在<code class="fe mu mv mw ml b">page</code>列中。以下是页面列的所有可能值:</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="1be6" class="mp kq it ml b gy mq mr l ms mt">pages=[x.page for x in spark.sql('''SELECT DISTINCT(page) FROM data''').collect()]<br/>print(pages)</span><span id="d535" class="mp kq it ml b gy mx mr l ms mt">['Cancel', 'Submit Downgrade', 'Thumbs Down', 'Home', 'Downgrade', 'Roll Advert', 'Logout', 'Save Settings', 'Cancellation Confirmation', 'About', 'Submit Registration', 'Settings', 'Login', 'Register', 'Add to Playlist', 'Add Friend', 'NextSong', 'Thumbs Up', 'Help', 'Upgrade', 'Error', 'Submit Upgrade']</span></pre><p id="288b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">“流失”指标是当用户进入“提交降级”页面，有效地将他们的帐户级别从“付费”降至“免费”。在这些数据中，我们只对使用“付费”服务的用户感兴趣。<br/> <br/>鉴于用户操作将构成该数据集中的要素，我通过计算用户完成每个操作的次数以及他们使用该应用程序的总时间来创建要素数据框架。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="5a64" class="mp kq it ml b gy mq mr l ms mt">labeler = lambda x: 1 if x&gt;0 else 0 # a small function o generate labels<br/>                                    #if a user goes through the submit downgrade page<br/>labelfunc = F.udf(labeler,IntegerType())<br/>convert_to_int=F.udf(int,IntegerType())<br/># this SQL Query will take care of the feature exctraction from the data<br/>features=spark.sql('''<br/>SELECT BIGINT(userId),<br/>gender,<br/>BIGINT(COUNT(page)) AS n_pages,<br/>BIGINT(COUNT(IF(page='Submit Downgrade',1,NULL))) AS n_sub_downgrade,<br/>BIGINT(COUNT(IF(page='Thumbs Down',1,NULL))) AS n_thumb_down,<br/>BIGINT(COUNT(IF(page='Home',1,NULL))) AS n_home,<br/>BIGINT(COUNT(IF(page='Downgrade',1,NULL))) AS n_downgrade,<br/>BIGINT(COUNT(IF(page='Roll Advert',1,NULL))) AS n_roll_advert,<br/>BIGINT(COUNT(IF(page='Cancellation Confirmation',1,NULL))) AS n_cancellation,<br/>BIGINT(COUNT(IF(page='About',1,NULL))) AS n_about,<br/>BIGINT(COUNT(IF(page='Submit Registration',1,NULL))) AS n_submit_registration,<br/>BIGINT(COUNT(IF(page='Cancel',1,NULL))) AS n_cancel,<br/>BIGINT(COUNT(IF(page='Login',1,NULL))) AS n_login,<br/>BIGINT(COUNT(IF(page='Register',1,NULL))) AS n_register,<br/>BIGINT(COUNT(IF(page='Add to Playlist',1,NULL))) AS n_add_playlist,<br/>BIGINT(COUNT(IF(page='NextSong',1,NULL))) AS n_nextsong,<br/>BIGINT(COUNT(IF(page='Thumbs Up',1,NULL))) AS n_thumb_up,<br/>BIGINT(COUNT(IF(page='Error',1,NULL))) AS n_error,<br/>BIGINT(COUNT(IF(page='Submit Upgrade',1,NULL))) AS n_submit_upgrade,<br/>BIGINT(CEILING(SUM(length))) AS total_length<br/>FROM data <br/>GROUP BY userId,gender<br/>''')<br/><br/>features = features.withColumn("gender", F.when(F.col("gender")=="M", 0).otherwise(1))<br/>features = features.withColumn("label",labelfunc("n_sub_downgrade"))<br/>features.createOrReplaceTempView("features")<br/>print("Nrows before dropping NaNs:",features.count())<br/>features=features.dropna()<br/>print("Nrows AFTER dropping NaNs:",features.count())</span><span id="0a28" class="mp kq it ml b gy mx mr l ms mt">Nrows before dropping NaNs: 22278<br/>Nrows AFTER dropping NaNs: 22261</span><span id="ba08" class="mp kq it ml b gy mx mr l ms mt">features.printSchema()</span><span id="c384" class="mp kq it ml b gy mx mr l ms mt">root<br/> |-- userId: long (nullable = true)<br/> |-- gender: integer (nullable = false)<br/> |-- n_pages: long (nullable = false)<br/> |-- n_sub_downgrade: long (nullable = false)<br/> |-- n_thumb_down: long (nullable = false)<br/> |-- n_home: long (nullable = false)<br/> |-- n_downgrade: long (nullable = false)<br/> |-- n_roll_advert: long (nullable = false)<br/> |-- n_cancellation: long (nullable = false)<br/> |-- n_about: long (nullable = false)<br/> |-- n_submit_registration: long (nullable = false)<br/> |-- n_cancel: long (nullable = false)<br/> |-- n_login: long (nullable = false)<br/> |-- n_register: long (nullable = false)<br/> |-- n_add_playlist: long (nullable = false)<br/> |-- n_nextsong: long (nullable = false)<br/> |-- n_thumb_up: long (nullable = false)<br/> |-- n_error: long (nullable = false)<br/> |-- n_submit_upgrade: long (nullable = false)<br/> |-- total_length: long (nullable = true)<br/> |-- label: integer (nullable = true)</span></pre><h1 id="a783" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">电子设计自动化(Electronic Design Automation)</h1><p id="eff4" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">既然计算了特征矩阵，就很容易将这个较小的数据集导入pandas，并利用pandas dataframe函数。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="6d8b" class="mp kq it ml b gy mq mr l ms mt">featuredf = features.toPandas()</span></pre><h2 id="d71e" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated">特征相关性和差异</h2><p id="f68c" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">值得探索的是，我们的特征之间是否存在某种相关性，以及这些相关性在涉及用户是否感到不适时是否显著。</p><p id="574f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我选择使用肯德尔的Tau相关性，因为我更感兴趣的是事物是否倾向于向同一个方向移动，而不是以相同的量移动。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="8edf" class="mp kq it ml b gy mq mr l ms mt">plt.figure(figsize=(12,12),facecolor='white')<br/>corr = featuredf.corr(method='kendall')<br/>mask = np.zeros_like(corr)<br/>mask[np.triu_indices_from(mask)] = True<br/>sns.heatmap(corr,annot=True, fmt='.1f', square=True, mask = mask)<br/>%matplot plt #command needed to plot on AWS EMR</span></pre><figure class="mg mh mi mj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nj"><img src="../Images/37106c11cc4ae8491ff829be8dea304c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQPtWMCImHix1E7tDle_lg.png"/></div></div></figure><p id="21c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到n_add_playlist、n_nextsong、n_thump_up和n_length等特征之间的明显相关性。这种相关性是有意义的，因为它们表示用户与服务交互的时间；因此，用户与服务的交互越多，他们的动作就越多，这也由n_length和n_pages的高相关性来表示。<br/>我主要对<code class="fe mu mv mw ml b">label</code>的相关性感兴趣，因为这是我试图预测的。我不会去调查n _ sub _ downgrade(因为<code class="fe mu mv mw ml b">label</code>是利用该专栏的信息得出的)。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="7305" class="mp kq it ml b gy mq mr l ms mt">plt.figure(figsize=(12,12),facecolor='white')<br/>plt.subplot(2,2,1)<br/>sns.scatterplot(x='n_pages',y='n_error',hue='label',data=featuredf,<br/>               palette = ['#000000', '#FF0000'])<br/>plt.subplot(2,2,2)<br/>sns.scatterplot(x='n_thumb_up',y='n_thumb_down',hue='label',data=featuredf,<br/>               palette = ['#000000', '#FF0000'])<br/>plt.subplot(2,2,3)<br/>sns.scatterplot(x='n_downgrade',y='total_length',hue='label',data=featuredf,<br/>               palette = ['#000000', '#FF0000'])<br/>plt.subplot(2,2,4)<br/>sns.scatterplot(x='n_submit_upgrade',y='n_downgrade',hue='label',data=featuredf,<br/>               palette = ['#000000', '#FF0000'])<br/>%matplot plt</span></pre><figure class="mg mh mi mj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nj"><img src="../Images/9404d9d205453e6528a5a8a7a1834f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyBPsxu1FPB7LHbrC0Evbg.png"/></div></div></figure><p id="32ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">很难确定是否有一个直接的关联图，可以清楚地区分哪些用户会呕吐，哪些不会。但是从第四个情节来看，很明显，多次进入提交升级页面的用户以后有流失的趋势。这些用户可能正在争论是否升级到该平台。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="43fc" class="mp kq it ml b gy mq mr l ms mt">plt.figure(figsize=(12,12),facecolor='white')<br/>plt.subplot(2,2,1)<br/>sns.boxplot(x='label',y='total_length',hue='label',data=featuredf,<br/>               palette = ['#95a5a6', '#FF0000'])<br/>plt.subplot(2,2,2)<br/>sns.boxplot(x='label',y='n_thumb_up',hue='label',data=featuredf,<br/>               palette = ['#95a5a6', '#FF0000'])<br/>plt.subplot(2,2,3)<br/>sns.boxplot(x='label',y='n_error',hue='label',data=featuredf,<br/>               palette = ['#95a5a6', '#FF0000'])<br/>plt.subplot(2,2,4)<br/>gendersums=featuredf.groupby('gender').sum()<br/>gendercount=featuredf.groupby('gender').count()<br/>vals=100*gendersums['label']/gendercount['userId']<br/>sns.barplot(x=['M' , 'F'],y=vals,<br/>               palette = ['#95a5a6', '#FF0000'])<br/>plt.xlabel('% of users who dropped from each gender');<br/>%matplot plt</span></pre><figure class="mg mh mi mj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nj"><img src="../Images/38457c619d5fd034e3f75e58c6b25b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZEzZOzqCumxPil-NU_SpQ.png"/></div></div></figure><p id="5659" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以在这些图中观察到一些差异。流失的用户倾向于使用该服务的时间更长。这项服务正在努力留住用户。抱怨的用户也倾向于在服务中发现更多的错误。在我最初分析的数据子集中，显示女性用户比男性用户有更高的流失率，然而，在更大的数据集中，这两个比率几乎相等。</p><h1 id="97e4" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">机器学习管道</h1><h2 id="63c2" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated"><strong class="ak">管道&amp;指标</strong></h2><p id="f93a" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">对于这个项目，我将使用一个以上的学习模型，并比较他们如何执行相互比较。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="0497" class="mp kq it ml b gy mq mr l ms mt">print("{} % of the users churned".format\<br/>      (np.round(100*featuredf['label'].sum()/featuredf.shape[0],2)))</span><span id="765f" class="mp kq it ml b gy mx mr l ms mt">22.92 % of the users churned</span></pre><p id="513a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鉴于只有约22%的用户表示不满，将准确性作为预测性能的衡量标准是不明智的。</p><p id="cacd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我选择使用ROC曲线下的面积来对每个学习者的表现进行排名。ROC曲线下的面积是使用真阳性率和假阳性率的图。当真阳性率最高时，面积最大，假阳性率最低。<br/>在分类方面做得好的分类器将具有接近1.0的分数。如果I分类器的得分接近0.5，那么它在分类方面做得很差。</p><h2 id="20e9" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated"><strong class="ak">车型</strong></h2><p id="6476" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我将使用4个模型进行拟合和分类。</p><ul class=""><li id="d500" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn lx ly lz ma bi translated">逻辑回归</li><li id="0ddf" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated">梯度提升树分类器</li><li id="7d28" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated">随机森林分类器</li><li id="7e34" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated">线性支持向量机</li></ul><p id="dc5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我选择这些算法是因为它们已经存在于spark框架中，并且它们有不同的分类方法。</p><p id="497a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">逻辑回归基于将特征组合到回归模型中的能力，然后在最后应用逻辑函数来对回归模型的结果进行分类。<br/>梯度提升树分类器是一种提升算法，它通过降低每次重复的错误率来调整其分离特征。<br/>随机森林分类器是一种bagging算法。它由多个弱学习者组成，他们最终对样本的可能结果进行投票。拥有多个弱学习者通常可以防止过度适应，同时提供一个强学习者。<br/>支持向量机试图通过使用所有可用的特征维度来分离类别。由此产生的SVM试图在离这两个类最远的地方画一条“线”。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="3f23" class="mp kq it ml b gy mq mr l ms mt">from pyspark.ml import Pipeline<br/>from pyspark.ml.classification import GBTClassifier, LogisticRegression, LinearSVC, RandomForestClassifier<br/>from pyspark.ml.feature import VectorAssembler,MinMaxScaler<br/>from pyspark.ml.tuning import CrossValidator, ParamGridBuilder<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/>from sklearn.metrics import roc_curve,auc</span><span id="1c7d" class="mp kq it ml b gy mx mr l ms mt">evaluator = BinaryClassificationEvaluator()</span><span id="c741" class="mp kq it ml b gy mx mr l ms mt">lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)<br/>rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=10, seed = 20)<br/>gbt = GBTClassifier(labelCol="label", featuresCol="features", maxIter=10, seed = 20)<br/>lsvc = LinearSVC(maxIter=10, regParam=0.1)<br/>model_list = [lr,rf,gbt,lsvc]</span></pre><h2 id="1858" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated">管道</h2><p id="a9fd" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">流水线包括使用<code class="fe mu mv mw ml b"> MinMaxScaler</code>缩放特征，然后使用<code class="fe mu mv mw ml b"> VectorAssembler</code>收集特征。然后，数据将由学习模型拟合，模型将使用交叉验证进行调整，最佳模型将用于进一步评估。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="6b54" class="mp kq it ml b gy mq mr l ms mt">columns = ['n_thumb_down','n_home','n_pages','n_roll_advert','n_about',\<br/>'n_add_playlist','n_nextsong',\<br/>'n_thumb_up','n_error','n_submit_upgrade','total_length', 'n_downgrade']<br/>assembler = VectorAssembler(inputCols= columns,outputCol='features_stage1')<br/>scaler = MinMaxScaler(inputCol="features_stage1", outputCol="features")</span></pre><h2 id="01b8" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated">装配调谐</h2><p id="0b48" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">使用<code class="fe mu mv mw ml b">ParamGrid</code>,我将调整选择用于评估的模型。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="2f68" class="mp kq it ml b gy mq mr l ms mt">lrgrid=ParamGridBuilder()\<br/>.addGrid(lr.maxIter,[5,10,15,20])\<br/>.addGrid(lr.regParam,[0.1,0.3,0.5])\<br/>.addGrid(lr.elasticNetParam,[0.3,0.5,0.8])\<br/>.build()<br/><br/><br/>rfgrid=ParamGridBuilder()\<br/>.addGrid(rf.numTrees,[10,20,30,40,50])\<br/>.addGrid(rf.maxDepth,[2,3,5])\<br/>.build()<br/><br/>gbtgrid=ParamGridBuilder()\<br/>.addGrid(gbt.maxIter,[5,10,15,20])\<br/>.build()<br/><br/>lsvcgrid=ParamGridBuilder()\<br/>.addGrid(lsvc.maxIter,[5,10,15,20])\<br/>.addGrid(lsvc.regParam, [0.01,0.1,0.5])\<br/>.build()<br/><br/>grids= [lrgrid, rfgrid, gbtgrid, lsvcgrid]</span></pre><p id="da59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将数据分成80%的训练和20%的测试。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="a3c0" class="mp kq it ml b gy mq mr l ms mt">train = features.sampleBy("label",fractions={0:0.8,1:0.8},seed=1)<br/>test = features.subtract(train)</span></pre><h2 id="e2ee" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated">培养</h2><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="3a3c" class="mp kq it ml b gy mq mr l ms mt">best_models = {}<br/>model_names= ['Logistic Regression','Random Forrest', 'G.B Trees', 'Linear SVC']<br/>plt.figure(figsize=(12,12))<br/>#run through each model and perform tuning, then we perform AUC ROC Scoring and plotting<br/>for n, model_name in enumerate(model_names):<br/>    print(n,model_name)<br/>    pipe=Pipeline(stages=[assembler,scaler, model_list[n]])<br/>    crossval = CrossValidator(estimator=pipe,<br/>                              estimatorParamMaps=grids[n],<br/>                                    evaluator=BinaryClassificationEvaluator(),<br/>                              numFolds=5)<br/>    cvModel=crossval.fit(train)<br/>    bestmodel=cvModel.bestModel<br/>    predictions=bestmodel.transform(test)<br/>    score = evaluator.evaluate(predictions,{evaluator.metricName: "areaUnderROC"})<br/>    print(""+model_name+": " + str(score))<br/>    #get all the variables in a dictionary to use later<br/>    best_models[model_name]=bestmodel<br/>    best_models[model_name+'_score']=score<br/>    best_models[model_name+'_predictions_variables']=[prob,pred,labels]</span></pre><p id="4b1e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">计分</strong></p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="ed93" class="mp kq it ml b gy mq mr l ms mt">scores=[]<br/>for model_name in model_names:<br/>    scores.append(best_models[model_name+'_score'])<br/>plt.figure(figsize=(8,8))<br/>sns.barplot(x=np.linspace(1,len(scores),len(scores)),y=scores)<br/>plt.xticks(ticks=np.linspace(0,len(scores)-1,len(scores)),labels=model_names, rotation=65)<br/>plt.ylabel('AUC ROC')<br/>plt.title('Model Scores')<br/>%matplot plt</span></pre><figure class="mg mh mi mj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nr"><img src="../Images/1d578bbf7fe63f1f9a79c2e71e17cb83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQI7XbkW344WO5R9VxTIlg.png"/></div></div></figure><h1 id="4d02" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">讨论</h1><p id="9e86" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">观察调整后每个模型的得分，其中3个模型的性能相当相似，只有一个模型的性能落后。</p><p id="4b69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个重要的问题是，我们能从模型中学到什么，有没有一个特征比其他的更重要？</p><p id="74b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看看随机森林模型，要素重要性是模型的一部分。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="477f" class="mp kq it ml b gy mq mr l ms mt">rfmodel=best_models['Random Forrest'].stages[-1]<br/>rfmodel.featureImportances</span><span id="e661" class="mp kq it ml b gy mx mr l ms mt">SparseVector(12, {0: 0.0113, 1: 0.1226, 2: 0.0419, 3: 0.1136, 4: 0.0001, 5: 0.0017, 6: 0.0188, 7: 0.0011, 8: 0.0005, 9: 0.5448, 10: 0.0026, 11: 0.1411})</span></pre><p id="9fec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从特征重要性可以看出，特征4、5和9没有值，因此它们在建模和预测过程中没有帮助，不应该被认为是重要的。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="365d" class="mp kq it ml b gy mq mr l ms mt">xticks=columns<br/>plt.figure(figsize=(12,8))<br/>sns.barplot(x=np.linspace(0,len(xticks),len(xticks)+1),y=rfmodel.featureImportances.values)<br/>plt.xticks(ticks=np.linspace(0,len(xticks),len(xticks)+1),labels=xticks, rotation=30)<br/>plt.ylabel('Coefficients')<br/>plt.title('Feature Importances')<br/>%matplot plt</span></pre><figure class="mg mh mi mj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nj"><img src="../Images/0cb28f13a32399b03b6771166fc3ce35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMiXFn7Hd9gGWumQ-lkdwg.png"/></div></div></figure><p id="fc20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图中有趣的是，用户使用服务的最有用的特征之一总长度。不幸的是，我们的音乐流服务似乎很难让其长期客户退出这项服务。虽然从前面的图表中我们可以看到，用户遇到的错误数量可能会导致客户流失，但在这里我们看到这不是一个决定性因素。</p><h2 id="9f12" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated">改进注意事项</h2><p id="a5b5" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">这些特性可以用更多的领域知识来进一步改进。在AWS上，开发期间当前可用的是120MB和16GB，在AWS上开发可能会很昂贵，但是在AWS上执行EDA和功能探索可以带来更深入的探索。</p><h2 id="a00d" class="mp kq it bd kr my mz dn kv na nb dp kz kb nc nd ld kf ne nf lh kj ng nh ll ni bi translated">其他注释</h2><p id="5e9e" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我最初在120MB的数据集上运行管道，这允许我快速开发数据。</p><p id="ea75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我在我的本地Linux机器上安装了Spark 2.4.5，这样我就可以不用每小时都被充电了。这花了一些时间，但最终，我对能够在闲暇时探索而不必感到匆忙感到满意。我还必须使用pip安装AWS服务，这允许我从数据科学纳米学位项目的AWS S3存储桶中获取120MB数据集。<br/>尽管我采取了所有这些先发制人的措施，但在设置AWS EMR集群以使其正常运行方面仍然存在一些困难，尤其是集群在1小时的活动后会进入睡眠状态，以及内存管理方面的问题。</p><p id="5b01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">谢谢你看我的帖子！</p></div></div>    
</body>
</html>