<html>
<head>
<title>Build a taxi driving agent in a post-apocalyptic world using Reinforcement Learning | Machine Learning from Scratch (Part VIII)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用强化学习|从零开始的机器学习(第八部分),在后启示录世界中构建一个出租车驾驶代理</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/build-a-taxi-driving-agent-in-a-post-apocalyptic-world-using-reinforcement-learning-machine-175b1edd8f69?source=collection_archive---------3-----------------------#2019-06-29">https://levelup.gitconnected.com/build-a-taxi-driving-agent-in-a-post-apocalyptic-world-using-reinforcement-learning-machine-175b1edd8f69?source=collection_archive---------3-----------------------#2019-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0828" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何构建自动驾驶出租车代理。从头开始构建MDP，并使用Python中的Q-learning解决它！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a0468471aea277b4c675c80660458dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xn4P4rYyjIVJpxMqpeQQzw.jpeg"/></div></div></figure><blockquote class="kr"><p id="856b" class="ks kt iq bd ku kv kw kx ky kz la lb dk translated">TL；博士为自动驾驶出租车问题搭建了一个简单的MDP。接载乘客，避免危险，并在指定地点让他们下车。建立一个代理，并使用Q-learning解决问题。</p></blockquote><p id="c9aa" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lb ij bi translated">你醒了。这是一个阳光明媚的日子。你的伴侣还在你旁边睡着。你花一分钟去欣赏美景，甚至微笑。</p><p id="f933" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">你的肚子在咕咕叫，所以你站起来四处看看。头奖！你看到的是你们俩昨晚周年纪念晚餐的剩菜。你花一分钟去抓你的某个隐私点。是啊，多么美妙的早晨！你有三分之一的豆子罐头，六个月前才过期。太好吃了！</p><p id="78d0" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">虽然你为没有给睡在你床上的人留下任何食物而感到难过，但你还是穿好衣服准备去工作。“你打算怎么办？不是吃饭和工作！?"—那些想法似乎不再有帮助了。该上车了！</p><p id="cf48" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">你试图通过无线电联系张诗钟，但没有成功。哦，吉米上周失踪了，他的双胞胎兄弟可能也失踪了。奇怪，你不记得上次眼睛湿润是什么时候了。</p><p id="bfd2" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">自从<em class="mc">事件</em>以来，发生了很多变化。街道每周都变得越来越危险，但是人们仍然需要交通工具。您收到一个提货请求，并查看了异常地图。你犹豫地接受。你的地图两个月前更新过。</p><p id="1778" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">你完成了任务，得到了半罐的荣誉。那对夫妇真大方！这就为制造无人驾驶出租车的想法留出了时间。你甚至有了一个名字——<em class="mc">安全驾驶室</em>。你读过很多关于这个叫埃隆·努斯克的疯子的报道，他试图在<em class="mc">事件</em>发生之前制造全自动驾驶汽车。</p><p id="5fed" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">你开始挠头。这是可能的还是只是幻想？毕竟，如果你完成了这件事，你就可以每隔一天吃一次了。进入强化学习。</p><p id="568e" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated"><a class="ae md" href="https://colab.research.google.com/drive/1FMo6Lpf1UtO1blfMyA4yznzDN7tlgIWm" rel="noopener ugc nofollow" target="_blank">谷歌合作笔记本中的完整源代码</a></p><h1 id="8f89" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">强化学习</h1><p id="6dff" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">强化学习(RL)关注的是产生算法(代理)，试图实现一些预定义的目标。这一目标的实现依赖于选择一系列行动——对好的行动给予奖励，对坏的行动给予惩罚——强化点由此而来。在具有状态的环境中，代理行为给出奖励和一组动作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2cda35b217cae04a56a7427ee99b5998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*vP3fSQAOGeFb1a7p61Z9wg.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">来源:<a class="ae md" href="https://phrasee.co/" rel="noopener ugc nofollow" target="_blank">https://phrasee.co/</a></figcaption></figure><p id="1937" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">深度强化学习(使用深度神经网络来选择行动)最近取得了一些伟大的成就:</p><ul class=""><li id="7f36" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb nl nm nn no bi translated"><a class="ae md" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank">玩雅达利游戏</a></li><li id="58ba" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated"><a class="ae md" href="https://www.youtube.com/watch?v=oo0TraGu6QY" rel="noopener ugc nofollow" target="_blank">玩毁灭战士</a></li><li id="4a92" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated"><a class="ae md" href="https://deepmind.com/blog/alphago-zero-learning-scratch/" rel="noopener ugc nofollow" target="_blank">主宰围棋比赛</a></li></ul><h2 id="95e7" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">马尔可夫决策过程</h2><p id="eb5d" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">马尔可夫决策过程(MDP)是RL问题的数学表述。MDP满足马尔可夫性质:</p><p id="091c" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">马尔可夫属性——当前状态完全代表环境(世界)的状态。也就是未来只取决于现在。</p><p id="2ef2" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">MDP可以由(<em class="mc"> S </em>，<em class="mc"> A </em>，<em class="mc"> R </em>，<em class="mc"> P </em>，<em class="mc"> γ </em>来定义，其中:</p><ul class=""><li id="5e38" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb nl nm nn no bi translated"><em class="mc"> S </em> —一组可能的状态</li><li id="8bf5" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated"><em class="mc"/>—一组可能的动作</li><li id="7a4d" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated"><em class="mc">R</em>——给出奖励的概率分布(状态，动作)对</li><li id="6301" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">给定(状态，动作)对，任何状态成为新状态的可能性的概率分布。也称为转移概率。</li><li id="ca40" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated"><em class="mc"> γ </em> —奖励折扣系数</li></ul><p id="1e08" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">贴现因子<em class="mc"> γ </em>允许我们注入这样的启发:现在的100美元比30天后的100美元更有价值。折扣系数为1时，未来奖励的价值与当前奖励的价值相同。</p><p id="cdc2" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">对未来奖励打折扣的另一个原因是:</p><blockquote class="og oh oi"><p id="c95e" class="lc ld mc le b lf lx jr lh li ly ju lk oj lz ln lo ok ma lr ls ol mb lv lw lb ij bi translated">“从长远来看，我们都会死”——j . m .凯恩斯</p></blockquote><h2 id="e9db" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">学问</h2><p id="2803" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">以下是学习在RL环境中是如何发生的:</p><p id="a982" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">对于时间步长<em class="mc"> t </em> =0，直到完成:</p><ol class=""><li id="f4e0" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb om nm nn no bi translated">环境给你的代理一个状态</li><li id="b9e2" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">您的代理选择一个操作(从一组可能的操作中)</li><li id="131a" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">随着新的状态，环境给予奖励</li><li id="b6df" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">继续下去，直到目标或其他条件得到满足</li></ol><p id="ae74" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">这一切的目的是什么？找到一个函数<em class="mc">π</em>∑，称为最优策略，它使累积折扣报酬最大化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/97a25d752b6e9147e284c71c2b8a7fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*CEMvXjG_feSCtkMCthB0YA.png"/></div></figure><p id="4458" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">其中<em class="mc"> rt </em>是在步骤<em class="mc"> t </em>接收的奖励，而<em class="mc"> γt </em>是在步骤t <em class="mc"> t </em>的折扣因子。</p><p id="822c" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">策略<em class="mc"> π </em>是将状态<em class="mc"> s </em>映射到动作<em class="mc"> a </em>的函数，我们的代理认为这是给定状态下的最佳选择。</p><h2 id="5a1e" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">价值函数</h2><p id="9d43" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">价值函数给出了代理人将获得的最大预期未来报酬，从某个状态<em class="mc"> s </em>开始，遵循某个策略<em class="mc"> π </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6d525d25559c198f16d28d9b7b4c0b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*2ZVnv4_RwXVwcGwUtF952w.png"/></div></figure><p id="3bb5" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">存在一个<strong class="le ir">最佳值函数</strong>，其对于所有状态都具有最高值。定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/c6528b7f5d1f28049e88fd357055f9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*W11hMqE83Su4eqhBUlCqog.png"/></div></figure><h2 id="967e" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">q函数</h2><p id="5279" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">类似地，让我们定义另一个被称为<em class="mc"> Q </em>的函数(状态-动作值函数)，它给出了从状态<em class="mc"> s </em>开始，采取动作a <em class="mc"> a </em>，并遵循策略<em class="mc"> π </em>的预期回报。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/563125a106a60fdfa04de70408877a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SF2TIXjPunxseH3viV3M-Q.png"/></div></div></figure><p id="6522" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">你可以把<em class="mc">Q</em>-函数看作给定状态下某个动作的“质量”。我们可以将最优<em class="mc">Q</em>-函数定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/35ca646309f6bad9045314fcd9bc94d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*ArO7a1PAj_LrvLzGxKPJdw.png"/></div></figure><p id="027b" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">两个最优函数<em class="mc">V</em>∫和<em class="mc">Q</em>∫之间存在关系。它由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d40f0bf0881cc946e4f20a51201affbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*o6Z8g3KpN4hnOjNR8bpzdQ.png"/></div></figure><p id="f77a" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">即在<em class="mc"> s </em>开始时的最大期望总报酬是所有可能行动中<em class="mc">Q</em>∫(<em class="mc">s</em>，<em class="mc"> a </em>)的最大值。</p><p id="e3e3" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">我们可以通过选择状态<em class="mc"> s </em>下给出最大报酬<em class="mc">Q</em>∫(<em class="mc">s</em>，<em class="mc"> a </em>)的行动<em class="mc"> a </em>来找到最优策略<em class="mc">π</em>∫:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/06f220028e034f9647c482639ccbc4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*DE1o2hq2MAXTT_FmXsg-FA.png"/></div></figure><p id="a70c" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">到目前为止，我们定义的所有功能之间似乎都有协同作用。更重要的是，我们现在可以为给定的环境构建一个最佳的代理。实践中能做到吗？</p><h2 id="7276" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">q学习</h2><p id="fb9c" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">Q-Learning是一种用于<a class="ae md" href="https://en.wikipedia.org/wiki/Temporal_difference_learning" rel="noopener ugc nofollow" target="_blank">时间差(TD)学习</a>的非策略算法。证明了通过足够的训练，它以概率1收敛于任意目标策略的动作值函数的近似。它学习最优策略，甚至当使用探索性(一些随机性)策略(偏离策略)选择动作时。</p><p id="3b82" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">给定一个状态<em class="mc"> s </em>和动作<em class="mc"> a </em>，我们可以用自身(递归)来表示<em class="mc"> Q </em> ( <em class="mc"> s </em>，<em class="mc"> a </em>):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/cff398eb51d6f351fba1b6ad0e520e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*jamiG5MkFVHLTFmLggemVg.png"/></div></figure><p id="28d6" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">这就是<strong class="le ir">贝尔曼方程</strong>。它将最大未来奖励定义为代理在状态<em class="mc"> s </em>收到的奖励<em class="mc"> r </em>，加上状态<em class="mc"> s </em>对于每一个可能的动作的最大未来奖励。</p><p id="0274" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">我们可以使用贝尔曼方程将<em class="mc">Q</em>-学习定义为<em class="mc">Q</em>∫的迭代近似:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/1e9e5871ec04de4db8f7b219fa81ba72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2APPuDNSpWMlaiin7R0C9g.png"/></div></div></figure><p id="dfa0" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">其中<em class="mc"> α </em>是学习率，其控制考虑的先前和新Q值之间的差异。</p><p id="5154" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">下面是我们要实现的通用算法:</p><ol class=""><li id="2cae" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb om nm nn no bi translated">初始化一个<em class="mc"> Q </em>值表</li><li id="bda4" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">遵守初始状态<em class="mc"> s </em></li><li id="93dd" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">选择动作<em class="mc"> a </em>并行动</li><li id="f1eb" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">观察奖励<em class="mc"> r </em>和新状态<em class="mc"> s </em></li><li id="c6d6" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">使用<em class="mc"> r </em>和来自<em class="mc"> s </em>的最大可能奖励更新<em class="mc"> Q </em>表</li><li id="5c54" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">将当前状态设置为新状态，并从<em class="mc">步骤2 </em>开始重复，直到一个终端状态</li></ol><p id="64f9" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">注意<em class="mc">Q</em>-学习只是解决RL问题的一种可能的算法。这里是其中一些的比较。</p><h2 id="bc02" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">探索与开发</h2><p id="3ecb" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">你可能已经注意到我们浏览了选择行动的策略。您实施的任何策略都必须选择尝试新事物或使用已知事物的频率。这就是所谓的勘探/开发权衡。</p><ul class=""><li id="8638" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb nl nm nn no bi translated">探索——寻找关于环境的新信息</li><li id="650c" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">利用——利用现有信息最大化回报</li></ul><p id="3b88" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">记住，我们的RL代理的目标是最大化期望的累积回报。这对我们的自动驾驶出租车意味着什么？</p><p id="8bc9" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">最初，我们的驾驶代理对接送乘客的最佳驾驶路线几乎一无所知。它也应该学会避免异常现象，因为它们对业务不利(乘客往往会消失，甚至更糟)！在这段时间里，我们期望进行大量的探索。</p><p id="8eb2" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">在获得一些经验之后，代理可以越来越经常地使用它来选择一个动作。最终，所有的选择都将基于学到的东西。</p><h1 id="37c1" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">在后世界末日的世界里开车</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/43e4f1736c0f5a65ca3a27f49142678d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rD4AfcZ8DS9YGBrerYySrA.png"/></div></div></figure><p id="70ef" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">你的搭档画了这张地图。每个街区代表城市的一个小区域。异常用亮圈标出。四个字母是“安全区”，接送发生的地方。</p><p id="61fe" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">让我们假设你的车是这个城市中唯一的交通工具(不太夸张)。我们可以把它分成一个5x5的网格，这样就有25个可能的出租车位置。</p><p id="0164" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">当前滑行位置是(行，列)坐标中的<code class="fe ox oy oz pa b">(3, 1)</code>。接送地点为:<code class="fe ox oy oz pa b">[(0,0), (0,4), (4,0), (4,3)]</code>。异常点在<code class="fe ox oy oz pa b">[(0, 2), (2, 1), (2, 2), (4, 2)]</code>。</p><h2 id="0784" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">环境</h2><p id="7ce0" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">我们将使用<a class="ae md" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI的健身房</a>将我们的城市地图编码到自动驾驶代理的环境中。这个健身房到底是什么？</p><blockquote class="og oh oi"><p id="a679" class="lc ld mc le b lf lx jr lh li ly ju lk oj lz ln lo ok ma lr ls ol mb lv lw lb ij bi translated">Gym是一个开发和比较强化学习算法的工具包。它支持教导代理从<a class="ae md" href="https://gym.openai.com/envs/Humanoid-v2/" rel="noopener ugc nofollow" target="_blank">行走</a>到玩游戏如<a class="ae md" href="https://gym.openai.com/envs/Pong-ram-v0/" rel="noopener ugc nofollow" target="_blank"> Pong </a>或<a class="ae md" href="https://gym.openai.com/envs/VideoPinball-ram-v0/" rel="noopener ugc nofollow" target="_blank"> Pinball </a>的一切。</p></blockquote><p id="8770" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">健身房最重要的实体是环境。它提供了一个统一且易于使用的界面。以下是最重要的方法:</p><ul class=""><li id="d59c" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb nl nm nn no bi translated"><code class="fe ox oy oz pa b">reset</code> -重置环境并返回随机初始状态</li><li id="e193" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated"><code class="fe ox oy oz pa b">step(action)</code>——采取行动，前进一步。它返回:</li></ul><p id="9f0e" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">观察——环境的新状态</p><p id="9c20" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">奖励——采取行动获得的奖励</p><p id="7e7b" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">完成—大多数环境被划分为定义明确的剧集，如果<code class="fe ox oy oz pa b">done</code>为<code class="fe ox oy oz pa b">True</code>，则表明该剧集已经完成</p><p id="6abc" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">info —关于环境的附加信息(可能对调试有用)</p><ul class=""><li id="d8ef" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb nl nm nn no bi translated"><code class="fe ox oy oz pa b">render</code> -渲染环境的一帧(用于可视化)</li></ul><p id="9816" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">环境的完整源代码在笔记本里。在这里，我们来看看健身房的注册:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="ac67" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">注意，我们设置了一个<code class="fe ox oy oz pa b">timestep_limit</code>，它限制了一集的步数。</p><h2 id="8c32" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">行为空间</h2><p id="10a0" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">我们的代理遇到500个州(5行x 5列x 5个乘客位置x 4个目的地)中的一个，它选择一个动作。以下是可能的操作:</p><ol class=""><li id="bd2a" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb om nm nn no bi translated">南方</li><li id="45bd" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">北方</li><li id="d7bf" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">东方</li><li id="4040" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">西方的</li><li id="db7f" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">收集</li><li id="baee" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb om nm nn no bi translated">摘下</li></ol><p id="caf9" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">您可能会注意到，在上图中，出租车在靠近城市边界时无法执行某些操作。我们将用-1对此进行处罚，如果出现这种情况，我们将不会移动出租车。</p><h2 id="712b" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">感受一下环境</h2><p id="24d9" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">让我们看看我们的编码环境:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/87c05a7d3bfc092b37f9c0aff1711ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*WgRcshgFklqiieT_CWBXxw.png"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><pre class="kg kh ki kj gt pe pa pf pg aw ph bi"><span id="2f4c" class="nu mf iq pa b gy pi pj l pk pl">Action Space Discrete(6)<br/>State Space Discrete(500)</span></pre><p id="9623" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">下面是数字映射的操作:</p><ul class=""><li id="3f68" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb nl nm nn no bi translated">0 =南方</li><li id="b31b" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">1 =北方</li><li id="0bdd" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">2 =东部</li><li id="8e72" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">3 =西方</li><li id="3a9e" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">4 =拾取</li><li id="cc06" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">5 =衰减</li></ul><h2 id="0589" class="nu mf iq bd mg nv nw dn mk nx ny dp mo ll nz oa mq lp ob oc ms lt od oe mu of bi translated">构建代理</h2><p id="7c01" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">我们的代理有一个与环境交互的简单界面。这是:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="c33f" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">当我们希望代理人做出决定并采取行动时，我们将使用<code class="fe ox oy oz pa b">choose_action</code>方法。然后，在观察到奖励和来自环境的新状态后，我们的代理将使用<code class="fe ox oy oz pa b">learn</code>方法从它的行为中学习。</p><p id="1ff0" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">让我们看一下实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="6680" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated"><code class="fe ox oy oz pa b">__init__</code>有趣的部分是我们的Q <em class="mc"> Q </em>表的初始化。最初，它全是零。我们可以用其他方法初始化它吗？</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="0253" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">我们的策略相当简单。我们从0和1之间的均匀分布中抽取一个随机数。如果这个数字小于ε，我们想探索，我们采取随机行动。否则，我们会根据现有知识采取最佳行动。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="1030" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">学习包括使用<em class="mc"> Q </em>学习等式更新<em class="mc"> Q </em>表，并在情节完成时降低探索率<em class="mc"> ϵ </em>。</p><h1 id="da68" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">培养</h1><p id="2018" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">现在我们的代理已经准备好行动，我们可以在我们创建的环境中训练它。让我们为50k集培训我们的代理，并记录一段时间内的集奖励:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="e67a" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">在了解任何有关环境的信息之前，让我们先来看看我们的代理驾驶:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pm pc l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="ba2e" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">回想一下，我们在注册环境时设置了<code class="fe ox oy oz pa b">timestep_limit</code>，这样我们的代理就不会无限期地停留在一个剧集中。</p><h1 id="35b1" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">估价</h1><p id="0f0e" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">让我们来看看随着培训的进行，奖励的变化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pn"><img src="../Images/98bf47e68b39c0cdc310c04fd6b54cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7NG4twC_yekqtmoBbzL6Q.png"/></div></div></figure><p id="c44a" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">注意，使用<a class="ae md" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html" rel="noopener ugc nofollow" target="_blank"> savgol_filter </a> <code class="fe ox oy oz pa b">savgol_filter(rewards, window_length=1001, polyorder=2)</code>平滑学习曲线</p><p id="330e" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">回想一下，我们的探索率应该随着代理的学习而降低。看一看:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi po"><img src="../Images/f76fe02aafc4f2eeb140f3353fca7971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OM_uITPUIWLm3mq23plXGQ.png"/></div></div></figure><p id="1148" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">下面是我们将如何测试我们的代理并记录进度:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="6fda" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">注意，我们希望我们的代理只使用它所拥有的经验，所以我们设置了<code class="fe ox oy oz pa b">explore=False</code>。以下是每集的总报酬:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/42cad95a6ca1c2420ab425701fecb63b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjP0rmRKXFNwGyAZUMBbeQ.png"/></div></div></figure><p id="bb07" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">我知道这张图表可能无法让你很好地了解代理人的能力。这是它在城市中行驶的视频:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pm pc l"/></div></figure><p id="ca86" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">很好，对吧？看起来它学会了避开异常情况，接送乘客。</p><h1 id="36dd" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">结论</h1><p id="0b37" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">恭喜你建立了自动驾驶出租车代理。你已经学会了如何</p><ul class=""><li id="97ab" class="ng nh iq le b lf lx li ly ll ni lp nj lt nk lb nl nm nn no bi translated">基于OpenAI的健身房提供的环境构建您自己的环境</li><li id="b6f5" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">实施并应用<em class="mc">Q</em>-学习</li><li id="59ba" class="ng nh iq le b lf np li nq ll nr lp ns lt nt lb nl nm nn no bi translated">建立一个代理，学习接送乘客和避开危险区域</li></ul><p id="141f" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated"><a class="ae md" href="https://colab.research.google.com/drive/1FMo6Lpf1UtO1blfMyA4yznzDN7tlgIWm" rel="noopener ugc nofollow" target="_blank">谷歌合作笔记本中的完整源代码</a></p><p id="da23" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">你能增加城市的规模吗？代理还学得好吗？在下面的评论里告诉我进展如何！</p></div><div class="ab cl pq pr hu ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="ij ik il im in"><p id="5269" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated"><em class="mc">最初发表于</em><a class="ae md" href="https://www.curiousily.com/posts/build-self-driving-taxi-using-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"><em class="mc">https://www.curiousily.com</em></a><em class="mc">。</em></p></div><div class="ab cl pq pr hu ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="ij ik il im in"><p id="7cec" class="pw-post-body-paragraph lc ld iq le b lf lx jr lh li ly ju lk ll lz ln lo lp ma lr ls lt mb lv lw lb ij bi translated">喜欢你读的吗？你想了解更多关于机器学习的知识吗？提升你对ML的理解:</p><div class="px py gp gr pz qa"><a href="https://leanpub.com/hmls" rel="noopener  ugc nofollow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd ir gy z fp qf fr fs qg fu fw ip bi translated">从零开始实践机器学习</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">“我不能创造的，我不理解”——理查德·费曼这本书将引导你走向更深的…</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">leanpub.com</p></div></div><div class="qj l"><div class="qk l ql qm qn qj qo kp qa"/></div></div></a></div></div></div>    
</body>
</html>