<html>
<head>
<title>Easy Guide to Create a Custom Write Data Source in Apache Spark 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Apache Spark 3中创建自定义写数据源的简单指南</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/easy-guide-to-create-a-write-data-source-in-apache-spark-3-f7d1e5a93bdb?source=collection_archive---------4-----------------------#2020-11-02">https://levelup.gitconnected.com/easy-guide-to-create-a-write-data-source-in-apache-spark-3-f7d1e5a93bdb?source=collection_archive---------4-----------------------#2020-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8a95" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在Apache Spark 3.0.x中创建自定义事务性写数据源的分步指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/abae2e6a4d1a7524233fa92b7caaec73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Nc-lbcgCHd67m6dR"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">格伦·卡斯滕斯-彼得斯在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="e004" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是在Apache Spark 3.0.x中编写自定义数据源系列的第二篇文章。在第一篇文章<a class="ae kv" rel="noopener ugc nofollow" target="_blank" href="/easy-guide-to-create-a-custom-read-data-source-in-apache-spark-3-194afdc9627a">中，我们了解了Apache Spark 3.0.x中的数据源API、它们的重要性以及read APIs的概述。首先，我们学习了创建一个简单的定制读数据源，然后创建了一个稍微复杂的位置感知的多分区读数据源。在本文中，我们将学习实现一个自定义的写数据源。</a></p><h1 id="18d3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">我们会学到什么？</h1><ul class=""><li id="1c42" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">在本文中，我们将了解数据源编写API。</li><li id="173b" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">对于分布式执行，每个接口及其在集群上的部署的重要性，驱动程序与执行器节点。</li><li id="fbec" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">我们将通过实现所有必要的接口来创建一个简单的写数据源。</li><li id="95e9" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">之后，我们将对其进行更新，并为其添加交易支持。</li><li id="a909" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">我们将了解任务失败、重试机制、推测任务以及如何实现最多一次 行为<strong class="ky ir"> <em class="na">。</em></strong></li></ul><h1 id="10ae" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">编写数据源API</h1><p id="bba5" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">Apache Spark数据源具有读写能力。当我们调用<code class="fe ne nf ng nh b">dataset.write().format(<strong class="ky ir">“</strong>output_format<strong class="ky ir">”</strong>).save()</code>时，使用write APIs以指定的输出格式保存数据帧。开箱即用，火花支持CSV，拼花，JDBC，兽人等。作为书写格式。</p><h1 id="00d4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">编写接口</strong></h1><ul class=""><li id="beb6" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">表格提供者</li><li id="f26d" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">桌子</li></ul><p id="8efc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个在<code class="fe ne nf ng nh b">read </code>和<code class="fe ne nf ng nh b">write </code>API中很常见。以下是只写接口</p><ul class=""><li id="826b" class="mk ml iq ky b kz la lc ld lf ni lj nj ln nk lr mr ms mt mu bi translated">支持写入</li><li id="8985" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">编写器</li><li id="d0d2" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">BatchWrite</li><li id="bf69" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">LogicalWriteInfo</li><li id="79a6" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">PhysicalWriteInfo</li><li id="d08f" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">数据写入工厂</li><li id="9839" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">数据写入器</li><li id="7972" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">WriterCommitMessage</li></ul><p id="c400" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们详细了解每个接口的重要性，并在此过程中创建一个写数据源。</p><h1 id="bb35" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">表格提供者</h1><p id="5b02" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">正如我们在上一篇<a class="ae kv" rel="noopener ugc nofollow" target="_blank" href="/easy-guide-to-create-a-custom-read-data-source-in-apache-spark-3-194afdc9627a">文章</a>中看到的，主数据源接口<code class="fe ne nf ng nh b">DatasourceV2,</code>被移除，新接口<a class="ae kv" href="http://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/sql/connector/catalog/TableProvider.html" rel="noopener ugc nofollow" target="_blank"> TableProvider </a>被引入。它是所有不需要支持DDL的定制数据源的基本接口。这个接口的实现应该有一个0参数的公共构造函数，对于Java，它是默认的构造函数。让我们看看它看起来怎么样。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="895b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">支持写入</h1><p id="f433" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">一个<code class="fe ne nf ng nh b">Table</code>接口定义了一个代表结构化数据的逻辑实体。它可以是基于文件系统的数据源的文件/文件夹，Kafka的主题，或者JDBC数据源的表。它可以与<code class="fe ne nf ng nh b">SupportsRead</code>和<code class="fe ne nf ng nh b">SupportsWrite</code>混合使用，分别增加读写能力。<code class="fe ne nf ng nh b">capabilities</code>方法返回表的所有功能。对于我们简单的write实现，让我们返回BATCH_WRITE。一个<code class="fe ne nf ng nh b">SupportsWrite</code>接口有一个<code class="fe ne nf ng nh b">newWriteBuilder()</code>方法，它返回定义数据源写行为的<code class="fe ne nf ng nh b">WriteBuilder</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="822a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">WriteBuider</h1><p id="b00a" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">现在，让我们实现这个接口，并从<code class="fe ne nf ng nh b">JdbcTable </code>实现中返回它的实例。WriteBuilder用于创建用于批量写入的<code class="fe ne nf ng nh b">BatchWrite</code>或用于将数据流写入流数据源的<code class="fe ne nf ng nh b">StreamingWrite </code>的实例。我们已经从表功能中返回了BATCH_WRITE，所以我们将实现<code class="fe ne nf ng nh b">buildForBatch().</code></p><p id="ea48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该接口提供的默认行为是使用<code class="fe ne nf ng nh b">BatchWrite</code>实现向给定表追加数据的能力。它可以与其他接口一起使用，以提供额外的功能，如覆盖(<code class="fe ne nf ng nh b">SupportsOverwrite</code>)、截断现有数据(<code class="fe ne nf ng nh b">SupportsTruncate</code>)。</p><p id="fbc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在另一篇文章中学习如何创建流写数据源。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="d6d2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">BatchWrite</h1><p id="8c37" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">顾名思义，这个接口用于将数据批量写入给定的数据源。它处理为每个分区创建的写作业和任务。这里处理作业级提交和中止。它返回<code class="fe ne nf ng nh b"><strong class="ky ir">DataWriterFactory.</strong></code> <strong class="ky ir">的实例，这三个接口的实例运行在驱动程序节点上，在调用</strong> <code class="fe ne nf ng nh b"><strong class="ky ir">dataset.write.save()</strong></code> <strong class="ky ir">时会创建一个写作业。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="0aa7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据写入工厂</h1><p id="dcf2" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">顾名思义，它是一个创建<code class="fe ne nf ng nh b">DataWriter </code>实例的工厂。BatchWrite实例在驱动程序节点上运行。<code class="fe ne nf ng nh b">BatchWrite </code>创建一个<code class="fe ne nf ng nh b">DataWriterFactory </code>实现的实例，并将其发送给执行者节点。这里要注意的一件重要事情是使它可序列化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="2edb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据写入器</h1><p id="75f2" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">这是主接口，它实际上完成了将数据写入目标系统的工作。对于每个数据帧分区，使用DataWriterFactory在executor节点上创建一个<code class="fe ne nf ng nh b">DataWriter </code>实例。为每个内部行调用一个<code class="fe ne nf ng nh b">write(record)</code>。当一个分区中的所有记录都被成功写入时，就会调用<code class="fe ne nf ng nh b">commit()</code>。如果在写入这些记录的过程中出现任何异常，就会调用<code class="fe ne nf ng nh b">abort()</code>。我们可以使用这些API来支持分区级事务。我们稍后会看到更多相关内容。</p><p id="505f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark并行写入每个分区。因此，<code class="fe ne nf ng nh b">DataWriter</code>实现不需要线程安全。为每个分区创建一个单独的<code class="fe ne nf ng nh b">DataWriter </code>实例。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="e970" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">其他接口</h1><ul class=""><li id="6483" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated"><strong class="ky ir"> LogicalWriteInfo </strong>:携带源数据帧模式、执行写操作所需的选项和分配给作业的查询id。</li><li id="e53b" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated"><strong class="ky ir"> PhysicalWriteInfo </strong>:携带源数据帧的分区计数信息。</li><li id="5a67" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated"><strong class="ky ir"> WriterCommitMessage: </strong>这是一个标记接口。它可以用于将分区id、任务id等信息从执行器节点上的数据写入器传输到驱动程序节点。</li></ul><p id="17fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样。我们已经成功创建了一个自定义写入数据源。现在，让我们看看如何将现有的数据帧写入JDBC表。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><h1 id="d608" class="ls lt iq bd lu lv nu lx ly lz nv mb mc jw nw jx me jz nx ka mg kc ny kd mi mj bi translated">添加事务支持</h1><p id="ef69" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">有了这些知识，让我们使这个实现更加成熟，并看看如何将事务支持添加到这个实现中。</p><p id="3d12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们前面看到的，<code class="fe ne nf ng nh b">DataWriter</code>接口有<code class="fe ne nf ng nh b">commit()</code>和<code class="fe ne nf ng nh b">abort()</code>方法。这些可用于在分区级别实现事务支持。对于Jdbc类型的数据源，可以做以下事情</p><ul class=""><li id="1ac8" class="mk ml iq ky b kz la lc ld lf ni lj nj ln nk lr mr ms mt mu bi translated">关闭连接的自动提交。</li><li id="f83c" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">在一个连接上继续写。</li><li id="7275" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">成功写入整个分区数据后，将调用datawriter.commit()。我们将从此方法调用connection.commit()。</li><li id="cab8" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">如果<code class="fe ne nf ng nh b">write(record)</code>抛出任何异常，则调用<code class="fe ne nf ng nh b">abort()</code>。我们可以调用<code class="fe ne nf ng nh b">connection.rollback()</code>来还原不完整的数据。</li></ul><p id="190f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于像文件数据源这样的非JDBC数据源，可以将数据写入像内存缓冲区这样的临时存储中，然后在commit()中，可以一次写入所有数据，也可以在abort()中完全清除数据。</p><h2 id="f05d" class="nz lt iq bd lu oa ob dn ly oc od dp mc lf oe of me lj og oh mg ln oi oj mi ok bi translated">作业级别交易</h2><p id="7017" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">一个<code class="fe ne nf ng nh b">BatchWrite</code>可用于添加一个作业级事务支持。这可以使用临时存储来完成。在我们的例子中，我们将传递一个临时表名，而不是传递用户指定的表名。分区的所有数据写入器都将写入该临时表。成功写入所有分区后，将调用<code class="fe ne nf ng nh b">BatchWrite.commit( WriterCommitMessage[] writerCommitMessages)</code>。在这个方法中，我们将把表重命名为原来的表。在任何异常情况下，我们将删除<code class="fe ne nf ng nh b">abort(WriterCommitMessage[] writerCommitMessages)</code>中的临时表。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="2b65" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">任务失败</h1><p id="43eb" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在分布式系统中，失败是很常见的。这可能是由于网络中断、节点故障等原因造成的。像Apache Spark这样健壮可靠的系统肯定知道如何处理这样的故障。它有一个任务失败的重试机制。对于每个spark作业，多个任务在执行器节点上运行。在我们的写作业中，为写每个数据源分区创建了一个单独的任务。单个任务的失败不应该完全是写作工作的失败。出于同样的原因，Spark有一个重试机制来在不同的节点上执行失败的任务。重试次数可通过<code class="fe ne nf ng nh b">spark.task.maxFailures.</code>进行配置</p><p id="59c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">执行写操作时，数据一致性应该是主要目标。重试机制给数据源实现者带来了挑战。如果数据源没有正确实现，有些数据可能会被多次写入。</p><p id="7344" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了增加分区级事务支持，我们在关闭自动提交的情况下在连接上写入数据。我们仅在所有内容都成功写入时提交，否则使用回滚丢弃所有内容。这可以确保即使重试任务因失败而运行，也不会多次写入记录。</p><h1 id="a487" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">投机任务</h1><p id="2697" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">Apache Spark还有另一个惊人的特性，叫做投机。当一个正在运行的任务花费太多时间来完成时，另一个推测性任务将在任何其他节点上为同一任务运行。这意味着，现在有<strong class="ky ir">多个任务同时运行来写入相同的数据</strong>。对于只读任务，这无关紧要。较早完成阅读的任务将被考虑在结果中，其他的将被忽略。</p><p id="a0c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，写作业中的情况有所不同。如果多个任务并行运行，试图将相同的数据写入目标系统，一致性就会受到影响。为了避免这种情况，Spark使用commit co-ordinator来确保只有一个正在运行的任务应该提交数据。可以使用下面的方法通过<code class="fe ne nf ng nh b">BatchWrite</code>实现来打开提交协调程序</p><pre class="kg kh ki kj gt ol nh om on aw oo bi"><span id="ce07" class="nz lt iq nh b gy op oq l or os">public class JdbcBatchWrite implements BatchWrite {<br/>    <br/>    public boolean useCommitCoordinator() {<br/>        return true;<br/>    }<br/>}</span></pre><p id="e715" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，只有当我们实现了分区级别的事务时，这才会起作用。否则，重复的条目将被写入目标系统。可以使用<code class="fe ne nf ng nh b">spark.speculation</code>配置推测功能。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><h1 id="debd" class="ls lt iq bd lu lv nu lx ly lz nv mb mc jw nw jx me jz nx ka mg kc ny kd mi mj bi translated">总结一下</h1><p id="33ce" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">可以实现定制的写数据源以写入目标系统。与read相比，它稍微复杂一些，因为需要添加事务支持，错误的处理可能会导致数据不一致。我们需要了解重试机制和推测性任务机制，以及分区级事务如何处理可能因此产生的数据不一致问题。</p><p id="3598" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个例子的完整源代码在这里分享<a class="ae kv" href="https://github.com/aamargajbhiye/big-data-projects/tree/master/Datasource%20spark3/src/main/java/com/bugdbug/customsource/jdbc" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="b448" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过实现<code class="fe ne nf ng nh b">SupportOverwrite </code>和<code class="fe ne nf ng nh b">SupportsTruncate </code>接口来添加覆盖和截断功能，可以进一步增强这个示例。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="f0ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读。你喜欢这篇文章吗？请在评论中告诉我你的想法。</p><h1 id="3f39" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">参考文献</strong></h1><div class="ot ou gp gr ov ow"><a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/connector/write/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">/docs/latest/API/Java/org/Apache/spark/SQL/connector/write的索引</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">编辑描述</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">spark.apache.org</p></div></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://github.com/aamargajbhiye/big-data-projects/tree/master/Datasource%20spark3" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">aamargajbhiye/大数据项目</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">github.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk kp ow"/></div></div></a></div><h1 id="cdbd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">进一步阅读</h1><div class="ot ou gp gr ov ow"><a rel="noopener  ugc nofollow" target="_blank" href="/easy-guide-to-create-a-custom-read-data-source-in-apache-spark-3-194afdc9627a"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">在Apache Spark 3中创建自定义读取数据源的简单指南</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">在Apache Spark 3.0.x中使用位置感知和多分区编写自定义读取数据源的分步指南…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="pf l"><div class="pl l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://medium.com/@aamargajbhiye/apache-spark-and-in-memory-hadoop-file-system-igfs-8b405997930" rel="noopener follow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">Apache Spark和内存中Hadoop文件系统(IGFS)</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">Apache Spark和内存中Hadoop文件系统(IGFS)</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">Apache Spark和内存中Hadoop文件系统(IGFS)medium.com</p></div></div><div class="pf l"><div class="pm l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://medium.com/@aamargajbhiye/apache-spark-setup-a-multi-node-standalone-cluster-on-windows-63d413296971" rel="noopener follow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">Windows上的Apache Spark独立集群</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">在Windows上设置Apache Spark多节点独立集群</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">medium.com</p></div></div><div class="pf l"><div class="pn l ph pi pj pf pk kp ow"/></div></div></a></div></div></div>    
</body>
</html>