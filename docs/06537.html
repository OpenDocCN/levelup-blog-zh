<html>
<head>
<title>Web Crawling with 25 Lines of Python Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用25行Python代码进行网络爬行</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/web-crawling-with-25-lines-of-python-code-e564c38a3c1?source=collection_archive---------2-----------------------#2020-12-04">https://levelup.gitconnected.com/web-crawling-with-25-lines-of-python-code-e564c38a3c1?source=collection_archive---------2-----------------------#2020-12-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eb23" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何在几分钟内编写一个强大的蜘蛛</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c0caffd2c130b4422447592a89acf152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DjxhDJemH6bXA7vC"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@jeroenbosch?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">耶鲁安博世</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="0c6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Web爬行和web抓取是两个非常相似且互补的领域。我所看到的对这两种状态的最佳定义是，爬虫，也称为蜘蛛，是一种设计用于在网站上移动的机器人，一页一页地爬行。另一方面，抓取是从网站中提取数据的行为。</p><p id="cb0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我将介绍如何使用递归函数创建一个非常简单但高效的爬虫，只需几行代码，以及如何使用它来抓取一些数据。</p><p id="08c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出于两个主要原因，我们将以维基百科为例。首先，它允许抓取和爬行，这是你应该在开始之前检查的；此外，它的结构对我们在本文中执行的操作有意义，因为我们可以使用每篇文章中引用的链接在每篇文章之间移动。</p><h1 id="7da8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">编写代码</h1><p id="d230" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们从进口开始。对于这项任务，我们只需要Python库BeautifulSoup和请求。我假设您已经安装了它们，但是如果您没有，您可以使用<em class="mp"> pip </em>来完成:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="ae0d" class="mv lt iq mr b gy mw mx l my mz">pip install beautifulsoup4</span><span id="d5a5" class="mv lt iq mr b gy na mx l my mz">pip install requests</span></pre><p id="3ec0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了这个设置，我们现在将编写完成全部工作的函数。在这种情况下，将爬行器作为一个函数来编写尤其重要，这样爬行器就可以多次递归调用自己，以便从一页接一页的页面中获取链接。</p><p id="3f3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该函数将只接收一个参数，即网站的URL，并将直接获取带有请求的页面并用BeautifulSoup解析它。这个URL是爬虫将开始的地方。我们还将实例化一个列表来跟踪我们抓取的页面。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="7cf8" class="mv lt iq mr b gy mw mx l my mz">pages_crawled = []<br/>def crawler(url):<br/>    page = requests.get(url)<br/>    soup = BeautifulSoup(page.text, 'html.parser')</span></pre><p id="5212" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从现在开始，所有的代码都在函数内部。</p><p id="5e88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在这里的任务包括获取起始页上其他页面的所有链接，然后在这些页面中的每一个页面上获取其中的所有链接，以此类推，无限期。</p><p id="aaf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到其他页面的链接通常存储在<code class="fe nb nc nd mr b">a </code>标签中。所以，我们需要做的是在页面上获取所有这些标签。<code class="fe nb nc nd mr b">find_all </code>方法是最好的选择:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="5c04" class="mv lt iq mr b gy mw mx l my mz">links = soup.find_all('a')</span></pre><p id="ad90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这行代码返回一个包含页面上所有<code class="fe nb nc nd mr b">a </code>标签的iterable。然后我们将遍历它，再次调用crawler函数，将每个链接作为参数传递。</p><p id="469c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但在我们这样做之前，必须满足一些条件:</p><ol class=""><li id="4da8" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">标签必须包含一个<code class="fe nb nc nd mr b">href </code>属性。<code class="fe nb nc nd mr b">href </code>代表超文本链接，是爬虫会找到链接的地方。检查这一点很重要，因为尽管链接通常在<code class="fe nb nc nd mr b">a </code>标签中，但并不是所有的标签都包含链接。</li><li id="4fe3" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">由于我们只对文章页面感兴趣，链接必须以“/wiki”开头，并且不包含分号，否则，爬虫最终会被重定向到维基百科的其他部分，甚至是其他完全不相关的网站。</li><li id="0377" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">链接一定还没有被抓取。为此，将抓取的链接附加到一个列表中，对于每个新链接，我们将检查它是否已经在这个列表中。</li></ol><p id="5cde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是这一切的样子:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="a6dc" class="mv lt iq mr b gy mw mx l my mz">for link in links:<br/>    if 'href' in link.attrs:<br/><br/>        if link['href'].startswith('/wiki') and ':' not in link['href']:<br/>            <br/>            if link['href'] not in pages_crawled:</span></pre><p id="1f7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">满足这些条件后，我们现在将链接添加到已爬网链接列表中，并为下一页创建完整的URL:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="1139" class="mv lt iq mr b gy mw mx l my mz">new_link = f"https://en.wikipedia.org{link['href']}"<br/>pages_crawled.append(link['href'])</span></pre><h2 id="0b1c" class="mv lt iq bd lu ns nt dn ly nu nv dp mc lf nw nx me lj ny nz mg ln oa ob mi oc bi translated">收集数据</h2><p id="f95e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在用另一个URL调用该函数之前，是时候收集您想要的数据了。从理论上讲，你可以抓取任何你想要的东西:页面标题、文章名称、目录、副标题、文章的整个文本等等。</p><p id="6494" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我实际上并没有从维基百科搜集数据，我只是用这个网站作为例子。但是假设我们想要得到页面的标题、文章的名称和文章的URL。我们可以做的是使用上下文管理器打开一个文本文件并写入信息。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="10d7" class="mv lt iq mr b gy mw mx l my mz">with open('data.csv', 'a') as file:<br/>    file.write(f'{soup.title.text}; {soup.h1.text};   {link["href"]}\n')</span></pre><p id="ba3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，使用分号作为分隔符很重要，因为当我们处理文本数据时，我们应该会在其中找到一些逗号。</p><p id="c0d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以再次调用该函数，代码将无限期地抓取越来越多的页面。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="c076" class="mv lt iq mr b gy mw mx l my mz">crawler(new_link)</span></pre><p id="68e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是完整的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="57ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我们的爬虫从主页开始，但它也可以从维基百科中的任何页面开始。此外，还添加了一个<code class="fe nb nc nd mr b">try </code>子句，以防止代码在将数据写入。csv文件或递归调用函数时。</p><h1 id="1c67" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">随机化爬行</h1><p id="8909" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">爬行维基百科的另一种可能性是随机进行。如果目标是让网站上的每一页都显示出来，这可能不是最聪明的想法，但这不是我们现在要做的。</p><p id="28fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">选择随机的文章来抓取仍然会产生大量的数据，因为维基百科几乎是无止境的，不会向他们的服务器发出太多的请求，这是我们总是需要考虑的事情，因为我们不想造成任何伤害。</p><p id="0cfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只需对代码做一些修改和一些新的导入就可以实现这一点。首先，我们需要向<code class="fe nb nc nd mr b">find_all</code>方法指定，我们不仅要寻找页面上的所有<code class="fe nb nc nd mr b">a</code>标签，也不要寻找包含<code class="fe nb nc nd mr b">href </code>属性的所有<code class="fe nb nc nd mr b">a </code>标签，而且我们希望所有包含<code class="fe nb nc nd mr b">href</code>属性的<code class="fe nb nc nd mr b">a</code>标签都有一个以“/wiki”开头的链接，并且不包含分号。</p><p id="76ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以通过指定使用Re模块来实现这一点。然后我们编写一个满足这些要求的正则表达式。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="772c" class="mv lt iq mr b gy mw mx l my mz">links = soup.find_all('a', <br/>{'href': re.compile('^\/wiki\/((?!:).)*$')})</span></pre><p id="7b6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们使用Numpy从<code class="fe nb nc nd mr b">links</code>中选择一个随机标签。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="7851" class="mv lt iq mr b gy mw mx l my mz">link = links[np.random.randint(1, len(links) + 1)]</span></pre><p id="7ab1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们之前建立的三个条件中的前两个已经满足，我们只需要检查最后一个。因此，如果链接还没有被抓取，我们只需要将它附加到抓取的链接列表中，获取数据，创建完整的URL并再次调用该函数。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="0366" class="mv lt iq mr b gy mw mx l my mz">if link not in pages_crawled:<br/>    pages_crawled.append(link)<br/>    <br/>    with open('data.csv', 'a') as file:<br/>        file.write(f'{soup.title.text}; {soup.h1.text};     {link["href"]}\n')</span><span id="4067" class="mv lt iq mr b gy na mx l my mz">    new_link = f"https://en.wikipedia.org{link['href']}"<br/>    random_crawl(new_link)</span></pre><p id="4d4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就这样了。如果您想要可重复的结果，您可以指定随机种子，或者如果您每次运行代码时想要不同的随机结果，您可以使用时间模块来生成不同的种子。</p><p id="c3fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是完整的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="d7c0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最后的想法</h1><p id="b039" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这就是我们如何用不超过25行代码爬取整个维基百科。这是一个非常简单有效的第一个网络爬虫，也是一个开始网络爬行的好方法。</p><p id="f8aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，请记住，正如已经提到的，维基百科允许你这样做，而其他网站可能不允许。但即使是维基百科也有一些条件，你可以在他们的robots.txt中看到:</p><blockquote class="of"><p id="a737" class="og oh iq bd oi oj ok ol om on oo lr dk translated">友好，低速的机器人是受欢迎的，但不是动态生成的网页。</p></blockquote><p id="c8ca" class="pw-post-body-paragraph kw kx iq ky b kz op jr lb lc oq ju le lf or lh li lj os ll lm ln ot lp lq lr ij bi translated">他们指定低速机器人是受欢迎的，因此，你一定要在你的代码中包含一些随机的停顿，以免网站的服务器过载。你可以使用<code class="fe nb nc nd mr b">time.sleep()</code>来完成这项工作。</p><p id="eec3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，当编写生产级代码时(本文中的情况并非如此，因为这只是一个示例)，让您的代码更好地处理异常并使用代理提供者(如<a class="ae kv" href="https://infatica.io/" rel="noopener ugc nofollow" target="_blank"> Infatica </a>)是一个好的做法，因为它们能够为您提供更好的IP地址基础设施，这样您就可以确保您的代码将继续运行。由于发送到服务器的请求数量，这在抓取网站时甚至更为重要。</p><p id="21c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！我希望你喜欢这本书，它可能会有用。如果你有问题，有建议，或者只是想保持联系，请随时通过<a class="ae kv" href="https://twitter.com/_otavioss" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae kv" href="https://github.com/otavio-s-s" rel="noopener ugc nofollow" target="_blank"> GitHub </a>或<a class="ae kv" href="https://www.linkedin.com/in/otavioss28/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p></div></div>    
</body>
</html>