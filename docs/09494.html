<html>
<head>
<title>Hands-On Practice with Time Distributed Layers using Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Tensorflow进行时间分布图层的实践练习</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/hands-on-practice-with-time-distributed-layers-using-tensorflow-c776a5d78e7e?source=collection_archive---------3-----------------------#2021-08-16">https://levelup.gitconnected.com/hands-on-practice-with-time-distributed-layers-using-tensorflow-c776a5d78e7e?source=collection_archive---------3-----------------------#2021-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d514" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我将指导您使用Tensorflow解决一个涉及一系列图像作为输入的问题，这是我在Elysium IIIT德里举办的最后一轮数据解放-机器学习竞赛中遇到的问题。</p><h1 id="3572" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">问题陈述</h1><blockquote class="lj lk ll"><p id="2314" class="jn jo lm jp b jq jr js jt ju jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj kk ij bi translated">你是洛杉矶警察局的技术人员。最近非法赛车的激增给市民带来了很多麻烦。所以现在警方已经处于戒备状态。昨天他们因为同样的原因带来了一个人。我们访问了他的仪表板摄像头，但我们不能确定他是否超速，因为硬盘损坏了。尽管如此，你还是设法得到了一些视频片段，但从这些片段来看，他的速度并不是光看就能得出结论的。作为一个积极性很高的人，你决定从另一辆车的仪表盘摄像头中取出视频片段，并记下速度。现在，您的任务是训练一个模型，该模型使用一系列dash-cam图像(即8帧)来确定汽车的速度。摄录机以20帧/秒的速度录制片段。</p></blockquote><p id="fb89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lq" href="https://www.kaggle.com/c/final-showdown/overview" rel="noopener ugc nofollow" target="_blank">比赛环节</a></p><p id="9484" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练数据的例子:</strong></p><div class="lr ls lt lu gt ab cb"><figure class="lv lw lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/ae95d5a152a6b347471b7aa9241f8bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*9tbRiyODldZkIA-IhWs3PQ.jpeg"/></div></figure><figure class="lv lw lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/5b7662a3b9e73fda72d09e19e2f37f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*WViWMr_jjiMd6wnLeWNTqw.jpeg"/></div></figure><figure class="lv lw lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/93f711b0c27543b0e6f15e91a2e6a7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*SztWt9zKyfLdbXo3D9om8g.jpeg"/></div></figure></div><div class="ab cb"><figure class="lv lw lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/3b7740be0e97846453a26b775fc9ad6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*1Q8AhrWx0LJEOOXsO7ExdQ.jpeg"/></div></figure><figure class="lv lw lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/e643cc0dc0a19178ff70257a9f1b7177.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6rtX9zniz36M7clqf1lw0A.jpeg"/></div></figure><figure class="lv lw lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/3c1af40052bd04fc08fd5375b6e5fee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pjeSmiXhNmUcVh4IR5_tRw.jpeg"/></div></figure></div><div class="ab cb"><figure class="lv lw mi ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/6086a2ade84731943974a61e35e4503a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*8D7n58JHPhqkYVoggvk4IA.jpeg"/></div></figure><figure class="lv lw mi ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/08c216ed910595d927d6bef5a3cd4dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*xPjljprzgFmJAPpT5AKRGg.jpeg"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk mn di mo mp translated">有序图像序列</figcaption></figure></div><blockquote class="lj lk ll"><p id="c2d8" class="jn jo lm jp b jq jr js jt ju jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj kk ij bi translated"><strong class="jp ir">输出</strong>:X、Y方向车速:[-1.2287469463，-0.0101401592]</p><p id="c884" class="jn jo lm jp b jq jr js jt ju jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj kk ij bi translated">速度值被缩放。</p></blockquote><h1 id="ad4c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">理解问题陈述</h1><p id="add0" class="pw-post-body-paragraph jn jo iq jp b jq mq js jt ju mr jw jx jy ms ka kb kc mt ke kf kg mu ki kj kk ij bi translated">这里的问题是训练一个模型，它可以仅使用dash相机拍摄的一系列图像来确定汽车的速度。我们必须<strong class="jp ir">输入</strong> <strong class="jp ir"> 8张图片</strong>到模型，它输出汽车速度的X和Y分量。简单来说，我们必须用仪表盘摄像头拍摄的8幅图像来预测汽车的速度。</p><h1 id="afa3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">了解时间分布图层</h1><p id="51fb" class="pw-post-body-paragraph jn jo iq jp b jq mq js jt ju mr jw jx jy ms ka kb kc mt ke kf kg mu ki kj kk ij bi translated">通常，在计算机视觉中，我们在建立一个可以对图像进行分类的模型(例如狗或猫)。您将输入一个单一的图像到模型，它给出0或1的输出(0是猫，1是狗)。</p><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mv"><img src="../Images/36bb8ff5860bd448929378cd08e786cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUBdDSVPpbC5VHvzTaZK0w.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">犬猫分类器</figcaption></figure><h1 id="13a0" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">但是在我们的例子中，每个样本有8个图像。</h1><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mw"><img src="../Images/0ece4e2b9a70125416b07c5cf2043fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l66eo4GkNkRfe8-Zywp4Hw.jpeg"/></div></div></figure><p id="e9bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">解决这个问题的一种方法是将所有图像合并成一个图像，并将合并后的图像输入到模型中，但是在这里我们将丢失可以帮助模型进行预测的关键信息。在我们的问题陈述中，<strong class="jp ir">图像中物体位置的微小差异可以帮助模型确定汽车的速度</strong>，但是通过合并图像，我们将丢失这些信息。</p><p id="5fb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们必须找到一种不丢失信息的方法，同时将8幅图像传递给模型并获得一个输出。我们的救世主来了来自Tensorflow <strong class="jp ir">的<strong class="jp ir">时间分布层</strong>。</strong></p><p id="0b31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个专门的层将同一个层应用于几个输入，并获得每个输入的输出，这样我们可以将它们组合起来，并将其传递给另一个层来进行预测。</p><p id="3528" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样，我们仅使用一个层，该层在8个单独的图像上执行其操作并给出输出。</p><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mx"><img src="../Images/171eedb1eebb215c2cee6e829c919e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlRx1_ci-ertAFULdjq2zQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">重要提示:<strong class="bd kn">所有卷积层共享相同的权重。所以技术上来说是同一层。为了便于理解，您可以将它们视为层的克隆。</strong></figcaption></figure><p id="8047" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">时间分布层将该层的相同实例应用于8个图像中的每一个。所以这一层没有8组不同的权重。同一组权重应用于所有图像。</p><p id="7eeb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，通过使用这一层，我们不会增加模型的复杂性(参数的数量),但让模型能够从8个不同的图像中学习，只需一层。</p><p id="1d01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">直观上，这些图像是在不同的时间戳拍摄的，没有任何重大差异，因此一层足以关键特征。就像在手机上用连拍模式拍照一样。因此，相同的图层可以应用于8幅图像，并从每幅图像中识别关键特征。</p><p id="dab5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lm">了解有关时间分布图层的详细信息:</em> <strong class="jp ir"> </strong> <a class="ae lq" href="https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00" rel="noopener"> <strong class="jp ir">如何在神经网络中处理时间分布数据</strong> </a></p><p id="e421" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我感谢这篇文章的作者Patrice Ferlet。它是黄金，在解决这个问题上帮助了我很多。</p><h1 id="6765" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">了解了解决这个问题所需的基础知识后，让我们深入研究一下代码</h1><figure class="lr ls lt lu gt lw"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="3507" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">提供的数据结构</strong>:</p><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nc"><img src="../Images/a3ba0b54fd93950912973d5950a2c12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gk0BkPdH7Mb1WdxAxzCtug.png"/></div></div></figure><p id="c140" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">导入必要的库:</strong></p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="6425" class="ni km iq ne b gy nj nk l nl nm"><strong class="ne ir">import</strong> <strong class="ne ir">cv2<br/>import</strong> <strong class="ne ir">numpy</strong> <strong class="ne ir">as</strong> <strong class="ne ir">np<br/>import</strong> <strong class="ne ir">matplotlib.pyplot</strong> <strong class="ne ir">as</strong> <strong class="ne ir">plt</strong><br/>%matplotlib inline<br/><strong class="ne ir">import</strong> <strong class="ne ir">pandas</strong> <strong class="ne ir">as</strong> <strong class="ne ir">pd</strong><br/><strong class="ne ir">import</strong> <strong class="ne ir">keras<br/>import</strong> <strong class="ne ir">json</strong><br/><strong class="ne ir">import</strong> <strong class="ne ir">tensorflow</strong> <strong class="ne ir">as tf</strong> <br/><strong class="ne ir">from</strong> <strong class="ne ir">keras.layers</strong> <strong class="ne ir">import</strong> Input<br/><strong class="ne ir">from</strong> <strong class="ne ir">keras</strong> <strong class="ne ir">import</strong> Sequential<br/><strong class="ne ir">from</strong> <strong class="ne ir">keras.layers</strong> <strong class="ne ir">import</strong> Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout<br/><strong class="ne ir">from</strong> <strong class="ne ir">keras.applications.inception_v3</strong> <strong class="ne ir">import</strong> InceptionV3<br/><strong class="ne ir">from</strong> <strong class="ne ir">keras.applications.vgg16</strong> <strong class="ne ir">import</strong> VGG16</span></pre><p id="e12f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">安装Google drive并提取数据:</strong></p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="ff42" class="ni km iq ne b gy nj nk l nl nm"><strong class="ne ir">from</strong> <strong class="ne ir">google.colab</strong> <strong class="ne ir">import</strong> drive<br/>drive.mount('/content/gdrive/')</span><span id="e44b" class="ni km iq ne b gy nn nk l nl nm"><strong class="ne ir">import</strong> <strong class="ne ir">zipfile</strong></span><span id="fa4d" class="ni km iq ne b gy nn nk l nl nm">final_showdown_zip = '/content/gdrive/My Drive/final-showdown.zip'</span><span id="2cfe" class="ni km iq ne b gy nn nk l nl nm">zip_ref = zipfile.ZipFile(final_showdown_zip, 'r')<br/>zip_ref.extractall('/content/gdrive/My Drive/final_showdown')<br/>zip_ref.close()</span></pre><p id="e4e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们问题中图像的维数是(720，1280，3)。为了更快地处理和训练数据，我们必须降低图像的维度。为了在降低维数的同时保持该比率，我们将维数除以4。(720/4,1280/4) =(180,320,3)</p><p id="4374" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">将图像大小从(720，1280，3)调整到(180，320，3)的功能，以加快计算速度，同时保持图像的纵横比:</strong></p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="312c" class="ni km iq ne b gy nj nk l nl nm"><strong class="ne ir">def</strong> get_img(img_path, printer=<strong class="ne ir">True</strong>):<br/>  original_img = cv2.imread(img_path, cv2.IMREAD_COLOR)<br/>  <strong class="ne ir">if</strong> printer: print ("original dim:",original_img.shape)</span><span id="e813" class="ni km iq ne b gy nn nk l nl nm">resized_img = cv2.resize(original_img, (320,180), interpolation=cv2.INTER_CUBIC)<br/>  <strong class="ne ir">if</strong> printer: print ("resized dim:", resized_img.shape)</span><span id="a8b2" class="ni km iq ne b gy nn nk l nl nm"><strong class="ne ir">return</strong> resized_img</span></pre><p id="4060" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">测试我们的功能:</strong></p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="9858" class="ni km iq ne b gy nj nk l nl nm">img_path = "/content/gdrive/My Drive/final_showdown/Train/Train/422/imgs/001.jpg"<br/>resized_img = get_img(img_path)<br/>plt.imshow(resized_img)</span></pre><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi no"><img src="../Images/0cbc9b060664e408e887971917c5327a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ge8v-gdHOjNihFU950ytUA.png"/></div></div></figure><p id="f29c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一步是在我们的时间分布层的样本中连接8个图像。</p><blockquote class="lj lk ll"><p id="f589" class="jn jo lm jp b jq jr js jt ju jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj kk ij bi translated">我们的时间分布层的输入形状=(每个样本的图像数量、图像高度、图像宽度、通道数量)</p></blockquote><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="77e9" class="ni km iq ne b gy nj nk l nl nm">prefix = "/content/gdrive/My Drive/final_showdown/Train/Train/400/imgs/00"<br/> <em class="lm">## Testing the Concatenation method</em></span><span id="61b0" class="ni km iq ne b gy nn nk l nl nm">X_sample = []<br/><strong class="ne ir">for</strong> idx <strong class="ne ir">in</strong> range (1, 9):<br/>  img_path = prefix + str(idx) + ".jpg"<br/>  resized_img = get_img(img_path, printer=<strong class="ne ir">False</strong>)<br/>  X_sample.append(resized_img)</span><span id="fa23" class="ni km iq ne b gy nn nk l nl nm">print (np.array(X_sample).shape)</span></pre><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ddea7663dc6da22a53ba6ce354a0ec93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*rewBhPfTSp7fxV1D3WbZyw.png"/></div></figure><p id="c33c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">将训练数据转换成所需的格式，以将数据输入模型。</strong></p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="5154" class="ni km iq ne b gy nj nk l nl nm">main_prefix="/content/gdrive/My Drive/final_showdown/Train/Train/"<br/>X_train_check=[]<br/><strong class="ne ir">for</strong> i <strong class="ne ir">in</strong> range(1,457):<br/>  path=main_prefix+str(i)+"/imgs/00"<br/>  X_sample = []<br/>  <strong class="ne ir">for</strong> idx <strong class="ne ir">in</strong> range(1, 9):<br/>    img_path = path + str(idx) + ".jpg"<br/>    resized_img = get_img(img_path, printer=<strong class="ne ir">False</strong>)<br/>    X_sample.append(resized_img)<br/>  X_train_check.append(np.array(X_sample))<br/>np.array(X_train_check).shape</span></pre><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/fe95627ad5203c7417b24b1fd3673df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*TcmkhaAXFnBk6KhhH7t0jA.png"/></div></figure><p id="bfa5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lm">我们的Numpy数组</em><strong class="jp ir"><em class="lm">X _ train _ check</em></strong><em class="lm">具有上述形状，这与我们的想法相对应(456个训练示例，每个示例中有8个图像，图像的尺寸)</em></p><p id="debf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从注释文件中提取X和Y方向的速度，并创建一个包含2列的NumPy数组(X和Y方向的速度)。</p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="cb7f" class="ni km iq ne b gy nj nk l nl nm">main_prefix="/content/gdrive/My Drive/final_showdown/Train/Train/"<br/>y_train_check=[]<br/><strong class="ne ir">for</strong> i <strong class="ne ir">in</strong> range (1,457):<br/>  path=main_prefix+str(i)+"/annotation.json"<br/>  f = open(path,)  <br/>  data = json.load(f) <br/>  f.close()<br/>  y_train_check.append(np.array((data[0]["velocity"])))<br/>np.array(y_train_check).shape</span></pre><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nr"><img src="../Images/fff4feca4ae12d024adde96d768a0268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCIxO6clHo5MoRtzfENlfA.png"/></div></div></figure><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="874c" class="ni km iq ne b gy nj nk l nl nm">print(np.array(y_train_check)[:5])</span></pre><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ns"><img src="../Images/b212086ccd38efce25873e880f003bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VWFRngy9j3i8KX_8IbTG3w.png"/></div></div></figure><h1 id="385e" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">模型结构</h1><p id="eb3b" class="pw-post-body-paragraph jn jo iq jp b jq mq js jt ju mr jw jx jy ms ka kb kc mt ke kf kg mu ki kj kk ij bi translated">我们将使用时间分布卷积层和LSTM来捕获顺序数据，然后使用密集层来获得最终输出。由于我们用于训练的数据量有限，我们使用<strong class="jp ir">预训练模型</strong> InceptionResNetV2作为时间分布层来从图像中捕捉细节。</p><p id="f280" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">LSTMs是一种特殊的RNN，能够学习长期依赖性。我们使用LSTM来捕获从上面的时间分布层获得的按时间顺序排列的信息，并从中获得有意义的信息。</p><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nt"><img src="../Images/3651475f317e2b9869c160aa76eae224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6SmOOW_YJwOrvIdqnIrBzQ.gif"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">模型的可视化解释。<a class="ae lq" href="https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00" rel="noopener">图像来源</a></figcaption></figure><h1 id="6d27" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">什么是预训练模型？</h1><p id="bef1" class="pw-post-body-paragraph jn jo iq jp b jq mq js jt ju mr jw jx jy ms ka kb kc mt ke kf kg mu ki kj kk ij bi translated">预先训练的模型先前已经在数据集上训练过，并且包含表示对其进行训练的任何数据集的特征的权重和偏差。学习到的特征通常可以转移到不同的数据中。</p><p id="0436" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Inception-ResNet-v2是一个卷积神经网络，它在ImageNet数据库中的超过一百万张图像上进行训练。该网络有164层，可以将图像分为1000个对象类别，如键盘、鼠标、铅笔和许多动物。因此，该网络已经学习了各种图像的丰富特征表示。</p><p id="6f0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的LSTM上使用这个预训练模型的重量将增加我们的准确性。我们使用Inception-ResNetv2模型，但我们只训练它的最后4层。再次完全训练庞大的模型将耗费大量的计算能力，并且无视预训练的使用。</p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="904b" class="ni km iq ne b gy nj nk l nl nm"><em class="lm">#Best Model to this problem was InceptionResNet V2</em></span><span id="039c" class="ni km iq ne b gy nn nk l nl nm">inceptionresnet=tf.keras.applications.InceptionResNetV2(                                       <br/>    include_top=<strong class="ne ir">False</strong>,<br/>    weights="imagenet",<br/>    input_tensor=<strong class="ne ir">None</strong>,<br/>    input_shape=(180,320,3)<br/>)<br/>  <br/><strong class="ne ir">for</strong> layer <strong class="ne ir">in</strong> inceptionresnet.layers[:-4]:                                                       <br/>    layer.trainable = <strong class="ne ir">False</strong></span><span id="5868" class="ni km iq ne b gy nn nk l nl nm"><em class="lm">#We train only the last 4 layers this Model while Freezing the other #Layers.</em></span></pre><p id="7d8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">建立模型:</strong></p><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="7cd1" class="ni km iq ne b gy nj nk l nl nm">model = Sequential()</span><span id="4c06" class="ni km iq ne b gy nn nk l nl nm"><em class="lm">#add Inception model for 8 input images (keeping the right shape)</em></span><span id="e025" class="ni km iq ne b gy nn nk l nl nm">model.add(TimeDistributed(inceptionresnet, input_shape=(8, 180, 320, 3)))     <br/>                     <br/><em class="lm">#Using TimeDistributed Layer to Feed the Image Sequence</em></span><span id="c296" class="ni km iq ne b gy nn nk l nl nm"><em class="lm"># now, flatten  each output to send 8 outputs with one dimension to #LSTM</em></span><span id="4a6d" class="ni km iq ne b gy nn nk l nl nm">model.add( TimeDistributed( Flatten() ))</span><span id="df05" class="ni km iq ne b gy nn nk l nl nm"><em class="lm">##Added LSTM to Capture the Sequencial Information</em></span><span id="80d0" class="ni km iq ne b gy nn nk l nl nm">model.add(LSTM (256, activation='relu', return_sequences=<strong class="ne ir">False</strong>))</span><span id="52f9" class="ni km iq ne b gy nn nk l nl nm"><em class="lm"># Dense Layer</em></span><span id="8969" class="ni km iq ne b gy nn nk l nl nm">model.add(Dense (64, activation='relu'))</span><span id="5d4b" class="ni km iq ne b gy nn nk l nl nm"><em class="lm">#Final Dense Layer</em></span><span id="670e" class="ni km iq ne b gy nn nk l nl nm">model.add(Dropout(.5))</span><span id="b6d4" class="ni km iq ne b gy nn nk l nl nm">model.add(Dense(2))  <br/>                     <br/><em class="lm"># Final Layer is of 2 Neurons [Velocity in X Direction and Y Direction]</em></span><span id="bd33" class="ni km iq ne b gy nn nk l nl nm">model.compile(optimizer='adam', loss='mean_squared_error', metrics=tensorflow.keras.metrics.RootMeanSquaredError())</span><span id="4d60" class="ni km iq ne b gy nn nk l nl nm">model.summary()</span></pre><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nu"><img src="../Images/bd7932580a724f0b500cfd914c9f2451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tX8ww9u4FlkR4OcmsehWsg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">模型概述描述了模型中使用的层和参数数量。</figcaption></figure><p id="60f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">相似模型结构:</strong></p><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nv"><img src="../Images/2e536df2b4cbbdd60adf6e7c13879d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0FUQ39KESKNNrsKh3h48ww.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">模型结构。<a class="ae lq" href="https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00" rel="noopener">图像来源</a></figcaption></figure><pre class="lr ls lt lu gt nd ne nf ng aw nh bi"><span id="526b" class="ni km iq ne b gy nj nk l nl nm">#Fitting the model to our training data r=model.fit(np.array(X_train_check),np.array(y_train_check),validation_split=0.2,batch_size=38,epochs=10)</span></pre><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nw"><img src="../Images/de122cc289400309fd0a50955973e452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORF8ZLU8OmHDoRxyg47Fzw.png"/></div></div></figure><figure class="lr ls lt lu gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nx"><img src="../Images/237ab39dce7ce97f84b618a9de1a9bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCvvlje5NU97GiHlSRuY0A.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">韵律学</figcaption></figure><p id="13db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">发现为<strong class="jp ir"> 10个时期</strong>训练模型有效地避免了<strong class="jp ir">过度拟合</strong>问题。</p><p id="6d83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">损失随着每个时期而减少。训练误差和验证损失在每个时期都显著下降，表明该模型表现良好。</p><p id="d1d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用这个模型对测试数据进行预测，给了我1.76的均方根误差，并帮助我在比赛中获得了第二名<strong class="jp ir">。</strong></p><p id="27aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lq" href="https://github.com/Sooryak12/Data-Unchained-By-IIIT-Delhi" rel="noopener ugc nofollow" target="_blank"> GitHub链接访问笔记本</a>。</p></div></div>    
</body>
</html>