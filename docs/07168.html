<html>
<head>
<title>A Noob’s Guide to Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林指南</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/a-noobs-guide-to-random-forest-d7398d56b01c?source=collection_archive---------14-----------------------#2021-01-28">https://levelup.gitconnected.com/a-noobs-guide-to-random-forest-d7398d56b01c?source=collection_archive---------14-----------------------#2021-01-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5164" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为一个新手，在了解随机森林的时候很容易迷路。但是不要害怕我的菜鸟朋友，因为这个博客会给你一个入门指南，让你在这个混乱的话题中生存下来。我们还将研究随机森林和决策树在数据集上的实现，这将有助于您进一步理解该算法。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/3473239cf720c15cb875c5f0c1cff91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/0*NRpxi_ezJ4GfVTNY.jpg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">来源:www.fromthegenesis.com</figcaption></figure><h1 id="db4d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="1854" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">随机森林是一种<strong class="jp ir"> <em class="me">集成学习</em> </strong>方法，主要用于分类问题，但也可用于回归。它结合了多个决策树的决策，然后将所有决策树中的大多数响应作为最终的裁决/输出。</p><p id="52d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">集成</strong>基本上是预测模型，它结合来自“弱学习者”的预测来创建一个“强预测者”。</p><p id="3253" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解和实现随机森林，你应该首先理解决策树的概念。如果你愿意，我已经在另一个博客中讨论了这个话题的理论，你可以在这里找到这个博客的链接。</p><h1 id="be63" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">工作</h1><p id="727b" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">为了理解随机森林的工作原理，我们需要理解<strong class="jp ir">“装袋”或“引导聚集”</strong>的含义。</p><p id="0fc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"><em class="me"/></strong>套袋是一种技术，我们为每棵树选择一定数量的样品进行替换。<strong class="jp ir"> <em class="me">采样与替换</em> </strong>表明，当选择数据集行进行采样时，我们可以再次选择部分或全部那些行用于其他决策树。</p><p id="eb02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在将样本分配给这些树之后，我们使用这些样本来训练这些树，然后继续使用所有这些样本来预测测试实例/行。</p><p id="8993" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们从所有的树中得到结果后，我们在分类的情况下从结果池中找出所有结果的多数标签，并在回归的情况下对结果进行平均。</p><p id="55e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这整个过程可以用下图来描述:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/8f4b61eb3a3a622260ecd726c390227e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/0*rakt_8G6Aftn7qxn.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">资料来源:towardsdatascience.com</figcaption></figure><p id="c61f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你仍然不明白这个过程，我们可以举一个装有樱桃、苹果和橘子的水果篮的例子。假设我们要在下图中找到某种水果的标签:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/ee0e3b4520f9863c0b42b0668280e1d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*oaPMwL4GvDRjv2Vb.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">来源:pinterest.com</figcaption></figure><p id="56aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种未知的水果具有以下特征:直径= 3，颜色为橙色，生长在夏季，形状为圆形。</p><p id="44b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们将使用随机森林算法来找出它的归属。</p><ul class=""><li id="b867" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">首先，我们从数据集中看到，我们为三个决策树各取一个样本，并使用各自的样本对它们进行训练，如下图所示:</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/e527d4290036db00926ff80182f55ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*C9aoWNGLIgw0PQF4.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">来源:pinterest.com</figcaption></figure><ul class=""><li id="6199" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">在对它们进行训练之后，我们将我们的测试实例“传递”到这些树中的每一棵树上，并在上面查看它们的预测。</li><li id="8e98" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">拿第一棵树来说，我们看到它首先询问直径是否≥3？对于测试实例来说，这是真的，所以，它走上了正确的道路。然后树问颜色是否是橙色的。这也是真的，所以我们沿着正确的道路前进，最终预测是一个橙子。</li><li id="102b" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">类似地，对于树2和树3，我们得到预测标签为“橙色”。</li><li id="5159" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">因此，在所有三个预测中出现次数最多的标签应该是“橙色”。</li></ul><p id="7d05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">工作非常简单，有了决策树的知识和实现，随机森林变得相当容易。</p><h1 id="c19d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">履行</h1><p id="fa0c" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">我在乳腺癌数据集上实现了这个算法，你可以在这里找到<a class="ae mf" href="https://github.com/tanya12181/Machine-Learning-Internship-at-Internity/blob/main/Task%209/Breast_cancer_data%5B1%5D.csv" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mw mx l"/></div></figure><ul class=""><li id="ca0c" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">导入numpy和pandas库。</li><li id="137c" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">pandas.read_csv()用于读取csv文件并将数据帧存储在变量数据集中。</li><li id="9cd4" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">我使用numpy.isnull()检查空值。sum()。在这个数据集中，没有任何空值。</li><li id="6ef7" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">我将我的dataframe转换成一个numpy数组，因为我个人更容易使用它的切片操作<em class="me">(我也可以使用NP . random . shuffle(&lt;array&gt;)轻松地打乱行，并且我不必在每次想要访问列或行时都使用dataframe.iloc)。</em></li><li id="8e96" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">我按照75:25的比例将数据集划分为训练集和测试集。我首先在数据集的3/4处找到索引。因为我使用的是Python 3.x，所以我必须使用整数除法，它使用“//”而不是“/”来进行普通除法。这个除法会给我一个整数值，而不是浮点数。</li><li id="2ef3" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">train = dataset[:idx，:]和test = dataset[idx::]。请注意，我没有将列车分为x列车和y列车。测试集的类似情况。当我们浏览这些函数时，您将会看到原因。</li></ul><h2 id="22f7" class="my lc iq bd ld mz na dn lh nb nc dp ll jy nd ne lp kc nf ng lt kg nh ni lx nj bi translated">决策树函数</h2><p id="e9df" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated"><strong class="jp ir">定义count_label( &lt;部分数据集&gt; ) </strong>:它为我们的响应变量中不同类型的值创建一个字典。在我们的例子中,“诊断”中不同类型的值不是0就是1。</p><p id="a967" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在“cnt”字典中，对于每种类型，它存储它在响应列中出现的次数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/affe3de310572997c09eb1ffb6c12c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*fzbRoszoaT_Uk8I4XgBJbA.png"/></div></figure><p id="8682" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义熵(数据集&gt;的&lt;部分)</strong>:它在整个数据集中寻找熵或随机性(特别是响应变量，在我们的例子中是“诊断”)。</p><p id="0373" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它首先获取响应变量列中每种类型值的出现次数，然后通过以下公式计算熵:</p><p id="bc28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">熵=</strong>σ<strong class="jp ir">(-n/总数)* log(n/总数)</strong>其中，</p><ul class=""><li id="5cf7" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">total =该部分中的行数，</li><li id="6439" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">n =特定类型的出现次数。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0f62f9b7819a63abb2475368a3d59ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*RzFcMW19ZPaLvnWwIOQp4g.png"/></div></figure><p id="3e38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义info_gain(left_split，right_split，parent_entropy): </strong>其中，left和right是数据集划分后的左右部分。</p><p id="55ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它通过以下方式告诉我们拆分后获得了多少信息:</p><ul class=""><li id="04d5" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">首先，求熵和它们的加权平均值的乘积之和。</li><li id="1aea" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">然后，用母熵减去这些信息。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nm"><img src="../Images/bcbf4bf049bdb7dc0a4c9246c18f1ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DutazDwjE-EY9zBiVjH6XQ.png"/></div></div></figure><p id="98fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">班级节点:</strong></p><p id="1bb3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建该类是为了便于存储最佳分割特征、节点的子节点(即左右分割)、给定特征的最佳问题和最终预测标注。</p><p id="239f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，对于所有不是叶节点的节点，我们将存储除预测标签之外的所有内容。</p><p id="8e6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了确定一个节点是否是叶子，我们使用方法<strong class="jp ir"> <em class="me"> is_leaf() </em> </strong>，如果预测标签不是none，则返回true。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nn"><img src="../Images/7a2e9051ddfa3f3dca3ab400707fa910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBNE_PR7I6ueviZNbmKwDg.png"/></div></div></figure><p id="db07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义分区(&lt;行的区段&gt;，问题值，特征):</strong></p><p id="6a11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，问题值对应于我们进行拆分所基于的特性或列。它根据这个“阈值”或问题值创建左右分区。</p><p id="07b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果该行的列值的值≥问题值，则将其追加到右分区。否则，它被附加到左分区。</p><p id="22b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它以numpy数组的形式返回分区。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/1d33fddf7f50e19d016eade944aa5646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*0jNWmgHr0bhnAS3FFWz2wg.png"/></div></figure><p id="565f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义best_split(rows，column): </strong></p><p id="f16b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，行定义了要分割的数据集部分，列是在分割中已经用完的要素列表。</p><p id="aa38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它返回最佳问题值和最佳属性或特征，基于这些，我们将获得最大的信息增益，并且得到的子代的熵将是最小的。</p><p id="911e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它首先检查该特征是否已经在早期迭代中用于分割。如果是，那么我们跳过这个特征，我们继续寻找这个特征的最佳可能问题值。</p><p id="701f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们找到该特征中每个值的信息增益，并将最佳可能值存储在ques变量中，该变量给出信息增益的最大值。</p><p id="f1d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="me">如果没有信息增益为&gt; 0的拆分，则返回ques和attr变量None。</em>T15】</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi np"><img src="../Images/aeda07323bf7e1140e323bdd8bfa3d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*n-RlT4LyfzfYkhjwxPkanQ.png"/></div></figure><p id="5224" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义build_tree(rows，max_depth，column): </strong></p><ul class=""><li id="3bd7" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">列是在分割中已经用完的特征的列表。</li><li id="1c9f" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">max_depth给了我们允许树生长的极限。</li><li id="296f" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">rows是数据集。</li></ul><p id="7f36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">它执行的任务:</strong></p><ul class=""><li id="1bd0" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">首先，它使用best_split函数找到最佳分割。</li><li id="c3e6" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">如果返回的属性值不是None，那么它将继续查找我们的响应变量是否只包含一种类型的值。它还更新列列表。</li><li id="4199" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">如果max_depth为0或属性/特征为None或分割为纯分割(类型数= 1)，则返回叶节点。</li><li id="9c87" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">否则，我们将行划分为左分区和右分区，并为这两个分区递归调用build_tree，使max_depth递减1，并更新列列表。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d7874f2904d9b1df44b20e3a23a50afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*wIFJiN04o5QK6cmcz8wAwg.png"/></div></figure><p id="1f43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义find_common_label(column): </strong>对于给定的特性，它返回最常见类型的值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/95d08c2d4efb214f5347862837d23b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*Q3LcYLzTdsNAME46C33soQ.png"/></div></figure><p id="12b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> define_root(rows，max_depth): </strong>调用build_tree()函数，存储我们决策树的根。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0f9018c2d63cadcedd63269f64d4ca21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*QYT7s47jIT0c9AglH5g_kg.png"/></div></figure><p id="5f6c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> define predict(test_set，root): </strong>它使用我们的训练数据建模决策树的根，并为测试集中的每个测试行给出预测。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6840f3f36731baf108d3778bb7b53557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*uOwVMpKYBQAd-ePDrAgntA.png"/></div></figure><p id="204d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">define traverse_tree(test_row，node):它将测试行中的特征值与我们的决策树中存储的问题值进行比较。</p><ul class=""><li id="415d" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">如果该节点的特征值≥问题值，则遍历正确的路径</li><li id="81ae" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">否则，遍历左侧路径。</li></ul><p id="7639" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果它到达叶节点，那么它返回预测的标签。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/440d2227b37f39571fd28af7bb339123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*eBo_yN_1evzhE8khWX9R-w.png"/></div></figure><p id="6ddd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决策树有很多功能。</p><h2 id="d8b4" class="my lc iq bd ld mz na dn lh nb nc dp ll jy nd ne lp kc nf ng lt kg nh ni lx nj bi translated">随机森林函数</h2><p id="1145" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated"><strong class="jp ir">定义random_forest_classifier(): </strong>它以树的数量、训练集、测试行(不包括响应列)、树的最大深度作为参数。</p><ul class=""><li id="0794" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated">每棵树都分配有来自训练集的样本。</li><li id="9a63" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">他们使用我们的决策树函数进行训练。</li><li id="3ebf" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">我们将每个树根附加到“树”列表，并将树预测附加到“每树预测”列表。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nv"><img src="../Images/a256eb31ad575d70dc4926909b978fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*74FJe2IhJMYOlLBpQp2uKw.png"/></div></div></figure><p id="5296" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义get_samples(训练集):</strong></p><p id="ad76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用np.random.choice()选择随机行索引。我选择样本大小为训练集的三分之一。</p><p id="99ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后将这些随机选择的行追加到样本列表中，然后返回。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5a9891c55de72fab52f9f21b3b165033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*J5ZD7Wa1J9ojiLUj99l0xw.png"/></div></figure><p id="2d80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义random _ forest _ predict(tree _ predictions，actual _ test _ response _ labels):</strong>我们对每棵树的每一行的预测进行计数，并找到多数标签，然后将其作为最终输出返回。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nx"><img src="../Images/b2eed86135455a896fefcb2f28026b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxqn61s_wFvhNn2daPJ6Uw.png"/></div></div></figure><p id="0e90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我取树的数量= 5，最大深度= 100。这些值给了我92.3%的准确率。</p><p id="f5fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">****************************************************************</p><h1 id="bd40" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">优势:</h1><ul class=""><li id="7f4b" class="mi mj iq jp b jq lz ju ma jy ny kc nz kg oa kk mn mo mp mq bi translated">它对预测值之间的多重共线性是稳健的。</li><li id="f51a" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">它可以有效地处理丢失的数据。</li><li id="fca6" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">它减少了决策树的过拟合问题。</li><li id="7ee4" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">它对分类和回归问题都有用。</li><li id="5267" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">它不要求我们对数据集中的值进行规范化，因为它是基于规则的搜索。</li></ul><h1 id="f988" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">缺点:</strong></h1><ul class=""><li id="ff97" class="mi mj iq jp b jq lz ju ma jy ny kc nz kg oa kk mn mo mp mq bi translated">大量的树可能会降低算法的速度，从而限制其在实时应用中的使用。</li><li id="705e" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">这使得我们更难解释每一个决策，这在决策树的情况下是一个很大的优势。</li><li id="e7dd" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">它需要大量的时间来训练多棵树，并且需要更多的计算能力来建立它们。</li></ul><p id="16c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">************************************************************</p><p id="4e4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，博客到此结束。这篇博客的决策树部分可能看起来相当冗长。我完全同意。我花了很多时间去理解算法和实现。我希望这篇博客能帮助你(哪怕是一点点)理解决策树和随机森林背后的工作原理。</p><p id="abbc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决不，我说这是一个完整的指南。我只能说我尽力了。</p><p id="1cc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，感谢所有花时间阅读这篇博客的读者。祝你有美好的一天😄！！！</p></div></div>    
</body>
</html>