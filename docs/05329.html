<html>
<head>
<title>Pseudo-English</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伪英语</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/pseudo-english-typing-practice-with-machine-learning-5700eb4dc54?source=collection_archive---------10-----------------------#2020-08-19">https://levelup.gitconnected.com/pseudo-english-typing-practice-with-machine-learning-5700eb4dc54?source=collection_archive---------10-----------------------#2020-08-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e8b0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过机器学习进行打字练习</h2></div><blockquote class="kf kg kh"><p id="60c6" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">本系列文章:<br/> 1。<a class="ae lf" href="https://medium.com/@bayan.bennett/typing-practice-with-machine-learning-introduction-aa3bb5d24134" rel="noopener">简介</a> <br/> 2。<a class="ae lf" rel="noopener ugc nofollow" target="_blank" href="/pseudo-english-typing-practice-with-machine-learning-5700eb4dc54">伪英语</a>(你在这里)<em class="iq"> <br/> 3。</em> <a class="ae lf" rel="noopener ugc nofollow" target="_blank" href="/keyboard-input-typing-practice-w-machine-learning-b5c5a9a362a7">键盘输入</a> <em class="iq"> <br/> 4。</em> <a class="ae lf" rel="noopener ugc nofollow" target="_blank" href="/inference-using-web-workers-f47266b7ef11"> Web Worker推断</a></p><p id="efb2" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">已完工项目位于:<a class="ae lf" href="https://www.bayanbennett.com/projects/rnn-typing-practice" rel="noopener ugc nofollow" target="_blank">https://www.bayanbennett.com/projects/rnn-typing-practice</a></p></blockquote><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/6cb5411d2c8fd8e54858d032a574c1fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wqg3PbI7nh8ZxH9CRi-Mrg.png"/></div></div></figure><h1 id="9ebe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">目标</h1><p id="dd14" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">使用递归神经网络生成类似英语的单词。</p><h1 id="c1b8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">琐碎的方法</h1><p id="57e4" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">在决定使用ML之前，首先我必须说服自己，这些琐碎的方法不能提供足够的结果。</p><h2 id="6b7d" class="ms lt iq bd lu mt mu dn ly mv mw dp mc mm mx my me mo mz na mg mq nb nc mi nd bi translated">随机字母</h2><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="4a67" class="ms lt iq nf b gy nj nk l nl nm">const getRandom = (distribution) =&gt; {<br/>  const randomIndex = Math.floor(Math.random() * distribution.length);<br/>  return distribution[randomIndex];<br/>}</span><span id="dc46" class="ms lt iq nf b gy nn nk l nl nm">const alphabet = "abcdefghijklmnopqrstuvwxyz";</span><span id="e2c5" class="ms lt iq nf b gy nn nk l nl nm">const randomLetter = getRandom(alphabet);</span></pre><p id="11e2" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">不出所料，和英语单词没有相似之处。生成的字符序列很难输入。以下是五个字母单词的几个例子:</p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="2cd9" class="ms lt iq nf b gy nj nk l nl nm">snyam   iqunm   nbspl   onrmx   wjavb   nmlgj<br/>arkpt   ppqjn   zgwce   nhnxl   rwpud   uqhuq<br/>yjwpt   vlxaw   uxibk   rfkqa   hepxb   uvxaw</span></pre><h2 id="a7c9" class="ms lt iq bd lu mt mu dn ly mv mw dp mc mm mx my me mo mz na mg mq nb nc mi nd bi translated">加权随机字母</h2><p id="33a9" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">如果我们生成的序列具有和英语一样的字母分布会怎么样？我从维基百科获得了<a class="ae lf" href="https://en.wikipedia.org/wiki/Letter_frequency" rel="noopener ugc nofollow" target="_blank">字母的频率，并创建了一个JSON文件，将字母表映射到它们相应的相对频率。</a></p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="6ed7" class="ms lt iq nf b gy nj nk l nl nm">// letter-frequencies.json<br/>{<br/>  "a": 0.08497,  "b": 0.01492,  "c": 0.02202,  "d": 0.04253,<br/>  "e": 0.11162,  "f": 0.02228,  "g": 0.02015,  "h": 0.06094,<br/>  "i": 0.07546,  "j": 0.00153,  "k": 0.01292,  "l": 0.04025,<br/>  "m": 0.02406,  "n": 0.06749,  "o": 0.07507,  "p": 0.01929,<br/>  "q": 0.00095,  "r": 0.07587,  "s": 0.06327,  "t": 0.09356,<br/>  "u": 0.02758,  "v": 0.00978,  "w": 0.02560,  "x": 0.00150,<br/>  "y": 0.01994,  "z": 0.00077<br/>}</span></pre><p id="a1a9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">这里的想法是创建一个大的字母序列，其分布与上面的频率紧密匹配。<code class="fe no np nq nf b">Math.random</code>具有均匀分布，因此当我们从序列中随机选择字母时，选择字母的概率与其频率相匹配。</p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="2bd7" class="ms lt iq nf b gy nj nk l nl nm">const TARGET_DISTRIBUTION_LENGTH = 1e4; // 10,000</span><span id="ed6e" class="ms lt iq nf b gy nn nk l nl nm">const letterFrequencyMap = require("./letter-frequencies.json");</span><span id="b6c3" class="ms lt iq nf b gy nn nk l nl nm">const letterFrequencyEntries = Object.entries(letterFrequencyMap);</span><span id="293e" class="ms lt iq nf b gy nn nk l nl nm">const reduceLetterDistribution = (result, [letter, frequency]) =&gt; {<br/>  const num = Math.round(TARGET_DISTRIBUTION_LENGTH * frequency);<br/>  const letters = letter.repeat(num);<br/>  return result.concat(letters);<br/>};</span><span id="a02b" class="ms lt iq nf b gy nn nk l nl nm">const letterDistribution = letterFrequencyEntries<br/>  .reduce(reduceLetterDistribution, "");</span><span id="29ca" class="ms lt iq nf b gy nn nk l nl nm">const randomLetter = getRandom(letterDistribution);</span></pre><p id="41a7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">元音数量的增加是显而易见的，但生成的序列仍然无法像一个英语单词。以下是五个字母单词的几个例子:</p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="e4ac" class="ms lt iq nf b gy nj nk l nl nm">aoitv   aertc   cereb   dettt   rtrsl   ararm<br/>oftoi   rurtd   ehwra   rnfdr   rdden   kidda<br/>nieri   eeond   cntoe   rirtp   srnye   enshk</span></pre><h2 id="0fd5" class="ms lt iq bd lu mt mu dn ly mv mw dp mc mm mx my me mo mz na mg mq nb nc mi nd bi translated"><a class="ae lf" href="https://en.wikipedia.org/wiki/Markov_chain" rel="noopener ugc nofollow" target="_blank">马尔可夫链</a></h2><p id="ec61" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">这将是下一个逻辑步骤，我们将创建字母序列对的概率。这就是我决定直接去RNNs的原因。如果有人想实现这种方法，我很想看看结果。</p><h1 id="5cf7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">递归神经网络</h1><p id="4f6c" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">神经网络通常是无记忆的，系统没有来自先前步骤的信息。RNNs是一种神经网络，其中网络的先前状态是当前步骤的输入。</p><ul class=""><li id="a256" class="nr ns iq kl b km kn kp kq mm nt mo nu mq nv le nw nx ny nz bi translated"><strong class="kl ir">输入</strong>:一个字符</li><li id="ad74" class="nr ns iq kl b km oa kp ob mm oc mo od mq oe le nw nx ny nz bi translated"><strong class="kl ir">输出</strong>:下一个字符概率的张量。</li></ul><p id="bf82" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">神经网络天生不擅长处理不同长度的输入，有办法解决这个问题<em class="kk">(就像变形金刚中的位置编码)</em>。对于RNNs，输入的大小是一致的，只有一个字符。自然语言处理对rnn有天然的亲和力，因为语言是单向的(LTR或RTL ),字符的顺序很重要。换句话说，虽然<em class="kk">联</em>和<em class="kk">解</em>这两个字只交换了两个字，但意思却相反。</p><p id="2e40" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">下面的模型基于<a class="ae lf" href="https://www.tensorflow.org/tutorials/text/text_generation#build_the_model" rel="noopener ugc nofollow" target="_blank"> Tensorflow文本生成和RNN </a>教程。</p><h2 id="37a1" class="ms lt iq bd lu mt mu dn ly mv mw dp mc mm mx my me mo mz na mg mq nb nc mi nd bi translated">嵌入的输入层</h2><p id="fe08" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">这是我第一次遇到嵌入层的概念。这是一个迷人的概念，我很高兴开始使用它。</p><p id="1e6d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">我在这里写了一篇总结嵌入的短文:<a class="ae lf" href="https://bayanbennett.com/posts/embeddings-in-machine-learning" rel="noopener ugc nofollow" target="_blank">https://bayanbennett . com/posts/embeddings-in-machine-learning</a></p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="502b" class="ms lt iq nf b gy nj nk l nl nm">const generateEmbeddingLayer = (batchSize, outputDim) =&gt;<br/>  tf.layers.embedding({<br/>    inputDim: vocabSize,<br/>    outputDim,<br/>    maskZero: true,<br/>    batchInputShape: [batchSize, null],<br/>  });</span></pre><h2 id="fd50" class="ms lt iq bd lu mt mu dn ly mv mw dp mc mm mx my me mo mz na mg mq nb nc mi nd bi translated">门控循环单元(GRU)</h2><p id="14d7" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">我没有足够的知识来证明<em class="kk">为什么选择</em>GRU，所以我推迟到前面提到的Tensorflow教程中的实现。</p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="90d3" class="ms lt iq nf b gy nj nk l nl nm">const generateRnnLayer = (units) =&gt;<br/>  tf.layers.gru({<br/>    units,<br/>    returnSequences: true,<br/>    recurrentInitializer: "glorotUniform",<br/>    activation: "softmax",<br/>  });</span></pre><h1 id="ff32" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">把所有的放在一起</h1><p id="ef43" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">由于我们是将一层的输出顺序输入到另一层的输入中，<code class="fe no np nq nf b">tf.Sequential</code>是我们应该使用的模型类。</p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="2013" class="ms lt iq nf b gy nj nk l nl nm">const generateModel = (embeddingDim, rnnUnits, batchSize) =&gt; {<br/>  const layers = [<br/>    generateEmbeddingLayer(batchSize, embeddingDim),<br/>    generateRnnLayer(rnnUnits),<br/>  ];<br/>  return tf.sequential({ layers });<br/>};</span></pre><h1 id="ee0e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">培训用数据</h1><p id="fa3d" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">我使用普林斯顿的WordNet 3.1数据集作为单词的来源。</p><blockquote class="kf kg kh"><p id="dc29" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">WordNet是一个大型的英语词汇数据库。名词、动词、形容词和副词被分组为认知同义词集(synset)……”<br/>——普林斯顿大学“关于WordNet。”<a class="ae lf" href="https://wordnet.princeton.edu/" rel="noopener ugc nofollow" target="_blank"> WordNet </a>。普林斯顿大学。2010.</p></blockquote><p id="8230" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">因为我只对单词感兴趣，所以我解析了每个文件，只提取了单词。带空格的单词被拆分成单独的单词。符合以下条件的单词也被删除:</p><ul class=""><li id="3b94" class="nr ns iq kl b km kn kp kq mm nt mo nu mq nv le nw nx ny nz bi translated">带有音调符号的单词</li><li id="adbc" class="nr ns iq kl b km oa kp ob mm oc mo od mq oe le nw nx ny nz bi translated">单字符单词</li><li id="2ac6" class="nr ns iq kl b km oa kp ob mm oc mo od mq oe le nw nx ny nz bi translated">含数字的单词</li><li id="4f7a" class="nr ns iq kl b km oa kp ob mm oc mo od mq oe le nw nx ny nz bi translated">罗马数字</li><li id="bf76" class="nr ns iq kl b km oa kp ob mm oc mo od mq oe le nw nx ny nz bi translated">重复的单词</li></ul><h1 id="5429" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据集生成器</h1><p id="87c9" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated"><code class="fe no np nq nf b">tf.LayersModel</code>和<code class="fe no np nq nf b">tf.Sequential</code>都有<code class="fe no np nq nf b">.fitDataset</code> <a class="ae lf" href="https://js.tensorflow.org/api/latest/#tf.Sequential.fitDataset" rel="noopener ugc nofollow" target="_blank">方法</a>，这是一种方便的方式— <em class="kk">拟合数据集</em>。我们需要创建一个<code class="fe no np nq nf b">tf.data.Dataset</code>，但首先这里有一些助手函数:</p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="d047" class="ms lt iq nf b gy nj nk l nl nm">// utils.js</span><span id="fb49" class="ms lt iq nf b gy nn nk l nl nm">const characters = Array.from("\0 abcdefghijklmnopqrstuvwxyz");<br/>const mapCharToInt = Object.fromEntries(<br/>  characters.map((char, index) =&gt; [char, index])<br/>);</span><span id="d2f2" class="ms lt iq nf b gy nn nk l nl nm">const vocabSize = characters.length;</span><span id="3a79" class="ms lt iq nf b gy nn nk l nl nm">const int2Char = (int) =&gt; characters[int];<br/>const char2Int = (char) =&gt; mapCharToInt[char];</span><span id="e0e4" class="ms lt iq nf b gy nn nk l nl nm">// dataset.js</span><span id="f72f" class="ms lt iq nf b gy nn nk l nl nm">const wordsJson = require("./wordnet-3.1/word-set.json");<br/>const wordsArray = Array.from(wordsJson);</span><span id="744d" class="ms lt iq nf b gy nn nk l nl nm">// add 1 to max length to accommodate a single space that follows each word<br/>const maxLength = wordsArray.reduce((max, s) =&gt; Math.max(max, s.length), 0) + 1;</span><span id="85c2" class="ms lt iq nf b gy nn nk l nl nm">const data = wordsArray.map((word) =&gt; {<br/>  const paddedWordInt = word<br/>    .concat(" ")<br/>    .padEnd(maxLength, "\0")<br/>    .split("")<br/>    .map(char2Int);<br/>  return { input: paddedWordInt, expected: paddedWordInt.slice(1).concat(0) };<br/>});</span><span id="5bb1" class="ms lt iq nf b gy nn nk l nl nm">function* dataGenerator() {<br/>  for (let { input, expected } of data) {<br/>    /* If I try to make the tensors inside `wordsArray.map`,<br/>     * I get an error on the second epoch of training */<br/>    yield { xs: tf.tensor1d(input), ys: tf.tensor1d(expected) };<br/>  }<br/>}</span><span id="2c98" class="ms lt iq nf b gy nn nk l nl nm">module.exports.dataset = tf.data.generator(dataGenerator);</span></pre><p id="a1dd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">注意，我们需要所有的输入都是相同的长度，所以我们用空字符填充所有的单词，这些字符将通过<code class="fe no np nq nf b">char2Int</code>函数转换成整数0。</p><h1 id="12c8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">生成和编译模型</h1><p id="fe8b" class="pw-post-body-paragraph ki kj iq kl b km mk jr ko kp ml ju kr mm mn ku kv mo mp ky kz mq mr lc ld le ij bi translated">这就是我们一直在努力打造的时刻:</p><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="9bb7" class="ms lt iq nf b gy nj nk l nl nm">const BATCH_SIZE = 500;</span><span id="5b60" class="ms lt iq nf b gy nn nk l nl nm">const batchedData = dataset.shuffle(10 * BATCH_SIZE).batch(BATCH_SIZE, false);<br/>const model = generateModel(vocabSize, vocabSize, BATCH_SIZE);<br/>const optimizer = tf.train.rmsprop(1e-2);</span><span id="009d" class="ms lt iq nf b gy nn nk l nl nm">model.compile({<br/>  optimizer,<br/>  loss: "sparseCategoricalCrossentropy",<br/>  metrics: tf.metrics.sparseCategoricalAccuracy,<br/>});</span><span id="e031" class="ms lt iq nf b gy nn nk l nl nm">model.fitDataset(batchedData, { epochs: 100 });</span></pre><p id="e5a0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">选择批量大小为500，因为这是我在不耗尽内存的情况下可以容纳的大小。</p><h1 id="2696" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">例子</h1><pre class="lh li lj lk gt ne nf ng nh aw ni bi"><span id="2eef" class="ms lt iq nf b gy nj nk l nl nm">ineco uno kam whya qunaben qunobin<br/>xexaela sadinon zaninab mecoomasph<br/>anonyus lyatra fema inimo unenones</span></pre><p id="5ad0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">它并不完美，但它产生的单词似乎来自另一种罗曼语或日耳曼语。<code class="fe no np nq nf b">model.json</code>和<code class="fe no np nq nf b">weights.bin</code>文件的大小只有44 kB。这一点很重要，因为更简单的模型通常运行推理更快，并且足够轻便，最终用户可以下载，而不会影响感知的页面性能。</p><p id="23e0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated">下一步是有趣的开始，构建一个打字练习web应用程序！</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="4c26" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mm kt ku kv mo kx ky kz mq lb lc ld le ij bi translated"><em class="kk">原籍:</em></p><div class="om on gp gr oo op"><a href="https://www.bayanbennett.com/posts/typing-practice-with-machine-learning-pseudo-english" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd ir gy z fp ou fr fs ov fu fw ip bi translated">机器学习打字练习:伪英语</h2><div class="ow l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">www.bayanbennett.com</p></div></div></div></a></div></div></div>    
</body>
</html>