<html>
<head>
<title>Generating YouTube Video Titles using LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LSTMs生成YouTube视频标题</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/generating-youtube-titles-using-lstms-19baa1948088?source=collection_archive---------13-----------------------#2020-02-16">https://levelup.gitconnected.com/generating-youtube-titles-using-lstms-19baa1948088?source=collection_archive---------13-----------------------#2020-02-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/19d1e178435e1e28a898d7a8d6a6d8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6DwzipLk8ygjP4zzhrgzuA.jpeg"/></div></div></figure><p id="9e87" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">文本生成是一种语言建模问题，并且是包括文本摘要、语音到文本和会话系统在内的许多自然语言处理任务的核心问题。语言模型可以使用序列中先前出现的单词作为上下文来学习单词出现的可能性。在本文中，我将使用YouTube趋势视频数据集来训练一个使用递归神经网络的文本生成语言模型，该模型将用于生成YouTube视频标题。</p><h2 id="41e9" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">生成N元符号序列</strong></h2><p id="76c7" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">语言建模需要输入数据作为一系列标记。数据清理和转换后的第一步是生成一系列n元语法标记。n元语法是来自文本或语音语料库的给定样本的n个项目的相邻序列。这些项目可以是单词、音节、音素、字母或碱基对。在这种情况下，n-grams是来自YouTube标题语料库的单词序列。标记化是从语料库中提取标记的过程。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="748f" class="kz la it mc b gy mg mh l mi mj">tokenizer = Tokenizer()</span><span id="0daa" class="kz la it mc b gy mk mh l mi mj">def get_sequence_of_tokens(corpus):<br/>  #get tokens<br/>  tokenizer.fit_on_texts(corpus)<br/>  total_words = len(tokenizer.word_index) + 1<br/> <br/>  #convert to sequence of tokens<br/>  input_sequences = []<br/>  for line in corpus:<br/>  token_list = tokenizer.texts_to_sequences([line])[0]<br/>  for i in range(1, len(token_list)):<br/>  n_gram_sequence = token_list[:i+1]<br/>  input_sequences.append(n_gram_sequence)<br/> <br/>  return input_sequences, total_words</span><span id="6f0e" class="kz la it mc b gy mk mh l mi mj">inp_sequences, total_words = get_sequence_of_tokens(corpus)</span></pre><p id="1c12" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从输出来看，标记[1，88]，[1，88，4064]，[1，88，4064，70]等表示从输入数据生成的n元短语。输出中的每个整数对应于文本词汇中的一个特定单词。例如</p><blockquote class="ml mm mn"><p id="2ac7" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">标题:我们想谈谈婚姻</p><p id="f0ee" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">(N元语法:代币)</p><p id="6e4a" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">我们想要:[1，88]</p><p id="59bc" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">我们想:[1，88，4064]</p><p id="dc30" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">我们想谈谈:[1，88，4064，70]</p><p id="0e58" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">我们想谈谈:[1，88，4064，70，368]</p><p id="b12c" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">我们想谈谈婚姻:[1，88，4064，70，368，1313]</p></blockquote><h2 id="26fc" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">填充序列并获得预测器和目标</strong></h2><p id="3e56" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">因为序列可以是可变长度的，所以使序列长度相等是很重要的。当使用神经网络时，我们通常将输入输入到网络中，期望得到输出。在实践中，批量处理数据比逐个处理更有效。这通过使用矩阵[批次大小x序列长度]来完成，其中序列长度对应于最长的序列。在这种情况下，我们用一个令牌(通常为0)填充序列，以适应矩阵的大小。用记号填充序列的过程称为填充。为了将数据输入到学习模型中，我需要创建预测器和标签。我将创建n-gram序列作为预测器，并将n-gram的下一个单词作为标签。例如</p><blockquote class="ml mm mn"><p id="2631" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">标题:人人炸鸡食谱</p><p id="a4d2" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">(预测值:标签)</p><p id="9b84" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">油炸:鸡肉</p><p id="c346" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">炸鸡:食谱</p><p id="fea3" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">炸鸡食谱:适合</p><p id="8c09" class="kb kc mo kd b ke kf kg kh ki kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx ky im bi translated">炸鸡食谱适合:所有人</p></blockquote><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="bb16" class="kz la it mc b gy mg mh l mi mj">def generate_padded_sequences(input_sequences):<br/>  max_sequence_len = max([len(x) for x in input_sequences])<br/>  input_sequences = np.array(pad_sequences(input_sequences,  maxlen=max_sequence_len, padding=’pre’))<br/>  predictors, label = input_sequences[:,:-1], input_sequences[:, -1]<br/>  label = ku.to_categorical(label, num_classes = total_words)<br/>  return predictors, label, max_sequence_len</span><span id="f386" class="kz la it mc b gy mk mh l mi mj">predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)</span></pre><h2 id="d759" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">长短期记忆(LSTM) </strong></h2><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ms"><img src="../Images/e2b92e68d6059456c2647a2d237fb177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-_gfwZVFm7uLNZyXqvAxA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">LSTM</figcaption></figure><p id="db09" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在递归神经网络中，激活输出在两个方向上传播，即从输入到输出和从输出到输入，这与激活输出仅在一个方向上传播的前馈神经网络不同。这在神经网络结构中产生了环路，充当神经元的“记忆状态”。因此，RNN会在不同的时间步骤中保持某种状态，或者“记住”随着时间的推移所学到的东西。内存状态有其优点，但也有缺点。消失渐变就是其中之一。在这个问题中，当学习大量层时，网络学习和调整早期层的参数变得非常困难。为了解决这个问题，开发了一种新型RNN；LSTM(长短期记忆)。</p><p id="915d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">LSTM包含一个附加状态(小区状态),该状态实质上使网络能够了解在长期状态中存储什么、丢弃什么以及从中读取什么。该模型中的LSTM包含三层</p><p id="99f6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">1.输入层:将单词序列作为输入</p><p id="3076" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2.LSTM图层:使用lstm单位计算输出。</p><p id="8e71" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">3.脱落层:帮助防止过度拟合的调整层</p><p id="a217" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">4.输出层:计算下一个单词作为输出的概率</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="2140" class="kz la it mc b gy mg mh l mi mj">def create_model(max_sequence_len, total_words):<br/>  input_len = max_sequence_len — 1<br/>  model = Sequential()<br/> <br/>  # Add Input Embedding Layer<br/>  model.add(Embedding(total_words, 10, input_length=input_len))<br/> <br/>  # Add Hidden Layer 1 — LSTM Layer<br/>  model.add(LSTM(100))<br/>  model.add(Dropout(0.1))<br/> <br/>  # Add Output Layer<br/>  model.add(Dense(total_words, activation=’softmax’))</span><span id="19bd" class="kz la it mc b gy mk mh l mi mj">  model.compile(loss=’categorical_crossentropy’, optimizer=’adam’)<br/> <br/>  return model</span><span id="c8aa" class="kz la it mc b gy mk mh l mi mj">model = create_model(max_sequence_len, total_words)</span><span id="cbe8" class="kz la it mc b gy mk mh l mi mj">model.fit(predictors, label, epochs=20, verbose=5)</span></pre><p id="348c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">生成文本</strong></p><p id="bf81" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">既然模型架构已经准备好了，并且已经使用数据对其进行了训练，那么就该根据输入的单词来预测YouTube的标题了。首先对输入单词进行标记化，然后对序列进行填充，然后将其传递给训练好的模型，以返回预测的序列。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="b33a" class="kz la it mc b gy mg mh l mi mj">def generate_text(seed_text, next_words, model, max_sequence_len):<br/>  for _ in range(next_words):<br/>  token_list = tokenizer.texts_to_sequences([seed_text])[0]<br/>  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1,  padding=’pre’)<br/>  predicted = model.predict_classes(token_list, verbose=0)<br/> <br/>  output_word = “”<br/>  for word,index in tokenizer.word_index.items():<br/>  if index == predicted:<br/>  output_word = word<br/>  break<br/>  seed_text += “ “+output_word</span><span id="9533" class="kz la it mc b gy mk mh l mi mj">  return seed_text.title()</span></pre><p id="b7ed" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">结果</strong></p><p id="94b0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">是时候查看一些结果了。输入如下:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="ca4b" class="kz la it mc b gy mg mh l mi mj">print(generate_text(“spiderman”, 5, model, max_sequence_len))</span><span id="5a5a" class="kz la it mc b gy mk mh l mi mj">print(generate_text(“lose”, 5, model, max_sequence_len))</span><span id="b4ba" class="kz la it mc b gy mk mh l mi mj">print(generate_text(“Dave”, 3, model, max_sequence_len))</span><span id="c818" class="kz la it mc b gy mk mh l mi mj">print(generate_text(“episode”, 4, model, max_sequence_len))</span><span id="1b4b" class="kz la it mc b gy mk mh l mi mj">print(generate_text(“fast”, 6, model, max_sequence_len))</span></pre><p id="7a2e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中整数表示除了输入单词之外的标题的期望长度。</p><p id="c632" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以下是生成的标题:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="9a26" class="kz la it mc b gy mg mh l mi mj">Spiderman The Voice 2018 Blind Audition</span><span id="8d31" class="kz la it mc b gy mk mh l mi mj">Lose The Item Behind The Wall</span><span id="1be2" class="kz la it mc b gy mk mh l mi mj">Dave Chappelle Equanimity Clip</span><span id="81be" class="kz la it mc b gy mk mh l mi mj">Episode 11 Albeet Alkebeer Woods</span><span id="39d4" class="kz la it mc b gy mk mh l mi mj">Fast Food Restaurants To Get A Snowman</span></pre><p id="8fb6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我得说这个模型做得很好。</p><p id="85af" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">(<em class="mo">完整代码和数据可用</em> <a class="ae mx" href="https://github.com/obie-china/Generating-Youtube-Titles-with-LSTM" rel="noopener ugc nofollow" target="_blank"> <em class="mo">此处</em> </a>)</p></div></div>    
</body>
</html>