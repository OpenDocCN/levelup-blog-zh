<html>
<head>
<title>How to run Scrapy spiders in your Python program</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Python程序中运行Scrapy spiders</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/how-to-run-scrapy-spiders-in-your-program-7db56792c1f7?source=collection_archive---------4-----------------------#2022-02-18">https://levelup.gitconnected.com/how-to-run-scrapy-spiders-in-your-program-7db56792c1f7?source=collection_archive---------4-----------------------#2022-02-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7d01" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习以编程方式运行Scrapy蜘蛛</h2></div><p id="d5e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们<a class="ae le" href="https://medium.com/codex/how-to-build-a-scraping-project-with-scrapy-and-mongodb-46e78b6549e3" rel="noopener">以前</a>介绍过<a class="ae le" href="https://medium.com/codex/how-to-build-a-scraping-project-with-scrapy-and-mongodb-46e78b6549e3" rel="noopener">如何用Scrapy和MongoDB </a>构建一个抓取项目。通常你会用<code class="fe lf lg lh li b">scrapy crawl</code>命令运行蜘蛛。您不会只运行它们一次，而是会安排它们重复运行，以便不断获取新数据。刮擦作业可以简单地通过<a class="ae le" href="https://en.wikipedia.org/wiki/Cron" rel="noopener ugc nofollow" target="_blank"> CRON作业</a>或更复杂的程序如<a class="ae le" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Airflow </a>来安排。</p><p id="85f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，有时您可能不希望安排被动运行擦除作业。相反，您可能希望基于某些条件以编程方式触发抓取作业。例如，您可能希望根据用户输入触发刮擦作业。</p><p id="5ea1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将介绍在您的程序中运行Scrapy spiders的不同方法，然后您将会很好地理解在您自己的案例中使用哪种方法。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/fddd93d5cf4d7986bb5bd224b5f99df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Epo8ZIhoEGihAt6v.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">图片来自<a class="ae le" href="https://pixabay.com/illustrations/developer-programmer-technology-3461405/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>。</figcaption></figure></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="097c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本帖的演示中，我们将使用<a class="ae le" href="https://medium.com/codex/how-to-build-a-scraping-project-with-scrapy-and-mongodb-46e78b6549e3" rel="noopener">上一篇文章</a>中介绍的蜘蛛。蜘蛛的代码可以从<a class="ae le" href="https://github.com/lynnkwong/run-scrapy-spiders-in-program" rel="noopener ugc nofollow" target="_blank">克隆到这里</a>。或者，你可以按照<a class="ae le" href="https://medium.com/codex/how-to-build-a-scraping-project-with-scrapy-and-mongodb-46e78b6549e3" rel="noopener">上一篇文章</a>中介绍的步骤，自己编写代码，以便更熟悉Scrapy框架。</p><p id="6eb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">特别是这篇文章中的演示，创建了一个新的蜘蛛<code class="fe lf lg lh li b">AuthorSpider</code>，它将用于演示如何使用Scrapy API同时运行多个蜘蛛。此外，为了简单起见，抓取的数据没有保存到MongoDB。如果您愿意，可以为MongoDB 添加<a class="ae le" href="https://github.com/lynnkwong/scrapy-quotes-demo/blob/main/scraping_proj/pipelines.py" rel="noopener ugc nofollow" target="_blank">管道。</a></p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="71fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<em class="mg">子进程</em>运行蜘蛛程序。</p><p id="1a2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如本文中<a class="ae le" href="https://medium.com/codex/how-to-execute-shell-commands-properly-in-python-5b90c1a9213f" rel="noopener">所介绍的，我们可以将<code class="fe lf lg lh li b">scrapy crawl</code>命令作为shell命令运行。由于<em class="mg">子进程</em>模块的安全性和其他便利特性，建议</a><a class="ae le" href="https://medium.com/codex/how-to-execute-shell-commands-properly-in-python-5b90c1a9213f" rel="noopener">使用<em class="mg">子进程</em>模块而不是<code class="fe lf lg lh li b">os.system()</code>函数来运行shell命令。</a></p><p id="03d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想同步运行蜘蛛并等待结果，你可以使用<code class="fe lf lg lh li b">subprocess.run()</code>:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="89a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们已经将抓取日志重定向到了<code class="fe lf lg lh li b">/tmp/scrapy.log</code>，所以作业应该会安静地完成。如果想查看脚本中的日志，可以在<code class="fe lf lg lh li b"><a class="ae le" href="https://github.com/lynnkwong/run-scrapy-spiders-in-program/blob/main/scraping_proj/settings.py" rel="noopener ugc nofollow" target="_blank">settings.py</a></code>中注释掉下面一行:</p><pre class="lk ll lm ln gt mj li mk ml aw mm bi"><span id="e3ba" class="mn mo it li b gy mp mq l mr ms">LOG_FILE = "/tmp/scrapy.log"</span></pre><p id="de27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为Scrapy将所有日志重定向到标准错误(STDERR ),所以我们只需要管道STDERR:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mh mi l"/></div></figure></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="5f82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果想异步运行蜘蛛，可以使用<code class="fe lf lg lh li b">subprocess.Popen()</code>:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="1489" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe lf lg lh li b">proc.poll()</code>如果作业仍在运行，则返回<code class="fe lf lg lh li b">None</code>，如果作业已完成，则返回退出代码。关于<code class="fe lf lg lh li b">subprocess.run()</code>和<code class="fe lf lg lh li b">subprocess.Popen()</code>的更多细节，请参考<a class="ae le" href="https://medium.com/codex/how-to-execute-shell-commands-properly-in-python-5b90c1a9213f" rel="noopener">这篇文章</a>。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="32dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用</strong> <code class="fe lf lg lh li b"><strong class="kk iu">CrawlerProcess</strong></code> <strong class="kk iu">在同一进程中运行多个蜘蛛</strong>。</p><p id="96e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面我们已经介绍了如何使用<em class="mg">子进程</em>模块在你的程序中运行Scrapy spiders。使用<em class="mg">子进程</em>是在你的程序中运行蜘蛛的一种幼稚的方式。当您只想在每个进程中运行一个spider时，这种方法很有效。如果你想在每个进程中运行多个蜘蛛，或者想在你的程序中直接获取和使用抓取的项目，你需要使用Scrapy的内部API。</p><p id="e1d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先展示使用Scrapy的内部API运行蜘蛛的代码，然后解释有趣的部分:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="d491" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你是Scrapy的新手，上面的代码可能很难理解。不要担心，我们现在将解释所有技术细节，这将使您更容易理解:</p><ul class=""><li id="007c" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">我们使用<code class="fe lf lg lh li b">get_project_settings()</code>函数根据<code class="fe lf lg lh li b"><a class="ae le" href="https://github.com/lynnkwong/run-scrapy-spiders-in-program/blob/main/scraping_proj/settings.py" rel="noopener ugc nofollow" target="_blank">settings.py</a></code>模块中的设置来获取项目的设置。返回的<code class="fe lf lg lh li b">settings</code>是类<code class="fe lf lg lh li b">scrapy.settings.Settings</code>的一个实例，其行为非常类似于字典。</li><li id="a749" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">我们使用<code class="fe lf lg lh li b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess" rel="noopener ugc nofollow" target="_blank">CrawlerProcess</a></code>类在一个进程中同时运行多个Scrapy蜘蛛。我们需要用项目设置创建一个<code class="fe lf lg lh li b">CrawlerProcess</code>的实例。</li><li id="6b52" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">如果我们想对蜘蛛进行自定义设置，我们需要为蜘蛛创建一个<a class="ae le" href="https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess" rel="noopener ugc nofollow" target="_blank">爬虫</a>的实例。爬虫在Scrapy中是一个很抽象也很重要的概念。当你想拥有Scrapy的高级配置时，你会遇到它。尤其是当你使用<code class="fe lf lg lh li b">from_crawler()</code>类方法的时候。Crawler对象提供了对所有Scrapy核心组件的访问，尤其是设置，这是扩展访问它们并将它们的功能与Scrapy挂钩的唯一方式。请注意，您需要继承项目的设置，否则项目设置将无效。在本例中，如果不使用<code class="fe lf lg lh li b">**settings</code>语法，日志将处于调试模式，并将打印在屏幕上，而不是发送到日志文件<code class="fe lf lg lh li b">/tmp/scrapy.log</code>。你可以自己尝试一下。</li><li id="c4ea" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">我们可以用<code class="fe lf lg lh li b">CrawlerProcess</code>实例的<code class="fe lf lg lh li b">crawl()</code>方法运行/抓取蜘蛛的爬虫。特别是，我们可以将键/值对传递给这个方法，它们将表现为自定义属性，就像之前的文章中介绍的那样。对于<code class="fe lf lg lh li b">quotes</code>蜘蛛，我们只想刮去带有<em class="mg">爱</em>标签的引号。</li><li id="7a04" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">我们可以在同一个进程中爬行多个蜘蛛/爬虫，它们直到调用<code class="fe lf lg lh li b">start()</code>方法才会开始运行。</li></ul><p id="d39c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<code class="fe lf lg lh li b">CrawlerProcess</code>在同一个进程中运行多个蜘蛛的代码可以在<a class="ae le" href="https://github.com/lynnkwong/run-scrapy-spiders-in-program/blob/main/scraping_script_with_api.py" rel="noopener ugc nofollow" target="_blank">这个脚本</a>中找到。如果您在克隆的项目文件夹中运行这个脚本，您将会看到引文和作者。请注意，您需要等待几秒钟才能完成抓取作业。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="5235" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用信号直接获取</strong>程序中 <strong class="kk iu">的项目。</strong></p><p id="ff83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，我们会将抓取的项目保存在一个数据库中，要么是MySQL之类的关系数据库，要么是MongoDB之类的NoSQL数据库。然而，有时我们会希望直接在程序中访问抓取的项目，并以某种方式处理它们。在这种情况下，我们可以使用爬行实例的<a class="ae le" href="https://docs.scrapy.org/en/latest/topics/signals.html#topics-signals" rel="noopener ugc nofollow" target="_blank">信号</a>,用于在某些事件发生时发出通知:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="65f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，信号是通过<code class="fe lf lg lh li b">Crawler.signals.connect()</code>方法相加的。在上面的代码中，我们将<code class="fe lf lg lh li b">quote_cralwer</code>连接到三个信号，即当蜘蛛启动时(<code class="fe lf lg lh li b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/signals.html#scrapy.signals.spider_opened" rel="noopener ugc nofollow" target="_blank">signals.spider_opened</a></code>)、当一个项目被刮除时(<code class="fe lf lg lh li b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/signals.html#scrapy.signals.item_scraped" rel="noopener ugc nofollow" target="_blank">signals.item_scraped</a></code>)和当蜘蛛关闭时(<code class="fe lf lg lh li b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/signals.html#scrapy.signals.spider_closed" rel="noopener ugc nofollow" target="_blank">signals.spider_closed</a></code>)。每个信号都与一个函数相关联，当信号被触发时，这个函数将被调用。这个函数像JavaScript中的<a class="ae le" href="https://developer.mozilla.org/en-US/docs/Glossary/Callback_function" rel="noopener ugc nofollow" target="_blank">回调函数</a>一样工作。</p><p id="0852" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些信号可以为连接的函数提供几个参数。你不需要全部都用，只能用需要的。例如，对于<code class="fe lf lg lh li b">signals.spider_opened</code>和<code class="fe lf lg lh li b">signals.spider_closed</code>的关联函数，我们只使用<code class="fe lf lg lh li b">spider</code>参数，这是蜘蛛发出的信号。您可以查看<a class="ae le" href="https://docs.scrapy.org/en/latest/topics/signals.html#spider-signals" rel="noopener ugc nofollow" target="_blank">信号文档</a>以了解每个信号参数的更多详细信息。</p><p id="12a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在程序中直接抓取项目的代码可以在<a class="ae le" href="https://github.com/lynnkwong/run-scrapy-spiders-in-program/blob/main/scraping_script_with_api_and_signals.py" rel="noopener ugc nofollow" target="_blank">这个脚本</a>中找到。如果您在克隆的项目文件夹中运行这个脚本，您将看到打开/关闭蜘蛛的信息，以及所有抓取的项目:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mh mi l"/></div></figure></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="563b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们介绍了在你的程序中运行Scrapy spiders的两种方法，即使用<em class="mg">子进程</em>模块和Scrapy内部API的<code class="fe lf lg lh li b">CrawlerProcess</code>类。使用<em class="mg">子流程</em>模块运行<code class="fe lf lg lh li b">scrapy crawl</code>命令非常简单。但是，您不能在同一个进程中同时运行多个spiders，并且您不能在您的程序中直接访问抓取的项目。这些高级特性可以通过内部API的<code class="fe lf lg lh li b">CrawlerProcess</code>类来实现。如果你是Scrapy的高级用户，并且想对蜘蛛如何在你的程序中运行有更多的控制，你可以直接使用<code class="fe lf lg lh li b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerRunner" rel="noopener ugc nofollow" target="_blank">CrawlerRunner</a></code>类，它由幕后的<code class="fe lf lg lh li b">CrawlerProcess</code>类使用。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="ae50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相关文章:</p><ul class=""><li id="082b" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated"><a class="ae le" href="https://medium.com/codex/how-to-build-a-scraping-project-with-scrapy-and-mongodb-46e78b6549e3?source=your_stories_page----------------------------------------" rel="noopener">如何用Scrapy和MongoDB构建一个抓取项目</a></li><li id="7511" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated"><a class="ae le" href="https://medium.com/codex/how-to-execute-shell-commands-properly-in-python-5b90c1a9213f?source=your_stories_page----------------------------------------" rel="noopener">如何在Python中正确执行shell命令</a></li></ul></div></div>    
</body>
</html>