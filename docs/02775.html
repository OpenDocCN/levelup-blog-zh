<html>
<head>
<title>Map-Reduce with Python and Hadoop on AWS EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AWS EMR上使用Python和Hadoop实现Map-Reduce</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/map-reduce-with-python-hadoop-on-aws-emr-341bdd07b804?source=collection_archive---------2-----------------------#2020-04-04">https://levelup.gitconnected.com/map-reduce-with-python-hadoop-on-aws-emr-341bdd07b804?source=collection_archive---------2-----------------------#2020-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/4d5833d3e102a2e754d113d9593236b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdNmGXaypjr8eDyiSobs2g.png"/></div></div></figure><p id="7037" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">让我们在AWS EMR上做一些基本的</em><strong class="kd iu"><em class="kz">Map-Reduce</em></strong><em class="kz">，用典型的字数示例，但是使用Python和Hadoop流。</em></p><p id="0f0a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">老实说，Hadoop现在作为一个框架已经过时了……但Map-Reduce没有，因为Map-Reduce是一种范式——或者说是一种通过将问题分成多个可以并行处理的子问题来解决问题的方法(这是<strong class="kd iu"> Map </strong>步骤)。一旦子问题得到解决，我们就可以收集和汇总答案(这就是<strong class="kd iu"> Reduce </strong>步骤)。</p><p id="1e74" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作为Hadoop框架的一个核心组件，Map-Reduce现在仍然适用，并且可能会一直适用，所以理解它并进行一些实践是很有好处的。</p><h1 id="3b58" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">1.启动EMR集群</h1><p id="c6d4" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">非常简单:只需在AWS管理控制台搜索栏中键入<em class="kz"> EMR </em>，然后在下一页中点击<em class="kz"> Create Cluster </em>:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/fdbdc140ae226ebc2800ddbf53b92b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6gCUQlBs3lFW3Rnl.png"/></div></div></figure><p id="780b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">点击<em class="kz">创建集群</em>后，您将被带到一个页面，在这里您可以定制您的集群。向下滚动到页面底部，直到看到<em class="kz">硬件配置</em>:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mi"><img src="../Images/9bbe82e87934536afd7af8293b59c1ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MB6QGjf5dutUxCbt.png"/></div></div></figure><p id="2ff1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在那里，您可以更改集群将使用的实例的类型，以及实例的数量。出于测试目的，您可以使用一个<em class="kz"> m4.large </em>实例类型，并保留默认的3个实例。</p><p id="48c1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，确保使用EC2密钥对。</p><p id="965c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">就这些了。点击<em class="kz">创建集群</em>，几分钟后你的集群就准备好了。</p><h1 id="1198" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">2.SSH到主节点</h1><p id="d22c" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">首先，您需要ssh到您的主节点。</p><p id="6cb8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一旦您的集群准备就绪，您将看到如下内容:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/540967323cb1678f6e751c1404c92402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O_vzrSk2GbVcA5Kk.png"/></div></div></figure><p id="b51e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从那里，您将能够获取您的主节点的公共DNS，并且您将使用SSH进入它，使用如下命令(使用<em class="kz"> hadoop </em>作为用户名):</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="14f8" class="mp lb it ml b gy mq mr l ms mt">ssh -i path\to\your\privatekey.pem hadoop@master-public-dns</span></pre><p id="acf5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，您应该以<em class="kz"> hadoop </em>用户的身份登录到主节点。</p><h1 id="2529" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">3.下载一些数据</h1><p id="a8ea" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">我们将从gutenberg.org下载一堆书。</p><p id="be31" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先让我们创建一个目录来保存书籍:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="3c9c" class="mp lb it ml b gy mq mr l ms mt">mkdir books-input</span></pre><p id="280b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后我们编写一个简短的脚本来下载几十本带有<code class="fe mv mw mx ml b">vim download_books.sh</code>或<code class="fe mv mw mx ml b">nano download_books.sh</code>的书:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="94ba" class="mp lb it ml b gy mq mr l ms mt">#!/bin/bash </span><span id="0dff" class="mp lb it ml b gy my mr l ms mt">for i in {1340..1400} <br/>do <br/>    wget "http://www.gutenberg.org/files/$i/$i.txt" <br/>done</span></pre><p id="4b55" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">保存脚本，然后确保它是可执行的:<code class="fe mv mw mx ml b">chmod +x download_books.sh</code>。我们现在可以开始下载这些书了(有些书在。txt格式，那也行):</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="2c40" class="mp lb it ml b gy mq mr l ms mt">./download_books.sh</span></pre><p id="b5f9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下载完成后，您可以使用以下命令列出文件夹中的文件:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="292a" class="mp lb it ml b gy mq mr l ms mt">ls -lh books-input</span></pre><p id="fd5d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您可以检查文件夹的总大小(在本例中为25MB):</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="e399" class="mp lb it ml b gy mq mr l ms mt">du -sh books-input</span></pre><p id="91d2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将用python编写映射器和缩减器，它们都将从<code class="fe mv mw mx ml b">stdin</code>获取输入，并将输出写入<code class="fe mv mw mx ml b">stdout</code>。</p><h1 id="6a11" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">4.编写映射器</h1><p id="76d2" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">映射器将读取每本书的每一行，将它们拆分成单词，并输出每个单词的元组(word，1)。这些元组的第二个元素(<code class="fe mv mw mx ml b">1</code>)实际上没有那么必要，但是<em class="kz">映射</em>步骤的一般思想是输出一系列键-值对。然后对这些值对进行排序(或者用Hadoop术语称之为<em class="kz">混洗</em>，以便在<em class="kz"> Reduce </em>步骤之前，所有与给定键相关的值都在同一个节点中结束。</p><p id="9af6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是映射器的代码:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="28c0" class="mp lb it ml b gy mq mr l ms mt">#!/usr/bin/env python3 </span><span id="95b7" class="mp lb it ml b gy my mr l ms mt">import sys <br/>import string </span><span id="ed30" class="mp lb it ml b gy my mr l ms mt">for line in sys.stdin: <br/>    line = line.strip() <br/>    words = line.split() <br/>    for w in words: <br/>        table = w.maketrans('', '', string.punctuation)<br/>        w = w.translate(table).lower() <br/>        print(w, '\t', 1)</span></pre><p id="cd2c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">关于这段代码的一些备注:<br/> -第一行说我们希望这个文件可以用python3 <br/>执行-我们删除了单词中的标点符号，把它们都变成了小写</p><p id="1021" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">假设这段代码在一个名为<code class="fe mv mw mx ml b">mapper.py</code>的文件中，用<code class="fe mv mw mx ml b">chmod +x mapper.py</code>使这个文件可执行。然后，您可以在本地测试它，如下所示:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="b310" class="mp lb it ml b gy mq mr l ms mt">printf 'My name is Karim\nWhat is your name' | ./mapper.py</span></pre><p id="9db9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在上面的命令中，我们打印了两行并通过管道将其作为映射器脚本的输入。</p><p id="1502" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输出只是一系列(单词，1)元组:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/1d31bb24ac2a4d84fb13d472d43a0c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZOjuMrVDvU8CuDL9.png"/></div></div></figure><p id="2c72" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们需要计算每个单词的总数。下面是<em class="kz">减少</em>步骤的代码(保存在<code class="fe mv mw mx ml b">reducer.py</code>脚本中):</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="ce08" class="mp lb it ml b gy mq mr l ms mt">#!/usr/bin/env python3<br/>from collections import defaultdict<br/>import sys</span><span id="f740" class="mp lb it ml b gy my mr l ms mt">word_count = defaultdict(int)</span><span id="8e54" class="mp lb it ml b gy my mr l ms mt">for line in sys.stdin:<br/>    try:<br/>        line = line.strip()<br/>        word, count = line.split()<br/>        count = int(count)<br/>    except:<br/>        continue</span><span id="00cf" class="mp lb it ml b gy my mr l ms mt">word_count[word] += count</span><span id="cd09" class="mp lb it ml b gy my mr l ms mt">for word, count in word_count.items():<br/>    print(word, count)</span></pre><p id="0a3a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里我们使用非常方便的<code class="fe mv mw mx ml b">defaultdict</code>来增加每个单词的计数。记住，这个脚本的输入将是我们的<code class="fe mv mw mx ml b">mapper.py</code>脚本的输出。同样，确保这个文件是可执行的。</p><p id="f841" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本地测试您的映射器+缩减器:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="9ae1" class="mp lb it ml b gy mq mr l ms mt">printf 'My name is Karim\nWhat is your name' | ./mapper.py | ./reducer.py</span></pre><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/ac9ef7f01439195b559bf78fb18c9906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HRc2Hbbk8xtKYVCr.png"/></div></div></figure><p id="f8b7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">是时候让Hadoop做自己的事了。首先，我们需要把数据(书籍)放在HDFS:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="5fff" class="mp lb it ml b gy mq mr l ms mt">hdfs dfs -mkdir books-input hdfs dfs -put books-input/*.txt books-input</span></pre><p id="6df4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因为我们想要使用Hadoop streaming，所以让我们首先找到所需的jar文件:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="e787" class="mp lb it ml b gy mq mr l ms mt">find /usr/lib/ -name *hadoop*streaming*.jar</span></pre><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/81e6b5ca7975692df4c6fd186425601f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U8yA40e8tjL1ix_A.png"/></div></div></figure><p id="f944" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们要找的是<code class="fe mv mw mx ml b">/usr/lib/hadoop/hadoop-streaming.jar</code>。</p><p id="fe35" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以我们开始吧:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="3b77" class="mp lb it ml b gy mq mr l ms mt">hadoop jar /usr/lib/hadoop/hadoop-streaming.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -in put books-input -output books-output</span></pre><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/fb47107754948309914639d8011e1c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y04nFRv3q0nL_kNA.png"/></div></div></figure><p id="f0e4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">会有一堆<em class="kz">地图</em>岗位首发，接着是几个<em class="kz">岗位减少</em>。浏览所有的书大约需要一分钟。</p><p id="e34d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输出现在应该位于HDFS文件夹中的books-output。您可以使用以下命令将其复制回主节点:</p><pre class="me mf mg mh gt mk ml mm mn aw mo bi"><span id="67c9" class="mp lb it ml b gy mq mr l ms mt">hdfs dfs -get books-output/*</span></pre><p id="ec14" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这将把一些<code class="fe mv mw mx ml b">part-0000x</code>文件放在主节点上的用户文件夹中，然后您可以打开并检查这些文件(例如用nano)。您应该会看到不同单词的列表，以及该单词在所有书籍中的数量:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/3eb2ab455710c5873b582bef05dc0598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-Kq6bcP_PC1bDXrW.png"/></div></div></figure><p id="30f2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">结论:我们对几本书的数据(大约25MB)进行了简单的字数统计。尽管这还不能称之为大数据，但是如果我们想在一万本书上运行它，这个过程是完全一样的。<br/>下一步:使用S3存储我们的输入、输出、映射器和缩减器；以及从控制台(与命令行相反)向正在运行的集群提交MapReduce流作业。</p></div></div>    
</body>
</html>