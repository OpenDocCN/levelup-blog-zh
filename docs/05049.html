<html>
<head>
<title>Real or Not? NLP with Disaster Tweets (Classification using Google BERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">真实与否？带有灾难推文的NLP(使用Google BERT分类)</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/real-or-not-nlp-with-disaster-tweets-classification-using-google-bert-76d2702807b4?source=collection_archive---------13-----------------------#2020-07-31">https://levelup.gitconnected.com/real-or-not-nlp-with-disaster-tweets-classification-using-google-bert-76d2702807b4?source=collection_archive---------13-----------------------#2020-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="ae39" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">目录</h1><ol class=""><li id="c13a" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">项目概述</li><li id="4d59" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">数据描述</li><li id="8655" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">探索性数据分析</li><li id="a574" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">特征工程</li><li id="648a" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">数据预处理</li><li id="4ef0" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">构建伯特模型</li><li id="d0e9" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">结果</li></ol><h1 id="dde8" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">项目概述</h1><p id="dfe4" class="pw-post-body-paragraph li lj iq kn b ko kp lk ll kq kr lm ln ks lo lp lq ku lr ls lt kw lu lv lw ky ij bi translated">推特已经成为紧急时刻的重要沟通渠道。无处不在的智能手机使人们能够实时宣布他们正在观察的紧急情况。正因为如此，越来越多的机构对有计划地监控推文感兴趣(如救灾组织和新闻机构)，但人们并不总是清楚一个人的话是否真的在宣布灾难。</p><p id="244f" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">以一条推文<em class="mc">为例，上面写着“昨晚看天空，它着火了”</em>。这条推文的作者明确使用了“闪亮”一词，但这是一种隐喻。这对于人类来说是显而易见的，尤其是有了视觉辅助，但是对于机器来说就不那么清楚了。</p><p id="b4bd" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">Kaggle在他们的平台上举办了这次挑战，数据集是由figure-figure-figure公司创建的，最初分享在他们的“人人数据”网站(【https://www.figure-eight.com/data-for-everyone/】T2)上。</p><h1 id="90e9" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">数据描述</h1><p id="478b" class="pw-post-body-paragraph li lj iq kn b ko kp lk ll kq kr lm ln ks lo lp lq ku lr ls lt kw lu lv lw ky ij bi translated"><strong class="kn ir">我需要什么文件？</strong></p><p id="df66" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">您需要train.csv、test.csv和sample_submission.csv。</p><p id="87ac" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">我希望数据格式是什么样的？</strong></p><p id="2b12" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">训练和测试集中的每个样本都有以下信息:</p><ul class=""><li id="8d50" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky mh la lb lc bi translated">一条推文的文本</li><li id="373a" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">那条推文中的一个关键词(虽然这可能是空白的！)</li><li id="af10" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">发送推文的位置(也可以是空白的)</li></ul><p id="944e" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">我预测的是什么？</p><p id="4beb" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">你在预测一条推文是否是关于一场真正的灾难。如果是，预测一个1。如果没有，预测0。</p><p id="437e" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">文件</strong></p><ul class=""><li id="319f" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky mh la lb lc bi translated">train.csv训练集</li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/3a92c663e14ff62997d746e2266e73af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KN4S4jLqhcP3JPzI"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">培训用数据</figcaption></figure><ul class=""><li id="5d36" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky mh la lb lc bi translated">test.csv —测试集</li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi my"><img src="../Images/8d692868a2d9fd6e7855fe6e1898b750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Zw0mkn4FjEfEvTko"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">测试数据</figcaption></figure><ul class=""><li id="e84a" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky mh la lb lc bi translated">sample_submission.csv —格式正确的示例提交文件</li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mz"><img src="../Images/84d449edf111a55d6382ae63ef583150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Cajm9KR1lIf-MEHn7G1kw.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">提交文件</figcaption></figure><p id="c054" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">列</strong></p><ul class=""><li id="eda4" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky mh la lb lc bi translated">id——每条推文的唯一标识符</li><li id="24d5" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">文本——推文的文本</li><li id="fcc3" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">位置—发送推文的位置(可以为空)</li><li id="58bd" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">关键词——推文中的特定关键词(可以为空)</li><li id="236b" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">target —仅在train.csv中，这表示一条推文是关于一场真正的灾难(1)还是(0)</li></ul><h1 id="9873" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">探索性数据分析</h1><ol class=""><li id="75d2" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">空值</li></ol><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/50970cadace23b7a2c0dc3f9438cd5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/0*2R5dH3gSoEmp2IK9"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">训练数据集中的空值</figcaption></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/0c7280f045939514817b7a41fa5a7ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/0*YPuyO1nPLTA4yQKl"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">测试数据集中的空值</figcaption></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nc"><img src="../Images/cee86e490ca8fe286e5c7ae153d487e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rg8fhgyEgr8BqJcS"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">训练和测试数据集中关键字和位置要素中缺失值的分布</figcaption></figure><ol class=""><li id="c42d" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky kz la lb lc bi translated">目标分布:在训练分布中，少数目标有非常高的概率成为真正的灾难tweet (class = 1)。如果测试数据集也是从列车分布中提取的，那么我们可以使用这些信息来改进我们的预测。</li></ol><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nd"><img src="../Images/05e4d264481c293c39784e2bee2ca818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9X-bTR-zHuB5fkTv"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">关键字的目标等于1的概率</figcaption></figure><h1 id="1b13" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">特征工程</h1><ol class=""><li id="945c" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">如何处理位置和关键字列？</li></ol><p id="b629" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">位置不是自动生成的，而是用户输入的。这就是为什么外景地很脏，有太多独特的价值在里面。不应该作为一个功能。</p><p id="da5e" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">幸运的是，关键字中有信号，因为其中一些单词只能在一种上下文中使用。关键词有非常不同的tweet计数和目标均值。关键字可以单独作为一个特性使用，也可以作为一个词添加到文本中。训练集中的每个关键词都存在于测试集中。如果训练集和测试集来自同一个样本，也可以对关键字使用目标编码。</p><h1 id="cba8" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">数据预处理</h1><ol class=""><li id="ff6d" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">清理文本特征</li></ol><p id="a682" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">训练集和测试集中的文本特征有噪声。清洁该特征的一种方法是移除:</p><ul class=""><li id="c82e" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky mh la lb lc bi translated">删除URL</li><li id="b0c4" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">移除表情符号</li><li id="699f" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">删除html内容</li><li id="0622" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky mh la lb lc bi translated">删除标点符号</li></ul><h1 id="0f4f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">构建伯特模型</h1><p id="a0d1" class="pw-post-body-paragraph li lj iq kn b ko kp lk ll kq kr lm ln ks lo lp lq ku lr ls lt kw lu lv lw ky ij bi translated">来自变压器的双向编码器表示(BERT)是由Google开发的用于NLP预训练的技术。BERT利用了Transformers，这是一种学习文本中单词(或子单词)之间上下文关系的注意力机制。一般来说，Transformer包括两个独立的机制——一个读取文本输入的编码器和一个为任务生成预测的解码器。</p><p id="b3cc" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">BERT是一个预训练的变压器编码器堆栈。它是在维基百科和图书语料库数据集上训练的。</p><p id="d730" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">BERT引入了上下文单词嵌入(一个单词可以根据它周围的单词有不同的意思)。转换器使用注意机制来理解使用单词的上下文。然后，该上下文被编码成向量表示。实际上，它在长期依赖方面做得更好。</p><p id="0e55" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">例如伯特做了什么</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ne"><img src="../Images/b333233b204a96dce96be5e3d54f42c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gk3XI7bfciVsV6n4"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">谷歌伯特的工作</figcaption></figure><p id="fe89" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">伯特是如何被训练的:</strong></p><p id="85e7" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">掩码语言模型(MLM):</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/cd467bfe8d01b9fcedcaf470f5e3f321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*Ldidzr_oOeJ6wy0U"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">掩蔽语言建模</figcaption></figure><p id="2190" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">下一句预测(NSP):</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ne"><img src="../Images/cf1123b006eaa6b7d2fa7ba72e26a2fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*apzxS48yxBmFMq7G"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">下一句预测</figcaption></figure><p id="6324" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">伯特的好处:</strong></p><ol class=""><li id="80ba" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky kz la lb lc bi translated">捕获电子邮件的语义和上下文</li><li id="314e" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">在小数据集上给出了良好的结果，因为它是在维基百科语料库上预先训练的</li></ol><p id="3d2a" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">构建BERT分类器:</strong></p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d813107c45c5dffa60dfd1b2950726f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/0*SedDvp1RaKgwGPnx"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">模型摘要</figcaption></figure><p id="da05" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">为创建分类器而对标准Bert模型进行的添加</p><ol class=""><li id="e38a" class="kl km iq kn b ko lx kq ly ks me ku mf kw mg ky kz la lb lc bi translated">λ层</li><li id="e9dc" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">具有tanh激活的密集层，用于将嵌入值保持在范围:[-1，+1]内</li><li id="ade1" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">具有softmax激活函数的密集层，该层的输出是属于某个类的输入的概率。</li></ol><p id="f7ca" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">训练:</strong></p><p id="b5ce" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">超参数:</p><p id="29d5" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">1)历元数= 15</p><p id="8b0c" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">2)批量大小= 16</p><p id="2319" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">3)Adams优化器，学习率= 1e — 5</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e4f448e9945c21377dd857dac03834ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/0*pl9t23dAnbjd7QPn"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">跨时代的验证损失</figcaption></figure><p id="cf75" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">验证损失在第15个时期后持续下降，因此我们针对15个时期训练我们的模型。</p><h1 id="0cf5" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结果</h1><p id="bf98" class="pw-post-body-paragraph li lj iq kn b ko kp lk ll kq kr lm ln ks lo lp lq ku lr ls lt kw lu lv lw ky ij bi translated">训练精度:0.95</p><p id="c135" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">测试精度:0.81</p><p id="9138" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">混淆矩阵:</strong></p><p id="4062" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">0:代表虚假推文</p><p id="425f" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">1:代表真实的推文</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e7e146af6946f2874dcaf456c259a11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*AfB0CzHiurygZdkcD3E7kg.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">混淆矩阵</figcaption></figure><p id="c147" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">准确度得分</strong> : 0.81</p><p id="777e" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated"><strong class="kn ir">报告</strong>:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/812bd26b0efe72b31a9b73cf6ec80604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*Tjq70t80bb2JUd5G5RY2Ag.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">精确回忆f1-分数和对预测的支持</figcaption></figure><p id="70e7" class="pw-post-body-paragraph li lj iq kn b ko lx lk ll kq ly lm ln ks lz lp lq ku ma ls lt kw mb lv lw ky ij bi translated">对于提交的样本，我们得到的公开分数是0.83</p><h1 id="376f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">链接到代码</strong></h1><div class="nk nl gp gr nm nn"><a href="https://github.com/gautam1998/NLP_Predict_Disaster_Tweets" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd ir gy z fp ns fr fs nt fu fw ip bi translated">Gautam 1998/NLP _ Predict _灾难_Tweets</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">这个代码是基于kaggle竞赛真实与否？灾难推文的NLP。本次比赛的目的是…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">github.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob ms nn"/></div></div></a></div><h1 id="3a04" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">参考文献</strong></h1><div class="nk nl gp gr nm nn"><a href="https://www.kdnuggets.com/2020/02/intent-recognition-bert-keras-tensorflow.html" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd ir gy z fp ns fr fs nt fu fw ip bi translated">使用Keras和tensor flow 2-KD块的BERT意图识别</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">如今，从文本中识别意图非常有用。通常，你会得到一篇短文(一两句话),然后…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">www.kdnuggets.com</p></div></div><div class="nw l"><div class="oc l ny nz oa nw ob ms nn"/></div></div></a></div><div class="nk nl gp gr nm nn"><a href="https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd ir gy z fp ns fr fs nt fu fw ip bi translated">NLP与灾难推文- EDA，清洁和伯特</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自多个数据源的数据</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">www.kaggle.com</p></div></div><div class="nw l"><div class="od l ny nz oa nw ob ms nn"/></div></div></a></div><div class="nk nl gp gr nm nn"><a href="https://www.kaggle.com/c/nlp-getting-started/discussion/123343" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd ir gy z fp ns fr fs nt fu fw ip bi translated">真实与否？灾难推文的NLP</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">预测哪些推文是关于真正的灾难，哪些不是</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">www.kaggle.com</p></div></div><div class="nw l"><div class="oe l ny nz oa nw ob ms nn"/></div></div></a></div></div></div>    
</body>
</html>