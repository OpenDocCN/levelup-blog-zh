# k-è¡¨ç¤ºå¾ªåºæ¸è¿›

> åŽŸæ–‡ï¼š<https://levelup.gitconnected.com/k-means-manual-implementation-9f6fd9375b86>

èšç±»ç®—æ³•æœ‰åŠ©äºŽå°†æ•°æ®åˆ†ç»„ä¸ºç›¸ä¼¼çš„ç»„æˆ–ç°‡ã€‚æœ‰è®¸å¤šä¸åŒçš„èšç±»æ–¹æ³•ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹æœ€æµè¡Œçš„èšç±»ç®—æ³•ä¹‹ä¸€:K-Meansã€‚è¯¥ç®—æ³•çš„ç›®æ ‡æ˜¯é€šè¿‡è´¨å¿ƒè¯†åˆ«æ¯ä¸ªèšç±»ã€‚æŽ¥ä¸‹æ¥çš„åŠ¨æœºæ˜¯ç†è§£ K-Means ç®—æ³•æ˜¯å¦‚ä½•å®žçŽ°çš„ã€‚åŒ…å«çš„ä»£ç ä¸æ˜¯ä¸ºäº†ç”¨äºŽç”Ÿäº§ï¼Œè€Œæ˜¯ä¸ºäº†ç†è§£ç®—æ³•å¦‚ä½•å­¦ä¹ èšç±»ã€‚

æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æˆ‘ä»¬å°†ä½¿ç”¨çš„ç›¸å…³åº“ã€‚

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
```

æ¯ä¸ªç®—æ³•éƒ½éœ€è¦æŸç§åˆå§‹åŒ–ã€‚ç»“æžœè¡¨æ˜Žï¼ŒK-Means å¯¹è´¨å¿ƒçš„åˆå§‹åŒ–éžå¸¸æ•æ„Ÿã€‚æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥åˆå§‹åŒ–è´¨å¿ƒðœ‡_k.ï¼Œæˆ‘ä»¬å®žçŽ°äº†æ–‡çŒ®ä¸­æ‰€è¯´çš„ *K-Means++* åˆå§‹åŒ–ã€‚æˆ‘ä»¬é€šè¿‡éšæœºé€‰æ‹©ä¸€ä¸ªæ•°æ®ç‚¹æ¥åˆå§‹åŒ–ç¬¬ä¸€è´¨å¿ƒðœ‡_1:

![](img/0f07741488f7a5fad4757841f6acf485.png)

å¯¹äºŽæ¯ä¸ªåŽç»­è´¨å¿ƒï¼Œæˆ‘ä»¬å¸Œæœ›é€‰æ‹©ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œè¯¥æ•°æ®ç‚¹å¾ˆæœ‰å¯èƒ½è¿œç¦»å‰ä¸€ä¸ªè´¨å¿ƒã€‚å‡è®¾å·²ç»é€‰æ‹©äº†å‰ k-1 ä¸ªè´¨å¿ƒ:

![](img/14b43bbf02b74988c2d25a35cf7bbb17.png)

æˆ‘ä»¬è¦é€‰æ‹©ä¸‹ä¸€ä¸ªï¼Œè®©å®ƒå¤§æ¦‚çŽ‡è¿œç¦»ä¸Šä¸€ä¸ªã€‚æ¯æ¬¡æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªè´¨å¿ƒï¼Œæˆ‘ä»¬ä»ŽåŽŸå§‹æ•°æ®é›†ä¸­åˆ é™¤ç›¸åº”çš„æ•°æ®ç‚¹ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†é›†åˆ{1ï¼Œ2ï¼Œâ€¦ï¼ŒN-k}ä¸Šçš„åˆ†å¸ƒðœˆ(å…¶ä¸­æ•°æ®é›†å·²ç»è¢«é‡æ–°ç´¢å¼•)ã€‚è¿™ç§åˆ†å¸ƒä½¿å¾—é€‰æ‹©ç¬¬ I ä¸ªæ•°æ®ç‚¹çš„æ¦‚çŽ‡ä¸Žå…¶è‡ªèº«å’Œæœ€åŽé€‰æ‹©çš„è´¨å¿ƒä¹‹é—´çš„å¹³æ–¹è·ç¦»æˆæ¯”ä¾‹ã€‚

![](img/1b71c8839c49c56ecf4ec04d0bc40a5e.png)

```
def initialize_centers(X,K):
    '''Implements K-Means++ initialization given the data matrix X and the number of cluster K
    Input:
        X - shape (N, D) - data matrix containing N examples of D-dimensional data
        K - a positive integer - the number of clusters

    Output:
        mu - shape(K,D) - K centroids each of which is a D-dimensional vector
    '''

    N,D = X.shape #find out the number of example and the dimesion of the data
    mu = np.zeros((K,D)) #initialize the variable that will store the K (D-dimensional) centroids
    idx = np.random.randint(0,X.shape[0]) #choose an index at random
    mu[0] = X[idx] #assign the respective data point as the first centroid
    X = np.delete(X,idx,axis=0) #remove the chosen data point from the data set.
    for k in range(1,K): #for each of the subsequent clusters
        #compute the square-norm of the different between each data point and the previous centroid
        nu = np.linalg.norm(X-mu[k-1],axis=1)**2 
        #divide these square-norms by their sum to get a distribution
        nu = nu / np.sum(nu)
        #choose an index at random from the distribution
        idx = np.random.choice(np.array(range(X.shape[0])),1,replace=False,p=nu)
        #assign the centroid of the k^th cluster as the respective data point
        mu[k] = X[idx]
        #remove the selected data point from the data matrix
        X = np.delete(X,idx,axis=0)
    #return the centroids
    return mu
```

ä½ å¯èƒ½çŸ¥é“ï¼Œè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡ä¼˜åŒ–â€œå¥½â€(æœ€å¤§åŒ–)æˆ–â€œåâ€(æœ€å°åŒ–)çš„æŸç§è¡¡é‡æ ‡å‡†æ¥å·¥ä½œã€‚åœ¨ K-Means ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦æœ€å°åŒ–çš„æŸå¤±å‡½æ•°(â€œåæ€§â€)ã€‚è¿™ä¸ªå‡½æ•°è®¡ç®—æ¯ä¸ªé›†ç¾¤ä¸­çš„â€œåæ€§â€ã€‚è¿™æ˜¯é€šè¿‡è®¡ç®—åˆ†é…ç»™ä¸€ä¸ªèšç±»çš„æ¯ä¸ªæ•°æ®ç‚¹ä¸Žè¯¥èšç±»çš„è´¨å¿ƒä¹‹é—´çš„å¹³æ–¹è·ç¦»æ¥å®žçŽ°çš„ã€‚å¤±çœŸæŸè€—å°±æ˜¯æ‰€æœ‰è¿™äº›å¹³æ–¹è·ç¦»çš„æ€»å’Œã€‚å› æ­¤ï¼Œå¦‚æžœä¸€ä¸ªæ•°æ®ç‚¹ç¦»è´¨å¿ƒä¸å¤Ÿè¿‘ï¼Œå®ƒå°†å¯¹æŸè€—äº§ç”Ÿå¾ˆå¤§çš„å½±å“ã€‚

```
def distorsion_loss(X,mu, Z):
    '''Given the data matrix X, the current set of centroids mu
    and the assignment-matrix Z (also called a latent variable),
    computes the distrortion-loss

    Input:
        X - shape (N, D) - data matrix containing N examples of D-dimensional data
        mu - shape(K,D) - K centroids each of which is a D-dimensional vector
        Z - shape (N, K) - each element in the row contains exactly one 1 and the rest zero
            indicating the assignment of the respective data point to the cluster.
            For example Z_{53}=1 means that the 5th data point belongs to cluster 3 (4th cluster)
    '''
    K = mu.shape[0] #get the number of cluster by looking at how many centroids there are
    loss = 0 #initialize the loss to zero
    for k in range(K): #for each cluster
        #get the data points assigned to that cluster
        X_k = X[Z[:,k]==1]
        #compute the square-dstiance between each of these data points and the centroid
        D=np.linalg.norm(X_k-mu[k],axis = 1)**2
        #accumulate the loss
        loss += np.sum(D)
    #return
    return loss
```

æˆ‘ä»¬çŽ°åœ¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒ K-Means ç®—æ³•ã€‚åŸ¹è®­å°†æŒ‰å¦‚ä¸‹æ–¹å¼è¿›è¡Œ:

1.  åˆå§‹åŒ–è´¨å¿ƒ:mu_init = initialize_centers(Xï¼ŒK)
2.  æ›´æ–°åˆ†é…çŸ©é˜µã€‚æ¯ä¸ªæ•°æ®ç‚¹è¢«åˆ†é…åˆ°æœ€è¿‘çš„è´¨å¿ƒã€‚
3.  æ›´æ–°è´¨å¿ƒã€‚æ–°çš„è´¨å¿ƒæ˜¯å®ƒä»¬å„è‡ªèšç±»å†…çš„æ•°æ®ç‚¹çš„å¹³å‡å€¼ã€‚
4.  è®°å½• distorion_lossã€‚å¦‚æžœå½“å‰æŸå¤±å’Œå…ˆå‰æŸå¤±ä¹‹é—´çš„å˜åŒ–å°äºŽå®¹å·®ï¼Œæˆ–è€…æˆ‘ä»¬å·²ç»è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåˆ™åœæ­¢ï¼Œå¦åˆ™è½¬åˆ°æ­¥éª¤ 2ã€‚

```
def train_kmeans(X,K,mu_init,max_iter=10,tol=1e-4):
    '''Train the K-Means algorithm using the data matrix X, the number of clusters K,
    and the initial centroids mu_init

    Input:
        X - shape (N, D) - data matrix containing N examples of D-dimensional data
        K - a positive integer - the number of clusters
        mu_init - shape(K,D) - initial K centroids each of which is a D-dimensional vector
        max_iter - positive integer (default 10) the maximum number of iterations
        tol - positive number (default 1e-4) the tolerance which determines that
                the distorion_loss has converged.
    '''
    N = X.shape[0] #read in the number of data points
    mu = mu_init #initialize mu with mu_init
    loss_trace = [] #initalize an array to keep track of the losses
    mu_trace = [] #initialize an array to keep track of the centroids
    Z_trace = [] #initialize an array to keep track of the assignment matrices
    for it in range(max_iter): #for each iteration
        #Update cluster indicators
        Z = np.zeros((N,K)) #initialize the assignment matrix
        for i in range(N): #for each data point
            #compute the square distance between the datapoint and all the centroids
            d = np.linalg.norm(X[i]-mu,axis=1)**2
            #select the index of the smallest distance (corresponding to the closest centroid)
            j = np.argmin(d)
            #update the assingment matrix by assigning the i^th data point to the j^th cluster
            Z[i,j]=1

        #Update centroids
        N_k = np.sum(Z,axis=0) #count the number of data points in each cluster
        for j in range(K): #for each cluster
            #sum up the data points
            mu[j] = np.sum(X[Z[:,j]==1],axis=0)
            if N_k[j] > 0: #if there is at least one data point in the cluster
                mu[j] = mu[j] / N_k[j] #divide to get the average, the centroid
        loss_ = distorsion_loss(X,mu,Z) #compute the loss
        loss_trace.append(loss_) #record the loss
        mu_trace.append(mu) #record the current value of mu
        Z_trace.append(Z) #record the current assignment matrix
        if len(loss_trace) > 1: #if we have at least 2 iterations
            if abs(loss_trace[-2]-loss_trace[-1]) < tol: #check convergence
                return mu, Z, loss_trace, mu_trace, Z_trace #return if loss has converged
    #return
    return mu, Z, loss_trace, mu_trace, Z_trace
```

æˆ‘ä»¬æ‰¾åˆ°äº†ã€‚K å‡å€¼ç®—æ³•ã€‚è®©æˆ‘ä»¬ç”Ÿæˆä¸€äº›çŽ©å…·æ•°æ®æ¥çœ‹çœ‹ K-Means çš„è¡¨çŽ°ã€‚

æˆ‘ä»¬å°†ç”Ÿæˆä»¥ä¸‹åˆ—ç‚¹ä¸ºä¸­å¿ƒçš„ 3 ä¸ªé›†ç¾¤:

![](img/2565c02fd64d499edca0a765bb6b1637.png)

è¿™äº›ä¸­å¿ƒå‘¨å›´çš„æ‰€æœ‰æ•°æ®ç‚¹å°†æ ¹æ®å…·æœ‰å•ä½(å„å‘åŒæ€§)åæ–¹å·®çŸ©é˜µçš„å¤šå…ƒæ­£æ€åˆ†å¸ƒè¿›è¡Œåˆ†å¸ƒ:

![](img/326465aee216c22f56dcceb05bf2ce09.png)

```
#centroids
mu_1_actual = np.array([2,2])
mu_2_actual = np.array([-2,-2])
mu_3_actual = np.array([10,0])
#covariance matrix
sigma = np.eye(2)

#Clusters
X1 = np.random.multivariate_normal(mu_1_actual,sigma,100)
X2 = np.random.multivariate_normal(mu_2_actual,sigma,100)
X3 = np.random.multivariate_normal(mu_3_actual,sigma,100)

#Data matrix:
X = np.concatenate([X1,X2,X3],axis=0)#Visualize the data and visualize the actual clusters.

fig = plt.figure(figsize=(20,5))
plt.subplot(121)
plt.scatter(X[:,0],X[:,1])
plt.subplot(122)
plt.scatter(X1[:,0],X1[:,1],c='r')
plt.scatter(X2[:,0],X2[:,1],c='g')
plt.scatter(X3[:,0],X3[:,1],c='b')
plt.show()
```

![](img/8719a813883d7861266d427010c0b193.png)

```
mu_init = initialize_centers(X,3)
mu,Z,loss_trace, mu_trace, Z_trace =train_kmeans(X,3,mu_init,max_iter=300,tol=1e-4)fig = plt.figure(figsize=(20,5))
plt.subplot(121)
plt.scatter((X[Z[:,0]==1])[:,0],(X[Z[:,0]==1])[:,1],color='r')
plt.scatter((X[Z[:,1]==1])[:,0],(X[Z[:,1]==1])[:,1],color='g')
plt.scatter((X[Z[:,2]==1])[:,0],(X[Z[:,2]==1])[:,1],color='b')
plt.scatter(mu[:,0],mu[:,1],color='black',marker='o',s=200,alpha=0.75)
plt.title('K-Means with K=3')
plt.xlabel('x_1')
plt.ylabel('x_2')
plt.subplot(122)
plt.plot(range(len(loss_trace)),loss_trace)
plt.title('Distorion Loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.show()
```

![](img/8b93f9eba0cbbe8ae5de880e34f3c6b2.png)

è¿˜ä¸é”™ã€‚çœ‹èµ·æ¥ç®—æ³•æ”¶æ•›äº†ä¸€äº›è¿­ä»£ã€‚

```
fig = plt.figure(figsize=(20,25))
for i in range(len(loss_trace)):
    plt.subplot(521+i)
    Z_i = Z_trace[i]
    mu_i = mu_trace[i]
    plt.scatter((X[Z_i[:,0]==1])[:,0],(X[Z_i[:,0]==1])[:,1],color='r')
    plt.scatter((X[Z_i[:,1]==1])[:,0],(X[Z_i[:,1]==1])[:,1],color='g')
    plt.scatter((X[Z_i[:,2]==1])[:,0],(X[Z_i[:,2]==1])[:,1],color='b')
    plt.scatter(mu_i[:,0],mu_i[:,1],color='black',marker='o',s=200, alpha=0.75)
    plt.title('Iteration:' + str(i+1) +'\nLoss:'+str(round(100*loss_trace[i])/100))
plt.show()
```

![](img/cb80c2512837e14e9c13ff7357ebed2b.png)

çŽ°åœ¨äº‹æƒ…å¯èƒ½æ²¡è¿™ä¹ˆç¾Žå¥½äº†ã€‚K-Means å¯¹åˆå§‹åŒ–éžå¸¸æ•æ„Ÿã€‚äº‹å®žä¸Šï¼Œå¦‚æžœæˆ‘ä»¬ç”¨ä¸åŒçš„åˆå§‹è´¨å¿ƒå†è¿è¡Œä¸€æ¬¡ï¼Œå®ƒçœ‹èµ·æ¥å¯èƒ½ä¸ä¼šè¿™ä¹ˆæ¼‚äº®ã€‚

```
mu_init2 = initialize_centers(X,3)
mu2,Z2,loss_trace2, mu_trace2, Z_trace2 =train_kmeans(X,3,mu_init2,max_iter=300,tol=1e-4)fig = plt.figure(figsize=(20,5))
plt.subplot(121)
plt.scatter((X[Z2[:,0]==1])[:,0],(X[Z2[:,0]==1])[:,1],color='r')
plt.scatter((X[Z2[:,1]==1])[:,0],(X[Z2[:,1]==1])[:,1],color='g')
plt.scatter((X[Z2[:,2]==1])[:,0],(X[Z2[:,2]==1])[:,1],color='b')
plt.scatter(mu2[:,0],mu2[:,1],color='black',marker='o',s=200,alpha=0.75)
plt.title('K-Means with K=3')
plt.xlabel('x_1')
plt.ylabel('x_2')
plt.subplot(122)
plt.plot(range(len(loss_trace2)),loss_trace2)
plt.title('Distorion Loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.show()
```

![](img/9759cd832edf71204847be65c6a06038.png)

è¯·æ³¨æ„ç®—æ³•å¦‚ä½•æ”¶æ•›åˆ°æ›´é«˜çš„æŸå¤±å€¼ï¼Œå¹¶ä¸”èšç±»ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚çŽ°åœ¨åœ¨å®žè·µä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“é›†ç¾¤åº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é¦–å…ˆè¦è¿è¡Œè¿™ä¸ªç®—æ³•ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å…‹æœè¿™ä¸ªéšœç¢å‘¢ï¼Ÿå®žé™…å‘ç”Ÿçš„æ˜¯ distortion_loss å‡½æ•°æœ‰è®¸å¤šå±€éƒ¨æœ€å°å€¼ã€‚æ­¤å¤–ï¼ŒK-Means ç®—æ³•ä¸èƒ½ä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚å› æ­¤ï¼Œåœ¨å®žè·µä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„åˆå§‹å€¼è¿è¡Œ K-Means å‡ æ¬¡ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€ä½Žæ”¶æ•›æŸå¤±çš„ç»“æžœã€‚æ­£å¦‚ä½ åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼ŒæŸå¤±é›†ä¸­åœ¨ 587 å·¦å³ï¼Œè€Œåœ¨ç¬¬äºŒæ¬¡è¿è¡Œä¸­ï¼Œå®ƒé›†ä¸­åœ¨ 2200 å·¦å³ã€‚

## k-è¡¨ç¤ºä½¿ç”¨ Sci-Kit å­¦ä¹ 

ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„é»„é‡‘æ ‡å‡† SciKit-Learn è½¯ä»¶åŒ…æ¥è¿è¡Œ K-Means ç®—æ³•ã€‚è¿™å®žé™…ä¸Šåšäº†æˆ‘ä»¬éœ€è¦çš„ã€‚å®ƒä¼šå¤šæ¬¡è¿è¡Œ K-Meansï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„åˆå§‹å€¼ï¼Œç„¶åŽé€‰æ‹©æœ€ä½³ç»“æžœã€‚

```
kmeans = KMeans(n_clusters=3, random_state=0,verbose=0).fit(X)clusters=kmeans.predict(X)
cluster_centers = kmeans.cluster_centers_
Z_sklearn = np.zeros((X.shape[0],3))
for i in range(X.shape[0]):
    Z_sklearn[i,clusters[i]]=1plt.scatter((X[Z_sklearn[:,0]==1])[:,0],(X[Z_sklearn[:,0]==1])[:,1],color='r')
plt.scatter((X[Z_sklearn[:,1]==1])[:,0],(X[Z_sklearn[:,1]==1])[:,1],color='g')
plt.scatter((X[Z_sklearn[:,2]==1])[:,0],(X[Z_sklearn[:,2]==1])[:,1],color='b')
plt.show()
```

![](img/42715bd48ef314d1295c877c4aa66c95.png)

é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæœ‰ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•æ¥ç»™é›†ç¾¤ç€è‰²..

```
plt.scatter(X[:,0],X[:,1],c=clusters)
plt.scatter(cluster_centers[:,0],cluster_centers[:,1],color='black',marker='o',s=200,alpha=0.75)
plt.show()
```

![](img/189436aab46fbfe19401dd848f781678.png)

## æ¯”è¾ƒç»“æžœâ€¦

```
#converged loss from sklearn
distorsion_loss(X,cluster_centers,Z_sklearn)586.9973311742276#converged loss from our good resultdistorsion_loss(X,mu,Z)586.9973311742275#difference between our centroids and the ones found by sklearn
np.linalg.norm(np.sort(mu,axis=0)-np.sort(cluster_centers,axis=0),axis=1)array([1.25607397e-15, 5.20417043e-18, 1.83102672e-15])
```

éƒ½å‡ ä¹Žä¸ºé›¶ã€‚

æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯¹ K-Means æœ‰æ‰€å¯å‘ï¼ŒK-Means æ˜¯å½“ä»Šæ— ç›‘ç£å­¦ä¹ ä¸­æœ€æµè¡Œçš„èšç±»ç®—æ³•ä¹‹ä¸€ã€‚åœ¨æˆ‘çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä½¿ç”¨é«˜æ–¯æ··åˆçš„è½¯èšç±»ç®—æ³•ã€‚

è¿™æ˜¯å®Œæ•´çš„ jupyter ç¬”è®°æœ¬ã€‚

å¦‚æžœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·èŠ±ç‚¹æ—¶é—´ç»™æˆ‘ä¸€äº›æŽŒå£°ã€‚