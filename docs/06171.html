<html>
<head>
<title>Markov Decision Processes Simplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化马尔可夫决策过程</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/markov-decision-processes-simplified-f5f8d37ab70f?source=collection_archive---------4-----------------------#2020-10-31">https://levelup.gitconnected.com/markov-decision-processes-simplified-f5f8d37ab70f?source=collection_archive---------4-----------------------#2020-10-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="77b2" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">AWS DeepRacer系列</h2><div class=""/><div class=""><h2 id="5d87" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">深入强化学习之前最基本的概念</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a822a88da56d7eca26ccdb217d39f122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uaoykDOTmofZIyRG"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">维多利亚诺·伊斯基耶多在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="3cf4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">欢迎来到我在AWS DeepRacer系列的第二篇文章！在本文中，我将介绍强化学习的基本方面，即<strong class="lh ja">马尔可夫决策过程</strong>。如果你没有看过我的第一篇文章，建议你去看看，对强化学习有个直观的认识。(<a class="ae le" href="https://towardsdatascience.com/casual-intro-to-reinforcement-learning-4a78b57d4686" rel="noopener" target="_blank">强化学习的非正式介绍</a>)</p><p id="1b08" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将从马尔可夫过程，马尔可夫回报过程，最后是马尔可夫决策过程开始。</p><h1 id="381a" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">概述</h1><ol class=""><li id="b250" class="mt mu iq lh b li mv ll mw lo mx ls my lw mz ma na nb nc nd bi translated"><strong class="lh ja">马尔可夫过程</strong></li><li id="c59a" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">马尔可夫奖励过程</strong></li><li id="d8de" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">马尔可夫决策过程</strong></li></ol></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="d856" class="mb mc iq bd md me nq mg mh mi nr mk ml kf ns kg mn ki nt kj mp kl nu km mr ms bi translated">马尔可夫过程</h1><p id="3c42" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated"><strong class="lh ja">马尔可夫决策过程</strong> ( <strong class="lh ja"> MDP </strong> ) <em class="ny"> </em>代表一个<em class="ny">环境</em>用于强化学习。这里我们假设<em class="ny">环境</em>是完全可观测的。这意味着在当前状态下，我们已经掌握了做出决策所需的所有信息。然而，在我们继续讨论MDP是什么之前，我们需要知道马尔可夫性质是什么意思。</p><p id="d141" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">马尔可夫性质</strong>陈述给定现在，未来独立于过去。这意味着当前的<em class="ny">状态</em>从历史中获取了所有相关信息。例如，如果我现在渴了，我想马上喝一杯。当我决定喝酒时，我是昨天还是一周前(过去的<em class="ny">状态</em>)口渴，对我来说是无关紧要的。现在是我做决定的唯一时间。</p><blockquote class="nz"><p id="ae13" class="oa ob iq bd oc od oe of og oh oi ma dk translated">鉴于现在，未来独立于过去</p></blockquote><p id="460b" class="pw-post-body-paragraph lf lg iq lh b li oj ka lk ll ok kd ln lo ol lq lr ls om lu lv lw on ly lz ma ij bi translated">除了<em class="ny">马尔可夫特性</em>之外，我们还有一个<strong class="lh ja">状态转移矩阵</strong>，它存储了从每个当前<em class="ny">状态</em>到每个后继<em class="ny">状态</em>的所有概率。假设我工作时有两种<em class="ny">状态</em>:工作(真实工作)和看视频。我在工作的时候，有70%的机会继续工作，30%的机会看视频。但是，如果我在工作中途看视频，我可能有90%的机会继续看视频，10%的机会回到实际工作中。也就是说，状态转移矩阵定义了从所有<em class="ny">状态</em>(工作，观看视频)到所有后续<em class="ny">状态</em>(工作，观看视频)的转移概率。</p><p id="5d63" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">了解了<em class="ny">马尔可夫性质</em>和<em class="ny">状态转移矩阵</em>之后，让我们继续讨论<strong class="lh ja">马尔可夫过程或马尔可夫链。</strong> <em class="ny">马尔可夫过程</em>是一种无记忆的随机过程，比如具有马尔可夫性质的状态序列。</p><p id="bd30" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以在下图中看到一个<em class="ny">马尔可夫过程</em>学生活动的例子。有几个<em class="ny">状态，从<em class="ny">类1 </em>到<em class="ny">睡眠</em>为最终状态。每个圆圈中的数字代表转移概率。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/f9b23466c201495aa0a1d8af5e0ec228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uYhgyTuNZnCHYPXQVAKpSg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">学生马尔可夫过程[图片来自大卫·希尔弗关于MDP的演讲]</figcaption></figure><p id="e8cb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以从<em class="ny">类1 </em>开始到<em class="ny">睡眠:</em>为止对这个过程进行采样<strong class="lh ja">集</strong></p><ol class=""><li id="3b4e" class="mt mu iq lh b li lj ll lm lo op ls oq lw or ma na nb nc nd bi translated">C1·C2·C3通过睡眠，</li><li id="e498" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">C1 FB FB C1 C2睡眠，</li><li id="2ed0" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">C1 C2 C3酒馆C2 C3帕斯睡眠，等等。</li></ol><p id="ad0d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们三个从同一个<em class="ny">状态</em>、<em class="ny">类1 </em>开始，到<em class="ny">睡眠</em>结束。然而，它们经历了一条不同的路径来达到最终的状态。每一次经历就是我们所说的<em class="ny">马尔可夫过程</em>。</p><blockquote class="nz"><p id="7b62" class="oa ob iq bd oc od oe of og oh oi ma dk translated">具有马尔可夫性质的随机状态序列是一个马尔可夫过程</p></blockquote></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="b997" class="mb mc iq bd md me nq mg mh mi nr mk ml kf ns kg mn ki nt kj mp kl nu km mr ms bi translated">马尔可夫奖励过程</h1><p id="9c20" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">至此，我们终于明白什么是<em class="ny">马尔可夫过程</em>了。一个<em class="ny">马尔可夫奖励过程</em> (MRP)是一个<em class="ny">马尔可夫过程</em>带<em class="ny">奖励</em>。很简单，对吧？它由<em class="ny">个状态、状态转移概率矩阵</em> <strong class="lh ja">加上</strong> <em class="ny">个奖励函数</em>和一个<em class="ny">折扣因子</em>组成。我们现在可以将我们以前的学生<em class="ny">马尔可夫过程</em>改为学生MRP，并增加<em class="ny">奖励</em>，如下图所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/faf70778fb2258ac07dc5fa18d87bb88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ofk_69nmgTrwSKg62OoJ6g.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">学生MRP[图片来自大卫·希尔弗关于MDP的演讲]</figcaption></figure><p id="62a7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了理解MRP，我们必须理解<em class="ny">返回</em>和<em class="ny">值函数</em>。</p><p id="dfa0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">回报</strong>是从现在开始的总折扣<em class="ny">奖励</em>。<em class="ny">贴现因子</em>是未来<em class="ny">奖励</em>的现值，其值介于0和1之间。当<em class="ny">折扣因子</em>为<em class="ny"> </em>接近0时，它更喜欢立即<em class="ny">奖励</em>而不是延迟<em class="ny">奖励</em>。当它接近1时，它认为延迟的<em class="ny">奖励</em>高于立即的<em class="ny">奖励</em>。</p><p id="6901" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，你可能会问“我们为什么要费心去添加一个折扣因子呢？”。嗯，有几个原因需要它。首先，我们希望通过将贴现因子设置为小于1来避免无限收益。第二，直接的回报实际上可能更有价值。第三，人类行为表现出对即时<em class="ny">回报</em>的偏好，比如选择现在购物而不是为未来储蓄。</p><p id="228e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用<em class="ny">奖励</em> ( <strong class="lh ja"> R </strong>)和<em class="ny">折扣因子</em>(<strong class="lh ja">γ</strong>)可以计算出<em class="ny">回报</em> ( <strong class="lh ja"> G </strong>)，如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b8bbe75c06bd5a7b537656d1fc80f429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*lZSgBsVxnk0QtAFFFaIf2w.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">返回[图片来自大卫·西尔弗关于MDP的演讲]</figcaption></figure><p id="e414" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从学生MRP中，我们可以得到一个样本<em class="ny">退货</em>，该退货从类别1开始，折扣系数为<strong class="lh ja"> 0.5 </strong> <em class="ny">。样本集是【C1 C2 C3传】，收益等于-2-2 *<strong class="lh ja">0.5</strong>-2 *<strong class="lh ja">0.25</strong>+10 *<strong class="lh ja">0.125</strong>=-2.25。</em></p><p id="56b3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">除了<em class="ny">收益</em>之外，我们还有一个<strong class="lh ja">价值函数</strong>，它是一个状态的预期收益。一个v <em class="ny">值函数</em>确定一个<em class="ny">状态</em>的值，该值指示一个<em class="ny">状态</em>的期望。使用<strong class="lh ja">贝尔曼方程</strong>，我们可以仅用当前<em class="ny">奖励</em>和下一个<em class="ny">状态值</em>来计算当前<em class="ny">状态</em>值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5e3b65e2c37f05ac32f2347c697f9747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*MOyTVXfsXXsn2cTslWpgmA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">价值[图片来自大卫·西尔弗关于MDP的演讲]</figcaption></figure><p id="9d66" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这意味着我们只需要下一个<em class="ny">状态</em>来计算一个<em class="ny">状态</em>的总<em class="ny">值</em>。换句话说，我们可以有一个递归函数，直到过程结束。</p><p id="c6a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">再来看看<em class="ny"> gamma </em>等于<strong class="lh ja"> 1 </strong>的学生MRP。下图表示在每个<em class="ny">状态</em>下具有<em class="ny">值</em>的学生MRP。这个值之前已经计算过了，现在我们想用等式来验证3类(红圈)中的<em class="ny">值</em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/a4e66444bc783e804ed2fd07a8f044d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0c2Hqq4UrMD_kQOEBeNoQA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">有价值观的学生MRP图片来自大卫·西尔弗关于MDP的演讲]</figcaption></figure><p id="fdda" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从类3中我们可以看到，<em class="ny">值</em>是通过将立即<em class="ny">奖励</em> (-2)与两个下一个<em class="ny">状态</em>的期望<em class="ny">值</em>相加计算出来的。为了计算下一个<em class="ny">状态</em>的预期<em class="ny">值</em>，我们可以将转移概率乘以<em class="ny">状态</em>的值。因此，我们得到<strong class="lh ja">-2</strong>+0.6 *<strong class="lh ja">10</strong>+0.4 *<strong class="lh ja">0.8</strong>，等于<strong class="lh ja"> 4.3 </strong>。</p><blockquote class="nz"><p id="97fc" class="oa ob iq bd oc od oe of og oh oi ma dk translated">马尔可夫奖励过程是一个有奖励和价值的马尔可夫过程</p></blockquote></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="33a9" class="mb mc iq bd md me nq mg mh mi nr mk ml kf ns kg mn ki nt kj mp kl nu km mr ms bi translated">马尔可夫决策过程</h1><p id="04a1" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">至此，我们已经了解了<em class="ny">马尔可夫奖励过程。</em>但是，当前<em class="ny">状态</em>和下一个<em class="ny">状态</em>之间没有动作。一个<strong class="lh ja">马尔可夫决策过程(MDP) </strong>是一个有决策的MRP。现在，我们可以选择几个动作在<em class="ny">状态</em>之间转换。</p><p id="dbf0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们看看下图中的学生MDP。这里的关键区别在于，学生在采取了一个<em class="ny">动作</em>后，会立即得到<em class="ny">奖励</em>。在MRP上，<em class="ny">状态</em>有即时<em class="ny">奖励</em>。这里的另一个区别是一个<em class="ny">动作</em>也可以引导学生进入不同的<em class="ny">状态</em>。基于学生MDP，如果一个学生采取<em class="ny"> Pub动作</em>，他可以在<em class="ny">类1 </em>、<em class="ny">类2 </em>或<em class="ny">类3 </em>中结束。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/12226088838c2bcd30a6fb9b812aa002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RVxVo-P-Ur6cqUWDFFLGw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">学生MDP[图片来自大卫·希尔弗关于MDP的演讲]</figcaption></figure><p id="1d87" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">考虑到这些行为，我们现在有了一个<strong class="lh ja">策略，</strong>，它将状态映射到行为。它定义了一个<strong class="lh ja">代理</strong>(在本例中是一个学生)的行为。<em class="ny">策略</em>是固定的(与时间无关)，它们依赖于<em class="ny">动作</em>和<em class="ny">状态</em>而不是时间步长。</p><p id="6137" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基于<em class="ny">策略</em>，我们有一个<strong class="lh ja">状态值函数</strong>和一个<strong class="lh ja">动作值函数</strong>。<em class="ny">状态值函数</em>是从当前状态开始，然后遵循一个策略的期望收益。另一方面，<em class="ny">动作值函数</em>是从当前状态开始，然后<strong class="lh ja">采取</strong> <strong class="lh ja">一个<em class="ny">动作</em>，</strong>然后遵循<em class="ny">策略</em>的期望收益。</p><p id="b2d1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过使用贝尔曼方程，我们可以得到如下的<em class="ny">状态值函数</em> (v)和<em class="ny">动作值函数</em> (q)的递归形式。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4b50a865d01e1ae02f2796c410b347be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*om5gXusFkXzXz3U-wtNezw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">国家价值函数[图片来自大卫·希尔弗关于MDP的演讲]</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/c18fc81cfdd75a613a557da595984ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Fg4kj_6VYjXpgsMhj1jEA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">行动价值函数[图片来自大卫·希尔弗关于MDP的演讲]</figcaption></figure><p id="18b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了让事情更清楚，我们可以在下面的图像中再次查看具有<em class="ny"> gamma </em> 0.1的学生MDP。假设在<em class="ny">三班</em>(红圈)中，一个学生有一个50:50 <em class="ny">的政策</em>。这意味着学生有50%的机会去学习或去酒吧。我们可以通过对每个动作后的每个预期收益求和来计算状态值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/ad2d6659758bd5191d3c9429b745252b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PoAXBBn5du7KHRCfEqGQpg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">有价值观的学生MDP[图片来自大卫·西尔弗关于MDP的演讲]</figcaption></figure><p id="28c2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">来自<em class="ny">研究</em>的<em class="ny">期望值</em>可以通过将<em class="ny">动作概率</em>与下一个<em class="ny">状态</em>的<em class="ny">期望值</em>(0.5 * 10)相乘来计算。相反，<em class="ny">发布动作</em>有多个分支，导致不同的<em class="ny">状态</em>。因此，我们可以通过将<em class="ny">动作概率</em> (0.5)乘以<em class="ny">动作值</em>从<em class="ny">公布</em>中计算出<em class="ny">期望值</em>。通过将即时<em class="ny">奖励</em>与来自所有可能<em class="ny">状态</em>的<em class="ny">期望值</em>相加，可以计算出<em class="ny">行动值</em>。可以用1 + 0.2*-1.3 + 0.4*2.7 + 0.4*7.4来计算。</p><p id="94e8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一个<em class="ny">代理</em>的目标是最大化它的<em class="ny">值</em>。因此，我们必须找到导致最大<em class="ny">值</em>的最优<em class="ny">值</em> <em class="ny">函数</em>。在前面的例子中，我们通过对所有可能的<em class="ny">动作</em>的所有预期<em class="ny">值</em>求和来计算<em class="ny">值</em>。现在，我们只关心给出最大<em class="ny">值</em>的<em class="ny">动作</em>。在已经知道最优<em class="ny">值函数</em>之后，我们有最优<em class="ny">策略</em>并且MDP被求解。下图显示了学生MDP对于每个<em class="ny">状态</em>的最优<em class="ny">值</em>和<em class="ny">策略</em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/dc7d889cd809a3a3a00efc8853f79899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2h1uq2JCxJW-WxjEs9otGg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">学生MDP与最优政策[图片来自大卫·西尔弗关于MDP的演讲]</figcaption></figure></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="7787" class="mb mc iq bd md me nq mg mh mi nr mk ml kf ns kg mn ki nt kj mp kl nu km mr ms bi translated">结论</h1><blockquote class="nz"><p id="ae59" class="oa ob iq bd oc od oe of og oh oi ma dk translated">总之，马尔可夫决策过程是一个带有行动的马尔可夫回报过程，在这个过程中，主体必须基于最优的价值和策略做出决策。</p></blockquote></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="5e86" class="mb mc iq bd md me nq mg mh mi nr mk ml kf ns kg mn ki nt kj mp kl nu km mr ms bi translated">参考</h1><p id="bf03" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">以下是我阅读的一些资料，有助于我对这个话题的理解。</p><ol class=""><li id="8dbc" class="mt mu iq lh b li lj ll lm lo op ls oq lw or ma na nb nc nd bi translated">David Silver关于RL的讲座——第2讲:马尔可夫决策过程。</li><li id="8a48" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><a class="ae le" href="https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da" rel="noopener" target="_blank"> Ayush Sing —强化学习:马尔可夫决策过程(第一部分)</a></li></ol></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/e862981ad69b99f380aa390d0571788b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2FRBDNplkrTog0V1"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@chewy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">耐嚼</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="942c" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">结束语</h1><p id="5ab7" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">感谢您花时间阅读这篇文章！我希望你对MDP有更多的了解，作为RL的基础。请务必关注我即将发表的关于强化学习和AWS DeepRacer细节的文章。我希望你有美好的一天！</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="2f65" class="mb mc iq bd md me nq mg mh mi nr mk ml kf ns kg mn ki nt kj mp kl nu km mr ms bi translated">关于作者</h1><p id="7aef" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">Alif Ilham Madani是一名有抱负的数据科学和机器学习爱好者，他热衷于从他人那里获得洞察力。他在印尼最顶尖的大学之一<a class="ae le" href="https://www.itb.ac.id/" rel="noopener ugc nofollow" target="_blank"><em class="ny">Institut Teknologi Bandung</em></a>主修电子工程。</p><p id="0f3f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你有任何话题要讨论，你可以通过<a class="ae le" href="https://www.linkedin.com/in/alif-ilham-madani/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae le" href="https://twitter.com/_alifim" rel="noopener ugc nofollow" target="_blank"> Twitter </a>与Alif联系。</p></div></div>    
</body>
</html>