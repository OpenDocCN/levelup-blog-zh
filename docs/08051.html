<html>
<head>
<title>Automating Multiple Document Downloads from a Webpage using Beautiful Soup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Beautiful Soup从网页自动下载多个文档</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/automating-multiple-document-downloads-from-a-webpage-using-beautiful-soup-b81f6a839024?source=collection_archive---------10-----------------------#2021-03-30">https://levelup.gitconnected.com/automating-multiple-document-downloads-from-a-webpage-using-beautiful-soup-b81f6a839024?source=collection_archive---------10-----------------------#2021-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5866" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我最喜欢的代码用途之一是让它为我完成无聊的任务。作为一名研究人员，我的部分职责包括收集信息，以便自己和他人进行分析。有时，谷歌搜索和下载出版物可能会很乏味和重复，所以我们为什么不建立一个“研究助理”来为我们做这些呢？</p><p id="e016" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的“研究助理”将会把亚洲开发银行<a class="ae kl" href="https://www.adb.org/sectors/energy/issues/clean-energy" rel="noopener ugc nofollow" target="_blank">清洁能源问题页面</a>上的所有出版物下载到我们的电脑上。包含我的代码的完整google colab文件可以在本文末尾找到。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/5f348c2fb3c3a0573c85ae47e69d7d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j6EcJqBW2-CJUyebY4H7Hg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">一些出版物可以在<a class="ae kl" href="https://www.adb.org/sectors/energy/issues/clean-energy" rel="noopener ugc nofollow" target="_blank">网站</a>上下载</figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="d9cf" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">打造我们的研究助手</h1><p id="7852" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">我们从导入抓取网站所需的两个主要库开始:Requests和BeautifulSoup。</p><p id="2a10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想浏览一个网站，Beautiful Soup是一个很好的python库。它非常有效地从HTML文档的大量信息中过滤(就像做汤一样)出你想要的信息。然而，它不能完成的任务之一是将感兴趣的网站信息放入您的编码环境中。这就是我们使用请求库的原因，因为它可以获取网站的html文档并将其带入我们的代码中。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="6443" class="mr lk iq mn b gy ms mt l mu mv"># Import the necessary libraries to webscrape the publications</span><span id="91b3" class="mr lk iq mn b gy mw mt l mu mv">import requests<br/>from bs4 import BeautifulSoup as soup</span></pre><p id="4361" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经导入了库，我们将定义我们想要抓取的URL。然后，我们可以将这个URL提供给请求库，并请求将网站信息拉入我们的代码中。一旦完成，我们将请求转换成一个实例，并将其传递给Beautiful Soup来创建一个可读性更好的打印输出。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="1f3d" class="mr lk iq mn b gy ms mt l mu mv"># First define the url of interest</span><span id="b10d" class="mr lk iq mn b gy mw mt l mu mv">url = "https://www.adb.org/sectors/energy/issues/clean-energy"<br/></span><span id="acc1" class="mr lk iq mn b gy mw mt l mu mv"># Once you have set the url, we can now use the requests library to get the content of the url's html page.</span><span id="cb55" class="mr lk iq mn b gy mw mt l mu mv">html_page = requests.get(url).content<br/></span><span id="8a5b" class="mr lk iq mn b gy mw mt l mu mv"># Now we have the html page we are going to use Beautiful Soup to put the information into a more readable format and then print it below. We call this a soup page.</span><span id="9b24" class="mr lk iq mn b gy mw mt l mu mv">soup_page = soup(html_page, "html")<br/>soup_page</span></pre><p id="d159" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以从下面的汤页打印结果中看到，这些信息仍然非常混乱，而且还不是一种易于收集的格式。幸运的是，漂亮的Soup library具有非常易用的功能，允许我们筛选数据以获得我们感兴趣的内容:网站上所有出版物的列表。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mx"><img src="../Images/9ecb3acb610ddafaa78bf64d8b53d695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DkOsmLOZJp-d0SarGjn0AA.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">我们的汤页面片段</figcaption></figure><p id="d5d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通读soup页面，注意每个单独的数据都由&lt;&gt;指定？我们可以使用Beautiful Soup来搜索这些数据位，以找到出版物下载链接。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="f922" class="mr lk iq mn b gy ms mt l mu mv"># First, notice that each &lt;&gt; that starts with "a" always contains text while &lt;&gt;'s not containing "a" look more like commands telling the html page where a button, or other design element should be. Let's use this to do our first filter.</span><span id="9f29" class="mr lk iq mn b gy mw mt l mu mv">soup_page.findAll("a")</span></pre><p id="6d16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这比以前好，但是仍然有大量我们不感兴趣的信息。滚动打印输出，找到详述出版物的行。每一行都有“class = views-list-image”。继续阅读soup页面，您会看到其他数据位也包含“class = ”,但是使用了不同的类名。我们可以使用这个类名进一步过滤。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="de33" class="mr lk iq mn b gy ms mt l mu mv"># Save the filtered information as an instance</span><span id="f613" class="mr lk iq mn b gy mw mt l mu mv">links = soup_page.findAll("a",{"class": "views-list-image"})<br/>links</span></pre><p id="4a12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有了出版物列表，我们就可以用美汤拉出感兴趣的属性。在我们的例子中，我们将把web链接拉到承载出版物的页面，以及出版物的标题。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="791f" class="mr lk iq mn b gy ms mt l mu mv"># Now that we have the links we can pull out the link to each of the pages where you can download the reports of interest</span><span id="75c2" class="mr lk iq mn b gy mw mt l mu mv">links[0].attrs["href"]</span><span id="8c2d" class="mr lk iq mn b gy mw mt l mu mv"><br/># Demonstrating using the same method to get the title name of the first report</span><span id="461d" class="mr lk iq mn b gy mw mt l mu mv">links[0].attrs["alt"]</span></pre><p id="5cab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经了解了如何获取出版物的网页链接和标题，我们将使用一个短循环来接收和保存每个出版物的网页链接。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="b858" class="mr lk iq mn b gy ms mt l mu mv"># A short loop to save the weblinks to each of the publications</span><span id="767f" class="mr lk iq mn b gy mw mt l mu mv">pub_links = []</span><span id="ac7b" class="mr lk iq mn b gy mw mt l mu mv">for link in links:<br/>   pub_links.append("https://www.adb.org" + link.attrs["href"])</span><span id="225f" class="mr lk iq mn b gy mw mt l mu mv">pub_links</span></pre><p id="ce04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可能已经注意到了我们现在保存的URL的一个小问题。如果您单击其中的任何一个，您将不会被直接带到报告的PDF，相反，您将最终出现在托管PDF下载和其他相关报告信息的网页上。</p><p id="4414" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用与上面相同的方法，在每个单独的出版物页面上找到我们要搜索的pdf的网络链接。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="00ba" class="mr lk iq mn b gy ms mt l mu mv"># Investigating what the soup page looks like --&gt; This is always good practice to do with every new web/soup page</span><span id="d29c" class="mr lk iq mn b gy mw mt l mu mv">html_page = requests.get(pub_links[0]).content<br/>soup_page = soup(html_page, "html")<br/>soup_page</span></pre><p id="5679" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">附在页面底部的google colab文件显示了一步一步的方法来为单个出版物soup页面找到正确的过滤设置。在本文中，我们将直接跳到正确的过滤器。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="2d7c" class="mr lk iq mn b gy ms mt l mu mv"># After the step by step investigation to figure out the right attributes to filter by we now have the PDF link for the first publication</span><span id="56b4" class="mr lk iq mn b gy mw mt l mu mv">pub_link = soup_page.findAll("a",{"title": links[0].attrs["alt"]})[0]<br/>pub_link</span></pre><p id="53a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从“pub_link”到上面代码片段的打印输出应该包含PDF的URL、标题、src、图像链接和图像宽度。使用本文前面描述的相同方法，我们现在将保存每个pdf的标题和网页链接。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="a20f" class="mr lk iq mn b gy ms mt l mu mv"># Here's a short loop that goes to each publication page and saves (1) the PDF's URL and (2) the publications title</span><span id="5e34" class="mr lk iq mn b gy mw mt l mu mv">pdfs = []<br/>pdf_names = []<br/>x = 0</span><span id="a7cb" class="mr lk iq mn b gy mw mt l mu mv">for link in pub_links:<br/>   html_page = requests.get(link).content<br/>   soup_page = soup(html_page, "html")<br/>   text = soup_page.findAll("a",{"title": links[x].attrs["alt"]})[0]<br/>   pdfs.append(text.attrs["href"])<br/>   pdf_names.append(text.attrs["title"])<br/>   x = x + 1</span></pre></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="d7de" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">下载出版物</h1><p id="0bf6" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">由于这是一个google colab文件，我们将使用google colab的一个名为files的库。Jupyter笔记本可能需要一个不同的选项。</p><p id="ea4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文件库允许我们自动下载我们用代码生成的文件。</p><pre class="kn ko kp kq gt mm mn mo mp aw mq bi"><span id="a176" class="mr lk iq mn b gy ms mt l mu mv"># Import the necessary libraries<br/>import urllib.request<br/>from google.colab import files</span><span id="bf52" class="mr lk iq mn b gy mw mt l mu mv">x = 0</span><span id="3804" class="mr lk iq mn b gy mw mt l mu mv"># First pull the PDF weblinks using the requests library</span><span id="f398" class="mr lk iq mn b gy mw mt l mu mv">for pdf in pdfs:<br/>   response = requests.get(pdf)</span><span id="5108" class="mr lk iq mn b gy mw mt l mu mv"><br/># Name the PDF's using the names we saved from the loop above</span><span id="ab09" class="mr lk iq mn b gy mw mt l mu mv">   with open(f'{pdf_names[x]}.pdf', 'wb') as f:<br/>      f.write(response.content)<br/>      print(f"{pdf_names[x]}")<br/>      x = x + 1</span><span id="97f4" class="mr lk iq mn b gy mw mt l mu mv"><br/># Download each of the named PDF's directly into your download folder</span><span id="735a" class="mr lk iq mn b gy mw mt l mu mv">for name in pdf_names:<br/>   files.download(f'/content/{name}.pdf')</span></pre><p id="221d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们运行上面的代码，你应该看到文件开始下载到你的电脑上。下图显示了我在文件下载过程中在google colab上看到的内容。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi my"><img src="../Images/8e92f12df383610368145871ba3356de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yh8feRtpMSpKBUmglPGg8Q.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">我们的清洁能源报告直接下载到我们的电脑上</figcaption></figure><p id="bf47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太棒了。现在我们有了自己的下载“研究助手”。我们的“研究助手”中使用的技术可以用于其他网页和其他文件下载。</p><p id="2f78" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望你能在其他项目中使用它！</p><div class="mz na gp gr nb nc"><a href="https://colab.research.google.com/drive/1u4605Be800_rBowtsw-VoiMPWg1fl0MF?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd ir gy z fp nh fr fs ni fu fw ip bi translated">美丽的刮网汤</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">这是一个简短的练习，演示了如何使用Beautiful Soup和请求来抓取出版物，然后自动将它们下载到您的计算机上。</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">colab.research.google.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq kw nc"/></div></div></a></div></div></div>    
</body>
</html>