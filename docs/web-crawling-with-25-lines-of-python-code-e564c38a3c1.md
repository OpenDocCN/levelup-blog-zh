# 用 25 行 Python 代码进行网络爬行

> 原文：<https://levelup.gitconnected.com/web-crawling-with-25-lines-of-python-code-e564c38a3c1>

## 如何在几分钟内编写一个强大的蜘蛛

![](img/c0caffd2c130b4422447592a89acf152.png)

[耶鲁安博世](https://unsplash.com/@jeroenbosch?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

Web 爬行和 web 抓取是两个非常相似且互补的领域。我所看到的对这两种状态的最佳定义是，爬虫，也称为蜘蛛，是一种设计用于在网站上移动的机器人，一页一页地爬行。另一方面，抓取是从网站中提取数据的行为。

在本文中，我将介绍如何使用递归函数创建一个非常简单但高效的爬虫，只需几行代码，以及如何使用它来抓取一些数据。

出于两个主要原因，我们将以维基百科为例。首先，它允许抓取和爬行，这是你应该在开始之前检查的；此外，它的结构对我们在本文中执行的操作有意义，因为我们可以使用每篇文章中引用的链接在每篇文章之间移动。

# 编写代码

让我们从进口开始。对于这项任务，我们只需要 Python 库 BeautifulSoup 和请求。我假设您已经安装了它们，但是如果您没有，您可以使用 *pip* 来完成:

```
pip install beautifulsoup4pip install requests
```

有了这个设置，我们现在将编写完成全部工作的函数。在这种情况下，将爬行器作为一个函数来编写尤其重要，这样爬行器就可以多次递归调用自己，以便从一页接一页的页面中获取链接。

该函数将只接收一个参数，即网站的 URL，并将直接获取带有请求的页面并用 BeautifulSoup 解析它。这个 URL 是爬虫将开始的地方。我们还将实例化一个列表来跟踪我们抓取的页面。

```
pages_crawled = []
def crawler(url):
    page = requests.get(url)
    soup = BeautifulSoup(page.text, 'html.parser')
```

从现在开始，所有的代码都在函数内部。

我们在这里的任务包括获取起始页上其他页面的所有链接，然后在这些页面中的每一个页面上获取其中的所有链接，以此类推，无限期。

到其他页面的链接通常存储在`a` 标签中。所以，我们需要做的是在页面上获取所有这些标签。`find_all` 方法是最好的选择:

```
links = soup.find_all('a')
```

这行代码返回一个包含页面上所有`a` 标签的 iterable。然后我们将遍历它，再次调用 crawler 函数，将每个链接作为参数传递。

但在我们这样做之前，必须满足一些条件:

1.  标签必须包含一个`href` 属性。`href` 代表超文本链接，是爬虫会找到链接的地方。检查这一点很重要，因为尽管链接通常在`a` 标签中，但并不是所有的标签都包含链接。
2.  由于我们只对文章页面感兴趣，链接必须以“/wiki”开头，并且不包含分号，否则，爬虫最终会被重定向到维基百科的其他部分，甚至是其他完全不相关的网站。
3.  链接一定还没有被抓取。为此，将抓取的链接附加到一个列表中，对于每个新链接，我们将检查它是否已经在这个列表中。

这就是这一切的样子:

```
for link in links:
    if 'href' in link.attrs:

        if link['href'].startswith('/wiki') and ':' not in link['href']:

            if link['href'] not in pages_crawled:
```

满足这些条件后，我们现在将链接添加到已爬网链接列表中，并为下一页创建完整的 URL:

```
new_link = f"https://en.wikipedia.org{link['href']}"
pages_crawled.append(link['href'])
```

## 收集数据

在用另一个 URL 调用该函数之前，是时候收集您想要的数据了。从理论上讲，你可以抓取任何你想要的东西:页面标题、文章名称、目录、副标题、文章的整个文本等等。

然而，我实际上并没有从维基百科搜集数据，我只是用这个网站作为例子。但是假设我们想要得到页面的标题、文章的名称和文章的 URL。我们可以做的是使用上下文管理器打开一个文本文件并写入信息。

```
with open('data.csv', 'a') as file:
    file.write(f'{soup.title.text}; {soup.h1.text};   {link["href"]}\n')
```

在这种情况下，使用分号作为分隔符很重要，因为当我们处理文本数据时，我们应该会在其中找到一些逗号。

现在，我们可以再次调用该函数，代码将无限期地抓取越来越多的页面。

```
crawler(new_link)
```

这是完整的代码:

请注意，我们的爬虫从主页开始，但它也可以从维基百科中的任何页面开始。此外，还添加了一个`try` 子句，以防止代码在将数据写入。csv 文件或递归调用函数时。

# 随机化爬行

爬行维基百科的另一种可能性是随机进行。如果目标是让网站上的每一页都显示出来，这可能不是最聪明的想法，但这不是我们现在要做的。

选择随机的文章来抓取仍然会产生大量的数据，因为维基百科几乎是无止境的，不会向他们的服务器发出太多的请求，这是我们总是需要考虑的事情，因为我们不想造成任何伤害。

我们只需对代码做一些修改和一些新的导入就可以实现这一点。首先，我们需要向`find_all`方法指定，我们不仅要寻找页面上的所有`a`标签，也不要寻找包含`href` 属性的所有`a` 标签，而且我们希望所有包含`href`属性的`a`标签都有一个以“/wiki”开头的链接，并且不包含分号。

我们可以通过指定使用 Re 模块来实现这一点。然后我们编写一个满足这些要求的正则表达式。

```
links = soup.find_all('a', 
{'href': re.compile('^\/wiki\/((?!:).)*$')})
```

然后我们使用 Numpy 从`links`中选择一个随机标签。

```
link = links[np.random.randint(1, len(links) + 1)]
```

由于我们之前建立的三个条件中的前两个已经满足，我们只需要检查最后一个。因此，如果链接还没有被抓取，我们只需要将它附加到抓取的链接列表中，获取数据，创建完整的 URL 并再次调用该函数。

```
if link not in pages_crawled:
    pages_crawled.append(link)

    with open('data.csv', 'a') as file:
        file.write(f'{soup.title.text}; {soup.h1.text};     {link["href"]}\n') new_link = f"https://en.wikipedia.org{link['href']}"
    random_crawl(new_link)
```

就这样了。如果您想要可重复的结果，您可以指定随机种子，或者如果您每次运行代码时想要不同的随机结果，您可以使用时间模块来生成不同的种子。

这是完整的代码:

# 最后的想法

这就是我们如何用不超过 25 行代码爬取整个维基百科。这是一个非常简单有效的第一个网络爬虫，也是一个开始网络爬行的好方法。

然而，请记住，正如已经提到的，维基百科允许你这样做，而其他网站可能不允许。但即使是维基百科也有一些条件，你可以在他们的 robots.txt 中看到:

> 友好，低速的机器人是受欢迎的，但不是动态生成的网页。

他们指定低速机器人是受欢迎的，因此，你一定要在你的代码中包含一些随机的停顿，以免网站的服务器过载。你可以使用`time.sleep()`来完成这项工作。

此外，当编写生产级代码时(本文中的情况并非如此，因为这只是一个示例)，让您的代码更好地处理异常并使用代理提供者(如 [Infatica](https://infatica.io/) )是一个好的做法，因为它们能够为您提供更好的 IP 地址基础设施，这样您就可以确保您的代码将继续运行。由于发送到服务器的请求数量，这在抓取网站时甚至更为重要。

就是这样！我希望你喜欢这本书，它可能会有用。如果你有问题，有建议，或者只是想保持联系，请随时通过 [Twitter](https://twitter.com/_otavioss) 、 [GitHub](https://github.com/otavio-s-s) 或 [Linkedin](https://www.linkedin.com/in/otavioss28/) 联系我。