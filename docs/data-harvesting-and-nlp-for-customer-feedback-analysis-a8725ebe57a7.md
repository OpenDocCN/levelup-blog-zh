# ç”¨äºå®¢æˆ·åé¦ˆåˆ†æçš„æ•°æ®æ”¶é›†å’Œè‡ªç„¶è¯­è¨€å¤„ç†

> åŸæ–‡ï¼š<https://levelup.gitconnected.com/data-harvesting-and-nlp-for-customer-feedback-analysis-a8725ebe57a7>

## ä½¿ç”¨æ¨ç‰¹åº”ç”¨ç¼–ç¨‹æ¥å£ã€MongoDB Atlas å’Œ spaCyã€‚

![](img/229e9b95bc20ca9449f019d08b98f0d8.png)

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ•°æ®æŒ–æ˜ç®¡é“ï¼Œä½¿ç”¨ Twitter API æå–å’Œåˆ†æå®¢æˆ·åé¦ˆã€‚æœ¬æ–‡é¢å‘å¸Œæœ›ä½¿ç”¨ Python å¼€å§‹ NLP ä¹‹æ—…çš„äººæˆ–å¸Œæœ›æ”¹è¿› EDA(æ¢ç´¢æ€§æ•°æ®åˆ†æ)è¿‡ç¨‹çš„æ•°æ®åˆ†æå¸ˆã€‚

æˆ‘å·²ç»å°†ç®¡é“åˆ’åˆ†ä¸º **4 ä¸ªé˜¶æ®µ**:

1.  **ä½¿ç”¨*æ¨ç‰¹åº”ç”¨ç¼–ç¨‹æ¥å£*** è·å–æ¨ç‰¹
2.  **ä½¿ç”¨ Python çš„ *spaCy åº“*åˆ†æå¹¶æå–æ¨æ–‡ä¸­çš„å…³é”®è¯ã€‚**
3.  **ä½¿ç”¨ *MongoDB å›¾é›†*åœ¨äº‘ä¸Šå­˜å‚¨æ¨æ–‡å’Œå…³é”®è¯ã€‚**
4.  **åœ¨ *MongoDB Atlas* ä¸Šåˆ›å»ºæ•°æ®å¯è§†åŒ–ã€‚**

# ä½¿ç”¨æ¨ç‰¹åº”ç”¨ç¼–ç¨‹æ¥å£è·å–æ¨ç‰¹

é¦–å…ˆï¼Œæ‚¨å°†éœ€è¦ä¸€ä¸ª [Twitter å¼€å‘äººå‘˜å¸æˆ·](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api)æ¥è·å– API å‡­æ®ã€‚æ‚¨å¯ä»¥è§‚çœ‹æ­¤[è§†é¢‘](https://www.youtube.com/watch?v=Lu1nskBkPJU&t=854s)äº†è§£æ›´å¤šè¯¦æƒ…ã€‚ä¸€æ—¦ä½ æˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªå¼€å‘è€…å¸æˆ·ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º`config.ini`çš„æ–‡ä»¶ï¼Œå°†ä½ æ‰€æœ‰çš„è¯ä¹¦ä¿å­˜åœ¨ä¸€ä¸ªåœ°æ–¹ã€‚

```
[twitter]
api_key = APIKEY
api_key_secret = APIKEYSECRET
access_token = ACCESSTOKEN
access_token_secret = ACCESSTOKENSECRET
bearer_token = BEARERTOKEN
```

åœ¨ç»§ç»­è·å–æ¨æ–‡ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦éªŒè¯ APIã€‚æˆ‘ä»¬å°†ä½¿ç”¨`tweepy`åº“æ¥ç®€åŒ–æµç¨‹ã€‚

```
import tweepy
from configparser import ConfigParser

#configuration
config = ConfigParser()
config.read('config.ini')

#twitter credentials
api_key = config['twitter']['api_key']
api_key_secret = config['twitter']['api_key_secret']
access_token = config['twitter']['access_token']
access_token_secret = config['twitter']['access_token_secret']

#authentication
def twitter_auth():
    auth = tweepy.OAuthHandler(api_key,api_key_secret)
    auth.set_access_token(access_token,access_token_secret)
    return auth

#api
def twitter_api():
    auth = twitter_auth()
    api = tweepy.API(auth)
    return api
```

å¯¹äºè¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘å°†è·å–æ ‡æœ‰â€œ@ VFSGlobalâ€(å› ä¸ºé‚£æ˜¯æˆ‘ç›®å‰å®ä¹ çš„å…¬å¸)çš„æ¨æ–‡ï¼Œå…³é”®è¯ä¸ºâ€œç´§æ€¥â€æˆ–â€œå¸®åŠ©â€ã€‚æˆ‘ä»¬ä¼šå¿½ç•¥æ‰€æœ‰çš„è½¬å‘ã€‚åŒæ ·çš„æŸ¥è¯¢å°†æ˜¯:`@VFSGlobal OR @Vfsglobalcare AND urgent OR help -filter:retweets`ã€‚

`Cursor()`æ˜¯ä¸€ä¸ª tweepy å‡½æ•°ï¼Œå…è®¸æˆ‘ä»¬æ›´æ”¹å‚æ•°ä»¥è·å–æ‰€éœ€çš„ tweetsã€‚

æœ€åä¸€ä¸ªåŠŸèƒ½:

```
#fetching tweets
def fetch_df_tweets():
    api = twitter_api()
    query_topic = '@VFSGlobal OR @Vfsglobalcare__ AND urgent OR help -filter:retweets'
    tweets = tweepy.Cursor(api.search_tweets, q=query_topic,count=200,tweet_mode='extended',result_type='recent').items(200)
    return converting_to_df(tweets)
```

`converting_to_df()`æ˜¯å°†æ‰€æœ‰æ¨ç‰¹ä¿¡æ¯è½¬æ¢æˆç†ŠçŒ«æ•°æ®å¸§çš„åŠŸèƒ½ã€‚æ•°æ®æ¡†æ¶ä½¿å­˜å‚¨å’Œæ“ä½œæ•°æ®é¡¹å˜å¾—æ›´å®¹æ˜“ã€‚ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨ MongoDB æ¥å­˜å‚¨æ¨æ–‡(ç¨åä¼šæœ‰æ›´å¤šå†…å®¹)ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°å°†æ•°æ®å¸§å‘å¸ƒåˆ°é›†åˆä¸­ã€‚

`converting_to_df()`åŠŸèƒ½:

```
columns = ['_id','User','Tweet','Date and Time']
data = []
for tweet in tweets:
    text = tweet.full_text.split() #split tweets into words
    resultwords  = filter(lambda x:x[0]!='@', text) #remove all @mentions
    result = ' '.join(resultwords) #merge all words
    data.append([tweet.id,tweet.user.screen_name,result.capitalize(), tweet.created_at])
df = pd.DataFrame(data,columns=columns)
return df
```

![](img/1acb9e907c31dbe97c1291df70b76eb5.png)

æ ·æœ¬æ•°æ®å¸§è¾“å‡º

# **ä½¿ç”¨ Python çš„ *spaCy åº“åˆ†æå¹¶æå–æ¨æ–‡ä¸­çš„å…³é”®è¯ã€‚***

æˆ‘ä»¬å°†ä½¿ç”¨ spaCy æ¨¡å‹æ¥å¤„ç†æ¨æ–‡ã€‚NLP è¿‡ç¨‹å°†åŒ…æ‹¬:æ ‡è®°åŒ–å’Œå¼•ç†åŒ–ã€‚åˆ†æå°†åŸºäºå…³é”®è¯çš„æ€»ä½“é¢‘ç‡ã€‚

NLP æ­¥éª¤:

1.  **å¥å­æ ‡è®°åŒ–(å°†æ¨æ–‡åˆ†è§£æˆå¥å­)ã€‚**
2.  **æ¶ˆé™¤åœç”¨è¯(ä¸é‡è¦çš„è¯)ã€‚**
3.  **è¯æ±‡åŒ–(å°†ä¸€ä¸ªè¯çš„è¯å½¢å˜åŒ–ç»„åˆèµ·æ¥ï¼Œä»¥ä¾¿ä½œä¸ºä¸€ä¸ªé¡¹ç›®è¿›è¡Œåˆ†æçš„è¿‡ç¨‹)ã€‚**

![](img/ab48a3fdece856173488f8d168336085.png)

æ¥æº:ä¸šåŠ¡æµç¨‹å­µåŒ–å™¨

é¦–å…ˆï¼Œç¬¬ä¸€æ­¥æ˜¯ç”¨`spacy.load("en_core_web_lg")`å¯¼å…¥ç©ºé—´æ¨¡å‹ã€‚æ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨`spacy.cli`ä¸‹è½½æ¨¡å‹ã€‚

```
import re
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import pandas as pd

# spacy.cli.download("en_core_web_lg") #download the model only once
nlp = spacy.load("en_core_web_lg")

def chunking(df):

    df = df['Tweet']
    all_sentences = []

    #sentence tokenization
    for sentence in df:
        all_sentences.append(sentence)

    #lemmatization
    lemma=[]
    for line in all_sentences:
        line = re.sub(r'[^\w\s]', '', line) #filtering the @mentions and punctuations
        if line !='':
            doc = nlp(line.lstrip().lower())
            for token in doc:
                lemma.append(token.lemma_)

    #Removing all stopwords
    lemma2 = []
    custom_stop_words = ['please','try','vfs','day','need','hi','apply','visa',' ']

    for word in lemma:
        if word not in custom_stop_words:
            lexeme = nlp.vocab[word]
            if lexeme.is_stop==False:
                lemma2.append(word)

    df2 = pd.DataFrame(lemma2)

    #skipping the search keywords
    searchfor = ["urgent","help"]
    df2 = df2[df2[0].str.contains('|'.join(searchfor)) == False]
    df2 = df2.value_counts().rename_axis('_id').reset_index(name='counts')

    print(df2)
    return df2
```

`value_counts()`ä¼šç»Ÿè®¡æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°ï¼Œå¹¶ä¿æŒè·Ÿè¸ªã€‚è¿™å°†æœ‰åŠ©äºæˆ‘ä»¬ç†è§£é¢‘ç‡ã€‚è¿™å¥è¯å°†ä¼šæ˜¯`_id`ã€‚

`custom_stop_words`æ˜¯æˆ‘ä¸æƒ³æ•°çš„ä¸€ä¸²å•è¯ã€‚

æ•´ä¸ª NLP è¿‡ç¨‹å¾ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä¸æƒ³æ·±å…¥ç ”ç©¶å®ƒï¼Œä½¿æµç¨‹å˜å¾—å¤æ‚å¹¶å¢åŠ æœ¬æ–‡çš„é•¿åº¦ã€‚

# **ä½¿ç”¨ *MongoDB Atlas* åœ¨äº‘ä¸Šå­˜å‚¨æ¨æ–‡å’Œå…³é”®è¯ã€‚**

è¿™ä¸€æ­¥ä½ éœ€è¦ä¸€ä¸ª [MongoDB Atlas](http://cloud.mongodb.com) è´¦å·ã€‚MongoDB å…è®¸ä½ å…è´¹åˆ›å»ºä¸€ä¸ªå…±äº«é›†ç¾¤ã€‚

![](img/55d0837222dd31ee8fd61ac3c600ec8e.png)

MongoDB ä»ªè¡¨æ¿

å•å‡»è¿æ¥>è¿æ¥æ‚¨çš„åº”ç”¨ç¨‹åº>é€‰æ‹© Python å¹¶å¤åˆ¶è¿æ¥å­—ç¬¦ä¸²ã€‚

å°†è¿æ¥å­—ç¬¦ä¸²æ·»åŠ åˆ°ä¹‹å‰åˆ›å»ºçš„`config.ini`æ–‡ä»¶ä¸­ã€‚

```
[mongodb]
connection_string = CONNECTIONSTRING
```

![](img/5ded854aaeff97e5fa5c337e48a0348b.png)

åœ¨ MongoDB Atlas ä¸­åˆ›å»ºä¸€ä¸ªæ•°æ®åº“ï¼ŒåŒ…å«ä¸¤ä¸ªé›†åˆ:count å’Œ tweetsã€‚

ä¸ºäº†è¿æ¥åˆ° MongoDBï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`pymongo`åº“ã€‚

```
from pymongo import MongoClient, errors
from configparser import ConfigParser

#configuration
config = ConfigParser()
config.read('config.ini')

# Connect to MongoDB
def get_database():

   # Provide the mongodb atlas url to connect python to mongodb using pymongo
   CONNECTION_STRING = config['mongodb']['connection_string']

   # Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient
   client = MongoClient(CONNECTION_STRING)

   # Create the database for our example (we will use the same database throughout the tutorial)
   return client['twitter']
```

æˆ‘ä»¬å°†ä½¿ç”¨ 2 ä¸ªå‡½æ•°æ’å…¥æ•°æ®ï¼Œå³ã€‚`insert_df_tweets()`å’Œ`insert_df_count()`ã€‚`insert_df_tweets()`å°†æ’å…¥æ¨æ–‡ï¼Œ`insert_df_count()`å°†æ’å…¥å…³é”®è¯çš„æ•°é‡ã€‚

```
def insert_df_tweets(df):
    # Get the database
    dbname = get_database()
    # Get the collection
    collection_tweets = dbname['tweets']
    # Insert dataframe into collection
    try:
        collection_tweets.insert_many(df.to_dict('records'),ordered=False)
    except errors.BulkWriteError:
        print("Skipping duplicate tweets")

def insert_df_count(df2):
    # Get the database
    dbname = get_database()
    # Get the collection
    collection_count = dbname['count']
    #Inserting dataframe into collection
    try:
        collection_count.insert_many(df2.to_dict('records'),ordered=False)
    except errors.BulkWriteError:
        print("Skipping duplicate values")
```

ä¸ºäº†é¿å…é‡å¤æ’å…¥å¤šæ¡æ¨æ–‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`try except`å—æ¥å¤„ç†é”™è¯¯ã€‚

å°†ä¸€åˆ‡æ•´åˆåœ¨ä¸€èµ·:

```
if __name__ == "__main__":  
    #fetch tweets in dataframe 
    df = fetch_df_tweets()
    #insert dataframe into mongodb
    insert_df_tweets(df)
    #chunking tweets 
    df2 = chunking(df)
    #insert dataframe into mongodb (chunks with counts)
    insert_df_count(df2)
```

ç¥è´ºæ‚¨ï¼Œæ‚¨å·²ç»æˆåŠŸåœ°å°†æ‰€æœ‰å€¼æ’å…¥åˆ°æ‚¨çš„ MongoDB ä¸­ã€‚

![](img/cf6a1ecab627ab7f1f7642c4b247f620.png)

æ‚¨çš„ä»ªè¡¨æ¿åº”è¯¥æ˜¯è¿™æ ·çš„ã€‚

![](img/8b095b5ce08d046008ae8a0dba852ced.png)

# **åœ¨ *MongoDB Atlas* ä¸Šåˆ›å»ºæ•°æ®å¯è§†åŒ–ã€‚**

ç”±äº MongoDB Atlas çš„å†…ç½®æ•°æ®å¯è§†åŒ–ç‰¹æ€§ï¼Œè¿™æ˜¯æœ€æœ‰è¶£å’Œæœ€å®¹æ˜“çš„éƒ¨åˆ†ã€‚

ç‚¹å‡»å³ä¸Šè§’çš„`Visualize Your Data`ã€‚

![](img/4c46d52297fcad7ee731c0ab8ff6b4bb.png)

ç‚¹å‡»`Add Dashboard`åˆ›å»ºä¸€ä¸ªæ–°çš„ä»ªè¡¨æ¿ã€‚

![](img/036bb2482fc88cd1d3f89b77473ec43d.png)

æ·»åŠ ä¸€ä¸ªæ–°å›¾è¡¨ï¼Œå¹¶é€‰æ‹©å½“å‰é›†ç¾¤ä½œä¸ºæ•°æ®æºã€‚

![](img/6ec107cc6144a57da32f6776b7b85097.png)

é€‰æ‹©â€œè®¡æ•°â€,å› ä¸ºæˆ‘ä»¬å°†åˆ†æå…³é”®è¯çš„é¢‘ç‡ã€‚

å°†`_id`æ·»åŠ åˆ° X è½´ï¼Œå°†`count`æ·»åŠ åˆ° Y è½´ã€‚

![](img/86905f2dff33cf0492e9086edb422772.png)

æœ€ç»ˆä»ªè¡¨æ¿

ç§å•Šã€‚æ‚¨å¾ˆå¿«å°±åˆ›å»ºäº†æ•°æ®å¯è§†åŒ–ã€‚æ”¹è¿›å¯è§†åŒ–è¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšï¼Œä½†æˆ‘å¸Œæœ›è¿™ç¯‡æ•™ç¨‹èƒ½è®©ä½ å¯¹å¦‚ä½•ä½¿ç”¨ä» Twitter æ”¶é›†çš„æ•°æ®æ„å»ºä»ªè¡¨æ¿å’Œå›¾è¡¨è¿›è¡Œåé¦ˆåˆ†ææœ‰ä¸€ä¸ªåŸºæœ¬çš„äº†è§£ã€‚

å¸Œæœ›æœ¬æ•™ç¨‹å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®æˆ–å»ºè®®æ¥æ”¹è¿›æ•´ä¸ªæµç¨‹ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ã€‚

## é“¾æ¥:

1.  [GitHub](https://github.com/Om-Kamath/Twitter_CFA)
2.  [MongoDB å›¾é›†](https://www.mongodb.com/atlas/database)
3.  [ç©ºé—´](https://spacy.io/)
4.  [Twitter å¼€å‘è€…å¹³å°](https://developer.twitter.com/en)

å¦‚æœä½ å–œæ¬¢æˆ‘çš„åšå®¢ï¼Œä½ å¯ä»¥ç»™æˆ‘ä¹°æ¯å’–å•¡ã€‚

# åˆ†çº§ç¼–ç 

æ„Ÿè°¢æ‚¨æˆä¸ºæˆ‘ä»¬ç¤¾åŒºçš„ä¸€å‘˜ï¼åœ¨ä½ ç¦»å¼€ä¹‹å‰:

*   ğŸ‘ä¸ºæ•…äº‹é¼“æŒï¼Œè·Ÿç€ä½œè€…èµ°ğŸ‘‰
*   ğŸ“°æŸ¥çœ‹[å‡çº§ç¼–ç å‡ºç‰ˆç‰©](https://levelup.gitconnected.com/?utm_source=pub&utm_medium=post)ä¸­çš„æ›´å¤šå†…å®¹
*   ğŸ””å…³æ³¨æˆ‘ä»¬:[Twitter](https://twitter.com/gitconnected)|[LinkedIn](https://www.linkedin.com/company/gitconnected)|[æ—¶äº‹é€šè®¯](https://newsletter.levelup.dev)

ğŸš€ğŸ‘‰ [**åŠ å…¥äººæ‰é›†ä½“ï¼Œæ‰¾åˆ°ä¸€ä»½ä»¤äººæƒŠå–œçš„å·¥ä½œ**](https://jobs.levelup.dev/talent/welcome?referral=true)