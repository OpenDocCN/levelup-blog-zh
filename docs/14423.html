<html>
<head>
<title>Kaggle knowledge points: BERT’s five pooling methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kaggle知识点:伯特的五种汇集法</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/kaggle-knowledge-points-berts-five-pooling-methods-b55d61dd9968?source=collection_archive---------6-----------------------#2022-11-27">https://levelup.gitconnected.com/kaggle-knowledge-points-berts-five-pooling-methods-b55d61dd9968?source=collection_archive---------6-----------------------#2022-11-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/45d92b12a51a100c689ba4433eebd147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-g3I0OWFtlwxNFPR.png"/></div></div></figure><p id="d447" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">BERT模型可以用于多个任务，也是当前NLP模型的必要方法。在文本分类中，我们会使用<code class="fe kz la lb lc b">[CLS]</code>的相应输出来完成文本分类，当然还有其他方法。</p><p id="95db" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这允许每个<code class="fe kz la lb lc b">token</code>对应的输出被使用<code class="fe kz la lb lc b">pooling</code>，然后在通过后被分类。本文将介绍几种构建和使用BERT的常用方法。</p><h1 id="1cda" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">方法1:平均汇集</h1><p id="3bb3" class="pw-post-body-paragraph kb kc it kd b ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky im bi translated"><code class="fe kz la lb lc b">token</code>计算每个对应输出的平均值，这里需要考虑<code class="fe kz la lb lc b">attention_mask</code>，即需要考虑有效输入<code class="fe kz la lb lc b">token</code>。</p><pre class="mg mh mi mj gt mk lc ml bn mm mn bi"><span id="2587" class="mo le it lc b be mp mq l mr ms">class MeanPooling(nn.Module):<br/>    def __init__(self):<br/>        super(MeanPooling, self).__init__()<br/>        <br/>    def forward(self, last_hidden_state, attention_mask):<br/>        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()<br/>        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)<br/>        sum_mask = input_mask_expanded.sum(1)<br/>        sum_mask = torch.clamp(sum_mask, min = 1e-9)<br/>        mean_embeddings = sum_embeddings/sum_mask<br/>        return mean_embeddings</span></pre><h1 id="2eff" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">方法2:最大池化</h1><p id="8da3" class="pw-post-body-paragraph kb kc it kd b ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky im bi translated">计算每个<code class="fe kz la lb lc b">token</code>对应输出的最大值，这里需要考虑<code class="fe kz la lb lc b">attention_mask</code>，也就是有效输入需要考虑<code class="fe kz la lb lc b">token</code>。</p><pre class="mg mh mi mj gt mk lc ml bn mm mn bi"><span id="4d33" class="mo le it lc b be mp mq l mr ms">class MaxPooling(nn.Module):<br/>    def __init__(self):<br/>        super(MaxPooling, self).__init__()<br/>        <br/>    def forward(self, last_hidden_state, attention_mask):<br/>        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()<br/>        embeddings = last_hidden_state.clone()<br/>        embeddings[input_mask_expanded == 0] = -1e4<br/>        max_embeddings, _ = torch.max(embeddings, dim = 1)<br/>        return max_embeddings</span></pre><h1 id="0da9" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">方法3:最小公摊</h1><p id="2828" class="pw-post-body-paragraph kb kc it kd b ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky im bi translated">计算每个<code class="fe kz la lb lc b">token</code>对应输出的最小值，这里需要考虑<code class="fe kz la lb lc b">attention_mask</code>，即需要考虑有效输入<code class="fe kz la lb lc b">token</code>。</p><pre class="mg mh mi mj gt mk lc ml bn mm mn bi"><span id="e888" class="mo le it lc b be mp mq l mr ms">class MinPooling(nn.Module):<br/>    def __init__(self):<br/>        super(MinPooling, self).__init__()<br/>        <br/>    def forward(self, last_hidden_state, attention_mask):<br/>        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()<br/>        embeddings = last_hidden_state.clone()<br/>        embeddings[input_mask_expanded == 0] = 1e-4<br/>        min_embeddings, _ = torch.min(embeddings, dim = 1)<br/>        return min_embeddings</span></pre><h1 id="14c0" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">方法4:加权池</h1><p id="becd" class="pw-post-body-paragraph kb kc it kd b ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky im bi translated">计算每个<code class="fe kz la lb lc b">token</code>对应输出的重量。这里的权重可以通过特征计算，也可以通过IDF计算。</p><pre class="mg mh mi mj gt mk lc ml bn mm mn bi"><span id="06e7" class="mo le it lc b be mp mq l mr ms">class WeightedLayerPooling(nn.Module):<br/>    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):<br/>        super(WeightedLayerPooling, self).__init__()<br/>        self.layer_start = layer_start<br/>        self.num_hidden_layers = num_hidden_layers<br/>        self.layer_weights = layer_weights if layer_weights is not None \<br/>            else nn.Parameter(<br/>                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)<br/>            )</span></pre><pre class="mt mk lc mu mv aw mw bi"><span id="9e0b" class="mx le it lc b gy my mz l na ms">    def forward(self, ft_all_layers):<br/>        all_layer_embedding = torch.stack(ft_all_layers)<br/>        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]</span><span id="0682" class="mx le it lc b gy nb mz l na ms">        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())<br/>        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()</span><span id="0700" class="mx le it lc b gy nb mz l na ms">        return weighted_average</span></pre><h1 id="a274" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">方法5:集中注意力</h1><p id="ac0d" class="pw-post-body-paragraph kb kc it kd b ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky im bi translated">将每个<code class="fe kz la lb lc b">token</code>特征单独添加到一个图层中进行关注度计算，增加模型的建模能力。</p><pre class="mg mh mi mj gt mk lc ml bn mm mn bi"><span id="81b5" class="mo le it lc b be mp mq l mr ms">class AttentionPooling(nn.Module):<br/>    def __init__(self, in_dim):<br/>        super().__init__()<br/>        self.attention = nn.Sequential(<br/>        nn.Linear(in_dim, in_dim),<br/>        nn.LayerNorm(in_dim),<br/>        nn.GELU(),<br/>        nn.Linear(in_dim, 1),<br/>        )</span></pre><pre class="mt mk lc mu mv aw mw bi"><span id="ad74" class="mx le it lc b gy my mz l na ms">    def forward(self, last_hidden_state, attention_mask):<br/>        w = self.attention(last_hidden_state).float()<br/>        w[attention_mask==0]=float('-inf')<br/>        w = torch.softmax(w,1)<br/>        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)<br/>        return attention_embeddings</span></pre><h1 id="9c76" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">总结</h1><p id="df0b" class="pw-post-body-paragraph kb kc it kd b ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky im bi translated">从模型复杂度来看:attention pooling &gt; weighted layer pooling &gt; mean pooling/min pooling/max pooling</p><p id="e784" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从模型精度来看:注意池&gt;加权层池&gt;平均池&gt;最大池&gt;最小池</p><p id="4fef" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用多种池的目的是增加BERT模型的多样性，并考虑在模型集成中使用它。</p><p id="dc29" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">喜欢这篇文章吗？成为一个媒介成员，通过无限制的阅读继续学习。如果你使用<a class="ae nc" href="https://machinelearningabc.medium.com/membership" rel="noopener">这个链接</a>成为会员，你将支持我，不需要你额外付费。提前感谢，再见！</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="e3bb" class="ld le it bd lf lg nk li lj lk nl lm ln lo nm lq lr ls nn lu lv lw no ly lz ma bi translated">分级编码</h1><p id="df3f" class="pw-post-body-paragraph kb kc it kd b ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky im bi translated">感谢您成为我们社区的一员！在你离开之前:</p><ul class=""><li id="2c15" class="np nq it kd b ke kf ki kj km nr kq ns ku nt ky nu nv nw nx bi translated">👏为故事鼓掌，跟着作者走👉</li><li id="3268" class="np nq it kd b ke ny ki nz km oa kq ob ku oc ky nu nv nw nx bi translated">📰查看<a class="ae nc" href="https://levelup.gitconnected.com/?utm_source=pub&amp;utm_medium=post" rel="noopener ugc nofollow" target="_blank">升级编码出版物</a>中的更多内容</li><li id="d385" class="np nq it kd b ke ny ki nz km oa kq ob ku oc ky nu nv nw nx bi translated">🔔关注我们:<a class="ae nc" href="https://twitter.com/gitconnected" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae nc" href="https://www.linkedin.com/company/gitconnected" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae nc" href="https://newsletter.levelup.dev" rel="noopener ugc nofollow" target="_blank">时事通讯</a></li></ul><p id="dddf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">🚀👉<a class="ae nc" href="https://jobs.levelup.dev/talent/welcome?referral=true" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">加入升级达人集体，找到一份惊艳的工作</strong> </a></p></div></div>    
</body>
</html>