# ä½¿ç”¨è½¬æ¢å™¨çš„å¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»

> åŸæ–‡ï¼š<https://levelup.gitconnected.com/multilingual-text-classification-with-transformers-2147fe179c6b>

## å¾®è°ƒçš„ mBERT/distill mBERT å¹¶å¯¼å‡ºåˆ° ONNX è¿›è¡Œæ¨ç†

![](img/317e08e26430a1237ebea143e9b95e98.png)

[æ¢æ°æ£®](https://unsplash.com/@ninjason?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹ç…§

ä¹‹å‰ï¼Œæˆ‘å†™è¿‡ä¸€ç¯‡å…³äºä½¿ç”¨ Python ä¸­çš„ spaCy è¿›è¡Œ[æ–‡æœ¬åˆ†ç±»çš„æ–‡ç« ã€‚spaCy èƒŒåçš„æ¶æ„å’Œæ¦‚å¿µå¾ˆç®€å•ï¼Œä½†å¯¹å¤§å¤šæ•°ç”¨ä¾‹æ¥è¯´å·²ç»è¶³å¤Ÿå¥½äº†ã€‚äº‹å®ä¸Šï¼Œæˆ‘å·²ç»åœ¨æˆ‘çš„ä¸€äº›é¡¹ç›®ä¸­å¹¿æ³›ä½¿ç”¨äº†å®ƒã€‚æ¯”å¦‚äºµæ¸å’Œå¹¿å‘Šåˆ†ç±»ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç©ºé—´æ¶æ„å­˜åœ¨ä»¥ä¸‹é—®é¢˜:](https://towardsdatascience.com/sarcasm-text-classification-using-spacy-in-python-7cd39074f32e)

*   æˆ‘å¿…é¡»ä¸ºæ¯ç§è¯­è¨€ç‹¬ç«‹è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹
*   æˆ‘å¿…é¡»ä¸ºæŸäº›è¯­è¨€(ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€ä¿„æ–‡ã€è¶Šå—æ–‡ç­‰)è®¾ç½®å’Œå®‰è£…ä¸“é—¨çš„åˆ†è¯å™¨ã€‚)

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†è§£é‡Šå¦‚ä½•ä½¿ç”¨`transformers`è®­ç»ƒå•ä¸ªå¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»æ¨¡å‹:

*   å¾®è°ƒé¢„è®­ç»ƒçš„æ‹¥æŠ±è„¸çš„æ–‡æœ¬åˆ†ç±»çš„å˜å½¢é‡‘åˆš
*   åŸºäº DistilmBERT æ¶æ„(ä¹Ÿæ”¯æŒæ™®é€šçš„ mBERT æ¶æ„)
*   å°†å¾®è°ƒæ¨¡å‹å¯¼å‡ºåˆ° ONNX è¿›è¡Œæ¨ç†

è®©æˆ‘ä»¬ç»§ç»­ä¸‹ä¸€éƒ¨åˆ†ï¼Œè®¾ç½®æ‰€æœ‰éœ€è¦çš„åŒ…ã€‚

# è®¾ç½®

å¼ºçƒˆå»ºè®®åœ¨å®‰è£…ä¹‹å‰åˆ›å»ºä¸€ä¸ªæ–°çš„è™šæ‹Ÿç¯å¢ƒã€‚æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼Œå®‰è£…`transformers`åŒ…ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
pip install transformers
```

> è¯·æ³¨æ„ï¼Œå®˜æ–¹æ–‡æ¡£æ˜¯åŸºäºå…‹éš†æœ€æ–°çš„å­˜å‚¨åº“å¹¶å°†å…¶å®‰è£…åœ¨æœ¬åœ°ã€‚æœ¬æ–‡åŸºäº PyPI çš„æœ€æ–°ç¨³å®šç‰ˆæœ¬ã€‚

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º`requirements.txt`çš„æ–°æ–‡ä»¶ï¼Œå¹¶åœ¨å…¶ä¸­æ·»åŠ ä»¥ä¸‹æ–‡æœ¬:

```
accelerate >= 0.12.0
datasets >= 1.8.0
sentencepiece != 0.1.92
scipy
scikit-learn
protobuf
torch >= 1.3
evaluate
```

> è¯·æ³¨æ„ï¼Œæœ¬æ•™ç¨‹æ˜¯åŸºäºå˜å‹å™¨ç‰ˆæœ¬ 4.24.0ï¼Œè¯·å‚è€ƒæ›´æ–°çš„[éœ€æ±‚æ–‡ä»¶](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/requirements.txt)å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯æ›´æ–°çš„ç‰ˆæœ¬ã€‚

è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€æœ‰å¿…éœ€çš„è½¯ä»¶åŒ…:

```
pip install -r requirements.txt
```

è½¬åˆ°ä¸‹é¢çš„[åº“](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)å¹¶å°† [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py) æ–‡ä»¶ä¿å­˜åˆ°å·¥ä½œç›®å½•ã€‚æ˜¯å®˜æ–¹å¯¹ä¸€ä¸ª`transformers`æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è®­ç»ƒè„šæœ¬ã€‚

> `run_glue.py`æ˜¯ç”¨äºå•æœºå¾®è°ƒçš„æ ‡å‡† Pytorch ç‰ˆæœ¬ã€‚è¿˜æœ‰å¦ä¸€ä¸ªåä¸º`run_glue_no_trainer.py`çš„è®­ç»ƒè„šæœ¬ï¼Œå®ƒæ˜¯ä½¿ç”¨`accelerate`åŒ…åœ¨å¤šå°æœºå™¨ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„ã€‚

æ‰“å¼€è®­ç»ƒè„šæœ¬å¹¶æ³¨é‡Šæ‰`check_min_version`å‡½æ•°(è¿™æ˜¯é˜²æ­¢å¼‚å¸¸æ‰€å¿…éœ€çš„ï¼Œå› ä¸ºæœ¬æ•™ç¨‹æ˜¯åŸºäº PyPI çš„`transformers`çš„æ—§ç¨³å®šç‰ˆæœ¬):

```
# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
# check_min_version("4.26.0.dev0")
```

# æ•°æ®é›†

è®­ç»ƒè„šæœ¬æ¥å— CSV æˆ– JSON ä½œä¸ºè®­ç»ƒ/éªŒè¯æ–‡ä»¶ã€‚æ¯ä¸ªæ–‡ä»¶éƒ½åº”è¯¥æœ‰ä»¥ä¸‹å¤´æˆ–é”®å€¼å¯¹:

*   `sentence` â€”ç¤ºä¾‹æ–‡æœ¬
*   `label`â€”ä»£è¡¨ç±»åˆ«çš„æ•°å­—ã€‚ä¾‹å¦‚ï¼Œ0 è¡¨ç¤ºé˜´æ€§ï¼Œ1 è¡¨ç¤ºé˜³æ€§ã€‚

```
|------------|--------|
|  sentence  | label  |
|------------|--------|
| bad movie  |   0    |
|------------|--------|
```

å¯¹äºé‚£äº›æ²¡æœ‰è‡ªå·±è®­ç»ƒæ•°æ®çš„äººï¼Œä½ å¯ä»¥ä½¿ç”¨ HuggingFace çš„[æ–¯å¦ç¦æƒ…æ„Ÿæ ‘åº“](https://huggingface.co/datasets/sst2)æ•°æ®é›†(æƒ…æ„Ÿåˆ†æ)ã€‚

å°†è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†æ”¾åœ¨ä¸`run_glue.py`æ–‡ä»¶ç›¸åŒçš„å·¥ä½œç›®å½•ä¸­ã€‚å½“å‰çš„åŸ¹è®­è„šæœ¬åªæ¥å—ä¸€ä¸ªåŸ¹è®­æ–‡ä»¶å’Œä¸€ä¸ªéªŒè¯æ–‡ä»¶ã€‚

# åŸ¹å…»

è¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹åŸ¹è®­:

```
python run_glue.py \
  --model_name_or_path distilbert-base-multilingual-cased \
  --train_file ./data/train.csv \
  --validation_file ./data/eval.csv \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --per_device_train_batch_size 16 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir ./output/
```

*   ä½¿ç”¨`bert-base-multilingual-cased`å¯¹ mBERT è€Œä¸æ˜¯ DistilmBERT è¿›è¡Œå¾®è°ƒã€‚ä¹Ÿå¯ä»¥åœ¨å…¶ä»–éå¤šè¯­è¨€`transformers`è½¦å‹ä¸Šè¿›è¡ŒåŸ¹è®­ã€‚
*   å°†`per_device_train_batch_size`çš„å€¼å¢åŠ æˆ–å‡å°‘åˆ° 4/8/32/64/ç­‰ã€‚åŸºäº GPU çš„å†…å­˜ã€‚
*   ç›¸åº”ä¿®æ”¹`train_file`ã€`validation_file`å’Œ`output_dir`çš„è·¯å¾„ã€‚

> å½“æ‚¨ç¬¬ä¸€æ¬¡è¿è¡Œè¯¥å‘½ä»¤æ—¶ï¼Œå®ƒä¼šå°†æ‰€éœ€çš„æ¨¡å‹ä¸‹è½½åˆ°ç¼“å­˜ä¸­ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å°†å¯¹æ•°æ®é›†è¿›è¡Œä»¤ç‰ŒåŒ–ï¼Œå¹¶å­˜å‚¨åœ¨åŒä¸€æ–‡ä»¶å¤¹ä¸­ã€‚

æ­¤å¤–ï¼Œè¿˜æœ‰å…¶ä»–å¯¹åŸ¹è®­æœ‰ç”¨çš„è¾“å…¥å‚æ•°:

*   `cache_dir` â€”é»˜è®¤æƒ…å†µä¸‹ï¼ŒHuggingFace ä¼šåœ¨`.cache/huggingface/` (Linux)ä¸‹è½½å¹¶å­˜å‚¨æ¨¡å‹ã€‚æ­¤å‚æ•°ç¡®ä¿ç¼“å­˜å°†ä¿å­˜åœ¨æ‰€éœ€çš„ä½ç½®ã€‚
*   `fp16` â€”ä»¥æ··åˆç²¾åº¦è®­ç»ƒæ¨¡å‹ã€‚ä½¿ç”¨æ··åˆç²¾åº¦å¯ä»¥åœ¨ä¸é™ä½æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹å°†è®­ç»ƒæ—¶é—´å‡å°‘ä¸€åŠã€‚
*   `save_strategy` â€”ä¸æŒ‰ç‰¹å®šæ—¶é—´é—´éš”ä¿å­˜æ£€æŸ¥ç‚¹ã€‚è¯¥æ ‡å¿—å°†åœ¨æ¯ä¸ªæ—¶æœŸä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚

```
CUDA_VISIBLE_DEVICES=2 python run_glue.py \
  --model_name_or_path distilbert-base-multilingual-cased \
  --train_file ./data/train.csv \
  --validation_file ./data/eval.csv \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --per_device_train_batch_size 16 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir ./output/ \
  --save_strategy epoch \
  --cache_dir models/ \
  --fp16
```

ç¯å¢ƒå˜é‡`CUDA_VISIBLE_DEVICES`å¯ç”¨äºåˆ†é…è®­ç»ƒæ‰€éœ€çš„ GPUã€‚ä»¥ä¸‹å‘½ä»¤å°†ä½¿ç”¨ç¬¬ä¸‰ä¸ª GPU è®­ç»ƒæ¨¡å‹(ç´¢å¼•ä» 0 å¼€å§‹):

```
CUDA_VISIBLE_DEVICES=2 python run_glue.py ...
```

æ ¹æ®æ•°æ®é›†çš„ä¸åŒï¼Œæ•´ä¸ªåŸ¹è®­å¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶ã€‚åœ¨åŸ¹è®­ç»“æŸæ—¶ï¼Œå®ƒå°†åœ¨`output_dir`ä¸­ç”Ÿæˆä»¥ä¸‹é‡è¦æ–‡ä»¶:

*   é…ç½®. json
*   pytorch_model.bin
*   special _ tokens _ map.json
*   tokenizer.json
*   tokenizer_config.json
*   vocab.txt

ä¸‹ä¸€èŠ‚å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨æ–°è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

# æ¨ç†

åœ¨åŒä¸€ä¸ªå·¥ä½œç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º`inference.py`çš„æ–°æ–‡ä»¶ã€‚åœ¨å…¶ä¸­è¿½åŠ ä»¥ä¸‹ä»£ç :

```
from transformers import AutoTokenizer, DistilBertForSequenceClassification, TextClassificationPipeline

# tokenizer initialization
tokenizer = AutoTokenizer.from_pretrained("./output/", local_files_only=True)
# model initialization, use BertForSequenceClassification for mBERT/BERT architecture
model = DistilBertForSequenceClassification.from_pretrained("./output/", local_files_only=True)
# pipeline initialization
pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)

text_list = ["I love this movie!", "I hate this movie!"]
# inference
result = pipe(text_list)
print(result)
```

ä¿å­˜æ–‡ä»¶å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤:

```
python inference.py
```

ä»¥ä¸‹ç»“æœå°†æ‰“å°åœ¨ç»ˆç«¯ä¸Š:

```
[{'label': 0, 'score': 0.9951834082603455}, {'label': 1, 'score': 0.9082725048065186}]
```

# ONNX

æˆ‘ä»¬ä¸ç›´æ¥ç”¨ Pytorch è¿›è¡Œæ¨ç†ï¼Œè€Œæ˜¯æ¢ç´¢å¦‚ä½•å°†æ¨¡å‹è½¬æ¢æˆ ONNX æ ¼å¼ï¼Œå¹¶ä½¿ç”¨ ONNX è¿è¡Œæ—¶è¿›è¡Œæ¨ç†ã€‚

ONNX è¿è¡Œæ—¶ç›¸æ¯” Pytorch å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿:

*   ML å‹å·çš„æ€§èƒ½åŠ é€Ÿ(é€‚ç”¨äº CPU å’Œ GPU)
*   èƒ½å¤Ÿåœ¨ä¸åŒçš„ç¡¬ä»¶å’Œæ“ä½œç³»ç»Ÿä¸Šè¿è¡Œ
*   æ”¯æŒå…¶ä»–è¯­è¨€çš„æ¨ç†(C#/C++/Java)
*   ç‹¬ç«‹äºåŸ¹è®­æ¡†æ¶(Pytorchã€TensorFlow ç­‰ã€‚)

`transformers`åº“è‡ªå¸¦äº†ç”¨äº ONNX æ”¯æŒçš„`[transformers.onnx](https://huggingface.co/docs/transformers/v4.24.0/en/serialization#export-to-onnx)`åŒ…ã€‚æŒ‰ç…§ä»¥ä¸‹æ–¹å¼å®‰è£…:

```
pip install transformers[onnx]
```

è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå°†æ–°å¾®è°ƒçš„æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼:

```
python -m transformers.onnx --model=output/ --feature=sequence-classification --framework=pt onnx_output/
```

æ‰€æœ‰æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨å¦‚ä¸‹:

```
|--------------------------------|-------------------------------------|
|            Feature             |             Auto Class              |
|--------------------------------|-------------------------------------|
|causal-lm, causal-lm-with-past  |  AutoModelForCausalLM               |
|default, default-with-past      |  AutoModel                          |
|masked-lm                       |  AutoModelForMaskedLM               |
|question-answering              |  AutoModelForQuestionAnswering      |
|seq2seq-lm, seq2seq-lm-with-past|  AutoModelForSeq2SeqLM              |
|sequence-classification         |  AutoModelForSequenceClassification |
|token-classification            |  AutoModelForTokenClassification    |
|--------------------------------|-------------------------------------|
```

ç»ˆç«¯å°†æ˜¾ç¤ºä»¥ä¸‹è¾“å‡ºï¼Œè¡¨æ˜å¯¼å‡ºæˆåŠŸ:

```
Validating ONNX model...
        -[âœ“] ONNX model output names match reference model ({'logits'})
        - Validating ONNX Model output "logits":
                -[âœ“] (2, 2) matches (2, 2)
                -[âœ“] all values close (atol: 1e-05)
All good, model saved at: onnx_output/model.onnx
```

å®ƒå°†åœ¨`onnx_ouput`æ–‡ä»¶å¤¹ä¸­ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:

*   model.onnx

åˆ›å»ºä¸€ä¸ªåä¸º`inference_onnx.py`çš„æ–°è„šæœ¬ï¼Œå¹¶åœ¨å…¶ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç :

```
from transformers import AutoTokenizer
from onnxruntime import InferenceSession
import numpy as np

# utility function to calculate probabilities
def softmax(vec):
    exponential = np.exp(vec)
    probabilities = exponential / np.sum(exponential)
    return probabilities

# initialization
sentiment_dict = {0: "positive", 1: "negative"}
tokenizer = AutoTokenizer.from_pretrained("./output/", local_files_only=True)
session = InferenceSession("onnx_output/model.onnx")

text_list = ["I love this movie!", "I hate this movie!"]
result = []

# loop over each item in text_list
for text in text_list:
    # ONNX accepts numpy as input
    inputs = tokenizer(text, return_tensors="np")

    # inference, returns numpy array
    outputs = session.run(output_names=["logits"], input_feed=dict(inputs))
    # [array([[ 4.1819444, -2.2060142]], dtype=float32)]

    # convert to probability score (0 to 1)
    probabilities = softmax(outputs[0][0])
    # [0.9951833  0.00481664]

    # get the index with the highest probability
    index = np.argmax(outputs)
    # 0

    print({'label': sentiment_dict[index], 'score': probabilities[index]})
    # {'label': positive, 'score': 0.9951833}
```

ä¿å­˜æ–‡ä»¶å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨æ–­:

```
python inference_onnx.py
```

> ä¸ä½¿ç”¨ Pytorch è¿›è¡Œæ¨ç†ç›¸æ¯”ï¼Œæ¨ç†é€Ÿåº¦åº”è¯¥æœ‰æ‰€æé«˜ã€‚

# ç»“è®º

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æœ¬æ•™ç¨‹çš„å­¦ä¹ å†…å®¹ã€‚

é¦–å…ˆç®€è¦ä»‹ç»äº†ä½œä¸ºå¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»æ›¿ä»£æ¶æ„çš„`transformers`ã€‚

ç„¶åï¼Œæœ¬æ–‡ä»‹ç»äº†æ‰€æœ‰å¿…éœ€åŒ…çš„è®¾ç½®å’Œå®‰è£…ã€‚å®ƒè¿˜è§£é‡Šäº†æ•°æ®é›†çš„æ ¼å¼ï¼Œå¹¶é“¾æ¥åˆ°ä¸€ä¸ªå…è´¹ä½¿ç”¨çš„æƒ…æ„Ÿåˆ†ææ•°æ®é›†ã€‚

éšåï¼Œå®ƒç»§ç»­ä½¿ç”¨ Pytorch è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚å®ƒè¿˜å¼ºè°ƒäº†ä¸€äº›é‡è¦çš„è®­ç»ƒå‚æ•°ï¼Œå¦‚æ··åˆç²¾åº¦è®­ç»ƒçš„ fp16ã€‚

æœ¬æ•™ç¨‹ç»§ç»­è¯¦ç»†è§£é‡Š ONNX ä»¥åŠå¦‚ä½•å°† Pytorch æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ã€‚æœ€åä¸€èŠ‚å±•ç¤ºäº†ä½¿ç”¨è½¬æ¢åçš„ ONNX æ¨¡å‹æ‰§è¡Œæ¨ç†çš„å®Œæ•´è„šæœ¬ã€‚

æ„Ÿè°¢ä½ é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚ç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼

# å‚è€ƒ

1.  [hugging faceâ€”distilbert-base-å¤šè¯­è¨€ç¯å¢ƒ](https://huggingface.co/distilbert-base-multilingual-cased)
2.  [æ‹¥æŠ±è„¸â€”Bert-base-å¤šè¯­è¨€ç¯å¢ƒ](https://huggingface.co/bert-base-multilingual-cased)
3.  [HuggingFace åšå®¢â€”å¯¼å‡ºåˆ° ONNX](https://huggingface.co/docs/transformers/v4.24.0/en/serialization#export-to-onnx)
4.  [æ‹¥æŠ±è„¸æ•°æ®é›†â€” SST](https://huggingface.co/datasets/sst2) 2
5.  [Github â€” HuggingFace æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)

# åˆ†çº§ç¼–ç 

æ„Ÿè°¢æ‚¨æˆä¸ºæˆ‘ä»¬ç¤¾åŒºçš„ä¸€å‘˜ï¼åœ¨ä½ ç¦»å¼€ä¹‹å‰:

*   ğŸ‘ä¸ºæ•…äº‹é¼“æŒï¼Œè·Ÿç€ä½œè€…èµ°ğŸ‘‰
*   ğŸ“°æŸ¥çœ‹[å‡çº§ç¼–ç å‡ºç‰ˆç‰©](https://levelup.gitconnected.com/?utm_source=pub&utm_medium=post)ä¸­çš„æ›´å¤šå†…å®¹
*   ğŸ””å…³æ³¨æˆ‘ä»¬:[Twitter](https://twitter.com/gitconnected)|[LinkedIn](https://www.linkedin.com/company/gitconnected)|[æ—¶äº‹é€šè®¯](https://newsletter.levelup.dev)

ğŸš€ğŸ‘‰ [**åŠ å…¥äººæ‰é›†ä½“ï¼Œæ‰¾åˆ°ä¸€ä»½ä»¤äººæƒŠå–œçš„å·¥ä½œ**](https://jobs.levelup.dev/talent/welcome?referral=true)