# 如何有效地转换 CSV 文件并以压缩形式上传到 AWS S3 (Python，Boto3)

> 原文：<https://levelup.gitconnected.com/efficiently-transforming-compressing-in-memory-and-ingesting-csv-files-to-aws-s3-using-python-da7bcec5f8f>

![](img/3313d097f4a68c4b9bc597630349cf27.png)

凯文·Ku 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

如果您一直从事数据工程领域的工作，那么您很可能已经参与了 CSV 文件的处理。尽管它不是最有效的分析格式，但 CSV 格式仍然在当前的数据格局中占有相当大的份额。这是一种受到广泛支持的格式，通常在源系统(如数据库)以文件形式提供数据时会遇到，这些数据将被接收到数据湖(随后被接收到专门的存储区，如服务于特定用例的数据仓库)。如果您是一家 AWS 商店，并且需要处理 CSV 文件并将它们上传到 S3，那么本文将介绍一种使用 Python 和 AWS SDK (boto3)的方法。虽然人们可以用许多方法来解决这个问题，但有几个考虑因素值得强调，使它成为一个相当可行的解决方案:

*   接收前压缩—网络通常是任何系统架构中的主要瓶颈。另一方面，CSV 格式在压缩方面效率不高，因此解压缩时通常体积较大。因此，将庞大的 CSV 文件接收到 AWS S3 可能是一项成本相当高的操作。总是建议在接收数据之前进行压缩。现在，在选择正确的压缩格式方面，可以有多种考虑因素，具体取决于下游处理将如何完成，例如，如果您使用 Spark 等大数据工具，建议使用可拆分的压缩格式。为了简单起见，我使用 gzip 作为压缩格式。
*   在 ETL 中避免 I/O——只要有可能，建议在 ETL 过程中避免磁盘 I/O。磁盘 I/O 是相当昂贵/缓慢的操作，因此，如果您可以在内存中做任何事情，它最终会有助于您的方法的整体优化。

有了这些基本的考虑，让我们看一个例子，看看如何转换 CSV 文件，压缩它(在运行中或在内存中)并通过 Python 的本地库和 AWS SDK (boto3)将它上传到 S3。

## 要求:

*   AWS 帐户
*   IAM 用户
*   S3 水桶
*   计算机编程语言
*   Boto3

假设您有一个简单的 CSV 文件，如下所示:

```
sensor_code,parameter_name,start_timestamp,finish_timestamp,sensor_valueDEX123,OCO_1,30/09/2021 5:00:00 PM,1/10/2021 1:00:00 AM,0.3008914DEX123,OCO_1,30/09/2021 6:00:00 PM,1/10/2021 2:00:00 AM,0.2821953DEX123,OCO_1,30/09/2021 7:00:00 PM,1/10/2021 3:00:00 AM,0.2513988DEX123,OCO_1,30/09/2021 8:00:00 PM,1/10/2021 4:00:00 AM,0.2153951DEX129,OCO_1,30/09/2021 9:00:00 PM,1/10/2021 5:00:00 AM,0.1723991DEX129,OCO_2,30/09/2021 10:00:00 PM,1/10/2021 6:00:00 AM,0.1423465DEX129,OCO_2,30/09/2021 11:00:00 PM,1/10/2021 7:00:00 AM,0.1424952DEX129,OCO_2,1/10/2021 12:00:00 AM,1/10/2021 8:00:00 AM,0.1455519DEX129,OCO_2,1/10/2021 1:00:00 AM,1/10/2021 9:00:00 AM,0.1682339
```

从概念上讲，这可以被认为是传感器度量的时间序列数据。您可能已经注意到时间戳格式不是“标准”格式，即 yyyy-mm-dd HH:MM:SS。如果您正在将数据加载到数据仓库中，它们在解析非标准时间格式时可能会有一些限制。例如，红移可以识别许多时间戳格式，但不是全部。上例中的格式是 Redshift 在向其加载数据时无法识别的(例如，通过来自 S3 的 COPY 命令)。因此，如果目标是将这些数据加载到 Redshift 中，那么就需要进行一些基本的转换，以便源文件中的时间戳格式是标准化的，从而可以在复制操作期间被 Redshift 识别。

让我们看一个示例代码，看看如何在 Python 中以最佳方式完成这项任务:

所以在上面的片段中发生了很多事情。为了更好地理解，我们来分解一下:

*   第 7 行:我们通过 *boto3.client()* 方法创建一个 S3 客户端。建议使用 *boto3。Session()* 然后用它创建 boto3.client(本文给出了一个[很好的解释](https://ben11kehoe.medium.com/boto3-sessions-and-why-you-should-use-them-9b094eb5ca8e))。为了简单起见，我只使用了 *boto3.client()*
*   第 9 行:我们使用内存中的字节缓冲区来存储字节，从而创建一个二进制流。简单地说，可以把它看作是将字节写入内存文件而不是磁盘上的实际文件的一种方式。这将用于存储数据的压缩(gzipped)对象将被上传到 S3。
*   第 10 行到第 11 行:我们通过 python 的 *open()* 函数打开我们的 CSV 源文件，使用基于“with”上下文管理器的方法来确保文件在结束时正确关闭，即使在异常的情况下。然后我们利用 *csv.reader()* 并将打开的文件对象传递给它。我们还指定了 CSV 文件的分隔符。
*   第 12 行:因为我们的 CSV 文件包含头，所以我们使用 python 的 next 函数来获取第一个项目/行。(在这种情况下是 header，它将是 list 类型)。
*   第 13 到 15 行:这是我们进行实际处理的地方，即将时间戳格式解析为标准格式。我们遍历 csv_rdr 对象，每次迭代都会产生一行。我们使用*datetime . datetime . strptime(x[2]，" %d/%m/%Y %I:%M:%S %p")* 按照时间戳格式( *%d/%m/%Y %I:%M:%S %p* )解析第 3 列中的时间戳值(由于 x[2])，将其转换为字符串，然后存储它。这为我们提供了标准 yyyy-mm-dd HH:MM:SS 格式的时间戳值。
*   第 16 行:我们将转换后的行添加到 *transformed_rows* 列表中。请注意，对于大文件的内存利用率来说，这不是一个优化的方法。为了有效处理大文件，请考虑使用生成器。
*   第 18 行:我们创建一个新的列表 *transformed_rows_header* ，它是两个列表的串联，即 *header* 和 *transformed_rows。*因此， *transformed_rows_header* 是一个列表的列表。这个*transformed _ rows _ header*的第一个元素是 header。每个后续元素都是经过解析的时间戳格式的转换行。在这一阶段，我们将数据转换为我们想要的状态，但是它存在于 Python 的对象(即列表)中。
*   第 19 行:类似于我们打开文件的方式，我们通过 *gzip 初始化一个上下文管理器。GzipFile* 来指定我们想要写一个 gzip 文件。我们将在第 8 行初始化的 *mem_file* BytesIO 缓冲区指定为我们的目标，即 gzip 操作的输出将被写入的位置。我们将“wb”指定为模式，以指定我们要写入的字节。对于 *compresslevel* ，我们使用 6，这也是默认设置，可以很好地平衡速度和压缩比。因此，在一个坚果壳中，我们准备将 gzip 文件的输出写到哪里(在本例中，写到内存缓冲区)
*   第 20 到 22 行:我们需要将转换后的数据(当前在列表的*transformed _ rows _ header*列表中)刷新为 CSV 格式，然后可以压缩并上传为文件。为此，我们初始化一个名为 *buff* 的内存中的字符串缓冲区(类似于内存中的字节 IO 缓冲区)，并使用“csv”模块将我们的列表的结果，即*transformed _ rows _ header*写入这个内存中的文件(buff)即字符串缓冲区。您可以认为它是将 CSV 数据写入文件，但在这种情况下，我们执行的是内存操作。由于前面讨论的原因，我们没有将数据写出到磁盘。因此，在这个阶段，我们的*“buff”*现在包含了纯 CSV 格式的转换数据。
*   第 23 行:我们获取内存中转换的 CSV 文件的内容( *buff* )，压缩它并写入内存字节缓冲区( *mem_file* )。此外，还要注意字符编码(即 UTF-8)。最后，我们在 *mem_file* bytes buffer 中将 CSV 数据转换为 gzip 压缩格式的字节。
*   第 24 行:我们将流的位置放在内存缓冲区的开始。把它想象成一个指向文件开始的光标。这样做是为了当我们上传到 S3 时，整个文件从头开始读取。
*   第 25 行:我们使用 *s3.put_object()* 方法将数据上传到指定的桶和前缀。在这种情况下，对于 Body 参数，我们指定保存压缩和转换后的 CSV 数据的 *mem_file* (内存字节缓冲区)

还有维奥拉。如果您已经处理了 AWS 方面的事情，例如，您有一个帐户、一个存储桶、一个有权限写入存储桶的 IAM 用户、一个配置的 AWS CLI 配置文件(或者一个角色，如果您已经在 AWS 中)，那么它应该读取文件、转换、压缩并将其上传到 S3 存储桶！

差不多就是这样了。本文用基本的转换逻辑演示了一个非常简单的场景。您也可以使用相同的逻辑，同时使用 Pandas 之类的库进行稍微高级一些的转换。编码快乐！