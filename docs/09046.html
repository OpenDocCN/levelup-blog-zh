<html>
<head>
<title>Generative Model of Inflected Words</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">屈折词的生成模式</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/generative-model-of-inflected-words-a4076dac53d6?source=collection_archive---------31-----------------------#2021-06-29">https://levelup.gitconnected.com/generative-model-of-inflected-words-a4076dac53d6?source=collection_archive---------31-----------------------#2021-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1985" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个项目中，我们的目标是建立词形变化的模型。我们研究了两种生成模型，即，<em class="kl">字符VAE，</em>仅具有连续变量的序列数据单序列变分自动编码器，<em class="kl"> MSVAE </em>(多空间变分自动编码器)，一种具有连续和离散潜变量的生成模型。相关的实验可以在这个报告中找到:<a class="ae km" href="https://github.com/akashrajkn/akruti" rel="noopener ugc nofollow" target="_blank">阿克鲁蒂</a>。</p><h1 id="bffc" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated"><strong class="ak">简介</strong></h1><p id="5904" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">语言使用词缀来传达信息，如重音，语调和语法意义。这些实体是更一般的一类实体的子集，这些实体形成单词的有意义的子部分(语素)。形态学是指语素组合的规则和过程。单词变形是一个过程，通过该过程，单词被修改以表达各种语法类别(例如，时态、重音、性别、语气等)。).对于NLP系统来说，学习和理解这样的过程是至关重要的；已经表明，明确地对形态学建模有助于机器翻译、解析和单词嵌入等任务(Dyer等人[1]；Cotterell等人[2])。涉及词频的计算方法显然不能模拟这种看不见的词。另一方面，基于启发式的方法需要手工制定语言特定的规则。</p><h2 id="098b" class="lq ko iq bd kp lr ls dn kt lt lu dp kx jy lv lw lb kc lx ly lf kg lz ma lj mb bi translated">我们能模拟屈折词的分布吗？</h2><p id="4155" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">我们希望研究生成模型来描述屈折词形的分布。vanilla variable auto encoder可用于编码字符序列。虽然它对于表示连续的潜在变量是有效的，但是我们不能对离散的形态特征进行编码。我们研究了一个生成模型，它可以在一个连续的潜在空间中对单词序列进行编码，在一个(近似)离散的潜在变量中对其形态特征进行编码。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/ea64c51f5d0f069bcf5be4f7a6a27664.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*TiE0G32_VxAQXHsAb5Ef5Q.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">字符变分自动编码器的生成过程。</figcaption></figure><h1 id="b98e" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">人物VAE</h1><p id="3b6a" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">处理神经模型中连续潜变量的有效方法是采用变分自动编码器。设<em class="kl"> x </em>表示一个字符序列(形成一个屈折词)<em class="kl"> z </em>表示潜在空间。给定潜在变量<em class="kl"> z </em>，VAE学习观察数据<em class="kl"> x </em>概率的生成模型，同时使用识别模型估计特定数据点的潜在变量。侧面显示了图形模型。</p><p id="9f19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似于Bowman等人(2015) [3]为记号序列开发的VAE，我们为字符序列构建了一个变分自动编码器。设<em class="kl"> q(。)</em>表示近似的后验。识别模型<em class="kl"> q(z|x) </em>参数化潜在空间上的近似后验概率。我们对潜在变量的先验使用标准高斯分布，即<em class="kl">p(z)∞N(z | 0，I) </em>。对于变分族，我们使用高斯分布。通过最大化数据的边际对数似然的变化下限来训练该模型:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/142661df69a72770bbc72ddd40c585d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EbgrO_03iYst3WMIK1QJWQ.png"/></div></div></figure><p id="52a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中θ和φ分别代表可训练网络和变分参数。接下来，我们讨论角色VAE模型的体系结构，</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mt"><img src="../Images/364586ec9e6966732f4f1ace60a876d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YDatwcJ2uNy7FlnEDBidDw.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">人物VAE建筑。<sos>是序列的开始字符。</sos></figcaption></figure><p id="980c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于编码器，我们使用双向门控循环单元。<em class="kl"> u = [h→;h←】</em>是输入序列<em class="kl"> x </em>的隐藏表示，其中<em class="kl"> h→ </em>和<em class="kl"> h← </em>分别代表编码器从正向和反向的最终隐藏状态。<em class="kl"> μ(u) </em>和<em class="kl"> σ(u) </em>是多层感知器，分别代表近似后验的均值和标准差，<em class="kl"> q(z|x) = N (z|μ(u)，σ(u)) </em>。在每个时间步，解码器使用(a)<em class="kl">x</em>的潜在表示，(b)在前一时间步预测的字符和(c)当前隐藏状态，来预测序列中下一个最可能的字符。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f7d541c024be41236df2a1380aa48bf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*cVCN7_7LgBToyruSgljh4Q.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">人物VAE潜在空间的t-SNE情节。每个数据点都是一个屈折词。具有相同引理的单词颜色相似。</figcaption></figure><h2 id="1658" class="lq ko iq bd kp lr ls dn kt lt lu dp kx jy lv lw lb kc lx ly lf kg lz ma lj mb bi translated">结果</h2><p id="82d0" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">图为人物<br/> VAE潜在空间的t-SNE图。图上的每个点代表一个屈折词，具有相同词条的词颜色相似。我们可以观察到共享相同引理的几个词彼此更接近。然而，我们看不到基于引理的明显聚类，因为潜在空间没有以任何方式受到限制。</p><h1 id="83b1" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">使用宽松变量的基本原理</h1><p id="0548" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">对于一个屈折词，神经网络应该学习是否存在形态特征。例如，单词“played”应该打开特征<em class="kl">时态=过去</em>。假设使用伯努利分布对每个形态特征进行建模。假设有<em class="kl">m</em>‘独立’形态特征，<em class="kl"/>(注:即使有些特征如<em class="kl">时态=现在</em>，<em class="kl">时态=过去</em>等。本质上是相关的，我们最初假设独立性，然后通过使用make:Masked auto encoder进行密度估计来诱导依赖性。为了进行推断，假设每个特征都有一个参数为α的伯努利先验。y上的分布为:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mv"><img src="../Images/f7d0cbec916b8aafe5a801a649327538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q9sE5FZTMTtaL6AWnNpDhA.png"/></div></div></figure><p id="5ec9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，<em class="kl"> x </em>是具有形态学标签的观察序列，y和λ表示变化参数。<em class="kl"> qλ </em>是参数<em class="kl"> bᵢ </em>的近似平均场伯努利后验。<em class="kl"> gθ(。)</em>是由<em class="kl"> θ </em>参数化的多层感知器，以<em class="kl"> x </em>为输入。使用平均场假设，学习目标<em class="kl"> L </em>可以写成如下:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mw"><img src="../Images/6f8e86e3487e1fc2590a19a1dbf91db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkZD6uDndsCnAqnPAEVDMg.png"/></div></div></figure><p id="b24d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上式中，涉及<em class="kl"> KL </em>散度的第二项可以解析计算，第一项相对于<em class="kl"> θ </em>的梯度可以用蒙特卡罗样本估计，但是相对于<em class="kl"> λ </em>的梯度是难以处理的，需要使用<strong class="jp ir">加强</strong>。在这种方法中，强化基线的选择会极大地影响收敛速度，因为它具有很高的方差。另一种加强方法是对分类变量进行<strong class="jp ir">放松，并采用直通估计器。不幸的是，这种方法有缺点:使用直通估计器会导致有偏差的梯度，因为它在梯度评估期间忽略了似然性中的Heaviside函数，并且具体分布不能模拟离散结果的似然性。我们用<strong class="jp ir">硬库马分布</strong>描述<a class="ae km" href="https://medium.com/nerd-for-tech/a-note-on-hard-kumaraswamy-distribution-b74278dc6877" rel="noopener">这个故事</a>【4】。来模拟形态特征的分布。这种分布满足了一个需要:一个可微分的替代离散变量，使无偏梯度估计。</strong></p><h1 id="50c9" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">多空间变分自动编码器</h1><p id="b16d" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">多空间变分自动编码器(MSVAE)是一种使用离散和连续隐变量的生成模型。MSVAE可以看作是带有辅助变量和序列VAE的生成模型的组合。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/04e50071bdf71f5af9d883c42a02ec7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*V1_6fuB6K7tCM7OgY2-klQ.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">(监督)MSVAE模型的图形模型。灰色变量表示观察到了各自的标签。</figcaption></figure><p id="9cec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">符号</em>:用<em class="kl"> x </em>表示一个字符序列(例如<em class="kl">玩</em>)。<em class="kl"> y </em>是一个<em class="kl"> M </em>维的二元向量；<em class="kl"> yᵢ </em>表示第<em class="kl"> i </em>个特征是否存在，即<em class="kl"> yᵢ ∈ {0，1} </em>。<em class="kl"> φ </em>代表连续潜在空间<em class="kl"> z </em>的变分参数，其上有一个标准的法线先验，即<em class="kl"> p(z) = N (z|0，I) </em>。<em class="kl"> θ </em>代表神经网络的可训练参数。<em class="kl"> λ </em>表示形态学特征的近似后验概率的变分参数。<em class="kl"> N </em>表示单词序列的个数。图中显示了受监督的(<strong class="jp ir"> y </strong>可用)MSVAE的图形模型。我们可以定义生成模型，<em class="kl"> pθ(x，</em> <strong class="jp ir"> <em class="kl"> y </em> </strong> <em class="kl">，</em><strong class="jp ir"><em class="kl">z</em></strong><em class="kl">)</em>如下:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi my"><img src="../Images/1d94d4fa231a3bf766301727b294e93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sp9yALCWjgg1k2cLKs3zfQ.png"/></div></div></figure><p id="3a89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="kl"> HK(a₀，b₀) </em>是硬库玛先验，<em class="kl"> fθ(。)</em>表示由<em class="kl"> θ </em>参数化的递归神经网络。我们可以通过最大化以下目标来培训MSVAE:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi my"><img src="../Images/e135313ea201043c4502f714869728ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*080Lxn1XQ4OqRaALzkqewg.png"/></div></div></figure><h2 id="c00a" class="lq ko iq bd kp lr ls dn kt lt lu dp kx jy lv lw lb kc lx ly lf kg lz ma lj mb bi translated"><strong class="ak">结果</strong></h2><p id="2dea" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">我们使用一个类似的架构，如在字符VAE部分所述。我们使用SIGMORPHON 2016数据集[5] —土耳其任务3数据。在解码时，为了预测每一步最可能的字符，我们使用三种类型的信息:(a)当前解码器状态，(b)形态学标签嵌入，以及(c)潜在变量。我们没有忽略潜在变量，而是使用平均向量作为潜在代表。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mz"><img src="../Images/337c658dc05dd497294f0a5dc6c684b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GqjvIFer0dU5vg4kxhcYkA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">潜在空间的t-SNE图。每个数据点都是土耳其语task 3数据集中的一个屈折词。(左)具有相同引理的单词颜色相同。(右)具有相同词性(pos)标签的单词颜色相同。</figcaption></figure><p id="2f04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在左图中，具有相同词条的单词被相似地着色。我们可以看到，具有相同引理的单词被粗略地聚集在一起。然而，集群并不明显。在右边，我们将相同的(连续的)潜在空间可视化，但根据词性标签进行着色:<em class="kl"> pos=V，pos=N </em>分别代表动词和名词。潜在空间没有根据词性标签进行聚类；这是意料之中的，因为我们希望连续空间只编码关于引理的信息。</p><h2 id="42a0" class="lq ko iq bd kp lr ls dn kt lt lu dp kx jy lv lw lb kc lx ly lf kg lz ma lj mb bi translated"><strong class="ak">形态学标签上的分布</strong></h2><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/4bf9867d150932ad6cf05adcba16bf51.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*z4SsKhOMJfRmbRgMMgrzQw.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">土耳其语(任务3)测试数据的平均精度和召回率。</figcaption></figure><p id="51a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在推理时，形态学特征不能被给予完全离散的处理。为了对此建模，我们可以使用硬分布(硬库马分布)和具体的随机变量，离散随机变量的连续松弛(具体分布)作为可能的候选。使用变换值计算每个预测的精度和召回率。我们使用MADE来归纳形态特征分布之间的相关性。</p><blockquote class="nb nc nd"><p id="497f" class="jn jo kl jp b jq jr js jt ju jv jw jx ne jz ka kb nf kd ke kf ng kh ki kj kk ij bi translated">硬库马比混凝土分配表现更好:</p><p id="976b" class="jn jo kl jp b jq jr js jt ju jv jw jx ne jz ka kb nf kd ke kf ng kh ki kj kk ij bi translated">1.硬库马可以模拟实际数据，即离散结果{0}和{1}，但具体的<br/> 2却不是这样。由于无偏的梯度估计，训练可能更稳定。</p></blockquote><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ef280c8c2d7505acd646d7fdf8b7ccb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Obit-s5wo09_iaDohxGW9w.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">来自土耳其数据集的示例单词(<strong class="bd kp"> koylar </strong>)显示了形态特征预测。x轴显示了不同的形态特征。第一行(实际)显示目标特征。</figcaption></figure><h1 id="1d70" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h1><p id="5057" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">我们研究了两个生成模型来描述屈折词形的分布。字符VAE作为一个有效地表示连续潜在变量的积木。然而，它的潜在空间没有捕捉到任何有意义的表现，因为我们没有以任何方式对它进行调节。另一方面，MSVAE从两个<br/>潜在表示中对屈折词形进行建模:捕获字符序列的连续空间和表示形态特征的近似离散潜在空间。遵循周和Neubig (2017) [6]，我们给它一个宽松的待遇。然而，我们使用硬库马，而不是通过有偏直通估计量使用具体分布。MSVAE的潜在空间可以根据屈折词的引理进行粗略聚类，但我们无法从引理中完全理清词法标签信息。</p><h1 id="0376" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">参考</h1><ol class=""><li id="19e9" class="ni nj iq jp b jq ll ju lm jy nk kc nl kg nm kk nn no np nq bi translated">Dyer，c .，Muresan，s .，和Resnik，P. (2008年)。广义词格翻译。马里兰大学帕克学院高级计算机研究所技术报告。</li><li id="1f38" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">Cotterell、h . schütze和j . e isner(2016年)。词嵌入的形态平滑和外推。《计算语言学协会第54届年会论文集》(第1卷:长篇论文)，第1651-1660页。</li><li id="2bd5" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">Bowman，S. R .、Vilnis，l .、Vinyals，o .、Dai，A. M .、Jozefowicz，r .和Bengio，S. (2015年)。从连续空间生成句子。arXiv预印本arXiv:1511.06349。</li><li id="e486" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated"><a class="ae km" href="https://medium.com/nerd-for-tech/a-note-on-hard-kumaraswamy-distribution-b74278dc6877" rel="noopener">https://medium . com/nerd-for-tech/a-note-on-hard-kumaraswamy-distribution-b 74278 DC 6877</a></li><li id="f3ec" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">r .科特雷尔、c .基洛夫、Sylak-Glassman、j .亚罗斯基、d .艾斯纳和M. (2016年)。sigmorphon 2016共享任务—形态学再反射。《第14届SIGMORPHON语音学、音韵学和形态学计算研究研讨会论文集》，第10-22页。</li><li id="ac24" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">周和纽比格(2017)。半监督标记序列转导的多空间变分编码解码器。arXiv预印本arXiv:1704.01691。</li><li id="61a9" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">代号:<a class="ae km" href="https://github.com/akashrajkn/akruti" rel="noopener ugc nofollow" target="_blank">https://github.com/akashrajkn/akruti</a></li></ol></div></div>    
</body>
</html>