<html>
<head>
<title>Predicting Titanic Survivors using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习预测泰坦尼克号幸存者</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/predicting-titanic-survivors-using-ml-8570ef4e89e8?source=collection_archive---------3-----------------------#2021-03-26">https://levelup.gitconnected.com/predicting-titanic-survivors-using-ml-8570ef4e89e8?source=collection_archive---------3-----------------------#2021-03-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c381" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用机器学习预测泰坦尼克号幸存者的数据科学方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7fae1d9d68a3bff125b658e083240b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVAqEYYZ2ic4K5vAueTzEg.png"/></div></div></figure><p id="5d5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我通过退休一直在哥伦比亚工程学习<a class="ae lq" href="https://online-exec.cvn.columbia.edu/applied-machine-learning" rel="noopener ugc nofollow" target="_blank">应用机器学习</a>。这是一个为期5个月的课程，我真的很喜欢，并会推荐。</p><p id="e9df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">教练都很好，很有帮助，尤其是罗伯特·曼里克兹和普内特·萨拉斯瓦特。他们举办“办公时间”网络研讨会，帮助完成复杂的作业，同时也提供我们所学内容的实际演练。</p><p id="8d73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有一个很棒的数据科学和机器学习资源叫做<a class="ae lq" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>。他们为数据科学家提供免费的数据集进行实践。还有比较机器学习的分析和建模的比赛。</p><p id="3365" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在一些“办公时间”的网络研讨会中，Robert向我们展示了“<a class="ae lq" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">泰坦尼克号——灾难中的机器学习</a>”数据集。目标是预测谁在泰坦尼克号上幸存。</p><p id="3128" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将提供数据科学和机器学习的实用介绍，而不是深入幕后的数学(数学很复杂！).虽然这些名字听起来很花哨，但实际上它们分别是统计学家和统计学的现代名称。我们不要在这里自欺欺人，这主要是复杂的数学。</p><h2 id="33e5" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">步骤1:确定项目范围</h2><p id="453b" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">这个项目的目标是确定谁是泰坦尼克号的幸存者。那将是我们的"<strong class="kw iu">决定因素</strong>"变量(我们正试图预测的)。我们将使用一个或多个"<strong class="kw iu">不确定的</strong>变量或"<strong class="kw iu">特性</strong>is输入。</p><p id="c6a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">理解我们的数据很重要。除了实际数据，Kaggle还提供了数据集的描述。这对于分析和特征工程来说肯定会派上用场。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/ae6ed91581ec8221e0cb330c4ef5fcbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ok5hctzzTCydUNKU63iLXQ.png"/></div></div></figure><p id="1355" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如"<strong class="kw iu"> sipsp </strong>"和"<strong class="kw iu"> parch </strong>"如果没有描述可能不明显，但可以肯定的是，将这两个相加将为您提供一个家庭的规模。这可能是一个有用的功能，可以设计和包含。</p><h2 id="673a" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">第二步:收集数据</h2><p id="faa5" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">在这种特殊情况下，这是非常容易的。Kaggle上已经提供给我们了，所以你只需要下载文件。我们将大部分使用“<strong class="kw iu"> train.csv </strong>”进行工作。</p><h2 id="ce13" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">步骤3:清理数据</h2><p id="a363" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">第一步是加载我们的数据。我正在使用Python 3(带有<strong class="kw iu"> pandas </strong>、<strong class="kw iu"> numpy </strong>、<strong class="kw iu"> seaborn </strong>、<strong class="kw iu"> matplotlib </strong>和<strong class="kw iu"> sklearn </strong>库)和Jupyter notebooks "<strong class="kw iu">jupyterlab</strong>"如果你想继续的话。</p><p id="0b61" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">加载必要的库。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="711e" class="lr ls it mr b gy mv mw l mx my">import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span></pre><p id="bc29" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Jupyter笔记本电脑的定制。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="a1f6" class="lr ls it mr b gy mv mw l mx my">%matplotlib inline</span><span id="97a1" class="lr ls it mr b gy mz mw l mx my">plt.rc('xtick', labelsize=15)     <br/>plt.rc('ytick', labelsize=15)<br/>plt.rc('axes', titlesize=16)</span><span id="1543" class="lr ls it mr b gy mz mw l mx my">sns.set_style('darkgrid')</span></pre><p id="8a7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从Kaggle加载我们的"<strong class="kw iu"> train.csv </strong>"和"<strong class="kw iu"> test.csv </strong>"数据集。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="c866" class="lr ls it mr b gy mv mw l mx my">train = pd.read_csv('./train.csv')<br/>test = pd.read_csv('./test.csv')</span></pre><p id="26d6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看数据集的形状。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8a8f" class="lr ls it mr b gy mv mw l mx my">train.shape, test.shape<br/>((891, 12), (418, 11))</span></pre><p id="1069" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“<strong class="kw iu">训练</strong>数据集具有891行和12列/特征。这包括我们将要预测的“<strong class="kw iu">确定性</strong>”变量“<strong class="kw iu">幸存</strong>”。“<strong class="kw iu">测试</strong>数据集具有418行和11列/特征。这是我们的数据集，没有我们的“<strong class="kw iu">幸存</strong>特征。看起来<strong class="kw iu">训练/测试</strong>的比例大致是<strong class="kw iu"> 70/30% </strong>。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="b422" class="lr ls it mr b gy mv mw l mx my">train.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/5b8ba2dcb519fa143103ad8bf3a68c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ff8i0XBvx6penNccQiC-sA.png"/></div></div></figure><p id="4cb5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们首先要检查的事情之一是是否有空值。这会给我们带来问题，所以我们需要想出在每种情况下该怎么做。处理这种情况的三种常用方法是:为缺失值分配平均值，完全删除行，或者使用机器学习来预测缺失值:)</p><p id="7ec4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到“<strong class="kw iu">年龄</strong>”特征对我们来说是个问题。它应该有891个完整的条目，但只有714个。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="9645" class="lr ls it mr b gy mv mw l mx my">train.isnull().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2b518cb6b5100dc359f0f9c0acfdba8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*Cf448xLaCBMRhQgODRezPw.png"/></div></figure><p id="0f71" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到，我们有177个数字空值需要进行排序。有一个名为“<strong class="kw iu"> Cabin </strong>”的“<strong class="kw iu">object</strong>”(string/text)特征有大量的缺失值。这个栏目对我们没有用，应该去掉。"<strong class="kw iu">已装载</strong>"类似，但只有两个值缺失。仅仅缺少两个条目就完全删除这个特性是没有意义的。删除这两行是有意义的。所以让我们现在就开始吧。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="d210" class="lr ls it mr b gy mv mw l mx my">train.drop(columns=['Cabin'], inplace=True)<br/>train.dropna(subset=['Embarked'], inplace=True)</span><span id="bb45" class="lr ls it mr b gy mz mw l mx my">test.drop(columns=['Cabin'], inplace=True)<br/>test.dropna(subset=['Embarked','Fare'], inplace=True)</span></pre><p id="5d6d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您还会注意到我对“<strong class="kw iu">训练</strong>”数据集所做的更改，以及我对“<strong class="kw iu">测试</strong>”数据集所做的更改。如果我们不这样做，未处理的“<strong class="kw iu">测试</strong>”数据集将在我们的模型上表现糟糕，即使对“<strong class="kw iu">训练</strong>”数据集的测试看起来不错。</p><p id="64c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在看起来怎么样？</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="229c" class="lr ls it mr b gy mv mw l mx my">train.isnull().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c95bffd93e7fcf630fb5ba1f5c65680c.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*fR-3vGhcx_xSZGphYfdZpQ.png"/></div></figure><p id="2730" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们仍然拥有“<strong class="kw iu">年龄</strong>特征中所有那些缺失的条目。我们不想删除该列或所有这些行。指定平均值(“<strong class="kw iu">平均值</strong>”)也没有用。我们将使用机器学习来计算出那些缺失值的合适年龄。我们将使用的机器学习模型是K近邻(KNN)，但这要求数据集中的所有值都是数字。我们将继续清理数据，并在最后回到“<strong class="kw iu">时代</strong>”功能，因为我们还有其他功能要先清理。</p><p id="5999" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看我们的数据…</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="9954" class="lr ls it mr b gy mv mw l mx my">train.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/3b280de66a4cafe9ef2a0731b4fd982e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N34b5qlnwMD7w62cs1wh2A.png"/></div></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="6a6c" class="lr ls it mr b gy mv mw l mx my">train.dtypes</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/dc73a36009abc93e3144a022cab70478.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*9PWHFmMpdI7qs2ih2070pw.png"/></div></figure><p id="70f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">需要指出的是，在机器学习中，我们只能处理数字数据。如果它们是分类数据，上面的“<strong class="kw iu"> object </strong>”类型特征可能仍然对我们有用，但是我们需要对它们做些什么才能使用它们。</p><p id="ae5a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是时候尝试一些<strong class="kw iu">特色工程</strong>。</p><p id="2ee8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“<strong class="kw iu">oaked</strong>”功能可能对我们有用，但我们应该详细说明这些字母的含义。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="9ed2" class="lr ls it mr b gy mv mw l mx my">train['Embarked'].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/ab149be68fc9b31cdc81f2896489ac8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*Nyj16tT0EXZuHLXKAQ9m8A.png"/></div></figure><p id="3583" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们从Kaggle的描述中可以看出“<strong class="kw iu"> S </strong>是“<strong class="kw iu">南安普顿</strong>”、“<strong class="kw iu"> C </strong>是“<strong class="kw iu">瑟堡</strong>”、“<strong class="kw iu"> Q </strong>是“<strong class="kw iu">皇后镇</strong>”所以我们会重新映射那些。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="7d55" class="lr ls it mr b gy mv mw l mx my">def convert_embarked(x):<br/>    if x == "C":<br/>        return "Cherbourg"<br/>    elif x == 'Q':<br/>        return "Queenstown"<br/>    elif x == 'S':<br/>        return "Southampton"</span><span id="e602" class="lr ls it mr b gy mz mw l mx my">train.Embarked = train.Embarked.map(convert_embarked)<br/>test.Embarked = train.Embarked.map(convert_embarked)</span></pre><p id="b52e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了确认它有效…</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="383b" class="lr ls it mr b gy mv mw l mx my">train['Embarked'].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7d9b1f66cee5630c9163441b7af2ded1.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*YRE8Tm9PSbTilZTfwsHzFg.png"/></div></figure><p id="483c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们知道"<strong class="kw iu"> SibSp </strong>"和"<strong class="kw iu"> Parch </strong>"合在一起会组成一个"<strong class="kw iu"> FamilySize </strong>"所以让我们为它创建一个新特性。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="b71c" class="lr ls it mr b gy mv mw l mx my">train['FamilySize'] = train['SibSp'] + train['Parch']<br/>test['FamilySize'] = test['SibSp'] + test['Parch']</span><span id="41f6" class="lr ls it mr b gy mz mw l mx my">train.groupby('FamilyCount').agg({"FamilyCount" : 'count', "Survived" : 'mean'}).sort_index()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4d489a105f9bbe4e01ea4325acfcfb15.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*gT_aUW-FbLexw49p2wmhLw.png"/></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="332d" class="lr ls it mr b gy mv mw l mx my">train.groupby('FamilySize').agg({"Survived" : 'mean'}).plot(kind='bar')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/44ea1dfb21037275ed8903e0356954bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K4KcxvDz2zwZmjoeeqE2Bg.png"/></div></div></figure><p id="4cb2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有趣的是，如果你是一个三口之家，你很有可能在泰坦尼克号上幸存。</p><p id="3e21" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将删除“<strong class="kw iu"> PassengerId </strong>”和“<strong class="kw iu"> Ticket </strong>”功能，因为我看不到任何保留它的明显理由。它既不是数字的，也不是分类的，似乎没有增加任何实际价值。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="7f4b" class="lr ls it mr b gy mv mw l mx my">train.drop(columns=['PassengerId','Ticket'], inplace=True)<br/>test.drop(columns=['PassengerId','Ticket'], inplace=True)</span></pre><p id="a856" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">乘客姓名对我们来说是个问题。这不是数字或分类，但我们能从中看出什么有用的东西吗？过去，每个人似乎都有一个头衔，所以如果我们能从名字中提取出来，我们就可以把它作为一个分类特征。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="3665" class="lr ls it mr b gy mv mw l mx my">train['Title'] = train['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())<br/>test['Title'] = test['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())</span></pre><p id="13f7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们来看看…</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="ff69" class="lr ls it mr b gy mv mw l mx my">train['Title'].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2eafb6b976af3ba1d35591ea74d9703f.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*ptFDX_1L0V78VT6Ia8IQwQ.png"/></div></figure><p id="2b05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那看起来真的很有趣。我现在要删除“<strong class="kw iu"> Name </strong>”功能。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="99ce" class="lr ls it mr b gy mv mw l mx my">train.drop(columns=['Name'], inplace=True)<br/>test.drop(columns=['Name'], inplace=True)</span></pre><p id="bce3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在看起来怎么样？</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="6b9f" class="lr ls it mr b gy mv mw l mx my">train.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/999547aab97c38f45f8526fb4a0b3b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVuIbH8w9MRUWRRTBMjRMQ.png"/></div></div></figure><p id="590a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">"<strong class="kw iu"> Pclass </strong>"是一个有趣的特性，因为它是数字，但实际上是分类的。我们应该相应地调整数据类型。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="e1b3" class="lr ls it mr b gy mv mw l mx my">train['Pclass'] = train['Pclass'].astype(str)<br/>test['Pclass'] = test['Pclass'].astype(str)</span></pre><p id="1818" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们看起来相当不错，但正如我提到的，我们不能使用"<strong class="kw iu">性别</strong>"、"<strong class="kw iu">着手</strong>、"<strong class="kw iu">职业</strong>"或"<strong class="kw iu">头衔</strong>"分类特征的当前形式。我们需要执行一个名为“<strong class="kw iu"> One-Hot Encoding </strong>”的函数，将这些分类特征转换成数字二进制形式。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="90f6" class="lr ls it mr b gy mv mw l mx my">train = pd.get_dummies(train, columns=['Sex','Embarked','Pclass','Title'], drop_first=True)<br/>test = pd.get_dummies(test, columns=['Sex','Embarked','Pclass','Title'], drop_first=True)</span></pre><p id="0dbe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所有的分类特征现在应该被扩展成二元特征，并且原始特征应该被丢弃。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="e80e" class="lr ls it mr b gy mv mw l mx my">train.dtypes</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f0b0ae4b8aa1641126db80037472e6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*AnNa1OA7JNAuJ5qwmultrg.png"/></div></figure><p id="258c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">既然我们所有的特征都是数字，我们可以看看如何使用K-最近邻来预测缺失的“<strong class="kw iu">年龄</strong>”值。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="bc7b" class="lr ls it mr b gy mv mw l mx my">from sklearn.neighbors import KNeighborsRegressor</span><span id="aeaf" class="lr ls it mr b gy mz mw l mx my"># train</span><span id="1b4c" class="lr ls it mr b gy mz mw l mx my">impute_train = train[train.Age.isnull()].drop(['Age'], axis=1)<br/>impute_train.describe()</span><span id="8f91" class="lr ls it mr b gy mz mw l mx my"># test</span><span id="40ec" class="lr ls it mr b gy mz mw l mx my">impute_test = test[test.Age.isnull()].drop(['Age'], axis=1)<br/>impute_test.describe()</span></pre><p id="81c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是我们缺少“<strong class="kw iu">年龄</strong>”的177行…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/741cc2db2f5fc93d876e9e824e8cb933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RR_AONvTylnYXwKv4KnWw.png"/></div></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="a746" class="lr ls it mr b gy mv mw l mx my"># train</span><span id="0be2" class="lr ls it mr b gy mz mw l mx my">knr = KNeighborsRegressor()</span><span id="fa4d" class="lr ls it mr b gy mz mw l mx my">titanic_knn_train = train[train.Age.notnull()]<br/>X_train = titanic_knn_train.drop(['Age'], axis = 1)<br/>y_train = titanic_knn_train.Age</span><span id="a847" class="lr ls it mr b gy mz mw l mx my">knr.fit(X_train, y_train)</span><span id="44ee" class="lr ls it mr b gy mz mw l mx my">imputed_train_ages = knr.predict(impute_train)<br/>imputed_train_ages</span><span id="5984" class="lr ls it mr b gy mz mw l mx my"># test</span><span id="d660" class="lr ls it mr b gy mz mw l mx my">knr = KNeighborsRegressor()</span><span id="ff63" class="lr ls it mr b gy mz mw l mx my">titanic_knn_test = test[test.Age.notnull()]<br/>X_test = titanic_knn_test.drop(['Age'], axis = 1)<br/>y_test = titanic_knn_test.Age</span><span id="633e" class="lr ls it mr b gy mz mw l mx my">X_test.isnull().sum()</span><span id="211b" class="lr ls it mr b gy mz mw l mx my">knr.fit(X_test, y_test)</span><span id="0cf5" class="lr ls it mr b gy mz mw l mx my">imputed_test_ages = knr.predict(impute_test)<br/>imputed_test_ages</span></pre><p id="5c3d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">"<strong class="kw iu">titanic _ KNN _ train</strong>"将是我们的具有"<strong class="kw iu">年龄</strong>"的712行，而"<strong class="kw iu"> imputed_train_ages </strong>"将是具有预测"<strong class="kw iu">年龄</strong>的177行。</p><p id="197a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是缺失的“<strong class="kw iu">年龄</strong>”值的预测年龄。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="25b1" class="lr ls it mr b gy mv mw l mx my">array([47.8, 31.2, 18.4, 32.5, 17.6, 28.6, 21.584, 17.6,<br/>       24.6, 28.7, 28.1, 31.4, 17.6, 26. , 49.4, 43.1,<br/>        8.334, 28.6, 28.1, 17.6, 28.1, 28.1, 28.6, 28.6,<br/>       22. , 28.1, 47.8, 13.4, 27. , 28.1, 22.6, 39. ,<br/>       29.2, 58.8, 12.8, 39. , 40. , 45.4, 28.8, 47.8,<br/>       17.6, 39. , 47.8, 28.6, 13.8, 26.6, 16.9, 18.9,<br/>       34.1, 33.8, 47.8, 21.5, 45.6, 17.6, 36.6, 58.8,<br/>       43.1, 43.4, 17.6, 29.6, 29.9, 28.1, 25.6, 39. ,<br/>       24.4, 40.4, 28.6, 29.6, 50.4, 32.5, 17.6, 17.6,<br/>       31.4, 18.4, 17.6, 36.4, 28.6, 37.8, 13.8, 28.6,<br/>       33.5, 36.6, 25.4, 28.7, 34.1, 47.8, 29.6, 26. ,<br/>       26. , 28.1, 40.6, 47.8, 28.1, 36.6, 35.9, 34.1,<br/>       41. , 36.6, 13.8, 26. , 22.4, 30.3, 20.3, 41.2,<br/>       28.1, 41.6, 32.5, 26.8, 30.8, 26.8, 28. , 33.5,<br/>       39. , 47.8, 30.8, 47.8, 28.1, 24.2, 26.8, 17.6,<br/>       29.5, 32.8, 28.1, 18.484, 37.2, 32.5, 28.6, 34.4,<br/>       25.8, 18.9, 47.8, 34.8, 36.6, 30. , 29.8, 29.8,<br/>       28.6, 17.6, 28.6, 31. , 40.6, 36.6, 21.5, 29.8,<br/>       17.6,  8.334, 61.8, 25.5, 17.6, 36.6, 28.6, 28.6,<br/>       43.4, 33.5, 41.4, 31.4, 32.5, 47.8, 34.8, 20.8,<br/>       47.8, 39. , 47. , 36.6, 33.5, 33.6, 40.6, 26.8,<br/>       28.1, 46.2, 39. , 34.8, 26.8, 39. , 26.6, 28.6,<br/>       16.8  ])</span></pre><p id="3730" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的“<strong class="kw iu"> imputed_train_ages </strong>”数据帧现在是这样的。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="cc36" class="lr ls it mr b gy mv mw l mx my"># test</span><span id="6a92" class="lr ls it mr b gy mz mw l mx my">impute_test['Age'] = imputed_test_ages</span><span id="baa5" class="lr ls it mr b gy mz mw l mx my"># train</span><span id="7823" class="lr ls it mr b gy mz mw l mx my">impute_train['Age'] = imputed_train_ages</span><span id="429e" class="lr ls it mr b gy mz mw l mx my">impute_train.head(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/9aeb1a7ffe03a4a81d5e2110753a53f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*407PdhXDHhLmiRN2wC3B2A.png"/></div></div></figure><p id="443d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后一步是连接两个数据帧。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="5703" class="lr ls it mr b gy mv mw l mx my"># train</span><span id="c1f1" class="lr ls it mr b gy mz mw l mx my">titanic_train_imputed = pd.concat([titanic_knn_train, impute_train], sort=False, axis=0)<br/>train = titanic_train_imputed</span><span id="b457" class="lr ls it mr b gy mz mw l mx my"># test</span><span id="f096" class="lr ls it mr b gy mz mw l mx my">titanic_test_imputed = pd.concat([titanic_knn_test, impute_test], sort=False, axis=0)<br/>test = titanic_test_imputed</span></pre><p id="1c96" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在看起来怎么样？</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="31b7" class="lr ls it mr b gy mv mw l mx my">train.isnull().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/bf3ee88192e15153b24a69f881457c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*uffRzTR_azzNOBywly_iGQ.png"/></div></figure><p id="1568" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">没有缺失值。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="4e78" class="lr ls it mr b gy mv mw l mx my">train['Age'].describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8785d33b681f785ac0e078376a8575fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*NFlQi_LAQ4UOd6oEMJT81w.png"/></div></figure><p id="a003" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">看起来没错。我们开始时有891行，现在有889行。这是891行减去我们删除的缺少“<strong class="kw iu">abowed</strong>”的两行。</p><p id="f8f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我现在唯一不太喜欢的是生成的“<strong class="kw iu">年龄</strong>”现在是浮点数(不是整数)。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="ccf0" class="lr ls it mr b gy mv mw l mx my"># train</span><span id="3c22" class="lr ls it mr b gy mz mw l mx my">train['Age'] = np.round(train['Age']).astype(int)</span><span id="00f3" class="lr ls it mr b gy mz mw l mx my"># test</span><span id="5c96" class="lr ls it mr b gy mz mw l mx my">test['Age'] = np.round(test['Age']).astype(int)</span></pre><p id="a393" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们来看看“<strong class="kw iu">年龄</strong>特征…</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="6783" class="lr ls it mr b gy mv mw l mx my">train['Age'].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9cf241bcb4b4d91572c677f0f11ca22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*ED3K4R_k413FhLz82YcLcw.png"/></div></figure><p id="a705" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此时保存我们处理过的数据通常是一个好的做法。这将使我们以后不必重复所有这些步骤。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="b090" class="lr ls it mr b gy mv mw l mx my"># train</span><span id="a307" class="lr ls it mr b gy mz mw l mx my">train.to_csv('./train_processed.csv')</span><span id="4067" class="lr ls it mr b gy mz mw l mx my"># test</span><span id="5ea5" class="lr ls it mr b gy mz mw l mx my">test.to_csv('./test_processed.csv')</span></pre><h2 id="b99f" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">步骤4:探索性数据分析(EDA)</h2><p id="ab9c" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">现在我们的数据已经清理干净，我们可以尝试闪烁一些见解。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="0bcd" class="lr ls it mr b gy mv mw l mx my">plt.figure(figsize = (12,10))<br/>train.corr()['Survived'].sort_values(ascending = True).plot(kind = 'barh')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/1f8f1ff6957dc30848a71da3727e8a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OyaHyRxM97peH4kineka3w.png"/></div></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="efc5" class="lr ls it mr b gy mv mw l mx my">df.corr()['Survived'].sort_values(ascending = False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/4e1154d394f01e3d4ef0c128cd39e156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*wIsb0xJx3bM0vHNsNNSz4A.png"/></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="fb00" class="lr ls it mr b gy mv mw l mx my">top_6 = train.corr()['Survived'].sort_values(ascending = False).keys()[0:7]</span><span id="3985" class="lr ls it mr b gy mz mw l mx my">print (top_6)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/8573661bd352d8f8370b7abb0bf087a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4FttU6ln7QjUYCxzZ8bSDg.png"/></div></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="4481" class="lr ls it mr b gy mv mw l mx my">top_6_train = train.loc[:,top_6].copy()<br/>top_6_train.head()<br/>sns.pairplot(top_6_train)<br/>top_6_train['Survived'].hist(bins=40)<br/>top_6_train.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/c3bc6ffbb0a89d570cf5690045ea288a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jWzLY7I6ONizT_oTKQ3DLA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/1d028e01d2f04fa60ccbb9b47cd5a0e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MuI-3tLCsrJjX2Kusep7Aw.png"/></div></div></figure><h2 id="5b74" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">步骤5:建立数据模型</h2><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="ea9a" class="lr ls it mr b gy mv mw l mx my">from sklearn.linear_model import LogisticRegression<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import preprocessing</span></pre><p id="ea79" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在要创建"<strong class="kw iu"> X </strong>"和"<strong class="kw iu"> y </strong>"数据集。"<strong class="kw iu"> y </strong>"将是我们的"<strong class="kw iu">确定的</strong>"变量为"<strong class="kw iu">幸存下来的</strong>"(我们正在努力预测的)和"<strong class="kw iu"> X </strong>"将是我们所有的特征输入。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="9c81" class="lr ls it mr b gy mv mw l mx my">train.columns</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/f6f8d50aae342519c1b127a8ba03dc98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ImT1ZMcVuOAuAXjuaF2Rug.png"/></div></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="c490" class="lr ls it mr b gy mv mw l mx my">y = train['Survived']</span><span id="8665" class="lr ls it mr b gy mz mw l mx my">X = train.loc[:, ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'Sex_male', 'Embarked_Queenstown', 'Embarked_Southampton', 'Pclass_2', 'Pclass_3', 'Title_Col', 'Title_Don', 'Title_Dr', 'Title_Jonkheer', 'Title_Lady', 'Title_Major', 'Title_Master', 'Title_Miss', 'Title_Mlle', 'Title_Mme', 'Title_Mr', 'Title_Mrs', 'Title_Ms', 'Title_Rev', 'Title_Sir', 'Title_the Countess']]</span></pre><p id="b84e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在想要将我们的数据分成“<strong class="kw iu">训练</strong>”和“<strong class="kw iu">测试</strong>”数据集。这将以70/30的比例完成。“<strong class="kw iu"> random_state </strong>”只是让我们能够稍后准确地再现这个输出。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="2767" class="lr ls it mr b gy mv mw l mx my">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)</span></pre><p id="a6c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">原始形状。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8a7d" class="lr ls it mr b gy mv mw l mx my">X.shape, y.shape<br/>((889, 28), (889,))</span></pre><p id="5740" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">火车形状。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="e435" class="lr ls it mr b gy mv mw l mx my">X_train.shape, y_train.shape<br/>((622, 28), (622,))</span></pre><p id="db24" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">测试形状。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="37e0" class="lr ls it mr b gy mv mw l mx my">X_test.shape, y_test.shape<br/>((267, 26), (267,))</span></pre><p id="f916" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们先试试<strong class="kw iu">逻辑回归</strong>模型。</p><p id="24ce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它是一种基于<strong class="kw iu">距离的</strong>型号。我们的“<strong class="kw iu">年龄</strong>”和“<strong class="kw iu">费用</strong>”特征会给我们带来问题，因为数值比其他特征大得多。</p><p id="67c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们希望调整所有的功能，使它们使用相同的比例。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="6b19" class="lr ls it mr b gy mv mw l mx my">scaler = preprocessing.StandardScaler().fit(X_train)<br/>X_scaled = scaler.transform(X_train)<br/>X_train = pd.DataFrame(X_scaled, columns=[X_train.columns])</span><span id="c02d" class="lr ls it mr b gy mz mw l mx my">X_train</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/416366dcdfbc7197f21ee4e6f2fc0035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rV-RNvtyfuzlPcd5uKLjUA.png"/></div></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="1a3d" class="lr ls it mr b gy mv mw l mx my">scaler = preprocessing.StandardScaler().fit(X_test)<br/>X_scaled = scaler.transform(X_test)<br/>X_test = pd.DataFrame(X_scaled, columns=[X_test.columns])</span></pre><p id="99d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">既然我们的“<strong class="kw iu"> X_train </strong>”和“<strong class="kw iu"> X_test </strong>”特性已经被适当地缩放，我们可以继续了。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="ef45" class="lr ls it mr b gy mv mw l mx my">lr = LogisticRegression(max_iter=500)<br/>lr.fit(X_train, y_train)</span></pre><p id="c983" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并检查准确度分数。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="4405" class="lr ls it mr b gy mv mw l mx my">lr.score(X_train, y_train), lr.score(X_test, y_test)<br/>(0.8263665594855305, 0.8389513108614233)</span></pre><p id="d981" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">训练时准确率为82%，测试时准确率为83%。</p><p id="f564" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在来做一些预测…</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="f33c" class="lr ls it mr b gy mv mw l mx my">y_hat = lr.predict(X_test)</span><span id="e524" class="lr ls it mr b gy mz mw l mx my">X_test_display = X_test.copy()<br/>X_test_display['Predicted_Survived'] = y_hat<br/>X_test_display['Survived_True'] = y_test.to_numpy()</span><span id="dbb5" class="lr ls it mr b gy mz mw l mx my">X_test_display.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/4ddc667267d2edc3b68547835c95923c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YNHpd9gqLdRl3WFng-JEJg.png"/></div></div></figure><p id="8f31" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你会看到在最后增加了“<strong class="kw iu">预测_幸存</strong>”和“<strong class="kw iu">幸存_真实</strong>”列。</p><p id="b622" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来我们来看一个“<strong class="kw iu">混淆矩阵</strong>”。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="03e7" class="lr ls it mr b gy mv mw l mx my">from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix</span><span id="c511" class="lr ls it mr b gy mz mw l mx my">confusion_matrix(y_test, y_hat)</span><span id="73cc" class="lr ls it mr b gy mz mw l mx my">array([[147,  24],<br/>       [ 19,  77]])</span></pre><ul class=""><li id="1f3e" class="nz oa it kw b kx ky la lb ld ob lh oc ll od lp oe of og oh bi translated">147是我们真正的阳性。换句话说，我们预测的是真的，这是真的。</li><li id="e27d" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated">77是我们真正的负片。换句话说，我们预测是错误的，这是错误的。</li><li id="ba1b" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated">24是我们的假阳性。换句话说，我们预测是积极的，但它不是。</li><li id="0af2" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated">19是我们的假阴性。换句话说，我们预测是负面的，但事实并非如此。</li></ul><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="b7a0" class="lr ls it mr b gy mv mw l mx my">print(classification_report(y_test, y_hat))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/b661069511f65893971a6725e6f78b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJ5RZyt8yNCJVl7qztvwTw.png"/></div></div></figure><p id="5296" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“<strong class="kw iu">f1-得分</strong>”用于衡量测试的准确性。“<strong class="kw iu">f1-得分</strong>”是精度和召回率之间的调和平均值。“<strong class="kw iu">f1-得分</strong>的范围为[0，1]。它告诉您您的分类器有多精确(它正确分类了多少个实例)，以及它有多健壮(它不会遗漏大量实例)。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="3a15" class="lr ls it mr b gy mv mw l mx my">plot_confusion_matrix(lr, X_test, y_test)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0f99bc7dd454a71ea2e38243907c951f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*nlU5Lqx-0lsT-opvf94KTg.png"/></div></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="a0e4" class="lr ls it mr b gy mv mw l mx my">train['Survived'].value_counts(normalize=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/01a1707b78607ba5f308246e1a540fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*b8y0_XfwToGNk_S4A4z-Sw.png"/></div></figure><p id="a2ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">验证我们模型的第一个测试是将它与基于最常见类的“<strong class="kw iu">猜测</strong>”进行比较。在我们的例子中，我们的模型比仅仅“<strong class="kw iu">猜测</strong>”要好，这是一个好的开始。</p><p id="891f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来我们将尝试<strong class="kw iu">决策树分类器</strong>模型。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="886e" class="lr ls it mr b gy mv mw l mx my">dtc = DecisionTreeClassifier(max_depth=3)<br/>dtc.fit(X_train, y_train)</span></pre><p id="5abe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">检查准确性。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8fe5" class="lr ls it mr b gy mv mw l mx my">dtc.score(X_train, y_train), dtc.score(X_test, y_test)<br/>(0.8344051446945338, 0.8239700374531835)</span></pre><p id="3fc5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">看起来它在训练中表现更好，但在测试中表现更差。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8008" class="lr ls it mr b gy mv mw l mx my">print(classification_report(y_test, y_hat))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/60caec52dbd5a25e03108a540fc1207d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u45HqS6LSHrLW1am7ZWdlQ.png"/></div></div></figure><p id="d2db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> ROC(受试者操作特征)</strong>和<strong class="kw iu"> AUC(曲线下面积)</strong>是检验模型性能的重要评价指标。也可以写成<strong class="kw iu"> AUROC(接收机工作特性下的区域)</strong></p><p id="fa39" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ROC是概率曲线，AUC代表可分性的程度或度量。它们告诉我们模型能够区分多少特征。AUC越高(最接近1)，模型预测真阳性和真阴性就越好。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="1dbe" class="lr ls it mr b gy mv mw l mx my">from sklearn.metrics import roc_curve<br/>from sklearn.metrics import roc_auc_score</span><span id="c00f" class="lr ls it mr b gy mz mw l mx my"># generate a no skill prediction (majority class)<br/>ns_probs = [0 for _ in range(len(y_test))]</span><span id="db08" class="lr ls it mr b gy mz mw l mx my"># predict probabilities<br/>lr_probs = lr.predict_proba(X_test)</span><span id="025b" class="lr ls it mr b gy mz mw l mx my"># keep probabilities for the positive outcome only<br/>lr_probs = lr_probs[:, 1]</span><span id="873a" class="lr ls it mr b gy mz mw l mx my"># calculate scores<br/>ns_auc = roc_auc_score(y_test, ns_probs)<br/>lr_auc = roc_auc_score(y_test, lr_probs)</span><span id="8108" class="lr ls it mr b gy mz mw l mx my"># calculate scores<br/>ns_auc = roc_auc_score(y_test, ns_probs)<br/>lr_auc = roc_auc_score(y_test, lr_probs)</span><span id="b071" class="lr ls it mr b gy mz mw l mx my"># summarise scores<br/>print('No Skill: ROC AUC=%.3f' % (ns_auc))<br/>print('Logistic: ROC AUC=%.3f' % (lr_auc))</span><span id="3c03" class="lr ls it mr b gy mz mw l mx my"># calculate roc curves<br/>ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)<br/>lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)</span><span id="6ae1" class="lr ls it mr b gy mz mw l mx my"># plot the roc curve for the model<br/>plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')<br/>plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')</span><span id="c179" class="lr ls it mr b gy mz mw l mx my"># axis labels<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')</span><span id="1844" class="lr ls it mr b gy mz mw l mx my"># show the legend<br/>plt.legend()</span><span id="c190" class="lr ls it mr b gy mz mw l mx my"># show the plot<br/>plt.show()</span></pre><p id="d9cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">左上方是“<strong class="kw iu">真阳性率</strong>”，右下方是“<strong class="kw iu">假阳性率</strong>”。橙色线越靠近左上角越好。左上方(1.0)表示“<strong class="kw iu">真正</strong>”和“<strong class="kw iu">真负</strong>”之间没有重叠，这是最完美的情况。随着错误越来越多，橙色线将越来越靠近蓝色线。</p><p id="8214" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在下图中，ROC AUC为<strong class="kw iu"> 0.876 </strong>。这意味着该模型有87.6%的机会能够区分正类和负类。</p><p id="31ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果ROC AUC为0.5，则意味着该模型不能区分“<strong class="kw iu">真阳性</strong>和“<strong class="kw iu">真阴性</strong>”。如果橙色线下降到蓝色线以下，并移向右下角，这意味着模型预测完全不正确。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/c9d316786d76ca88f0346184a3e07b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZNGUmJ7Wz2ciqBRcTNXzQ.png"/></div></div></figure><p id="bbf3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">他们看起来都很相似，但是<strong class="kw iu">逻辑回归</strong>表现稍好。</p><p id="ff4b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望您对此感兴趣，并再次特别感谢Robert Manriquez的出色培训和课程支持。</p><p id="d02e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望你喜欢这篇文章。如果你这样做了，请考虑关注我以后的文章，并为这篇文章鼓掌，因为这有助于回报我的努力:)</p><h1 id="301c" class="os ls it bd lt ot ou ov lw ow ox oy lz jz oz ka mc kc pa kd mf kf pb kg mi pc bi translated">迈克尔·惠特尔</h1><ul class=""><li id="8643" class="nz oa it kw b kx mk la ml ld pd lh pe ll pf lp oe of og oh bi translated"><strong class="kw iu"> <em class="pg">如果你喜欢这个，请</em> </strong> <a class="ae lq" href="https://whittle.medium.com/" rel="noopener"> <strong class="kw iu"> <em class="pg">跟我上媒</em> </strong> </a></li><li id="3eea" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated"><strong class="kw iu"> <em class="pg">更多有趣的文章，请</em> </strong> <a class="ae lq" href="https://medium.com/trading-data-analysis" rel="noopener"> <strong class="kw iu"> <em class="pg">关注我的刊物</em> </strong> </a></li><li id="2e4b" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated"><strong class="kw iu"> <em class="pg">有兴趣合作吗？</em> </strong> <a class="ae lq" href="https://www.linkedin.com/in/miwhittle/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="pg">我们上LinkedIn </em> </strong> </a>连线吧</li><li id="e635" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated"><strong class="kw iu"> <em class="pg">支持我和其他媒体作者</em> </strong> <a class="ae lq" href="https://whittle.medium.com/membership" rel="noopener"> <strong class="kw iu"> <em class="pg">在此报名</em> </strong> </a></li><li id="872e" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated"><strong class="kw iu"> <em class="pg">请别忘了为文章鼓掌:)←谢谢！</em>T49】</strong></li></ul></div></div>    
</body>
</html>