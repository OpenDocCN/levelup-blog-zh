# 使用 Node.js 实现文件拆分器

> 原文：<https://levelup.gitconnected.com/implementation-of-a-file-splitter-using-node-js-c9373c5dd9af>

![](img/e56d5b8248e5bc058e1be70587cfa835.png)

照片由[在](https://unsplash.com/@drew_beamer?utm_source=medium&utm_medium=referral) [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上绘制的光束器

在本文中，我将向您展示我使用 Node.js 实现的文件拆分器。这个文件拆分器可以获取一个大的 csv 文件，并根据指定的块大小将其拆分为许多较小的文件。块大小将是每个文件中包含的记录数。有许多不同的原因可以让你拆分一个大文件。这些原因可能包括希望将大量数据加载到数据库实例中，将数据发送给同事，加载到网站或 FTP 服务器，或者将数据下载到机器上。在所有这些情况下，处理较小的数据块可能比一次性处理单个大规模数据集更容易。另一个原因甚至可能包括安全目的。

出于安全原因，您可能希望将数据分布在多个位置，从而使数据更难被破坏。既然我们已经理解了为什么我们可能想要使用文件拆分器的一些原因，那么让我们检查一下源代码中的实现。我们将在这个例子中查看的代码库可以在这里找到:[https://github.com/mwiginton/file-splitter](https://github.com/mwiginton/file-splitter)

这段代码的逻辑驻留在我们的`index.js file`中。让我们从文件的最顶端开始，在这里我们将导入本例中使用的所有包:

我们列出的第一个进口产品是`fs`包。这是一个包，我们将使用它从源文件(大文件，我们希望将它分割成小块)中读取数据，以及向小文件块中写入数据。我们将看到的下一个进口是`csv-parser`包。这个包可以将 CSV 数据转换成 JSON 对象。拥有包含 CSV 数据的 JSON 对象对于将原始文件中的大数据分割成较小文件的小数据块非常重要。最后导入的包是`fast-csv`包。这个包将帮助我们获取较小的 JSON 数据块，并将每个数据块写入它自己的较小文件中。

既然我们已经介绍了完成这项工作所需的所有包，那么让我们进入这个脚本的逻辑。让我们跳到我们的`index.js`文件的第 83 行，在那里我们看到有一个对我们的`driver()`函数的函数调用。这是我们脚本的入口点。让我们进入我们的`driver()`函数，我们可以看到执行的第一步是获取一个由我们的`createList()`函数返回的数组`fileLines`。这个函数负责读取我们的初始大文件(在我们的例子中是 GitHub 存储库中包含的‘100000 Sales records . CSV ’),并返回一个列表，其中包含我们的原始文件的每一行，表示为一个 JSON 对象。现在让我们进入`index.js`文件第 5 行的函数，检查它的行为。

我们做的第一件事是初始化一个名为`processedJson`的空数组。这将是包含我们的原始 CSV 数据(表示为 JSON 对象)的每一行的数组。我们要做的下一件事是利用我们的`fs`包为我们正在处理的文件打开一个可读的流。然后，我们使用`pipe`方法来传输我们的 csv 对象，该对象将监听我们指定为事件链中的下一步的数据事件。在这种情况下，每次读取新的一行数据时，我们将获取当前行，并将其推送到包含 JSON 对象的数组中。这个逻辑全部储存在一个叫`csvToJsonParsing`的承诺里。我们等待这个承诺得到解决，然后我们将返回包含 JSON 对象和 CSV 数据的最终数组。这就结束了我们的`createList()`函数的逻辑。

如果我们跳出`createList()`函数，进入`driver()`函数的下一步，你会看到我们正在调用一个`fileSplitter()`方法，在这里我们传递 JSON 对象的数组。这个方法负责获取这个 JSON 数组，将它分成更小的块，并将每个更小的块写入它们自己的文件。让我们进入这个函数调用并检查它的行为。

让我们从函数的最开始说起。我们从声明和初始化分割文件所需的所有变量开始。
第一个变量`startingPoint`将是我们原始 JSON 数组的索引，我们将从这里开始写入当前较小的文件。下一个变量`linesWritten`将记录我们已经写入较小文件的总行数(在我们的例子中，我们总共有 100k 条记录正在写入)。为了简单起见，我们的`chunkSize`变量被硬编码为 5000。此变量表示每个文件中的记录数。因此对于我们的演示，每个小文件将包含 5000 条记录。我们将使用的最后一个变量是`numChunks`变量。一旦我们的脚本运行完毕，这就是最终我们将拥有的较小文件的总数。为了得到这个数字，我们将记录的总数(100k)除以我们的`chunkSize` (5k)，得到总共 20 个更小的文件。既然我们已经讨论了这个练习中要用到的所有变量。让我们继续探索分割文件的逻辑。

我们将从一个外部 for 循环开始，在这里我们将从 0 迭代到`numChunks`(我们正在写入的较小文件的总数)。在这个循环中，我们首先要检查我们写的总行数是否大于或等于我们正在处理的总行数。如果是这种情况，我们已经将所有的行写入一个较小的文件，我们可以结束这个过程。如果这个条件不满足，我们可以继续我们的文件分割过程。我们将声明一个数组`jsonChunk`,它将包含要写入当前较小文件的记录。

我们的下一步将是建立一个内部 for 循环，它将从我们当前的`startingPoint`值开始迭代，直到我们达到当前的`startingPoint`值和`chunkSize`的和。一旦进入循环，我们将首先把当前的 JSON 记录推到我们的`jsonChunk`数组中。接下来，我们将检查当前索引是否小于 JSON 对象的总数。如果满足这个条件，我们将增加`linesWritten`变量，这样我们就可以跟踪我们写的所有行。我们将遇到的下一个条件是检查我们当前的索引是否等于我们当前的`startingPoint`和`chunkSize — 1`之和(因为我们的外循环中的零索引)。一旦满足这个条件，我们就达到了该文件的 5k 块大小，然后我们需要为下一个文件更新我们的`startingPoint`为当前索引+ 1 或`j + 1`(因为在外部循环中索引为零)。

在我们更新我们的`startingPoint`之后，我们准备开始写入我们当前的文件。我们通过使用`fs`包来打开一个写流，并设置我们想要写入的文件的名称。我们使用标准的`‘file-’+ i +’.csv’`,因此每个文件都是根据我们的`numChunks`变量的外部循环中的当前索引来命名的。然后，我们使用`fastcsv`包来编写更小的`jsonChunk`数组，并将该信息传输到`writeStream`。一旦我们完成了对当前较小文件的写入，我们就可以重复这个过程，直到我们所有的原始 JSON 对象都被写入它们各自的较小文件。这就结束了我们的文件分割的逻辑行为。为了测试和运行这个脚本，您可以简单地克隆存储库，并且从您的项目目录中，简单地运行命令`node index.js`。

我想做的最后一点说明是，在这个例子中，为了简单起见，有些东西是硬编码的，比如将我们的`chunkSize`设置为 5000 以及文件名。如果我们想进一步改进这段代码，我们可以考虑删除这些硬编码的值，并在运行脚本时将它们作为命令行参数传递，以便使这个工具更具动态性。如果您也想用您自己的文件进行测试，您可以随时用您选择的文件删除我们在这个存储库中的示例文件。

对于那些花时间从头到尾阅读这篇文章的人，我非常感谢您的时间。如果你有任何建设性的反馈，请随时告诉我。我欢迎任何和所有的反馈。

*如果你喜欢阅读这篇文章，请考虑使用* [*我的推荐链接*](https://medium.com/@michelle.wiginton00/membership) *注册 Medium。这种订阅保证了可以无限制地访问我的文章以及其他许多学科的数千名天才作家的文章。*