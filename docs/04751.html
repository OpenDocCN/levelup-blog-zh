<html>
<head>
<title>The Million-Dollar Matrices</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">百万美元矩阵</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/the-million-dollar-matrices-1e9536656806?source=collection_archive---------20-----------------------#2020-07-12">https://levelup.gitconnected.com/the-million-dollar-matrices-1e9536656806?source=collection_archive---------20-----------------------#2020-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e8ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练机器学习模型通常是一个耗时的过程。自然语言处理模型也不例外。新的模型架构需要在专业硬件集群上进行多天甚至数周的训练，才能达到相当的效果，或者在特殊情况下达到新的最先进的效果，这种情况并不少见。这些陈述中经常忽略的是这些实验的货币成本。男孩可以变得昂贵！💸</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/a4428efa7e792f58b27dfea3ae6a2448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7QLG59Y0YSOx9SZXQJh2g.jpeg"/></div></div></figure><p id="3527" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">NLP领域的成功秘诀似乎已经变成创建比以往任何时候都更大的模型，在数百千兆字节的原始文本数据的合并上训练它们更长时间。即使这种方法达不到令人垂涎的<a class="ae lb" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">胶水排行榜</a>上的最先进位置，也有可能投入更多的计算来解决这个问题。训练来自不同初始化的多个模型来创建一个集合通常会达到目的。</p><blockquote class="lc"><p id="541b" class="ld le it bd lf lg lh li lj lk ll kn dk translated"><em class="lm">“我们利用互联网上找到的每一个单词，在1024个你甚至买不起的TPU豆荚上训练了8个100亿参数模型的集合”</em></p></blockquote><p id="3206" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">通常都是这样。</p><p id="2e82" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是有很好的理由来解释为什么这个故事会变成这样。<em class="ko">因为管用！</em></p><p id="337f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些训练运行——创建用于提交的模型——只是计算的冰山一角。下面是大量的超参数评估试验、消融研究和用于比较的基准模型。其中的每一个都需要一次以上的运行，以解决由于初始化和其他随机因素造成的统计差异。</p><p id="042c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有这些都是说，每一个新的最先进的结果都包含了大量的计算。对于从AWS或GCP know购买计算资源人来说，这可能很贵。真的很贵。</p><p id="ba73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那么最顶尖模特的培训价格是多少呢？</p><p id="2a9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了训练submission worth模型，而不是一个整体，BERT large的大小估计为200.000美元！当查看排行榜的顶部时，伯特出生仅两年，认为3.4亿参数很小是不是很奇怪。训练GPT-2或T5，分别有15亿和110亿个参数，估计花费大约100万美元！考虑到所有的辅助训练和实验，相信这些论文中的一篇的最终价格标签是数百万美元是合理的。</p><p id="4513" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有这些都是为了科学，当然，还有一组精心调整的矩阵。</p><p id="af5c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">百万美元矩阵。</em></p><p id="921a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">蛮力有其优点，这一点应该已经很明显了。但是从新的角度来看这个问题，用大脑代替动物肯定会变得更加主流。这是NLP内部的一个研究领域，我发现越来越有兴趣跟踪——弄清楚我们如何使用更小的模型、更少的数据和/或更有效的训练程序。为此，最近发表了一些论文，讨论如何创建更有效的模型。无论是在训练中还是在现实世界中。下面，你会发现我认为值得强调的三个例子。</p><p id="3d7c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">伊莱克特拉</strong></p><p id="ba96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ELECTRA提出了一种新的变压器架构预训练程序，它比以前使用的屏蔽语言建模(MLM)更具参数效率。虽然能够在相同的计算上达到与MLM训练的模型相当的性能，但它也能够实现更高的性能。</p><p id="069b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae lb" href="https://medium.com/dair-ai/bert-is-extremely-inefficient-this-is-how-to-solve-it-688b09350f10" rel="noopener">伊莱克特拉——解决伯特培训前流程的缺陷</a></p><p id="1cf3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">龙前</strong></p><p id="2483" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">变压器的注意头需要计算序列长度的二次复杂度。这使得处理更长的文档非常昂贵，这就是为什么BERT和它的许多亲戚被限制为512个令牌的原因。Longformer引入了一种显示线性复杂性的窗口注意力机制，允许我们在相同的计算中处理更长的序列。</p><p id="a420" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> TinyBERT </strong></p><p id="d96c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管上述论文带来了训练方面的改进，但最终仍然有可能得到一个庞大的模型。因为我们可能不仅仅想用我们的模型来争夺排行榜的位置，它的大小很重要吗？这将直接影响推理时间和成本。</p><p id="da03" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是<em class="ko">知识提炼</em>可以证明超级有用的地方。它可以将较大的模型简化成较小的模型，而不会损失太多的性能。TinyBERT就是一个例子，它比BERT base小7倍，快9倍，而性能却是它的96%!</p><p id="454b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">tiny Bert——体型的确很重要，但你如何训练它可能更重要。🐣</p><h1 id="10f2" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="5026" class="pw-post-body-paragraph jq jr it js b jt mq jv jw jx mr jz ka kb ms kd ke kf mt kh ki kj mu kl km kn im bi translated"><a class="ae lb" href="https://arxiv.org/abs/2004.08900" rel="noopener ugc nofollow" target="_blank">训练NLP模型的成本:简明概述</a></p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="d895" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你觉得这个总结有帮助，请考虑阅读我的其他文章！我已经写了一堆，肯定还会有更多的。👋🏼🤖</p></div></div>    
</body>
</html>