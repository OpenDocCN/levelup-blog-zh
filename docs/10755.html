<html>
<head>
<title>C4.5 Decision Tree. Explained from bottom up</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">C4.5决策树。自下而上解释</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/c4-5-decision-tree-explained-from-bottom-up-67468c1619a7?source=collection_archive---------1-----------------------#2022-01-08">https://levelup.gitconnected.com/c4-5-decision-tree-explained-from-bottom-up-67468c1619a7?source=collection_archive---------1-----------------------#2022-01-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/49a30a7b52db0495613ad21db1fcf132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*cZBhSUzsYkTWJgzl.jpg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">决策图表</figcaption></figure><p id="cda9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">C4.5决策树是一个很难理解的复杂算法。确实需要很多背景知识。这个博客试图整理所有需要的信息，并以一种结构化的方式呈现给读者，让他们不用做额外的研究就能理解。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kw"><img src="../Images/172da7e120f781ab28d66aa1f3bca3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYMF_nb4aI-YyCFedpN2Ow.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">本博客涵盖的内容</figcaption></figure><h1 id="13d5" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">信息熵</h1><p id="cc8f" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated">信息熵是给定示例中杂质的度量。让我们详细说明一下</p><p id="f8a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mi"> Shanon熵或自我信息:</em></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/2f5f58a919e829343d4fe99e9fc6c95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*ddZhbwgI-GynnXtbWutVFQ.png"/></div></figure><p id="f948" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中b是基数。</p><p id="5fed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于二进制数，基数为2，对于数据集中的百分比1，函数的绘图如下:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/5ad859b67a5392dc2644a8bc7a7a13be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*f06YOi6YrKeMTHUGS-YwhQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">src:<a class="ae ml" href="https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2020/01/decision-tree-algorithm-explained . html</a></figcaption></figure><p id="6b29" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，如果一个事件的概率接近1或0，信息熵趋向于0，因为输出或多或少是可预测的。</p><p id="3d2f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在统计学中，熵测量数据集中杂质(异质性)的<em class="mi">水平。完全同质数据集的熵为0，而偏斜数据集的熵更接近1，如下图所示:</em></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mm"><img src="../Images/f6f8f4e968719f4e25860acdcf19e48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*awa3gie6RSsOQEyOH-CQuw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">用熵测量样品的杂质</figcaption></figure><h1 id="764c" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">信息增益</h1><p id="f4c6" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated">它是一个参数，用于计算变换前后数据集熵的变化。信息增益有助于特征选择。其工作原理如下。</p><p id="e0f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">选择数据集中的一个要素，并基于该要素的值，将数据集分割成多个较小的数据集。从父数据集的总熵到所有新的子数据集的平均熵的变化被计算为信息增益。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mn"><img src="../Images/022647d16ca38a00853dd6a68ababf7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NALr83wy_Tkt7aj1cAdDDQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">信息增益公式</figcaption></figure><p id="1270" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">信息增益越高，特征划分数据集就越准确，因为得到的数据集越同质，熵就越低。这表明要素已准确分割(分类)数据集。另一方面，如果信息增益较低，则意味着生成的数据集或多或少与父数据集一样具有异构性，因此该要素不会提供太多价值。</p><h2 id="8d4b" class="mo lg iq bd lh mp mq dn ll mr ms dp lp kj mt mu lt kn mv mw lx kr mx my mb mz bi translated">示例:</h2><p id="be1e" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated">考虑如下数据集:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi na"><img src="../Images/a7ebd8747fdb8ff7dbf4a62484cc40e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HNCRbKIJxA8ntinjwljeYg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">https://www.saedsayad.com/decision_tree.htm</figcaption></figure><p id="1374" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了预测<em class="mi">打高尔夫</em>是还是不是，要考虑的特征是<em class="mi">前景、温度、湿度</em>和<em class="mi">有风。</em></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nb"><img src="../Images/59cd121ba639f9e905d58c31b6f6220c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MshQ-gpXdjT_2YKqaihPTg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">打高尔夫球熵</figcaption></figure><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nc"><img src="../Images/e0a1332d79b5cfeae815e28f2ec1bfbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mjdfRe7evvuF0eDNpuI5OA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">按Outlook拆分时PlayGolf的熵</figcaption></figure><p id="e6e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，从Outlook功能中获得的信息如下:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nd"><img src="../Images/bbd1615e988c183071ef9b956f5f7c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5LfxYr458j8HxUgUsViSzg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">信息增益的计算</figcaption></figure><h1 id="fcb5" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">迭代二分法3 (ID3)决策树</strong></h1><p id="7adc" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated"><strong class="ka ir">迭代二分法3 (ID3) </strong>是一种决策树算法，使用<em class="mi">熵</em>和<em class="mi">信息增益。</em>该算法计算数据集中每个特征的信息增益。从中选择具有最高增益的一个，并基于该特征划分数据集。</p><p id="d427" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">例如:</strong></p><p id="6058" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据上一节中的公式为“玩高尔夫”数据集中的每个要素计算增益，结果如下:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ne"><img src="../Images/e76fc3e36206001bfcbe0bcb772b658d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04PPr2JljNpQygUu3WSHLw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">https://www.saedsayad.com/decision_tree.htm</figcaption></figure><p id="1deb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于<em class="mi">前景</em>是具有最高信息增益的特征，所以选择它。增益的计算可以参考来源网站。</p><p id="ad48" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如下图所示，按<em class="mi"> Outlook </em>拆分的数据集<em class="mi">打高尔夫</em>的值更加同质。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nf"><img src="../Images/61745e35ca75d60bf31ba54ad5d4b6b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9tc5qmLUU3vENjroNMDn6w.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">https://www.saedsayad.com/decision_tree.htm</figcaption></figure><p id="e1ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">重复该过程，直到所有的目标值都落入相同的类中，或者没有更多的特征需要分割，在这种情况下，选择在目标标签中具有最大计数的类作为节点。</p><p id="2693" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在该示例中，Outlook功能为“阴”的数据集中的目标值始终为“是”。因此数据集不会被进一步分割。然而，对于其他2个数据集，在展望特征为晴天和雨天的情况下，重复该过程，直到得到解决方案。</p><h1 id="7349" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">C4.5</h1><p id="06a5" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated">C4.5决策树是对ID3决策树的修改。与使用信息增益的ID3不同，C4.5使用增益比作为<em class="mi">优度函数</em>来分割数据集。</p><p id="dfb5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">信息增益函数倾向于偏好具有更多类别的特征，因为它们倾向于具有更低的熵。这导致训练数据的过度拟合。增益比通过使用称为分割信息或内在信息的公式惩罚具有更多类别的特征来缓解这个问题。</p><p id="4bdb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">考虑Outlook功能的拆分信息的计算。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ng"><img src="../Images/09ac34e3e1007c13d7e254959b127416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnVuwsQQ7KtX_rqQTUDfkA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">拆分的计算</figcaption></figure><p id="adb1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在将分离信息的值应用于增益比:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3d5d534ee1fe2f946b08f01166dee3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*gg0WJ68rHNgRXgHgwgsiQA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">计算增益比的公式</figcaption></figure><p id="ca65" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，如果该功能只有一个类别，如果outlook只有一个类别，比如阳光明媚，则拆分信息应该是:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ni"><img src="../Images/20495bf9a0713c78b9e09e4ced817dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRHmhBxR4goExPXqMe3CoA.png"/></div></div></figure><p id="c63f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从而导致无限的增益比。由于增益比倾向于具有较少类别的特征，所以分支将较少，从而防止过度拟合。</p><h2 id="bdc9" class="mo lg iq bd lh mp mq dn ll mr ms dp lp kj mt mu lt kn mv mw lx kr mx my mb mz bi translated">参考资料:</h2><div class="nj nk gp gr nl nm"><a href="https://machinelearningmastery.com/what-is-information-entropy" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">信息熵简介——机器学习掌握</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">信息论是数学的一个分支，研究在噪声信道中传输数据。一块基石…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">machinelearningmastery.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa js nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">熵(信息论)</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">在1-信息论中，随机变量的熵是“信息”、“惊喜”或“意外”的平均水平</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">en.wikipedia.org</p></div></div><div class="nv l"><div class="ob l nx ny nz nv oa js nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://machinelearningmastery.com/information-gain-and-mutual-information/#:~" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">机器学习的信息增益和互信息-机器学习掌握</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">信息增益计算熵的减少或以某种方式转换数据集带来的惊喜。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">machinelearningmastery.com</p></div></div><div class="nv l"><div class="oc l nx ny nz nv oa js nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://medium.com/coinmonks/what-is-entropy-and-why-information-gain-is-matter-4e85d46d2f01" rel="noopener follow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">什么是熵，为什么信息增益在决策树中很重要？</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">根据维基百科，熵指的是无序或不确定性。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">medium.com</p></div></div><div class="nv l"><div class="od l nx ny nz nv oa js nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://www.saedsayad.com/decision_tree.htm" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">决策图表</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">地图&gt;数据科学&gt;预测未来&gt;建模&gt;分类&gt;决策树决策树构建…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">www.saedsayad.com</p></div></div><div class="nv l"><div class="oe l nx ny nz nv oa js nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://en.wikipedia.org/wiki/ID3_algorithm" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">ID3算法-维基百科</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">在决策树学习中，ID3(迭代二分法3)是Ross Quinlan发明的一种算法，用于生成一个决策树</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">en.wikipedia.org</p></div></div><div class="nv l"><div class="of l nx ny nz nv oa js nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://stats.stackexchange.com/questions/346504/information-gain-vs-gain-ratio" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">信息增益与增益比</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">感谢您为交叉验证提供答案！请务必回答问题。提供详细信息并分享…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">stats.stackexchange.com</p></div></div><div class="nv l"><div class="og l nx ny nz nv oa js nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">一步一步的C4.5决策树示例- Sefik Ilkin Serengil</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">决策树仍然是当今数据科学界的热门话题。这里，ID3是最常见的常规决策…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">sefiks.com</p></div></div><div class="nv l"><div class="oh l nx ny nz nv oa js nm"/></div></div></a></div></div></div>    
</body>
</html>