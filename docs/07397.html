<html>
<head>
<title>Coding a Deep Neural Network from Scratch using numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用numpy从头开始编码深度神经网络</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/coding-a-deep-neural-network-from-scratch-17bbc507e7c0?source=collection_archive---------8-----------------------#2021-02-15">https://levelup.gitconnected.com/coding-a-deep-neural-network-from-scratch-17bbc507e7c0?source=collection_archive---------8-----------------------#2021-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/0c83c52fb63f85be75bed2c049ba3f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/1*82NoZku9Ki3F-AM6U7TORA.gif"/></div></figure><p id="1c2c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">几周前，我写了一篇关于感知机编码的文章，目的是巩固我自己的理解，并希望在此过程中提供价值。出于同样的原因，我选择挑战自己，从头开始编写一个深度神经网络。像Tensorflow和Torch这样的框架允许我们轻松地利用神经网络，而不必重新发明轮子，但我相信，对于任何想进入机器学习领域的人来说，理解这些算法的底层技术概念是很重要的。</p><p id="ac4a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">本文假设您对神经网络及其工作方式有一个基本的了解，所以我将只是浏览代码，剖析每个函数。你可以在这里找到这个项目的<a class="ae ks" href="https://github.com/mohdabdin/Neural-Network-from-scratch/blob/main/MLP_using_numpy.py" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">全部代码</strong> </a>。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h1 id="9bb6" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">概观</h1><p id="dcb5" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">在本文中，我将实现一个具有两个隐藏层的深度神经网络，该网络使用带动量的随机下降，并使用MNIST数据集来训练和测试模型。这个数据集是最常用于初级深度学习的，由从0到9的带标签的手写数字组成。我承认有更多有趣的应用，但为了简单起见，我们将使用这个数据集。</p><p id="4263" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我将一步一步地解释每一个组成部分，虽然我不会过多地讨论数学细节，但我会尽最大努力保持直观，并提供一些我认为有用的资源，这些资源深入探讨了某些主题。你需要做的只是对神经网络、线性代数和微积分有一些基本的了解。所以让我们开始吧！</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h1 id="77fc" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">构造器</h1><p id="4477" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">首先让我们从进口开始</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="4b70" class="mm lb iq mi b gy mn mo l mp mq">import numpy as np<br/>import mnist</span></pre><p id="a1e6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正如我在概述中提到的，我们将使用MNIST数据集来训练我们的网络并测试其准确性。有许多方法可以导入这个数据集，但是我发现这个由Hyeonseok Jung设计的项目是最方便的，因为我们使用的是numpy数组。</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="a489" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里我们定义超参数:</p><ul class=""><li id="3a85" class="mt mu iq jw b jx jy kb kc kf mv kj mw kn mx kr my mz na nb bi translated"><strong class="jw ir"> eta(学习率)</strong>:学习率表示权重收敛所采取的步长大小。</li><li id="8259" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated"><strong class="jw ir"> alpha(动量因子)</strong>:我们将使用动量来加速我们的学习过程并提高准确性，但使用动量的主要动机是通过改变权重变化的方向来防止我们的模型陷入局部最小值。我发现<a class="ae ks" href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" rel="noopener" target="_blank"> <strong class="jw ir">这篇文章</strong> </a>提供了一个简洁的势头的交代。</li><li id="96d6" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">简单地说，这是我们训练模型所要经历的迭代次数。</li><li id="1ef3" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">最后，我们将分批训练我们的网络，这意味着我们将分批检查样本。这有助于我们的模型训练更快，使用更少的内存。</li></ul><p id="2422" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">请记住，这些超参数的值是任意的，例如，在少数情况下，根本不使用动量可能更有利。</p><h1 id="b594" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">一键编码</h1><p id="217c" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">首先让我们定义一个函数，使我们的模型能够理解输出。</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="e89a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这个函数的作用是将我们当前的标签(基本上是从0到9)转换成一个大小为10的numpy数组，在对应于数字的位置填充0和1。例如，一个标签'<strong class="jw ir"> 1 </strong>'会变成<strong class="jw ir"> [0，1，0，0，0，0，0，0，0] </strong>，'<strong class="jw ir"> 5 </strong>'会变成<strong class="jw ir"> [0，0，0，0，0，1，0，0] <br/> </strong>等等…</p><h1 id="4dd2" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">初始化权重和偏差</h1><p id="3d77" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">接下来，我们需要在矩阵中随机初始化权重，以符合我们的网络架构。</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="2f4c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如前所述，我们将有2个隐藏层，1个输入和1个输出。第一个权重矩阵的形状为(100，784+1)，这里的加号是我们的偏差单位。网络的第一层，即输入层，将具有形状(50，785)。在单层网络中，我们有一组权重用于计算我们的输出，这里隐藏层充当其前一层的输出。</p><p id="3570" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">此外，我们还定义了一个函数，使用方向参数将偏差单位添加到层中，这是因为我们的输入向量将处于列方向，而对于隐藏层，偏差单位将被添加到行中。</p><h1 id="6649" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">计算向前传球</h1><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="0f43" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在我们可以进入神经网络的核心。该函数将输入作为一个参数，返回每一层的激活和未激活结果，反向传播函数稍后将使用这些结果来计算每一层的梯度并相应地调整权重。</p><p id="2a4a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我使用sigmoid函数压缩每个输入并返回一个激活的输入。还在激活后向每层添加偏置单元。</p><h1 id="aa74" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">预测和损失函数</h1><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="f81e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，这里的predict函数基本上反转了我们之前所做的一键编码，返回实际的数字，而不是大小为10的数组。我们将用它来衡量我们模型的准确性，然后在一些测试样本上使用我们训练好的模型。</p><p id="9186" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们的第二个功能是计算损失，我们的模型将尝试在每次迭代中最小化损失。我不会详细介绍这个函数的推导过程，因为我还不确定具体怎么做，但我发现如果你想深入了解数学细节，这篇文章 会很有帮助。但本质上它所做的只是返回一个值，表明我们的预测离正确的标签有多远。</p><h1 id="030c" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated"><strong class="ak">计算向后传递</strong></h1><p id="f521" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">现在谈谈我们学习过程的主要部分。这里我们将使用向前传递函数计算的值来获得重量变化。</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="fb94" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从输出层开始，直观地说，我们将从最后一层的激活输出中减去标签，以获得权重变化，我们将使用它来应用随机梯度下降。然后，第一个增量用于计算它之前的层的增量，直到我们得到所有层的权重变化。最后，我们将每个增量与相应层的激活向量矩阵相乘，以获得梯度。</p><h1 id="7d68" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">将所有这些放在一起Fit()函数</h1><p id="cb01" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">既然我们已经完成了繁重的工作，我们可以开始把所有东西放在一起，并定义将训练网络的函数。</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="9895" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们将每个隐藏层的大小设为100，然后我们将使用我们之前定义的函数初始化我们的权重，该函数接受我们的层的大小。然后，我们初始化将用于动量的先前权重。如果我们不想在我们的模型中使用动量，也可以不使用这些，或者让alpha等于0，但仍然可以成功，所以如果你想让事情变得简单，请随意这样做。我们还定义了列表train _ losses和train_acc来跟踪损失和准确性。</p><p id="d5d2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在我们的内部循环中，我们将枚举X(图像)和y(标签)。首先，我们对标签进行一次性编码。然后，网络将计算前向传递，获得损失，计算后向传递，获得梯度，并使用它们来更新我们的权重。之后，我们在每个时期检查我们的模型的准确性，以便在它训练时给我们关于它的性能的反馈。</p><h1 id="172f" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">导入和准备我们的数据集</h1><p id="4d6e" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">在我们的类之外使用下面的函数可能更有意义，但是我会把所有的东西都放在同一个类中。</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="0136" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正如我在开始提到的，我将使用<a class="ae ks" href="https://github.com/hsjeong5/MNIST-for-Numpy" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">这个项目</strong> </a> <strong class="jw ir"> </strong>来导入数据集作为一个numpy数组。<em class="nm"> prep_data() </em>函数获取图像输入(X)和标签(y ),并将其转换为50个一批的数据，因此我们将处理(1200，50，784)个图像，而不是一个大小为(60000，784)的numpy数组。最后，我们使用归一化函数将像素亮度转换为0到1之间的值。</p><h1 id="3e3f" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">训练我们的模型</h1><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="4098" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在我们已经准备好了模型，我们将它初始化为mlp，加载我们的训练数据，使用prep_data函数准备它，最后调用fit()函数开始训练过程。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/986887e4da72019bfcc996e284ff4998.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*qmQ_iUqxc23a7I-CoXsE9w.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">训练迭代</figcaption></figure><p id="e9db" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我发现该模型在大约200次迭代(n_iter=200)后能够达到0.95的准确率。随意摆弄超参数，观察精度的变化。</p><h1 id="7cb1" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">结论和最终想法</h1><p id="cc41" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">这是一个有趣的挑战，我决定接受它来理解深度神经网络，我希望你能从中发现价值。我自己还有很多需要了解的地方，如果不是以前做过这件事的人的很多教程的帮助，我不会走到这一步。我个人发现，当我编写一个算法时，我对它背后的思想有了更强的理解。我花了很多时间看其他人的类似项目，下载他们的代码，一行一行地去理解。我也鼓励你在这里下载完整的代码<a class="ae ks" href="https://github.com/mohdabdin/Neural-Network-from-scratch/blob/main/MLP_using_numpy.py" rel="noopener ugc nofollow" target="_blank"><strong class="jw ir"/></a><strong class="jw ir"/>并使用变量，打印向量的形状等。</p><p id="adba" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">也请随时联系我，我仍然是一名学生，我很乐意在这个令人兴奋的领域联系和认识更多的人！</p><h1 id="518e" class="la lb iq bd lc ld nh lf lg lh ni lj lk ll nj ln lo lp nk lr ls lt nl lv lw lx bi translated">参考</h1><p id="2be0" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated"><strong class="jw ir">【1】</strong><em class="nm">二元交叉熵aka Log Loss——逻辑回归中使用的代价函数</em>。(2020年11月9日)。检索自Analytics vid hya:<a class="ae ks" href="https://www.analyticsvidhya.com/blog/2020/11/binary-cross-entropy-aka-log-loss-the-cost-function-used-in-logistic-regression/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/11/binary-cross-entropy-aka-log-loss-the-cost-function-used-in-logistic-regression/</a></p><p id="24f4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">【2】</strong>布沙耶夫诉(2017年12月3日)。<em class="nm">带动量的随机梯度下降</em>。检索自《走向数据科学:<a class="ae ks" href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" rel="noopener" target="_blank">https://Towards Data Science . com/random-gradient-descent-with-momentum-a 84097641 a5d</a></p></div></div>    
</body>
</html>