<html>
<head>
<title>Brief Introduction to Regularization: Ridge, Lasso, and Elastic Net</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化简介:脊、套索和弹性网</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/brief-introduction-to-regularization-ridge-lasso-and-elastic-net-be62a7955dd?source=collection_archive---------4-----------------------#2020-03-07">https://levelup.gitconnected.com/brief-introduction-to-regularization-ridge-lasso-and-elastic-net-be62a7955dd?source=collection_archive---------4-----------------------#2020-03-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/86cd845c8060859764cd952fee30aafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NYOHGE8J-JdXvhTQ8tjeZw.jpeg"/></div></div></figure><div class=""/><p id="1fd1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">监督机器学习是目前许多组织用来识别和解决业务问题的时髦词汇。常用的算法有两种—分类和回归。在本文中，我们将关注回归。回归分析是预测连续结果的模型。一些例子包括预测住房市场价格，零售商店的销售，天气预测等等！</p><h1 id="1453" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">什么是回归？</h1><p id="684d" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">回归搜索变量之间的关系。</p><p id="eaa1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">例如，你可以观察一个地方的几所房子，并试图了解它们的价格如何取决于<strong class="kd jf">特征</strong>，例如房子的面积、可用的公用设施类型、屋顶类型、角色、所在城市、卧室数量等等。</p><p id="a0e4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是一个回归问题，每个房子的相关数据代表一个<strong class="kd jf">观察值</strong>。假设房子的面积、可用的公用设施类型、屋顶类型、角色、所在城市是独立的特征，而价格取决于它们。</p><p id="fe17" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一般来说，在<strong class="kd jf">多元分析</strong>中，你会希望找到<strong class="kd jf"> </strong>一个函数，它将一些特征或变量充分地映射到其他特征或变量。<br/>相关特征称为因变量、输出或响应。独立特征称为独立变量、输入或预测值。</p><p id="d375" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">回归问题通常有一个连续无界的<strong class="kd jf">变量</strong>。然而，<strong class="kd jf">输入通常是</strong>连续的、离散的、<strong class="kd jf">或者可能是</strong>分类数据<strong class="kd jf">，如</strong>性别、国籍、品牌等等。</p><p id="2ea2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">标准的做法是用𝑦表示输出，用𝑥.表示输入如果有两个或更多的独立变量，<strong class="kd jf">它们将被</strong>表示为<strong class="kd jf">，因为</strong>向量𝐱 = (𝑥₁，…，𝑥ᵣ)，其中𝑟 <strong class="kd jf">是输入的</strong>数量。</p><h1 id="59bf" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">什么时候需要回归？</h1><p id="afa3" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">通常，您希望回归来回答某个现象是否以及如何影响另一个现象，或者几个变量是如何相关的。例如，你可以用它来计算经验或性别是否会影响薪水，以及影响到什么程度。</p><p id="26c0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当您想要使用一组新的预测因子来预测一个响应时，回归是非常有用的。例如，在给定室外温度、一天中的时间以及住户人数的情况下，您将尝试预测一个家庭接下来一个小时的用电量。回归被应用于各种领域:供应链规划、市场营销等等。</p><p id="2598" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我们需要了解回归的基础知识，以及在使用各种模型之前，方程的哪些参数发生了变化。简单线性回归，也称为普通最小二乘法(OLS)，试图最小化残差。在这种情况下，误差是实际值与其预测值之间的差异。</p><p id="62f6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个模型的等式被称为成本函数。</p><p id="c820" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个常见的问题是显示多重共线性的数据特征，这被解释为预测变量彼此相关并与响应变量相关。为了描述这一点，假设我们正在做一项研究，研究一个反应变量——患者体重，我们的预测变量是身高、性别和饮食。这里的问题是，身高和性别也是相关的，并且可以夸大其系数的标准误差，这可能使他们看起来在统计上无关紧要。</p><p id="9012" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对此的一般解决方案是:<strong class="kd jf">以引入一些偏差为代价减少方差</strong>。这种方法称为正则化，几乎总是有利于模型的预测性能。为了让它深入人心，我们来看看下面的情节。</p><p id="7b3a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">随着模型复杂性的增加，在线性回归的情况下可以认为是预测因子的数量，估计的方差也增加，但偏差减少。不带偏见的OLS会把我们放在画面的右边，这远非最佳。这就是我们调整的原因:以一些偏差为代价降低方差，从而在图上向左移动，向最优方向移动。</p><p id="a3ca" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这张图说明了什么是偏差和方差。想象靶心是我们正在估计的真实总体参数，<em class="mc"> β </em>，它的射击点是我们从四个不同的估计器得到的估计值——低偏差和方差，高偏差和方差，以及它们的组合。</p><p id="be95" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">希望偏差和方差都较低，因为较大的值会导致模型的预测较差。事实上，该模型的误差可以分解为三部分:由较大方差产生的误差，由显著偏差产生的误差，以及剩余部分——无法解释的部分。</p><p id="daba" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了产生更精确的复杂数据模型，我们在OLS方程中加入了一个惩罚项。一个罚分加一个<em class="mc"/><strong class="kd jf"><em class="mc"/></strong>偏向某个值。这些被称为L1正则化(拉索回归)和L2正则化(岭回归)。</p><h1 id="31a0" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">里脊回归</h1><p id="0620" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">岭回归使用L2正则化，它将以下惩罚项添加到OLS方程中。</p><p id="2611" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">L2项等于系数大小的平方。在这种情况下，如果lambda(λ)为零，则该方程是基本的OLS，但是如果它大于零，则我们向系数添加一个约束。该约束导致λ的值<strong class="kd jf"> </strong>越大，最小化系数(又名收缩率)趋向于零。缩小系数导致<strong class="kd jf"> </strong>更低的方差，并相继导致<strong class="kd jf"> </strong>更低的误差值。因此，岭回归降低了模型的复杂性，但并没有减少变量的数量，而是缩小了它们的影响。</p><h1 id="973e" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">岭回归中偏差-方差的权衡</h1><p id="abcc" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">在偏差和方差的公式中加入正则化系数，我们得到</p><p id="21d5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从这里可以看到<strong class="kd jf">随着<em class="mc"> λ </em>变大，方差减小，偏差增大</strong>。这就提出了一个问题:为了减少方差，我们愿意接受多大的偏差？或者说:<em class="mc"> λ </em>的最佳值是多少？有两种方法可以解决这个问题。一种更传统的方法将是确定λ这样的<strong class="kd jf"> </strong>某种信息准则，例如AIC或BIC，是那个<strong class="kd jf"> </strong>最小。</p><h1 id="2b15" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">套索回归</h1><p id="70d0" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">套索回归使用L1罚项，代表最小绝对收缩和选择算子。应用于L2的罚值绝对等于系数大小的<strong class="kd jf"> </strong>值:</p><p id="3ff6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">与岭回归相似，λ值为0时，基本的 OLS方程<strong class="kd jf">消失，但是给定适当的<strong class="kd jf"/>λ值，拉索回归可以将一些系数驱动为零。λ的值<strong class="kd jf"> </strong>越大，越多的特征被缩小到零。这将<strong class="kd jf"> </strong>完全消除一些特征，并为我们提供<strong class="kd jf"> </strong>预测值子集，帮助减轻多重共线性和模型复杂性。没有向零收缩的预测值表示它们<strong class="kd jf"> </strong>重要，因此L1正则化允许特征选择(稀疏选择)。因此，对于λ的顶值，许多系数在lasso下正好为零，这与岭回归中的<strong class="kd jf"> </strong>不同。</strong></p><h1 id="4ffd" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">弹性网</h1><p id="6975" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">第三种常用的回归模型是<strong class="kd jf"> </strong>弹性网，它包括来自L1和L2正则化的<strong class="kd jf"> </strong>罚值:</p><p id="6e80" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">除了设置和选择一个λ值，弹性网还允许我们调整阿尔法参数，其中𝞪 = 0对应山脊，𝞪 = 1对应套索。简而言之，如果你把α设为0，罚函数就变成了L1(山脊)项，如果我们把α设为1，就得到L2(套索)项。因此，我们将选择一个介于0和1之间的alpha值来优化弹性网络。实际上，这可能会收缩一些系数，并将一些系数设置为0以进行稀疏选择。</p><h1 id="0976" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">在数据集上实现线性、脊形和套索</h1><p id="a83a" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">因此，现在让我们通过将两种算法应用于一个<a class="ae md" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" rel="noopener ugc nofollow" target="_blank">房屋数据集</a>来研究Lasso和Ridge，然后比较它们的性能。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/d139177ae863c2a315778200bc90811a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OkvhjybSZaCrfJ9QBJliMA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">加载数据。</figcaption></figure><p id="a3b2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">加载数据后，我们会进行探索性的数据分析，清理数据并应用不同的预处理技术。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mn"><img src="../Images/a0052df2c6b8588040b71b6b636a5f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ic25SA9xDMN7SG3UBG3RKA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">检查缺少的值</figcaption></figure><p id="4920" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在对上述数据集运行线性回归和带有超参数调整的线性回归之后。代码和结果如下:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mo"><img src="../Images/37843c0492278649e844fdbba7218456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qn4nITuDv-0uVF-JU_ATxA.png"/></div></div></figure><p id="dba7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上面的输出显示了两个评估指标之一的RMSE。运行基本线性回归和从网格搜索cv和评估指标中获得最佳参数后运行回归。</p><p id="dabd" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">运行套索和套索与超参数调整以上数据集。代码和结果如下:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/444ef57886900984e74b588d3c68cd83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G8Pa7EHh9DkjUv6pjGWihQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">套索代码</figcaption></figure><p id="2ae4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">套索回归也可用于要素选择，因为不太重要的要素的coeﬃcients会减少到零。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/a7d55cb4f1ae7202fb1838d04cf4cccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZfCJMeEXgeWe6L5NLEihw.png"/></div></div></figure><p id="6c92" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以上输出显示了Lasso回归模型的RMSE值分别对应于训练和测试数据上的不同alpha值。</p><p id="2340" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在上述数据集上运行岭和带有超参数调谐的岭之后。代码和结果如下:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/d5bf8888893e7e983f6f2d86125b1986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_Cd7C4GQwNQff-AdEFTZQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">山脉</figcaption></figure><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ms"><img src="../Images/d72d55de232e779f663a251afc189db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEbWvrkWHodec1Or5Uzw6w.png"/></div></div></figure><p id="9406" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以上输出显示了岭回归模型的RMSE值，分别对应于训练数据和测试数据上的不同α值。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/7d6af7185d736b7ba95841c61dc7ba48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iLnmetRk18evTjiNFDx9SQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">所有模型的比较</figcaption></figure><p id="08ad" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正如你从上面的比较图表中看到的，我们可以看到，在OLS方程中加入一个惩罚项。复杂数据上的模型精度更高。</p><h1 id="2911" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">弹力网</strong></h1><p id="ef1a" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">弹性网最早出现是由于对lasso的批评，其变量选择可能过于依赖数据，因此不稳定。解决方案是结合岭回归和套索的惩罚，以获得两全其美。弹性网络旨在最小化以下损失函数:</p><p id="c6c2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中α是脊(α = 0)和套索(α = 1)之间的混合参数。</p><h1 id="09cd" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">结束注释</h1><p id="dc6d" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">总而言之，套索、脊和弹性网之间有一些显著的区别:</p><blockquote class="mu mv mw"><p id="e3d3" class="kb kc mc kd b ke kf kg kh ki kj kk kl mx kn ko kp my kr ks kt mz kv kw kx ky im bi translated"><em class="je">套索做稀疏选择，而山脊不做。当您有高度相关的变量时，岭回归会使两个系数相互缩小。<br/> Lasso有些冷漠，通常会挑一个而不是对面。弹性网可能是试图收缩和同时进行稀疏选择的两者之间的折衷。<br/>里奇惩罚较大的错误，就像惩罚较小的错误一样(因为它们在惩罚期限内被平方)。<br/>套索对他们的惩罚更均匀。这可能重要，也可能不重要。在使用强预测器的预测问题中，与套索相比，脊会降低预测器的有效性。</em></p></blockquote><p id="9afc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您可以在我的GitHub资源库中找到完整的代码。</p><div class="is it gp gr iu na"><a href="https://github.com/Nikhileshorg/Basic-classification-and-Regression-models" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd jf gy z fp nf fr fs ng fu fw jd bi translated">nikhileshorg/基本分类回归模型</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">基本分类和回归模型。为Nikhileshorg/基本分类和回归模型做出贡献…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">github.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no ja na"/></div></div></a></div><p id="1959" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你对这篇文章有什么疑问吗？留下评论，提出你的问题，我会尽力回答。</p><p id="b150" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">感谢阅读！❤</p></div></div>    
</body>
</html>