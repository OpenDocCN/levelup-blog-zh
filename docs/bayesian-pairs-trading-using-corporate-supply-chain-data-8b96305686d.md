# ä½¿ç”¨ä¼ä¸šä¾›åº”é“¾æ•°æ®çš„è´å¶æ–¯å¯¹äº¤æ˜“

> åŽŸæ–‡ï¼š<https://levelup.gitconnected.com/bayesian-pairs-trading-using-corporate-supply-chain-data-8b96305686d>

## ä½¿æŠ•èµ„è€…èƒ½å¤Ÿä¸ºç»™å®šçš„ä¼ä¸šä¾›åº”é“¾æž„å»ºå¥—æœŸä¿å€¼å’Œç»Ÿè®¡å¥—åˆ©ç­–ç•¥ã€‚

# ä»‹ç»

é…å¯¹äº¤æ˜“æ˜¯ç»Ÿè®¡å¥—åˆ©çš„ç»å…¸æ–¹æ³•ï¼Œæœ‰ç€æ‚ ä¹…çš„åŽ†å²ã€‚ç¡®å®šé…å¯¹äº¤æ˜“çš„ä¼ ç»Ÿæ–¹æ³•æ˜¯é€šè¿‡ frequentist ç»Ÿè®¡åæ•´æ£€éªŒï¼Œå¦‚ Engle-Granger ä¸¤æ­¥æ£€éªŒã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´å¤æ‚çš„è´å¶æ–¯æ–¹æ³•æ¥ä½¿ç”¨æ¦‚çŽ‡è§„åˆ’è¿›è¡Œé…å¯¹äº¤æ˜“ï¼Œè¿™æ˜¯è´å¶æ–¯æœºå™¨å­¦ä¹ çš„ä¸€ç§å½¢å¼ã€‚ä¸Žæ›´ç®€å•çš„ frequentist åæ•´æ£€éªŒä¸åŒï¼Œæˆ‘ä»¬çš„è´å¶æ–¯æ–¹æ³•å…è®¸æˆ‘ä»¬éšç€æ—¶é—´çš„æŽ¨ç§»ç›‘æŽ§ä¸€å¯¹è‚¡ç¥¨ä¹‹é—´çš„å…³ç³»ï¼Œè¿™å…è®¸æˆ‘ä»¬è·Ÿè¸ªåæ•´å‚æ•°ç¨³å®šæˆ–çªç„¶å˜åŒ–çš„è‚¡ç¥¨å¯¹ã€‚å½“ç»“åˆä¸€ä¸ªç®€å•çš„å‡å€¼å›žå½’äº¤æ˜“ç®—æ³•ï¼Œæˆ‘ä»¬è¯æ˜Žè¿™æ˜¯ä¸€ä¸ªå¯è¡Œçš„ç†è®ºäº¤æ˜“ç­–ç•¥ï¼Œä¸ºè¿›ä¸€æ­¥çš„è¯„ä¼°å’Œé£Žé™©ç®¡ç†åšå¥½å‡†å¤‡ã€‚

ä¼˜ç§€é…å¯¹äº¤æ˜“å€™é€‰äººçš„ä¸€ä¸ªæ½œåœ¨æ¥æºæ˜¯ä¼ä¸šä¾›åº”é“¾ã€‚æ‰¾åˆ°ä¸€å®¶å…¬å¸çš„å®žé™…ä¾›åº”å•†å’Œå®¢æˆ·å¾€å¾€éžå¸¸å›°éš¾ã€‚è®¸å¤šå…¬å¸ä¸æƒ³å‘ä»–ä»¬çš„ç«žäº‰å¯¹æ‰‹é€éœ²è¿™äº›ä¿¡æ¯ã€‚åˆ©ç”¨æ··åˆæ•°æ®æºæ¥æä¾›å…¬å¸ä¾›åº”é“¾ç”Ÿæ€ç³»ç»Ÿçš„å®Œæ•´ç”»é¢ï¼Œ [AlphaWave Data å…¬å¸ä¾›åº”é“¾ API](https://rapidapi.com/alphawave/api/corporate-supply-chain/endpoints) ä½¿æŠ•èµ„è€…èƒ½å¤Ÿä¸ºç»™å®šçš„å…¬å¸ä¾›åº”é“¾æž„å»ºå¯¹å†²å’Œå»ºç«‹ç»Ÿè®¡å¥—åˆ©ç­–ç•¥ã€‚è¯¦ç»†æè¿°è¿™ä¸€åˆ†æžçš„ Jupyter ç¬”è®°æœ¬ä¹Ÿå¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com/drive/1e_SiiZn7WEW3OUNG-ftN3riPNjiz0M0C?usp=sharing) å’Œ [Github](https://github.com/AlphaWaveData/Jupyter-Notebooks/blob/master/AlphaWave%20Corporate%20Supply%20Chain%20API%20Example.ipynb) ä¸ŠèŽ·å¾—ã€‚

# è½¯ä»¶

å¯¹äºŽè¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å‡ ç§åŸºäºŽ Python çš„ç§‘å­¦è®¡ç®—æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨äº†è‚¡ç¥¨å›žæº¯æµ‹è¯•å¼•æ“Žä»¥åŠ [AlphaWave æ•°æ®å…¬å¸ä¾›åº”é“¾ API](https://rapidapi.com/alphawave/api/corporate-supply-chain/endpoints) ã€‚

```
import json
import requests
import pymc3 as pm
import numpy as np
import pandas as pd
import theano as th
import seaborn as sns
import sklearn.decomposition
import matplotlib.pyplot as plt
%matplotlib notebook
sns.set()import warnings
warnings.filterwarnings('ignore')
```

# é…å¯¹äº¤æ˜“çš„è‚¡ç¥¨é€‰æ‹©

å› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ª pairs äº¤æ˜“ç®—æ³•ï¼Œä¸€ä¸ªå…±åŒæ•´åˆè‚¡ç¥¨çš„æ½œåœ¨æ¥æºæ˜¯å…¬å¸ä¾›åº”é“¾ã€‚ä½¿ç”¨ [AlphaWave æ•°æ®å…¬å¸ä¾›åº”é“¾ API](https://rapidapi.com/alphawave/api/corporate-supply-chain/endpoints) ï¼Œæˆ‘ä»¬å¯ä»¥èŽ·å¾—ç»™å®šè‚¡ç¥¨ä»£ç çš„ä¾›åº”å•†å’Œå®¢æˆ·åˆ—è¡¨ã€‚è¦ç”¨ Python è°ƒç”¨è¿™ä¸ª APIï¼Œå¯ä»¥é€‰æ‹© API æŽ§åˆ¶å°ä¸­æä¾›çš„ä¸€ä¸ªå—æ”¯æŒçš„ Python ä»£ç ç‰‡æ®µã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¦‚ä½•ç”¨ Python è¯·æ±‚è°ƒç”¨ API çš„ä¾‹å­ã€‚æ‚¨éœ€è¦åœ¨ä¸‹é¢çš„ä»£ç å—ä¸­æ’å…¥æ‚¨è‡ªå·±çš„ **x-rapidapi-host** å’Œ **x-rapidapi-key** ä¿¡æ¯ã€‚

```
url = "[https://corporate-supply-chain.p.rapidapi.com/api/v1/resources/supplychain](https://corporate-supply-chain.p.rapidapi.com/api/v1/resources/supplychain)"querystring = {"ticker":"IBM"}headers = {
    'x-rapidapi-host': "YOUR_X-RAPIDAPI-HOST_WILL_COPY_DIRECTLY_FROM_RAPIDAPI_PYTHON_CODE_SNIPPETS",
    'x-rapidapi-key': "YOUR_X-RAPIDAPI-KEY_WILL_COPY_DIRECTLY_FROM_RAPIDAPI_PYTHON_CODE_SNIPPETS"
    }response = requests.request("GET", url, headers=headers, params=querystring)print(response.text)
```

è¾“å‡º:

![](img/548371ff06f01f2949f2d72c27d45c03.png)

```
# Create DataFrame.  
df = pd.DataFrame.from_dict(response.json())
df
```

è¾“å‡º:

![](img/2b165ca12b585147805bf349d8492100.png)

å°±æ˜¯è¿™æ ·ï¼

çŽ°åœ¨ï¼Œå¯¹äºŽç»™å®šçš„è‚¡ç¥¨ä»£ç (æœ¬ä¾‹ä¸­ä¸º IBM)ï¼Œæ‚¨åœ¨ pandas æ•°æ®æ¡†æž¶ä¸­æœ‰äº†ä¸€ä¸ªä¾›åº”å•†å’Œå®¢æˆ·åˆ—è¡¨ã€‚å¦‚æžœæ‚¨æ„¿æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºæœ¬é¢æ•°æ®ã€æŠ€æœ¯æŒ‡æ ‡æˆ–å…¶ä»–æ›¿ä»£æ•°æ®æ¥æºè¿›ä¸€æ­¥ç­›é€‰ä¾›åº”å•†å’Œå®¢æˆ·åˆ—è¡¨ï¼Œä»¥èŽ·å¾—è‰¯å¥½çš„é…å¯¹äº¤æ˜“å€™é€‰åˆ—è¡¨ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†é€‰æ‹© IBM çš„å®¢æˆ· ABC æ¥ç»§ç»­æˆ‘ä»¬çš„ pairs trading åˆ†æžã€‚

# å¸‚åœºæ•°æ®

ä¸ºäº†è¿™æ¬¡æŽ¢ç´¢ï¼Œæˆ‘ä»¬ä»Žé‡‘èžæ•°æ®ä¾›åº”å•† [IEX äº‘](https://iexcloud.io/)èŽ·å–äº†å‡ å¹´çš„æ¯æ—¥æ”¶ç›˜æ•°æ®ã€‚æ‚¨éœ€è¦åœ¨ä¸‹é¢ä»£ç å—ä¸­çš„ **YOUR_IEX_API_KEY_HERE** å ä½ç¬¦ä¸­æ’å…¥æ‚¨è‡ªå·±çš„ IEX API å¯†é’¥ã€‚

```
IEX_API_Key = 'YOUR_IEX_API_KEY_HERE'tickers = [ 'IBM', 'ABC', ]#Create an empty string called `ticker_string` that we'll add tickers and commas to
ticker_string = ''#Loop through every element of `tickers` and add them and a comma to ticker_string
for ticker in tickers: 
    ticker_string += ticker 
    ticker_string += ',' 

#Drop the last comma from `ticker_string`
ticker_string = ticker_string[:-1]#Create the endpoint and years strings
endpoints = 'chart'
years = '5'#Interpolate the endpoint strings into the HTTP_request string
HTTP_request = f'[https://cloud.iexapis.com/stable/stock/market/batch?symbols={ticker_string}&types={endpoints}&range={years}y&cache=true&token={IEX_API_Key}'](https://cloud.iexapis.com/stable/stock/market/batch?symbols={ticker_string}&types={endpoints}&range={years}y&cache=true&token={IEX_API_Key}')#Send the HTTP request to the IEX Cloud API and store the response in a pandas DataFrame
stock_data = pd.read_json(HTTP_request)#Create an empty list that we will append pandas Series of stock price data into
series_list = []#Loop through each of our tickers and parse a pandas Series of their closing prices over the last 5 years
for ticker in tickers: 
    series_list.append(pd.DataFrame(stock_data[ticker]['chart'])['close'])#Add in a column of dates
series_list.append(pd.DataFrame(stock_data['IBM']['chart'])['date'])#Copy the 'tickers' list from earlier in the script, and add a new element called 'Date'. 
#These elements will be the column names of our pandas DataFrame later on.
column_names = tickers.copy()
column_names.append('Date')#Concatenate the pandas Series togehter into a single DataFrame
stock_data = pd.concat(series_list, axis=1)#Name the columns of the DataFrame and set the 'Date' column as the index
stock_data.columns = column_names
stock_data.set_index('Date', inplace = True)stock1_name, stock2_name = 'IBM','ABC'
orig_data = stock_data.loc['2018-01-01':,]
data = orig_data.diff().cumsum()
data1 = data[stock1_name].ffill().fillna(0).values
data2 = data[stock2_name].ffill().fillna(0).valuesplt.figure(figsize = (18,8))
ax = plt.gca()
plt.title("Potentially Cointegrated Stocks")
orig_data[stock1_name].plot(ax=ax,color=sns.color_palette()[1],linewidth=2)
orig_data[stock2_name].plot(ax=ax,color=sns.color_palette()[2],linewidth=2)
plt.ylabel("Price (USD)")
plt.legend()
plt.show()
```

![](img/450a1261109db30ac9a0c168d30b7321.png)

# è´å¶æ–¯å»ºæ¨¡

æˆ‘ä»¬çš„è‚¡ç¥¨è´å¶æ–¯æ¨¡åž‹ä¸ä¼šå›ºæœ‰åœ°å¯¹ä¸¤åªè‚¡ç¥¨çš„æ•´åˆé¡ºåºåšå‡ºå‡è®¾ã€‚ä½†æ˜¯ï¼Œä½œä¸ºä¸€ä¸ªç®€åŒ–çš„å‡è®¾ï¼Œæˆ‘ä»¬å°†å‡è®¾ä¸€å¯¹åæ•´è‚¡ç¥¨çš„æŸç§çº¿æ€§ç»„åˆæœ¬èº«æ˜¯å¹³ç¨³çš„ï¼Œå¹¶ä¸”æ˜¯æ­£æ€åˆ†å¸ƒçš„ã€‚

ä½¿ç”¨ PyMC3 è¯­æ³•ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹æ¨¡åž‹æè¿°:

```
with pm.Model() as model:

    # inject external stock data
    stock1 = th.shared(data1)
    stock2 = th.shared(data2)

    # define our cointegration variables
    beta_sigma = pm.Exponential('beta_sigma', 50.)
    beta = pm.GaussianRandomWalk('beta', sd=beta_sigma,
                                 shape=data1.shape[0])

    # with our assumptions, cointegration can be reframed as a regression problem
    stock2_regression = beta * stock1# Assume prices are Normally distributed, the mean comes from the regression.
    sd = pm.HalfNormal('sd', sd=.1)
    likelihood = pm.Normal('y',
                           mu=stock2_regression,
                           sd=sd,
                           observed=stock2)with model:
    stock1.set_value(data1)
    stock2.set_value(data2)
    trace = pm.sample(2000,tune=1000,cores=4)rolling_beta = trace[beta].T.mean(axis=1)plt.figure(figsize = (18,8))
ax = plt.gca()
plt.title("Beta Distribution over Time")
pd.Series(rolling_beta,index=orig_data.index).plot(ax=ax,color='r',zorder=1e6,linewidth=2)
for orbit in trace[beta][:500]:
    pd.Series(orbit,index=orig_data.index).plot(ax=ax,color=sns.color_palette()[0],alpha=0.05)
plt.legend(['Beta Mean','Beta Orbit'])
#plt.savefig("beta distrib.png")plt.show()
```

![](img/3a6e58d664bae31642b1b097db029ec7.png)

è¯·æ³¨æ„ï¼Œð›½(ä¸¤åªè‚¡ç¥¨ä¹‹é—´çš„å…³ç³»)ä¼¼ä¹Žç»å¸¸çªç„¶å˜åŒ–ã€‚

# äº¤æ˜“ç­–ç•¥

çŸ¥é“ä¸¤åªè‚¡ç¥¨å¯èƒ½æ˜¯ä¹Ÿå¯èƒ½ä¸æ˜¯åæ•´çš„ï¼Œå¹¶ä¸èƒ½æ˜Žç¡®å®šä¹‰äº¤æ˜“ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä»¥ä¸‹ç®€å•çš„å‡å€¼å›žå¤å¼äº¤æ˜“ç®—æ³•ï¼Œå®ƒåˆ©ç”¨äº†å‡è®¾çš„è‚¡ç¥¨ç»„åˆçš„å‡å€¼å›žå¤è¡Œä¸ºã€‚æ¯å½“æˆ‘ä»¬çš„æŠ•èµ„ç»„åˆå›žåˆ°å‡å€¼æ—¶ï¼Œæˆ‘ä»¬å°±äº¤æ˜“ã€‚å½“ç®—æ³•ä¸äº¤æ˜“æ—¶ï¼Œæˆ‘ä»¬åŠ¨æ€åœ°æ›´æ–°ð›½å’Œå®ƒçš„å…¶ä»–å‚æ•°ï¼Œä»¥é€‚åº”å¯èƒ½å˜åŒ–çš„åæ•´æ¡ä»¶ã€‚ä¸€æ—¦äº¤æ˜“å¼€å§‹ï¼Œæˆ‘ä»¬è¢«è¿«ä»¥å›ºå®šçš„ä»·æ ¼äº¤æ˜“è¿™ä¸¤åªè‚¡ç¥¨ï¼Œå› æ­¤æˆ‘ä»¬çš„ð›½åœ¨äº¤æ˜“æœŸé—´è¢«é”å®šã€‚è¯¥ç®—æ³•çš„å…·ä½“å®žçŽ°å¦‚ä¸‹:

å®šä¹‰ä¸€ä¸ªâ€œä¿¡å·â€ï¼Œè¿™åº”è¯¥æ„å‘³ç€-å¦‚æžœð›½ä¿æŒç›¸å¯¹é™æ­¢ï¼Œå½’é›¶ã€‚

å®šä¹‰ä¸€ä¸ªâ€œå¹³æ»‘ä¿¡å·â€ï¼Œä¸€ä¸ªâ€œä¿¡å·â€çš„ 15 å¤©ç§»åŠ¨å¹³å‡çº¿ã€‚

å¦‚æžœæˆ‘ä»¬ä¸äº¤æ˜“â€¦

*   æ›´æ–°ð›½ï¼Œè¿™æ ·å®ƒå°±ä¸ä¼šåœ¨æˆ‘ä»¬ä¸äº¤æ˜“çš„æ—¶å€™ä¿æŒä¸å˜ã€‚
*   å¦‚æžœå¹³æ»‘ä¿¡å·é«˜äºŽé›¶*ä¸”*å‘ä¸‹ç§»åŠ¨ï¼Œåšç©ºæˆ‘ä»¬çš„æŠ•èµ„ç»„åˆã€‚
*   å¦‚æžœå¹³æ»‘åŽçš„ä¿¡å·ä½ŽäºŽé›¶*ä¸”*å‘ä¸Šç§»åŠ¨ï¼Œåšå¤šæˆ‘ä»¬çš„æŠ•èµ„ç»„åˆã€‚

å¦‚æžœæˆ‘ä»¬åšå¤šäº¤æ˜“â€¦

*   å¦‚æžœå¹³æ»‘åŽçš„ä¿¡å·ä½ŽäºŽå®ƒçš„èµ·å§‹å€¼ï¼Œå°±å¹³ä»“ï¼›æˆ‘ä»¬å¯èƒ½åç¦»äº†å¹³å‡å€¼ã€‚
*   å¦‚æžœå¹³æ»‘ä¿¡å·ä¸Šå‡ç©¿è¿‡é›¶çº¿ï¼Œæˆ‘ä»¬å°±è¾¾åˆ°äº†å¹³å‡å€¼ã€‚ç»“æŸäº¤æ˜“ã€‚

å¦‚æžœæˆ‘ä»¬åšç©ºäº¤æ˜“â€¦

*   å¦‚æžœå¹³æ»‘åŽçš„ä¿¡å·é«˜äºŽå®ƒçš„èµ·å§‹å€¼ï¼Œå°±å¹³ä»“ï¼›æˆ‘ä»¬å¯èƒ½åç¦»äº†å¹³å‡å€¼ã€‚
*   å¦‚æžœå¹³æ»‘åŽçš„ä¿¡å·è½åœ¨é›¶çº¿ä¸Šï¼Œæˆ‘ä»¬å°±è¾¾åˆ°äº†å¹³å‡å€¼ã€‚ç»“æŸäº¤æ˜“ã€‚

```
def getStrategyPortfolioWeights(rolling_beta,stock_name1,stock_name2,data,smoothing_window=15):data1 = data[stock_name1].ffill().fillna(0).values
    data2 = data[stock_name2].ffill().fillna(0).values# initial signal rebalance
    fixed_beta = rolling_beta[smoothing_window]
    signal = fixed_beta*data1 - data2
    smoothed_signal = pd.Series(signal).rolling(smoothing_window).mean()
    d_smoothed_signal = smoothed_signal.diff()
    trading = "not"
    trading_start = 0leverage = 0*data.copy()
    for i in range(smoothing_window,data1.shape[0]):
        leverage.iloc[i,:] = leverage.iloc[i-1,:]if trading=="not":# dynamically rebalance the signal when not trading
            fixed_beta = rolling_beta[i]
            signal = fixed_beta*data1 - data2
            smoothed_signal = pd.Series(signal).rolling(smoothing_window).mean()
            d_smoothed_signal = smoothed_signal.diff()if smoothed_signal[i]>0 and d_smoothed_signal[i]<0:leverage.iloc[i,0] = -fixed_beta / (abs(fixed_beta)+1)
                leverage.iloc[i,1] = 1 / (abs(fixed_beta)+1)trading = "short"
                trading_start = smoothed_signal[i]elif smoothed_signal[i]<0 and d_smoothed_signal[i]>0:fixed_beta = rolling_beta[i]
                leverage.iloc[i,0] = fixed_beta / (abs(fixed_beta)+1)
                leverage.iloc[i,1] = -1 / (abs(fixed_beta)+1)trading = "long"
                trading_start = smoothed_signal[i]else:
                leverage.iloc[i,0] = 0
                leverage.iloc[i,1] = 0elif trading=="long":# a failed trade
            if smoothed_signal[i] < trading_start:
                leverage.iloc[i,0] = 0
                leverage.iloc[i,1] = 0
                trading = "not"# a successful trade
            if smoothed_signal[i]>0:
                leverage.iloc[i,0] = 0
                leverage.iloc[i,1] = 0
                trading = "not"elif trading=="short":# a failed trade
            if smoothed_signal[i] > trading_start:
                leverage.iloc[i,0] = 0
                leverage.iloc[i,1] = 0
                trading = "not"# a successful trade
            if smoothed_signal[i]<0:
                leverage.iloc[i,0] = 0
                leverage.iloc[i,1] = 0
                trading = "not"

    return leverage
```

# ç»“æžœ

ä½œä¸ºä¸€ç§å¤šç©ºç®—æ³•ï¼Œäººä»¬æœŸæœ›è¿™ç§ç®—æ³•åœ¨å¸‚åœºä¸‹è·Œæ—¶è¡¨çŽ°å¼ºåŠ²ã€‚è¿™é‡Œçš„å›žæº¯æµ‹è¯•åŒ…æ‹¬ 2018 å¹´åº•çš„å¸‚åœºä¸‹è·Œï¼Œ2019 å¹´åˆéšåŽçš„è¾ƒå°è·Œå¹…ï¼Œä»¥åŠ 2020 å¹´ 3 æœˆçš„å† çŠ¶ç—…æ¯’æŠ›å”®ã€‚

```
portfolioWeights = getStrategyPortfolioWeights(rolling_beta,stock1_name, stock2_name,data).fillna(0)def backtest(pricingDF,leverageDF,start_cash):
    """Backtests pricing based on some given set of leverage. Leverage works such that it happens "overnight",
    so leverage for "today" is applied to yesterday's close price. This algo can handle NaNs in pricing data
    before a stock exists, but ffill() should be used for NaNs that occur after the stock has existed, even
    if that stock ceases to exist later."""

    pricing = pricingDF.values
    leverage = leverageDF.values

    shares = np.zeros_like(pricing)
    cash = np.zeros(pricing.shape[0])
    cash[0] = start_cash
    curr_price = np.zeros(pricing.shape[1])
    curr_price_div = np.zeros(pricing.shape[1])

    for t in range(1,pricing.shape[0]):

        if np.any(leverage[t]!=leverage[t-1]):# handle non-existent stock values
            curr_price[:] = pricing[t-1]     # you can multiply with this one
            curr_price[np.isnan(curr_price)] = 0
            trading_allowed = (curr_price!=0)
            curr_price_div[:] = curr_price    # you can divide with this one
            curr_price_div[~trading_allowed] = 1

            # determine new positions (warning: leverage to non-trading_allowed stocks is just lost)
            portfolio_value = (shares[t-1]*curr_price).sum()+cash[t-1]
            target_shares = trading_allowed * (portfolio_value*leverage[t]) // curr_price_div

            # rebalance
            shares[t] = target_shares
            cash[t] = cash[t-1] - ((shares[t]-shares[t-1])*curr_price).sum()

        else:

            # maintain positions
            shares[t] = shares[t-1]
            cash[t] = cash[t-1]

    returns = (shares*np.nan_to_num(pricing)).sum(axis=1)+cash
    pct_returns = (returns-start_cash)/start_cash
    return (
        pd.DataFrame( shares, index=pricingDF.index, columns=pricingDF.columns ),
        pd.Series( cash, index=pricingDF.index ),
        pd.Series( pct_returns, index=pricingDF.index)
    )shares, cash, returns = backtest( orig_data, portfolioWeights, 1e6 )plt.figure(figsize = (18,8))
ax = plt.gca()
plt.title("Return Profile of Algorithm")
plt.ylabel("Percent Returns")
returns.plot(ax=ax,linewidth=3)
vals = ax.get_yticks()
ax.set_yticklabels(['{:,.0%}'.format(x) for x in vals])
plt.show()
```

![](img/29d80f2cfe44bdb10a71dd2e83a2a7fe.png)

æ­£å¦‚æˆ‘ä»¬å¯èƒ½å¸Œæœ›çš„é‚£æ ·ï¼Œåœ¨å¸‚åœºä¸‹è·Œä¸­è¡¨çŽ°å¼ºåŠ²ã€‚ç”±äºŽæˆ‘ä»¬çš„æŠ•èµ„ç»„åˆåªæœ‰ä¸¤åªè‚¡ç¥¨ï¼Œå›žæŠ¥çŽ‡æœ‰äº›è¿‡é«˜ã€‚å¯¹äºŽè¿™ä¸ªç®—æ³•çš„æœ€ç»ˆç‰ˆæœ¬ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šäº¤æ˜“ 100 å¯¹æˆ–æ›´å¤šæ¥å‡å°‘æ³¢åŠ¨ã€‚

# ç»“è®ºå’Œæ½œåœ¨çš„æœªæ¥æ–¹å‘

ä½¿ç”¨ [AlphaWave æ•°æ®å…¬å¸ä¾›åº”é“¾ API](https://rapidapi.com/alphawave/api/corporate-supply-chain/endpoints) æ¥è¯†åˆ«è‚¡ç¥¨å¯¹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå¥å£®çš„åŽŸåž‹ï¼Œå®ƒå°†è¢«å†…ç½®åˆ°ä¸€ä¸ªæ›´å¤æ‚çš„è‚¡ç¥¨å¯¹äº¤æ˜“ç®—æ³•ä¸­ã€‚è¿™ç§ç®—æ³•å’Œæ–¹æ³•æœ‰è®¸å¤šåœ°æ–¹å¯ä»¥æ”¹è¿›ï¼ŒåŒ…æ‹¬æ‰©å¤§æŠ•èµ„ç»„åˆï¼Œä¸ºð›½ä½•æ—¶é€‚åˆäº¤æ˜“å»ºç«‹æ ‡å‡†ï¼Œåœ¨æ›´å¤šæ—¶æœŸå†…è¿›è¡Œå›žæº¯æµ‹è¯•ï¼Œä½¿ç”¨ç®€åŒ–å‡è®¾è¾ƒå°‘çš„è´å¶æ–¯æ¨¡åž‹ï¼Œä»¥åŠè°ƒæŸ¥è‚¡ç¥¨ä¹‹é—´æ½œåœ¨çš„éžçº¿æ€§å…³ç³»ã€‚è¿™ä¸€èŠ‚çš„å‰©ä½™éƒ¨åˆ†å°†ä»‹ç»å…¶ä¸­çš„æ¯ä¸€é¡¹ã€‚

## è¿‡æ»¤Î²ä»¥ä¸¢å¼ƒâ€œåâ€å¯¹

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦ç”¨ð›½æ¥äº¤æ˜“ä¸¤åªè‚¡ç¥¨ï¼Œä¸è€ƒè™‘ð›½'s çš„è¡Œä¸ºã€‚é™åˆ¶äº¤æ˜“åªå‘ç”Ÿåœ¨ð›½ç¨³å®šçš„æ—¶å€™ï¼Œå¯èƒ½æœ‰åŠ©äºŽè¯†åˆ«é€‚åˆé…å¯¹äº¤æ˜“çš„è‚¡ç¥¨ï¼Œå› ä¸ºæœŸæœ›æˆ‘ä»¬çš„ç®—æ³•åœ¨å¯ä»¥æƒ³è±¡çš„æ‰€æœ‰å¯èƒ½çš„è‚¡ç¥¨é…å¯¹ä¸­å®Œç¾Žæ— ç¼ºæ˜¯ä¸åˆç†çš„ã€‚

## é™„åŠ å›žæº¯æµ‹è¯•

è™½ç„¶ç›®å‰çš„å›žæº¯æµ‹è¯•å¾ˆæœ‰å¸Œæœ›ï¼Œä½†è¯¥ç®—æ³•å°šæœªç»è¿‡å„ç§é…å¯¹å’Œæ—¶é—´æ®µçš„ä¸¥æ ¼å›žæº¯æµ‹è¯•ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™é¡¹å·¥ä½œå°†å¤§å¹…æ‰©å¤§è§„æ¨¡ï¼Œå¯¹å…¶åœ¨æ›´å¹¿æ³›çš„è‚¡ç¥¨èŒƒå›´å†…çš„è¡¨çŽ°è¿›è¡Œåˆ†æžï¼Œå°†æ˜¯æœç€å¯ä»¥åœ¨å¸‚åœºä¸Šäº¤æ˜“çš„ç®—æ³•ç‰ˆæœ¬è¿ˆå‡ºçš„ä¸€æ­¥ã€‚

å¯¹å“ªäº›è‚¡ç¥¨æœ€é€‚åˆé…å¯¹çš„æ›´æ·±å…¥çš„åˆ†æžä¹Ÿå¯ä»¥æ¥è‡ªäºŽè®¡ç®—å¤§åƒä¸–ç•Œä¸­æ¯ä¸€ä¸ªå¯èƒ½é…å¯¹çš„ð›½ï¼Œä¾‹å¦‚ç½—ç´  3000 æŒ‡æ•°ä¸­çš„è‚¡ç¥¨ã€‚è¿™ç§å…³ç³»æ˜¯å¦æ˜¾è‘—åº”è¯¥ä»Žé«˜æ€§èƒ½è‚¡ç¥¨å¯¹æ˜¯å¦å…·æœ‰æŸç§ç‰¹æ®Šçš„å¸‚åœºç»“æž„å…³ç³»(ä¾‹å¦‚:å…¬å¸ä¾›åº”é“¾ã€ç›¸åŒè¡Œä¸šã€ç±»ä¼¼äº§å“ç­‰)æ¥åˆ¤æ–­ã€‚)ï¼Œä»¥å¸®åŠ©é¿å…è¿‡åº¦æ‹Ÿåˆã€‚

## æ¨¡åž‹ç®€åŒ–å‡è®¾

åœ¨æ•´ä¸ªè°ƒæŸ¥ä¸­ï¼Œå‡è®¾ä¸€å¯¹åæ•´è‚¡ç¥¨(ð‘)çš„çº¿æ€§ç»„åˆå˜åŒ–å¾ˆå°ï¼Œå¯¹åˆ†æžæ¥è¯´ä¸é‡è¦ã€‚äººä»¬å¯ä»¥æƒ³è±¡ï¼Œå¦‚æžœð‘çš„æ³¢åŠ¨å¾ˆå¤§ï¼Œé‚£ä¹ˆè¿™ä¸ªå‡è®¾å°±æ˜¯é”™è¯¯çš„ã€‚æ­¤å¤–ï¼Œè¿˜å‡è®¾ð‘æ˜¯é™æ­¢çš„ã€‚äººä»¬å¯ä»¥è®¾æƒ³è¿™æ ·ä¸€ç§æƒ…å†µï¼Œå…¶ä¸­ð‘ä¸æ˜¯ç¨³å®šçš„ï¼Œä½†ä»ç„¶æ˜¯å‡å€¼å›žå¤çš„ï¼Œè¿™æ ·å®ƒçš„å‡å€¼å‡ ä¹Žæ˜¯ç¨³å®šçš„ã€‚åœ¨æˆ‘ä»¬ç›®å‰çš„åˆ†æžä¸­ï¼Œè¿™ä¸¤åªè‚¡ç¥¨å¯èƒ½å¹¶ä¸ç†æƒ³ï¼Œä½†åœ¨ð‘.çš„ä¸åŒç»Ÿè®¡æ¨¡åž‹ä¸­ï¼Œå®ƒä»¬å¯èƒ½çœ‹èµ·æ¥æ›´æœ‰åº

æ”¾æ¾å¯¹ð‘çš„ç®€åŒ–å‡è®¾å¯èƒ½ä¼šæ­ç¤ºæˆ‘ä»¬å¯ä»¥æˆåŠŸè¿›è¡Œé…å¯¹äº¤æ˜“çš„é¢å¤–è‚¡ç¥¨ï¼Œå› æ­¤åœ¨æˆ‘ä»¬æ— æ³•æ‰¾åˆ°è¶³å¤Ÿå¤šå…·æœ‰ç†æƒ³ð›½å±žæ€§çš„é…å¯¹çš„æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªå¯Œæœ‰æˆæ•ˆçš„æœªæ¥æ–¹å‘ã€‚

## åæ•´åˆ†ç»„

ä¸Žæ ‡å‡†çš„ frequentist åæ•´æ£€éªŒä¸åŒï¼Œæˆ‘ä»¬çš„è´å¶æ–¯æ¨¡åž‹å¾ˆå®¹æ˜“æ‰©å±•åˆ°ä¸‰ä¸ªæˆ–æ›´å¤šçš„è‚¡ç¥¨ï¼Œè€Œæˆ‘ä»¬çš„æ¨¡åž‹å‡ ä¹Žæ²¡æœ‰å˜åŒ–ã€‚

ä¸»è¦çš„å˜åŒ–æ˜¯æˆ‘ä»¬çŽ°åœ¨æœ‰äº†é¢å¤–çš„ð›½å˜é‡ã€‚è™½ç„¶è¿˜ä¸æ¸…æ¥šè¯•å›¾åœ¨æ›´å¤§çš„åæ•´æŠ•èµ„ç»„åˆ(ä¸‰ä¸ªæˆ–æ›´å¤š)ä¸Šäº¤æ˜“ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Œä½†ä½¿ç”¨è´å¶æ–¯ç»Ÿè®¡å’Œæ¦‚çŽ‡è§„åˆ’æ¥æž„å»ºæ˜¯å®Œå…¨å¯è¡Œçš„ã€‚

## æ·±åº¦è´å¶æ–¯åˆ†æž

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå‡è®¾æ˜¯è‚¡ç¥¨çš„ä¸€ä¸ª*çº¿æ€§*ç»„åˆäº§ç”Ÿä¸€äº›å¹³ç¨³(æˆ–ä½Žé˜¶ç§¯åˆ†)åˆ†å¸ƒð‘.æœ‰äº›è‚¡ç¥¨å¯èƒ½ä¼šæœ‰éžçº¿æ€§å…³ç³»ã€‚æˆ‘ä»¬å¯ä»¥å¯¹è‚¡ç¥¨åº”ç”¨è´å¶æ–¯ç¥žç»ç½‘ç»œæ¥æ•æ‰éžçº¿æ€§å…³ç³»ï¼Œè€Œä¸æ˜¯å‡è®¾ç®€å•çš„çº¿æ€§åŠ æƒã€‚å¦‚æžœæˆ‘ä»¬ä»Ž PyMC3 å’Œ Theano è½¬å‘ Edgar å’Œ TensorFlowï¼Œè¿™å°†å˜å¾—å®Œå…¨å¯è¡Œã€‚

***æ¥è‡ªã€Šèµ°å‘æ•°æ®ç§‘å­¦ã€‹ç¼–è¾‘çš„æç¤º:*** *è™½ç„¶æˆ‘ä»¬å…è®¸ç‹¬ç«‹ä½œè€…æ ¹æ®æˆ‘ä»¬çš„* [*è§„åˆ™å’ŒæŒ‡å¯¼æ–¹é’ˆ*](https://towardsdatascience.com/questions-96667b06af5) *å‘è¡¨æ–‡ç« ï¼Œä½†æˆ‘ä»¬å¹¶ä¸è®¤å¯æ¯ä¸ªä½œè€…çš„è´¡çŒ®ã€‚ä½ ä¸åº”è¯¥åœ¨æ²¡æœ‰å¯»æ±‚ä¸“ä¸šå»ºè®®çš„æƒ…å†µä¸‹ä¾èµ–ä¸€ä¸ªä½œè€…çš„ä½œå“ã€‚è¯¦è§æˆ‘ä»¬çš„* [*è¯»è€…æœ¯è¯­*](https://towardsdatascience.com/readers-terms-b5d780a700a4) *ã€‚*