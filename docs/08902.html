<html>
<head>
<title>Concrete Strength Modeling with 5 Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于5种机器学习算法的混凝土强度建模</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/concrete-strength-modeling-with-5-machine-learning-algorithms-447ea3d558d1?source=collection_archive---------9-----------------------#2021-06-16">https://levelup.gitconnected.com/concrete-strength-modeling-with-5-machine-learning-algorithms-447ea3d558d1?source=collection_archive---------9-----------------------#2021-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="05bb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">寻找混凝土强度的最佳算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/dd03cf9703fc528c1997b3702e744f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*oFYJ1Xd8fFH4b_j133Ej0Q.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><p id="2273" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，我们将实现回归算法，以找到混凝土强度的最佳性能模型。让我给你简单介绍一下数据集。混凝土的强度取决于如下所示的各种因素:</p><p id="3d61" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">独立变量:水泥、矿渣、灰分、水、超塑性、粗骨料、细骨料和龄期。</p><p id="dc09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因变量或目标变量:强度</p><p id="1641" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以从<a class="ae lq" href="https://www.kaggle.com/pritech/predicting-the-strength-of-concrete" rel="noopener ugc nofollow" target="_blank">这里</a>得到数据。</p><p id="897e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们练习寻找模型的性能。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="f09a" class="lw lx it ls b gy ly lz l ma mb"># Importing Libraries</span><span id="b398" class="lw lx it ls b gy mc lz l ma mb"># Data Handling Libraries</span><span id="4bb7" class="lw lx it ls b gy mc lz l ma mb">import pandas as pd<br/>import numpy as np<br/>from collections import OrderedDict</span><span id="8751" class="lw lx it ls b gy mc lz l ma mb"># Import Data Visualization Library</span><span id="0bca" class="lw lx it ls b gy mc lz l ma mb">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import scipy.stats as sci</span><span id="27a2" class="lw lx it ls b gy mc lz l ma mb"># Statistic Libraries</span><span id="08f1" class="lw lx it ls b gy mc lz l ma mb">import warnings<br/>warnings.filterwarnings("ignore")</span><span id="1465" class="lw lx it ls b gy mc lz l ma mb"># Import Machine Learning Libraries<br/>from sklearn.linear_model import LinearRegression , Lasso ,Ridge<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.ensemble import AdaBoostRegressor <br/>from sklearn.ensemble import RandomForestRegressor <br/>from sklearn.ensemble import GradientBoostingRegressor<br/>from xgboost import XGBRegressor<br/>from sklearn.svm import SVR<br/>from sklearn.neighbors import KNeighborsRegressor</span><span id="8944" class="lw lx it ls b gy mc lz l ma mb"># Importing Validating matrix<br/>from sklearn.model_selection import cross_val_score,GridSearchCV<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import r2_score,mean_squared_error</span></pre><p id="1494" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用pandas库读取数据集的excel文件。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="ae34" class="lw lx it ls b gy ly lz l ma mb">df = pd.read_excel("Capstone Project.xlsx")</span><span id="3661" class="lw lx it ls b gy mc lz l ma mb"># creating another set of copy<br/>df2 = df.copy()</span></pre><p id="7116" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用head方法查看数据。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="bbf5" class="lw lx it ls b gy ly lz l ma mb"># displays the first five rows of data<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/db86ef90d6cd17cc75fb84572c146723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*R2qh-r6jF12Ia0rD6lC_RQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><blockquote class="me mf mg"><p id="fa02" class="ku kv mh kw b kx ky ju kz la lb jx lc mi le lf lg mj li lj lk mk lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">了解统计数据的描述。</em> </strong></p></blockquote><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="28af" class="lw lx it ls b gy ly lz l ma mb"># gives the descriptive stats<br/>df.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/15fe5e317f817e8e2e5c84226860de63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WQgneQDU8CnJElLDQphMAQ.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><p id="0bb5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">统计描述是非常昂贵的信息较少，我们可以做一个自定义的描述报告如下所示。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="2f3f" class="lw lx it ls b gy ly lz l ma mb"># Preparing Custom EDA Report</span><span id="27c3" class="lw lx it ls b gy mc lz l ma mb">def custom_summary(data):<br/>    result = []<br/>    for col in data.columns:<br/>        stats = OrderedDict({"column_name":col,<br/>                    "Count":round(data[col].count(),2),<br/>                    "Minimum":round(data[col].min(),2),<br/>                    "Quartile 1":round(data[col].quantile(0.25),2),<br/>                    "Mean":round(data[col].mean(),2),<br/>                    "Median":round(data[col].median(),2),<br/>                    "Mode":round(data[col].mode(),2),<br/>                    "Quartile 3":round(data[col].quantile(0.75),2),<br/>                    "Maximum":round(data[col].max(),2),<br/>                    "Variance":round(data[col].var(),2),<br/>                    "Std. Dev.":round(data[col].std(),2),<br/>                    "Kurtosis":round(data[col].kurt(),2),<br/>                    "Skewness":round(data[col].skew(),2),<br/>                    "IQR":data[col].quantile(0.75)-data[col].quantile(0.25)})<br/>        result.append(stats)<br/>        if data[col].skew()&lt;-1:<br/>            sk_label = "Highly Negatively Skewed"<br/>        elif -1&lt;=data[col].skew()&lt;-0.5:<br/>            sk_label = "Moderately Negatively Skewed"<br/>        elif -0.5&lt;= data[col].skew()&lt;0:<br/>            sk_label = "Fairly Symmetric (Negative)"<br/>        elif 0&lt;=data[col].skew()&lt;0.5:<br/>            sk_label= "Fairly Symmetric (Positive)"  <br/>        elif 0.5&lt;=data[col].skew()&lt;1:   <br/>            sk_label= "Moderately Positively Skewed"  <br/>        elif data[col].skew()&gt;1:<br/>            sk_label="Highly Positively Skewed"<br/>        else:<br/>            sk_label='error'<br/>        stats['skewness comment']=sk_label</span><span id="5cf1" class="lw lx it ls b gy mc lz l ma mb">#Outlier comment<br/>        upperlim = stats['Quartile 3']+(1.5*stats['IQR'])    <br/>        lowerlim = stats['Quartile 1']-(1.5*stats['IQR'])<br/>        if len([x for x in data[col] if x &lt; lowerlim or x &gt; upperlim]) &gt; 0:<br/>            outliercomments = 'Has Outlier'<br/>        else:<br/>            outliercomments = 'Has no outliers'<br/>        stats['Outlier Comment'] = outliercomments</span><span id="4450" class="lw lx it ls b gy mc lz l ma mb">result_df = pd.DataFrame(result)<br/>    return result_df</span><span id="47c6" class="lw lx it ls b gy mc lz l ma mb">custom_summary(df)</span></pre><p id="a242" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上述自定义统计报告的结果非常惊人。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mq"><img src="../Images/0bd315ed58826cb1c4ae97fc89f664d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Auv5un1uMMVFK4rbV-I-Ag.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">数据的自定义描述。作者的照片</figcaption></figure><p id="b12b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将使用四分位数和标准方法制作两个函数来检测异常值。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="8cf9" class="lw lx it ls b gy ly lz l ma mb"># Replacing the outlier</span><span id="ba1f" class="lw lx it ls b gy mc lz l ma mb">def replaceoutlier(data,col,method = 'Quartile',strategy = 'Median'):<br/>    col_data = data[col]<br/>   <br/>    # Using Quartile to set values<br/>    <br/>    if method == 'Quartile':<br/>        q2 = data[col].median()<br/>        q1 = data[col].quantile(0.25)<br/>        q3 = data[col].quantile(0.75)<br/>        iqr = q3 - q1<br/>        low_limit = q1 - 1.5*iqr<br/>        up_limit = q3 + 1.5*iqr<br/>   <br/>    # Using STD to set values<br/>    <br/>    elif method == 'Standard Deviation':<br/>        col_mean = data[col].mean()<br/>        col_std = data[col].std()<br/>        cutoff = col_std*2<br/>        low_limit = col_mean - cutoff<br/>        up_limit = col_mean + cutoff<br/>        <br/>    <br/>    else:<br/>        print("Error: Pass a correct method")</span><span id="b2a4" class="lw lx it ls b gy mc lz l ma mb"># Printing Outliers<br/>    <br/>    outliers = data.loc[(col_data &lt; low_limit) | (col_data &gt; up_limit) , col ]</span><span id="fd96" class="lw lx it ls b gy mc lz l ma mb">    outlier_density = round((len(outliers)/len(data)),2)</span><span id="bcf7" class="lw lx it ls b gy mc lz l ma mb">    if len(outliers) == 0:<br/>        print(f'Feature\'{col}\'does not have any outlier')<br/>    else:<br/>        print(f'Total no. of outliers are: {len(outliers)}\n')<br/>        print(f'Outlier percxentage: {outlier_density}\n')<br/>        print(f'Outliers for \'{col}\'are : {np.sort(np.array(outliers))}\n')</span><span id="c0db" class="lw lx it ls b gy mc lz l ma mb">        display(data[(col_data &lt; low_limit) | (col_data &gt; up_limit)])<br/>    <br/>    # Replacing Outliers<br/>    <br/>    if strategy == 'Median':<br/>        data.loc[(col_data &lt; low_limit) | (col_data &gt; up_limit) , col ] = q2</span><span id="c563" class="lw lx it ls b gy mc lz l ma mb">    elif strategy == 'Mean':<br/>        data.loc[(col_data &lt; low_limit) | (col_data &gt; up_limit) , col ] = col_mean</span><span id="c75b" class="lw lx it ls b gy mc lz l ma mb">    else:<br/>        print("Error: Pass a correct Strategy")<br/>              <br/>    return data</span></pre><p id="a8c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第二个函数用于绘制密度和箱线图。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="5b52" class="lw lx it ls b gy ly lz l ma mb">def odt_plots(data,col):<br/>    f,(ax1,ax2,ax3) = plt.subplots(1,3,figsize = (20,20))<br/>    kwargs = {'fontsize':20,'color': 'black'}<br/>    <br/>    # Descriptive Stats: Box Plot<br/>    <br/>    sns.boxplot(data[col],ax = ax1,orient = 'v', color = 'red')<br/>    ax1.set_title(col +' ' + 'Box Plot',**kwargs)<br/>    ax1.set_xlabel('Box Plot',**kwargs)<br/>    ax1.set_ylabel("Values",**kwargs)<br/>   <br/>    #Plotting histogram with outlier<br/>    <br/>    sns.distplot(data[col],ax = ax2,color='red',fit = sci.norm )<br/>    ax2.axvline(data[col].mean(), color = 'green')<br/>    ax2.axvline(data[col].median(),color = 'black' , linestyle = 'dashed')</span><span id="39a1" class="lw lx it ls b gy mc lz l ma mb">    ax2.set_title(col +' ' + 'Distribution with Outliers',**kwargs)<br/>    ax2.set_xlabel('Density',**kwargs)<br/>    ax2.set_ylabel("Values",**kwargs)<br/>    <br/>    #Plotting histogram without outlier<br/>    <br/>    y= replaceoutlier(df,col)<br/>    <br/>    sns.distplot(y[col],ax = ax3,color='red',fit = sci.norm )<br/>    ax3.axvline(y[col].mean(), color = 'green')<br/>    ax3.axvline(y[col].median(),color = 'black' , linestyle = 'dashed')</span><span id="6bd1" class="lw lx it ls b gy mc lz l ma mb">    ax3.set_title(col +' ' + 'Distribution without Outliers',**kwargs)</span><span id="c3af" class="lw lx it ls b gy mc lz l ma mb">    ax3.set_xlabel('Density',**kwargs)<br/>    ax3.set_ylabel("Values",**kwargs)<br/>    <br/>    plt.show()</span></pre><p id="10c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，是时候调用这个函数了。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="6df9" class="lw lx it ls b gy ly lz l ma mb">for col in df.columns:<br/>    odt_plots(df,col)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mr"><img src="../Images/6b375d52622d895c1e20a5b271a169d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p9iLekGDgAndyS-OyVL3UQ.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">方框图和密度图。作者的照片</figcaption></figure><p id="9ff5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将在seaborn库的帮助下绘制每个特征的散点图和直方图。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="d62d" class="lw lx it ls b gy ly lz l ma mb">print(sns.pairplot(df))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ms"><img src="../Images/e7353e2df7b6cfd47026dcf88ed29b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jflAO-WyTg7M1GO46-Tcdg.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><p id="b5a7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将使用回归进行多元分析。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="4f85" class="lw lx it ls b gy ly lz l ma mb">for col in list(df.columns):<br/>    if col!= 'strength':<br/>        ## not all in 1 x axis        <br/>        fig,ax1 = plt.subplots(figsize=(20,15),sharex=False) <br/>        <br/>        sns.regplot(x=df[col],y=df['strength'],ax=ax1).set_title(<br/>                          f'Relationship between{col} and strength')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mt"><img src="../Images/d16855c95fb37f49744a612f3b562cb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmogdP3l6QVSHQMxvFiYhg.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><p id="bde3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是时候查看相关特性的热图了。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="f016" class="lw lx it ls b gy ly lz l ma mb">corr=df.corr()<br/>fig,ax = plt.subplots(figsize = (15,12))<br/>sns.heatmap(corr,annot=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mu"><img src="../Images/9ace209b6d87d2b4b4593097576a3e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bk0RUrHXn-8_bObmkQVY9A.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><p id="73cd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将使用自定义函数来查找相关数。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="cc26" class="lw lx it ls b gy ly lz l ma mb">def corr_with_target(data,tcol):<br/>    ind_var = data.drop([tcol],axis=1).columns<br/>    corr_result = []</span><span id="50c9" class="lw lx it ls b gy mc lz l ma mb">    for col in ind_var:<br/>        corr_result.append(data[tcol].corr(data[col]))<br/>    view_col = pd.DataFrame([ind_var,corr_result],index =<br/>                    ['Variable','Correlation']).T.sort_values<br/>                    ('Correlation',ascending=False)</span><span id="96e4" class="lw lx it ls b gy mc lz l ma mb">    return view_col</span><span id="5235" class="lw lx it ls b gy mc lz l ma mb">corr_with_target(df,'strength')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3aedf9970328e220be02ff200d093137.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*1CxxeXYvLwAsSBycLiVIgw.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><blockquote class="me mf mg"><p id="6ef8" class="ku kv mh kw b kx ky ju kz la lb jx lc mi le lf lg mj li lj lk mk lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">现在，我们来做多重共线性检查。</em>T9】</strong></p></blockquote><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="1813" class="lw lx it ls b gy ly lz l ma mb">def VIF_colinearity(ind_var):<br/>    from statsmodels.stats.outliers_influence import<br/>                                variance_inflation_factor<br/>    vif = pd.DataFrame()<br/>    vif["vif_factor"] = [variance_inflation_factor(ind_var.values,<br/>                               i) for i in range(ind_var.shape[1])]<br/>    vif["Feature"] = ind_var.columns<br/>    <br/>    return vif.sort_values("vif_factor",ascending= False)</span><span id="0c42" class="lw lx it ls b gy mc lz l ma mb">VIF_colinearity(df.drop('strength',axis=1))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mw"><img src="../Images/977adb89b9711b10fbe55690f28ff997.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*NGyh-762HrfEWVY_IXEPCQ.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><blockquote class="me mf mg"><p id="f202" class="ku kv mh kw b kx ky ju kz la lb jx lc mi le lf lg mj li lj lk mk lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">现在，我们将应用PCA来减少多重共线性特征。</em>T13】</strong></p></blockquote><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="b616" class="lw lx it ls b gy ly lz l ma mb">def apply_PCA(x):<br/>    <br/>    col = []<br/>    ncom = len(x.columns)<br/>    <br/>    from sklearn.preprocessing import StandardScaler<br/>    x = StandardScaler().fit_transform(x)<br/>    <br/>    <br/>    from sklearn.decomposition import PCA<br/>    <br/>    for i in range(1,ncom):<br/>        pca = PCA(n_components= i)<br/>        p_components = pca.fit_transform(x)<br/>        exp_var_ratio = np.cumsum(pca.explained_variance_ratio_)<br/>        if exp_var_ratio[i-1]&gt; 0.9:<br/>            n_components = i<br/>            break<br/>            <br/>    print("Explained Variance Ratio after PCA is: ",exp_var_ratio)<br/>    <br/>    #creating dataframe<br/>    <br/>    for j in range(1, n_components + 1):<br/>        col.append("pc" +str(j))<br/>    pcom = pd.DataFrame(data = p_components,columns=col)<br/>    <br/>    return pcom</span><span id="fac9" class="lw lx it ls b gy mc lz l ma mb">transform_df = apply_PCA(df.drop("strength",axis=1))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mu"><img src="../Images/c43c7a68dd005c91e78e83cf89d3dd81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NH-KNR3WBOEm-FMy3twAVw.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><p id="c084" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在应用PCA之后制作新的数据帧。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="7c0c" class="lw lx it ls b gy ly lz l ma mb">transform_df = transform_df.join(df[['strength']],how = 'left')<br/>transform_df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/43b28276264e74f280b751395b7a200a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*KYbZJ9_cviCExT53Jk8gUQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><div class="my mz gp gr na nb"><a href="https://betterprogramming.pub/perform-xgboost-knn-modeling-with-dimension-reduction-technique-9f4ca52feeaf" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">使用降维技术执行XGBoost、KNN建模</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">基于MNIST数据集的机器学习算法建模</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">better编程. pub</p></div></div><div class="nk l"><div class="nl l nm nn no nk np ko nb"/></div></div></a></div><blockquote class="me mf mg"><p id="ca61" class="ku kv mh kw b kx ky ju kz la lb jx lc mi le lf lg mj li lj lk mk lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">现在，创建一个函数来拆分数据和构建模型。</em> </strong></p></blockquote><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="0fb2" class="lw lx it ls b gy ly lz l ma mb"># Function for train test split<br/>def train_and_testsplit(data,tcol,test_size = 0.3):<br/>    x = data.drop(tcol,axis=1)<br/>    y = data[tcol]<br/>    <br/>    return train_test_split(x,y,test_size=test_size,random_state=50)</span></pre><p id="f670" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用于建立模型的函数。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="e40a" class="lw lx it ls b gy ly lz l ma mb"># Function for model building</span><span id="cfb8" class="lw lx it ls b gy mc lz l ma mb">def build_model(model_name,model,data,tcol):<br/>    x_train,x_test,y_train,y_test = train_and_testsplit(data,tcol)<br/>    model.fit(x_train,y_train)<br/>    y_pred = model.predict(x_test)<br/>    rmse = np.sqrt(mean_squared_error(y_test,y_pred))<br/>    r2score = r2_score(y_test,y_pred)<br/>    temp = [model_name , rmse ,  r2score]<br/>    <br/>    return temp</span></pre><p id="1afc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，为多个模型制作一个函数。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="e438" class="lw lx it ls b gy ly lz l ma mb"># building multiple models</span><span id="7251" class="lw lx it ls b gy mc lz l ma mb">def multiple_models(data,tcol):<br/>    col_names = ["Model_Name","RMSE", "R_Square"]<br/>    result = pd.DataFrame(columns= col_names)</span><span id="a1f3" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Linear_Regression",lr,transform_df,"strength")</span><span id="3b04" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Lasso_Regression",Lasso(),transform_df,"strength")</span><span id="d95f" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Ridge_Regression",Ridge(),transform_df,"strength")</span><span id="168b" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Decision_Tree_Regressor",DecisionTreeRegressor(),transform_df,"strength")</span><span id="4c45" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Ada_Boost_Regressor",AdaBoostRegressor(),transform_df,"strength")</span><span id="8aff" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Random_Forest_Regressor",RandomForestRegressor(),transform_df,"strength")</span><span id="11a8" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Gradient_Boosting_Regressor",GradientBoostingRegressor(),transform_df,"strength")</span><span id="bde7" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("XG_Boost_Regressor",XGBRegressor(),transform_df,"strength")</span><span id="1442" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("Support_Vactor_Regressor",SVR(),transform_df,"strength")</span><span id="e8c8" class="lw lx it ls b gy mc lz l ma mb">    result.loc[len(result)] = build_model("K_Nearest_Neighbor_Regressor",KNeighborsRegressor(),transform_df,"strength")<br/>    <br/>    return result</span><span id="f661" class="lw lx it ls b gy mc lz l ma mb">multiple_models(transform_df,"strength")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f86085393ee5faf43e983317c2211809.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*LbknFPll8YX26XivD9HNLQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">作者的照片</figcaption></figure><p id="0044" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lq" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lq" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="6136" class="nr lx it bd ns nt nu nv nw nx ny nz oa jz ob ka oc kc od kd oe kf of kg og oh bi translated">推荐文章</h1><p id="9034" class="pw-post-body-paragraph ku kv it kw b kx oi ju kz la oj jx lc ld ok lf lg lh ol lj lk ll om ln lo lp im bi translated"><a class="ae lq" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae lq" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae lq" href="https://pub.towardsai.net/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30" rel="noopener ugc nofollow" target="_blank">Python中的异常处理概念</a> <br/> 4。<a class="ae lq" href="https://pub.towardsai.net/deep-learning-88e218b74a14?source=friends_link&amp;sk=540bf9088d31859d50dbddab7524ba35" rel="noopener ugc nofollow" target="_blank">为什么LSTM在深度学习方面比RNN更有用？</a> <br/> 5。<a class="ae lq" href="https://pub.towardsai.net/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=friends_link&amp;sk=6844935e3de14e478ce00f0b22e419eb" rel="noopener ugc nofollow" target="_blank">神经网络:递归神经网络的兴起</a> <br/> 6。<a class="ae lq" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae lq" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lq" href="https://pub.towardsai.net/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a" rel="noopener ugc nofollow" target="_blank">concat()、merge()和join()与Python </a> <br/>的区别9。<a class="ae lq" href="https://pub.towardsai.net/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b" rel="noopener ugc nofollow" target="_blank">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae lq" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>