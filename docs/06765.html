<html>
<head>
<title>Neural Networks Visualized</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化神经网络</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/neural-networks-visualized-6cc657f9d7c5?source=collection_archive---------2-----------------------#2020-12-29">https://levelup.gitconnected.com/neural-networks-visualized-6cc657f9d7c5?source=collection_archive---------2-----------------------#2020-12-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/804060a15756cc5de7161b7a48aea34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gFCO3CSMGS8hL9mfELVeQA.jpeg"/></div></div></figure><p id="2f17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我们将通过一个小的神经网络，看看它是如何分类数据的。我们不会在公式上花太多时间。相反，我们将专注于可视化网络节点中的功能。最后，你会对三层神经网络的每一步有一个直观的感觉。</p><p id="d528" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要阅读这篇文章，你应该熟悉神经网络和感知机。你应该知道“sigmoid”、“激活函数”、“偏置节点”之类的概念。如果你正在阅读<a class="ae kw" href="https://pragprog.com/titles/pplearn" rel="noopener ugc nofollow" target="_blank">编程机器学习</a>，或者另一个ML介绍，那么你已经知道你需要的一切。让我们跳进来吧！</p><h1 id="66a2" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">功能机</h1><p id="6b12" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">当你忽略细节时，事情总是更简单。以神经网络为例:从远处看，它们非常简单。神经网络是实现功能的机器。例如，想象一个获取云覆盖范围并输出下雨几率的网络。一旦你用历史数据训练了它，这个神经网络就是一个函数，它接受一个数字并返回另一个数字。你可以在一张纸上画出这个函数，结果会和神经网络本身一样好。很简单！</p><p id="8653" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当你看得更近时，事情变得令人困惑。神经网络中有如此多的部分，很难看出它们是如何组合在一起构成最终功能的。隐藏节点计算什么样的中间函数？激活功能的作用是什么？即使是疲惫的研究人员也很难理解大型网络中的每个神经元在做什么。</p><p id="d6eb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">谢天谢地，较小的网络更容易被搜索到。如果你走过一个三层的神经网络，你就能理解它的全部。这就是我们要做的。我们将逐步通过一个神经网络，看看它的各个部分是如何对端到端功能做出贡献的。</p><p id="1a43" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们从一个小数据集开始:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/770abc6369b0fa178d1ab2a65bbb8490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*5DSkwlc58tw4CVOS47N00A.png"/></div></figure><p id="1cb5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个数据集包含两个类:绿色三角形和蓝色正方形。从现在开始，我们将用值1编码绿色三角形，用0编码蓝色正方形。</p><p id="3f49" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们有一个分类器——例如，一个神经网络——根据这些数据进行训练。我们不关心这里的训练部分。我们关心的是:在训练之后，分类器实现了一个函数。该函数接受一个点，并返回两个类中的一个:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/b3afaafa4f70ce6caa64e63468bb964f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByRJPXs_8XIlLLgSSV2TYA.png"/></div></div></figure><p id="1ddd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们来详细看看这个分类器。然而，在我们转向神经网络之前，让我们先来看一个更简单的分类器:感知器。</p><h1 id="5d4c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">穿过感知器</h1><p id="78c4" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">让我们回顾一下感知器是什么样子的:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mg"><img src="../Images/986a9d7d5f2ee6e0e410043756a0d179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NEUEYWrsbZsbvQ5NRR6wKA.png"/></div></div></figure><p id="8792" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感知器的输入是一个点的坐标(A和B)，加上一个固定值为1的“偏差”输入。输出是一个介于0和1之间的数字。接近1的数字表示“蓝色正方形”，接近0的数字表示“绿色三角形”。</p><p id="a535" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当感知器进行预测时，它首先计算中间节点<em class="mh"> z </em>的值。它将输入乘以权重，然后将结果相加，如下所示:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mi"><img src="../Images/803f49df052db894066bd588c045807f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5wkairzR-aZcYxnpPphww@2x.png"/></div></div></figure><p id="1d91" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你习惯于代数，你会认识到这是一个平面的方程。如果你不懂代数，不用担心:我们可以画出这个函数，看看它是什么样子。我根据上面的数据训练了一个感知器，并在训练后绘制了<em class="mh"> z </em>的值:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/d845e16963e72e886c7491ff5905dfc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u68WgdphOGKvQ7Tl_M6iTw.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">输入的加权和，包括偏差输入。</figcaption></figure><p id="0085" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">明确地说，这是一个平面，因为我们使用的数据有两个输入和一个输出，总共是三维的。随着维度的增加，这个平面将会变成一个令人费解的高维线性形状。尽管如此，核心思想在任何维度上都是一样的:感知机的前半部分生成一个“直”的形状。</p><p id="c427" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感知器的另一半通过一个s形驱动那个形状，产生输出<em class="mh"> ŷ </em>。这是ŷ的样子:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/4a5bcb63a7a49255cab41ac5a631bb15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mm3Ea7B-Z1gY28X1Ns0UfQ.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">加权和的sigmoid。</figcaption></figure><p id="3240" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">s形弯曲<em class="mh"> z </em>，将它变成值在0和1之间的曲线曲面。更接近0的值被归类为蓝色正方形，更接近1的值被归类为绿色三角形。为了得到一个直截了当的预测，我们可以将<em class="mh"> ŷ </em>四舍五入到最接近的整数。高于0.5的值变成1，低于0.5的值变成0——就像这样:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/39e6c350b40bfc598b71c04989f75e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1y4sy1h9Wqu6iEn4DS2Uw.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">感知器的输出，四舍五入到最接近的整数。</figcaption></figure><p id="eafd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">等等——让我在水平面上挤压这个函数，使它更具可读性:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/2942cf460ffba2f3624af959f88ead04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*FjZjHK20Xh-TJ_g9KfHCmA.png"/></div></figure><p id="014d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就是当我们使用感知机对数据进行分类时得到的结果:一个直接的“决策边界”。边界一侧的点被归类为绿色三角形。另一边的点被归类为蓝色方块。</p><p id="1269" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">概括一下，这是感知器及其内部功能的示意图:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/2e6c35fcc5a84cc59db533515c572742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCPUj8KilOZ9z_uZUgbahQ.png"/></div></div></figure><p id="1573" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你熟悉感知机，你就会知道它们有一个严重的缺点。我们刚刚描述的过程会产生一个直接的决策边界。不幸的是，并不是所有的数据集都可以用一条直线整齐地划分。例如，看看这些数据:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/3e5eed087ff5cbfcdfc987f00bcb209e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*ybXKXH2PtpjZyk2QWrxfJw.jpeg"/></div></figure><p id="382a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这一次，我们需要一个弯曲的边界来区分蓝色正方形和绿色三角形。感知器无法绘制弯曲的边界，但神经网络可以。</p><h1 id="5a32" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">升级到神经网络</h1><p id="e4a2" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">让我们从感知器升级到神经网络。作为第一步，我们可以用一堆感知器并行运行它们。例如，假设我们想要一个三层神经网络，其中三个节点位于隐藏层。为此，我们需要并行运行三个感知器，如下所示:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mp"><img src="../Images/9fda23484965b41bdcb89e77626f04ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5FOHuLU15ScTGUrkTj5L4w.png"/></div></div></figure><p id="eede" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你看到三个感知器了吗？它们重叠是因为它们共享相同的输入。另一方面，每个感知器都有自己的权重，所以它输出不同的值<em class="mh"> z. </em>这三个<em class="mh"> z </em>依次产生三个输出。我把输出称为<em class="mh"> h </em>，因为它们将是我们神经网络的隐藏节点。</p><p id="b78e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们处理了神经网络的前半部分。为了完成它，我们需要用一个输出层来结束它，就像这样:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/1f4c94d67281fcad57fc5b460d1615a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkIwggKIieSZVixrRDUZtw.png"/></div></div></figure><p id="307d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如今，大多数神经网络会用ReLU来代替乙状结肠。我懒得做那件事。ReLU可能有不同的形状，但其核心思想与sigmoid相同:将平面变成非线性形状。</p><p id="7f2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">说到激活功能，最后的softmax有些大材小用了。记住softmax是如何工作的:它接受一组输入(“逻辑”)并调整它们的大小，使它们的总和为1。在只有两个输出的情况下，就像在这种情况下，每个输出都是1减去另一个——一旦我们知道了一个，我们也就知道了另一个。所以你可能会想:为什么我们需要冗余输出？难道我们不能用一个更简单的函数来代替softmax，返回一个介于0和1之间的值，比如sigmoid？</p><p id="f332" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">是的，我们可以。在这种情况下，我承认我用了一个softmax来保持传统。大多数神经网络示例的末尾都有一个softmax，我想让这个网络感觉熟悉一些。为了弥补两个冗余输出，我们只考虑其中一个，忽略另一个。</p><p id="db27" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">撇开softmax的题外话，现在我们有了一个神经网络。让我们看看里面发生了什么。</p><h1 id="598d" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">可视化网络的功能</h1><p id="2730" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">你已经看到我们神经网络的前半部分和三个并联的感知器是一样的。这意味着网络隐藏节点中的函数看起来像感知器的输出。但是不要相信我的话:让我们看看它们。</p><p id="b242" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我又一次在实践中尝试了这个实验。我训练了一个类似上面的神经网络，并绘制了其隐藏节点的内容:</p><div class="mb mc md me gt ab cb"><figure class="mr jr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/a620e35ca088f669c7487b345c949693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*jZJmo7JMb_qm2WRa2RthKA.png"/></div></figure><figure class="mr jr mx mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/63656a3f1512359e3d724eab1c3fbabc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Wa51Ohok6wDGFYktrslKGA.png"/></div></figure></div><div class="ab cb"><figure class="mr jr my mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/0e5f04ce7b3ee54281badd639ee2ff05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vVdG4FXPkkWmLDUe2-RXtw.png"/></div></figure><figure class="mr jr my mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/69d278434f1b910f6f195b2a93130f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*0DSMxwiadUgjlv0gSOUJLQ.png"/></div></figure></div><p id="d0a6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">看到了吗？来自前一层的三个节点具有明显的sigmoid-y形状。唯一突出的节点是附加偏置节点，它的固定值为1-水平面。</p><p id="3fd7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，网络将logits计算为隐藏节点的加权和，计算如下:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/e749281ef1b0c16ac67a009787eca07c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oP3EiNk--iAjysJqnqcCfQ.png"/></div></div></figure><p id="55f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于有两个logits，这个网络用不同的权重计算两次。然而，我们决定忽略网络的一个输出——因此我们也可以忽略它的匹配逻辑。我们将跳过第一个逻辑，专注于第二个。下面是它的功能图:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/837b01208aad175f33651b1d8669bf2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jupw_c-r_V2N0GIMHd-Hag.png"/></div></div></figure><p id="99e9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你对实际的数值感兴趣，这些是我在实验中得到的重量:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/fa76b1e44ffe204beee9e28a1a3dac35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RlT19JdITLGDB7CZYB1xhA.png"/></div></div></figure><p id="b47a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们快完成了。接下来，logits通过softmax得到输出，每个输出等于1减去另一个输出。我们决定关注第二个输出。这是:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/70c464291db7ab5a4efce317460a6bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CepiSPPh-1eWU2K1_dZeTw.png"/></div></div></figure><p id="ce0c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">而这就是整个神经网络实现的功能。当它接近0时，该点被分类为蓝色正方形。当它接近1时，该点被分类为绿色三角形。像我们对感知器所做的一样，让我们将函数四舍五入到最接近的整数:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/ca22886b5cb5721fcfe7634080f091b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*54KusPhfCh6G-818x3_fsw.png"/></div></div></figure><p id="402f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们可以将函数压缩到二维来查看决策边界</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/7ba19a70dec5f014a8892c3d8afad551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*FmAnefupQpR05Whl-9wk_w.png"/></div></figure><p id="137f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">向我们的神经网络致敬:这是区分蓝色正方形和绿色三角形的一个不错的机会。仍然有一些错误分类的点——但是考虑到这个网络只有三个隐藏节点。多几个隐藏节点可能会使边界更容易弯曲，分类也更好。</p><p id="50d2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">总结一下:</p><ol class=""><li id="afc1" class="nb nc iq ka b kb kc kf kg kj nd kn ne kr nf kv ng nh ni nj bi translated">神经网络的前半部分产生sigmoid-y函数，就像感知器一样。</li><li id="4f13" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">第二部分使用这些函数，加上一个偏差，来计算一个logit——一个更复杂的函数，有峰有谷。</li><li id="a337" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">logit通过一个s形线，生成一个近似于数据点的表面。</li></ol><p id="8c3e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是神经网络后半部分的视觉回顾:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/8ed7cb377d921a58904badc2ef51dcae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AMiRHBv7jdEOmAk0f_DJ-Q.png"/></div></div></figure><p id="f08a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就是三层神经网络如何产生巧妙的弯曲决策边界。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/51cd52c93786d2dc6dfa3debd1cd0908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96g8PQjTFSJPBv6KCz5hzQ.jpeg"/></div></div></figure></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ae815976d39b730ca0b1dc526066865d.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*CLGEpC-MfhtV-7XY.png"/></div></figure><p id="6838" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mh">本帖是</em> <a class="ae kw" href="http://www.progml.com/" rel="noopener ugc nofollow" target="_blank">编程机器学习</a> <em class="mh">的衍生，我的机器学习零到英雄入门，从基础到深度学习。前往</em> <a class="ae kw" href="http://www.pragprog.com/titles/pplearn" rel="noopener ugc nofollow" target="_blank"> <em class="mh">此处</em> </a> <em class="mh">为电子书，</em> <a class="ae kw" href="https://www.amazon.com/gp/product/1680506609/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=ductyp-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1680506609&amp;linkId=21357a11b4a7bc9be95476540d1d3a09" rel="noopener ugc nofollow" target="_blank"> <em class="mh">此处</em> </a> <em class="mh">为纸质书，如有疑问或评论，欢迎来到</em> <a class="ae kw" href="https://forum.devtalk.com/tag/book-programming-machine-learning" rel="noopener ugc nofollow" target="_blank"> <em class="mh">论坛</em> </a> <em class="mh">！</em></p></div></div>    
</body>
</html>