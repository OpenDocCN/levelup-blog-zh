<html>
<head>
<title>How to build a scraping project with Scrapy and MongoDB</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Scrapy和MongoDB构建一个抓取项目</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/how-to-build-a-scraping-project-with-scrapy-and-mongodb-46e78b6549e3?source=collection_archive---------1-----------------------#2022-02-12">https://levelup.gitconnected.com/how-to-build-a-scraping-project-with-scrapy-and-mongodb-46e78b6549e3?source=collection_archive---------1-----------------------#2022-02-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bce9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">成为Python中真正的“蜘蛛侠”</h2></div><p id="e6ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Scrapy是一个多功能的强大的网络抓取框架，可以用来抓取网站并从网页中提取结构化数据。适用于需要抓取很多网站或者很多页面的大型抓取项目。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/1122db30a4b238bdc0d40fa9eb52a666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FuxltT3FGMD3KyaI.jpg"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">图片来自<a class="ae le" href="https://pixabay.com/illustrations/spiderman-character-superhero-5561671/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>。</figcaption></figure><p id="2aa9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将演示如何使用Scrapy来抓取整个<a class="ae le" href="http://quotes.toscrape.com/" rel="noopener ugc nofollow" target="_blank">行情网站</a>并抓取所有作者的所有行情。提取的数据将存储在MongoDB中，因为提取的标签是可变长度的字符串列表，这更适合MongoDB这样的文档数据库。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9319" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在开始之前，我们需要安装本教程所需的软件包。建议<a class="ae le" href="https://medium.com/codex/how-to-create-virtual-environments-with-venv-and-conda-in-python-31814c0a8ec2" rel="noopener">创建一个虚拟环境</a>并在那里安装软件包，这样它们就不会搞乱系统库。为简单起见，我们将使用<a class="ae le" href="https://medium.com/codex/how-to-create-virtual-environments-with-venv-and-conda-in-python-31814c0a8ec2" rel="noopener"> <em class="mc"> conda </em> </a>来创建虚拟环境。我们需要安装用于网页抓取的<em class="mc"> Scrapy </em>包，以及包含MongoDB使用工具的库<em class="mc"> pymongo </em>。</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="462f" class="mi mj it me b gy mk ml l mm mn">(base) $ <strong class="me iu">conda create --name scrapy python=3.10</strong><br/>(base) $ <strong class="me iu">conda activate scrapy</strong><br/>(scrapy) $ <strong class="me iu">pip install -U Scrapy==2.5.1 pymongo==4.0.1</strong></span></pre><p id="0a39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们将把抓取的数据存储在MongoDB中，所以我们需要一个可用的MongoDB服务器。实际上，你通常会有一个专用的MongoDB服务器，比如由<a class="ae le" href="https://medium.com/codex/how-to-use-mongodb-atlas-to-manage-your-server-and-data-d97a6e7663c5" rel="noopener"> MongoDB Atlas </a>托管的服务器。在这项工作中，我们将在Docker 容器中启动一个<a class="ae le" href="https://hub.docker.com/_/mongo" rel="noopener ugc nofollow" target="_blank"> MongoDB服务器:</a></p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="734f" class="mi mj it me b gy mk ml l mm mn">$ docker network create mongo-net</span><span id="8a17" class="mi mj it me b gy mo ml l mm mn">$ docker run --detach --network <!-- -->mongo-net<!-- --> --name mongo-server \<br/>    --env MONGO_INITDB_ROOT_USERNAME=admin \<br/>    --env MONGO_INITDB_ROOT_PASSWORD=pass \<br/>    --env <!-- -->MONGO_INITDB_ROOT_DATABASE=admin \<br/>    --volume mongo-data:/data/db \<br/>    --publish 27017:27017 \<br/>  <!-- -->  mongo:5.0.6</span></pre><p id="0f10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MongoDB服务器已经启动，我们现在可以创建数据库并在其中插入文档。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="b435" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本帖中，我们将使用我们的老朋友<a class="ae le" href="http://quotes.toscrape.com/" rel="noopener ugc nofollow" target="_blank">quotes.toscrape.com</a>进行演示。我们可以使用其他网站进行演示，但这不会更有趣，因为我们感兴趣的是抓取技术，而不是网站本身。quotes.toscrape.com非常简单，在我们之前的文章中已经使用过。因此，我们应该更熟悉网站的结构，以及要使用的标签和XPaths。</p><p id="d626" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于大的抓取项目，我们应该将蜘蛛和配置文件放在一个<strong class="kk iu"> <em class="mc">项目</em> </strong>中，该项目将所有相关模块放在一起。我们可以使用<code class="fe mp mq mr me b">scrapy</code>命令行工具来创建一个抓取项目:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="d8f9" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">scrapy startproject scraping_proj</strong></span></pre><p id="bbf8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如控制台中所提示的，我们现在可以进入项目文件夹，然后生成一个蜘蛛:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="37fd" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">cd scraping_proj</strong><br/>$ <strong class="me iu">scrapy genspider quotes quotes.toscrape.com</strong></span></pre><ul class=""><li id="8631" class="ms mt it kk b kl km ko kp kr mu kv mv kz mw ld mx my mz na bi translated"><code class="fe mp mq mr me b">quotes</code> —要创建的蜘蛛的名称。</li><li id="20e6" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b">quotes.toscrape.com</code> —将被蜘蛛抓取的网站。</li></ul><p id="10df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以给你的蜘蛛取任何名字。对于良好的做法，它应该是描述性的，并反映网站和项目，你是刮。</p><p id="6f6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们用Linux中的<code class="fe mp mq mr me b">tree</code>命令检查项目文件夹的结构:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="d801" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">tree . -I __pycache__</strong></span></pre><p id="2d66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mp mq mr me b">-I</code>选项忽略匹配指定模式的文件。这里我们想忽略包含编译后的Python字节码的<code class="fe mp mq mr me b">__pycache__</code>文件夹。这是项目文件夹的结构:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="266b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在不需要接触大部分文件，但是对它们有一个大致的了解是有好处的:</p><ul class=""><li id="eed8" class="ms mt it kk b kl km ko kp kr mu kv mv kz mw ld mx my mz na bi translated"><code class="fe mp mq mr me b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/commands.html#configuration-settings" rel="noopener ugc nofollow" target="_blank">scrapy.cfg</a></code> —项目的配置文件。通常，它只包含设置文件的位置以及如何部署项目的配置。<code class="fe mp mq mr me b">scrapy.cfg</code>文件所在的目录被称为项目根目录。</li><li id="dde5" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b">scraping_proj</code> —在项目根目录下，有一个项目文件夹(这里是<code class="fe mp mq mr me b">scraping_proj</code>)，所有与抓取相关的模块都在这个文件夹下。</li><li id="8cd6" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b">spiders</code> —重要的是，项目文件夹里面有一个<code class="fe mp mq mr me b">spiders</code>文件夹，所有的蜘蛛都在里面。</li><li id="d350" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b">items.py</code> —定义提取数据格式的模块。这不是必须的，但是为要提取的数据定义项目类型是一个很好的做法，这样数据就可以标准化并且不会包含随机字段。</li><li id="c487" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b">pipelines.py </code> —定义如何处理报废物品的模块。可以有多个管道以定义的顺序处理项目。</li><li id="5f4e" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html" rel="noopener ugc nofollow" target="_blank">middlewares.py</a> </code> —定义钩子框架的模块，可以为零碎的请求和响应添加自定义功能。</li><li id="0239" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b">settings.py</code> —项目设置文件。我们可以在这个文件中定义中间件和管道。此外，我们可以定义一些环境或系统变量，这些变量可以被所有的蜘蛛或管道使用。</li></ul><p id="4f56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">稍后当我们将抓取的数据写入MongoDB时，我们将更新<code class="fe mp mq mr me b">items.py</code>、<code class="fe mp mq mr me b">pipelines.py</code>和<code class="fe mp mq mr me b">settings.py</code>。现在，让我们为<code class="fe mp mq mr me b">quotes.py</code>中的第一个蜘蛛添加代码:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">quotes.py</figcaption></figure><p id="4c41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不熟悉Python中的<a class="ae le" href="https://medium.com/codex/how-to-use-the-assignment-expression-walrus-operator-in-python-13811859f80f" rel="noopener"> walrus操作符(:=) </a>，你可能会发现<a class="ae le" href="https://medium.com/codex/how-to-use-the-assignment-expression-walrus-operator-in-python-13811859f80f" rel="noopener">这篇文章</a>很有帮助。此外，如果你想知道更多关于<a class="ae le" href="http://quotes.toscrape.com" rel="noopener ugc nofollow" target="_blank">quotes.toscrape.com</a>网站和使用的HTML元素和XPaths的信息，请查看<a class="ae le" href="https://medium.com/codex/simple-web-scraping-using-requests-beautiful-soup-and-lxml-in-python-4f5903c67db2" rel="noopener">这篇文章</a>。</p><p id="8019" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以用<code class="fe mp mq mr me b">scrapy crawl</code>爬这只蜘蛛了:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="12d5" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">scrapy crawl quotes</strong></span></pre><p id="92a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">干杯！数据被成功抓取并打印在控制台中。如果我们想将数据存储在JSON文件中，我们可以在命令行上添加<code class="fe mp mq mr me b">-o</code>选项:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="8403" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">scrapy crawl quotes -o quotes_all.json</strong></span></pre><p id="e518" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">抓取的数据现在将存储在<code class="fe mp mq mr me b">quotes_all.json</code>中。但是数据还是打印在控制台里，现在挺吵的。如果您仔细检查刮擦日志，您会注意到刮擦数据记录在<a class="ae le" href="https://lynn-kwong.medium.com/stop-using-print-in-your-python-code-for-logging-use-the-logging-module-like-a-pro-66fb0427d636" rel="noopener">记录模块</a>的<code class="fe mp mq mr me b">DEBUG</code>模式中。我们可以使用<code class="fe mp mq mr me b">-L</code>选项来更改日志记录级别。让我们将日志记录级别更改为<code class="fe mp mq mr me b">INFO</code>:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="4c61" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">scrapy crawl quotes -o quotes_all.json -L INFO</strong></span></pre><p id="0664" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意<code class="fe mp mq mr me b">-L</code>和<code class="fe mp mq mr me b">INFO</code>都应该是大写的。这一次，数据不在控制台中打印，但仍存储在<code class="fe mp mq mr me b">quotes_all.json</code>中。如果不相信，可以删除这个文件，重新运行上面的命令。</p><p id="faaa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你已经注意到的，在<code class="fe mp mq mr me b">start_requests()</code>方法中，我们检查蜘蛛是否有标签属性，如果有，我们将只抓取带有特定标签的引号。在命令行上，可以使用<code class="fe mp mq mr me b">-a</code>选项将属性传递给蜘蛛。您可以将任何属性传递给蜘蛛。但是，如何使用这些自定义属性取决于您。</p><p id="ed1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们刮掉所有带有标签<code class="fe mp mq mr me b">love</code>的引文:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="69d7" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">scrapy crawl quotes -o quotes_love.json -L INFO -a tag=love</strong></span></pre><p id="7303" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自定义属性用<code class="fe mp mq mr me b">-a NAME=VALUE</code>格式指定。您可以多次重复<code class="fe mp mq mr me b">-a NAME=VALUE</code>来指定多个属性。上面的命令应该只抓取带有<code class="fe mp mq mr me b">love</code>标签的引号。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="af7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面我们已经成功地抓取了报价并保存在一个JSON文件中。在这一节中，我们将学习如何在MongoDB中存储抓取的数据，这是一个非常流行的用于存储文档的NoSQL数据库，在Python中通常指JSON对象或字典。</p><p id="92a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于搜集到的报价主要包含文本而非数字数据，因此将它们存储在NoSQL数据库中比存储在MySQL之类的关系数据库中更方便。此外，<code class="fe mp mq mr me b">tags</code>字段对于不同的引号有不同数量的字符串，这使得MongoDB成为一个更好的解决方案，因为MongoDB擅长存储<a class="ae le" href="https://www.mongodb.com/unstructured-data" rel="noopener ugc nofollow" target="_blank">非结构化数据</a>。</p><p id="7350" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先为报价创建一个<code class="fe mp mq mr me b">Item</code>类。如上所述，项目类别在<code class="fe mp mq mr me b">items.py</code>模块中定义。定义了一个<code class="fe mp mq mr me b">Item</code>类后，哪些字段将被删除就更清楚了:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">items.py</figcaption></figure><p id="06e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们需要更新蜘蛛以产生数据作为<code class="fe mp mq mr me b">QuoteItem</code>，而不是现在的字典:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">quotes.py</figcaption></figure><p id="e6f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mp mq mr me b">quotes.py</code>的整个模块可以在这里找到<a class="ae le" href="https://gist.github.com/lynnkwong/564ef4b3123695b0e1a2bc5020aa9d53" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="2b37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为一个<code class="fe mp mq mr me b">Item</code>类，<code class="fe mp mq mr me b">QuoteItem</code>像一个普通的类一样工作，我们需要创建并产生一个它的实例，使用抓取的数据作为属性。</p><p id="a10f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您再次运行蜘蛛，您可以看到它完全像以前一样工作。您可以尝试上面演示的所有不同的命令行选项，结果将是相同的。</p><p id="c96e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想以不同的方式处理和保存抓取的数据，我们应该创建一个定制的项目管道。每一个被抓取的物品都将被发送到物品管道并在那里被处理。在本教程中，我们将只创建一个管道。在后面的教程中，我们将创建多个管道，并了解它们如何按照指定的顺序处理项目。</p><p id="d674" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上所述，项目管道在<code class="fe mp mq mr me b">pipelines.py</code>模块中定义。然而，当定义了多个管道时，我们会想要创建一个<code class="fe mp mq mr me b">pipelines</code>文件夹，并将管道定义模块存储在其中，以便更好地组织代码。本教程中的<code class="fe mp mq mr me b">pipelines.py</code>模块代码如下:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">管道. py</figcaption></figure><p id="344c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于Scrapy要求的管道的特殊方法的解释，请查看<code class="fe mp mq mr me b">pipelines.py</code>中的文档字符串。现在请不要运行蜘蛛，因为我们还没有完成。我们需要添加项目管道到<code class="fe mp mq mr me b">settings.py</code>。此外，正如您在上面的代码中看到的<code class="fe mp mq mr me b">from_crawler()</code> <a class="ae le" href="https://medium.com/codex/how-to-use-the-magical-staticmethod-classmethod-and-property-decorators-in-python-e42dd74e51e7" rel="noopener">类方法</a>一样，我们也需要在<code class="fe mp mq mr me b">settings.py</code>中为MongoDB定义一些设置:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">settings.py</figcaption></figure><p id="65ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mp mq mr me b">settings.py</code>注意事项:</p><ul class=""><li id="4656" class="ms mt it kk b kl km ko kp kr mu kv mv kz mw ld mx my mz na bi translated">有些字段是Scrapy自动生成的，如<code class="fe mp mq mr me b">BOT_NAME</code>、<code class="fe mp mq mr me b">SPIDER_MODULES</code>等，不需要修改。</li><li id="69d1" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><code class="fe mp mq mr me b">settings.py</code>中有很多注释，为设置配置提供了有用的指导和参考。如果您喜欢更简洁的模块，可以安全地删除它们。</li><li id="8a58" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated">我们将创建的项目管道添加到<code class="fe mp mq mr me b">ITEM_PIPELINES</code>字典中。键是项目管道类<code class="fe mp mq mr me b">MongoDBPipeline</code>的相对路径，值是管道运行的顺序:项目从较低值到较高值的类。因为我们这里只有一个管道，所以你给它什么值并不重要。300是注释中的默认值，因此保留为默认值。</li><li id="54ac" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated">我们还为日志定义了一些全局设置，包括默认的日志级别、日志格式和日志文件。</li><li id="e233" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated">最后，在<code class="fe mp mq mr me b">settings.py</code>中还指定了MongoDB的一些配置，以便在需要时可以方便地导入。所有实现<code class="fe mp mq mr me b">from_crawler()</code> <a class="ae le" href="https://medium.com/codex/how-to-use-the-magical-staticmethod-classmethod-and-property-decorators-in-python-e42dd74e51e7" rel="noopener">类方法</a>的类都可以访问这些配置。</li></ul><p id="1efa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们再次运行蜘蛛:</p><pre class="lg lh li lj gt md me mf mg aw mh bi"><span id="5192" class="mi mj it me b gy mk ml l mm mn">$ <strong class="me iu">scrapy crawl quotes</strong></span></pre><p id="fc53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一次蜘蛛是无声无息地爬行，控制台里没有任何日志。这是因为日志现在保存在一个日志文件中，这就是<code class="fe mp mq mr me b">/tmp/scrapy.log</code>。请检查此日志文件，确保其中没有错误。如果有，您可以根据错误相应地更新代码。您可能会发现<a class="ae le" href="https://medium.com/codex/how-to-debug-python-scripts-and-api-code-in-the-console-and-in-vs-code-a0b825ad7d41" rel="noopener"> PDB </a>对调试蜘蛛很有帮助。</p><p id="40dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们检查数据是否保存在MongoDB中。在激活<code class="fe mp mq mr me b">scrapy</code>环境的情况下，在控制台中运行以下Python代码:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="0302" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">干杯！数据已成功保存在MongoDB中。</p><p id="07aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，如果您多次运行蜘蛛。退回的文件数量也会成倍增加。这是因为在这个例子中，我们没有文档的唯一密钥。实际上，通常有一个唯一的源id，可以用来唯一地标识正在被刮擦的物品。如果没有，您可以<a class="ae le" href="https://medium.com/codex/understand-the-encoding-decoding-of-python-strings-unicode-utf-8-f6f97a909ee0" rel="noopener">对抓取的数据进行哈希处理</a>并为文档生成一个唯一的密钥。然后，您可以使用带有<code class="fe mp mq mr me b">upsert</code>选项的<code class="fe mp mq mr me b"><a class="ae le" href="https://docs.mongodb.com/manual/reference/method/db.collection.updateOne/" rel="noopener ugc nofollow" target="_blank">update_one</a></code>方法来插入或更新文档。将会有一篇关于如何使用MongoDB的文章。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9a41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">恭喜，你已经走到这一步了。您已经学习了如何使用Scrapy创建一个scraping项目，以及如何将数据保存到MongoDB。你现在可以使用Scrapy来抓取你感兴趣的网站。请注意，您只能抓取带有普通HTML代码的网页。对于那些用JavaScript代码动态渲染的，需要使用一些特殊的工具，比如<a class="ae le" href="https://medium.com/codex/how-to-scrape-javascript-webpages-using-proxycrawl-in-python-a4de7182d996" rel="noopener"> ProxyCrawl </a>或者<a class="ae le" href="https://medium.com/codex/how-to-scrape-javascript-webpages-using-selenium-in-python-21d56731bb1f" rel="noopener"> Selenium </a>。请查阅相应的文章以供参考。在接下来的文章中，我们将讨论一些更高级的主题，比如使用代理抓取和缓存响应，这对您的应用程序很有帮助。</p><p id="b979" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章中展示的代码的GitHub库可以从<a class="ae le" href="https://github.com/lynnkwong/scrapy-quotes-demo" rel="noopener ugc nofollow" target="_blank">这里</a>克隆。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="96ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相关文章:</p><ul class=""><li id="1b34" class="ms mt it kk b kl km ko kp kr mu kv mv kz mw ld mx my mz na bi translated"><a class="ae le" href="https://medium.com/codex/simple-web-scraping-using-requests-beautiful-soup-and-lxml-in-python-4f5903c67db2?source=your_stories_page----------------------------------------" rel="noopener">使用Python中的请求、漂亮的汤和lxml进行简单的Web抓取</a></li></ul></div></div>    
</body>
</html>