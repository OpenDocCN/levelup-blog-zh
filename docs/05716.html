<html>
<head>
<title>Create Custom AR Filters With Vonage Video API and DeepAR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Vonage Video API和DeepAR创建自定义AR滤镜</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/create-custom-ar-filters-with-vonage-video-api-and-deepar-5f38ed3a43bf?source=collection_archive---------20-----------------------#2020-09-24">https://levelup.gitconnected.com/create-custom-ar-filters-with-vonage-video-api-and-deepar-5f38ed3a43bf?source=collection_archive---------20-----------------------#2020-09-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8cde63239410aca8e317f33bcd4f1c62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SZcU1JcUqU6kIdoXjto3AA.png"/></div></div></figure><p id="7c56" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用人工智能的增强是视频通话的未来。它将帮助我们打破(虚拟)坚冰，帮助导航互动视频中的新兴用例，如购物和远程协助，并创造全新的体验。这些体验的未来发明者可能正“坐”在虚拟教室里，由虚拟视频塑造，前无古人。</p><p id="37e3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">作为Vonage视频API开发人员，您可以将我们的视频API与<a class="ae kw" href="https://www.deepar.ai/" rel="noopener ugc nofollow" target="_blank"> DeepAR SDK </a>集成，以丰富您用户的视频参与体验。</p><p id="a8a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以通过抽象背景(替换、模糊、像素化对象)来使视频通话更加专业，或者通过改善照明和微妙的肤色效果来改善用户体验。</p><p id="028f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">或者从DeepAR的各种动画滤镜中获得更多乐趣。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kx"><img src="../Images/0ea973b1b268abec73de11739bfe648d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o9WAKBBLcvymD1Oz"/></div></div></figure><h1 id="2b01" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">我们日常生活中的增强现实</h1><p id="a22a" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">你很有可能已经参加过一次视频会议，无论是工作会议、医生会诊还是家庭聚会。</p><p id="8a8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这种新的以视频为中心的会议模式中，许多新的应用可以增加视频通信的价值。让我们回顾一下增强被用来改善用户体验和帮助我们更有效地进行远程交互的几种常见方式:</p><h2 id="0bfc" class="mf ld iq bd le mg mh dn li mi mj dp lm kj mk ml lq kn mm mn lu kr mo mp ly mq bi translated">背景抽象</h2><p id="f7ee" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">视频通信的改进之一是通过简单的背景模糊、像素化或替换功能来抽象用户背景中的一切。好处包括:</p><ul class=""><li id="f3aa" class="mr ms iq ka b kb kc kf kg kj mt kn mu kr mv kv mw mx my mz bi translated">启用呼叫者隐私</li><li id="48b0" class="mr ms iq ka b kb na kf nb kj nc kn nd kr ne kv mw mx my mz bi translated">通过消除干扰来促进观众参与</li><li id="7848" class="mr ms iq ka b kb na kf nb kj nc kn nd kr ne kv mw mx my mz bi translated">创造品牌体验</li></ul><h2 id="7d88" class="mf ld iq bd le mg mh dn li mi mj dp lm kj mk ml lq kn mm mn lu kr mo mp ly mq bi translated">扬声器增强</h2><p id="56ee" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">另一个通过AR将视频通话提升到更高水平的例子是对前景扬声器应用过滤器。实施这些类型的过滤器可以让您的用户:</p><ul class=""><li id="2b2e" class="mr ms iq ka b kb kc kf kg kj mt kn mu kr mv kv mw mx my mz bi translated">借助光线、肤色和美颜滤镜，轻松进行视频通话</li><li id="3c4c" class="mr ms iq ka b kb na kf nb kj nc kn nd kr ne kv mw mx my mz bi translated">用有趣的敌意来取悦和娱乐</li><li id="cfa3" class="mr ms iq ka b kb na kf nb kj nc kn nd kr ne kv mw mx my mz bi translated">与3D对象甚至虚拟人脸绘画进行实时交互</li></ul><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9d4c7d18474281284ab4ec9aef82d7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/0*J_8CAFnlzBC1o3ey"/></div></figure><p id="6ff5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Vonage视频API的灵活性使其在与DeepAR等AR合作伙伴集成时成为可能。在这篇博文中，我们将展示如何结合使用这两种API来实现上述用例。</p><h1 id="ed79" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">DeepAR是什么？</h1><p id="c17c" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">DeepAR是一个移动和HTML5优化的AR引擎，用于增强现实、人脸过滤器、背景分割、化妆、美容过滤器和animoji。</p><p id="8e81" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://www.deepar.ai/augmented-reality-sdk" rel="noopener ugc nofollow" target="_blank"> DeepAR SDK </a>让开发者可以访问DeepAR引擎，管理跨设备支持，并附带一个DeepAR Studio工具来帮助你创建AR资产。</p><h2 id="e62d" class="mf ld iq bd le mg mh dn li mi mj dp lm kj mk ml lq kn mm mn lu kr mo mp ly mq bi translated">使用DeepAR</h2><p id="23f1" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">要将DeepAR SDK框架添加到您的iOS应用程序中，您需要从他们的网站<a class="ae kw" href="https://developer.deepar.ai/" rel="noopener ugc nofollow" target="_blank">https://developer.deepar.ai/</a>下载zip文件。一旦有了zip文件，就需要解压缩它，并将框架文件拖到Xcode项目中。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/c970686f08a0c97b7faab9bc9b92f6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YYcbfEIkPMH66J5a"/></div></div></figure><p id="15dc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不要忘记将框架的<code class="fe nh ni nj nk b">embed</code>设置为“嵌入&amp;符号”，否则应用程序在运行时可能找不到库。</p><p id="129b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦你有了这个设置，下一步就是去<a class="ae kw" href="https://developer.deepar.ai/" rel="noopener ugc nofollow" target="_blank">https://developer.deepar.ai/</a>创建一个项目来获得你以后需要的应用密钥。</p><h1 id="8b74" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">将狮子添加到捕获的视频输入中</h1><p id="0902" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">在这篇博文中，我们将创建一个UIView，你可以在其中显示前置摄像头的内容，当检测到你的脸部时，会用一只很酷的狮子来增强。</p><p id="ffa6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你想在一个地方看到这篇文章的所有代码，你可以在<a class="ae kw" href="https://github.com/DeepARSDK/vonage-ios-swift" rel="noopener ugc nofollow" target="_blank"> DeepAR GitHub </a>上找到。</p><p id="5ff7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在代码中，你需要处理两个类；一个是DeepAR SDK的CameraController，这个类负责访问设备摄像头并从中获取图像。一旦它有了视频提要，它就把它们发送给ARView实例，这是另一个你必须实例化的类。</p><p id="a443" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">ARView负责显示应用了AR滤镜的相机的内容。ARView是一个常规的UIView子视图，因此您需要将它添加到应用程序视图层次结构中的任何父视图中。</p><p id="5af9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">了解了这一点，代码可能如下所示:</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="64a9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上面的代码中，首先创建CameraController，然后创建ARView实例，使用它作为全屏的框架，因为我们将显示它占据整个屏幕。</p><p id="9c8c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意，我们将<code class="fe nh ni nj nk b">self</code>指定为ARViewDelegate，因此您需要实现/遵守ARView delegate协议。我们以后会需要它。</p><p id="3bba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦有了这两个实例，就将CameraController实例的ARView属性分配给我们的视图，并将其插入到我们的<code class="fe nh ni nj nk b">parentView</code>中。</p><p id="683f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">剩下的唯一一件事就是通过调用<code class="fe nh ni nj nk b">startCamera</code>初始化视图并从摄像机中捕获视频。</p><p id="b91d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您运行您的代码，您会注意到没有🦁因为我们还没有激活这个效果。</p><p id="37eb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，您需要实现<code class="fe nh ni nj nk b">didInitialize</code>，ARViewDelegate的一个方法，一旦ARView完成初始化，我们将调用它，然后我们可以激活lion过滤器。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="708d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，您需要在项目中添加一个名为“lion”的文件。你可以在https://developer.deepar.ai/downloads得到的免费过滤包中找到它。下载完成后，您需要将文件“lion”拖到Xcode项目的根文件夹中，并检查它是否以您的应用程序为目标，因此它是与它捆绑在一起的。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6f016fb0a617822ef3139a81cc1ac407.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/0*IbTAb22kXWV2afGL"/></div></figure><p id="3a81" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您此时运行该应用程序，您应该会看到您的面部有一个很酷的狮子覆盖图，它会跟随您的所有动作和手势。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d7a08a3707e23468a3969db02ec2f3c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/0*U04aJN0XPgP_bj0B"/></div></figure><h1 id="1792" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">OpenTok和自定义视频驱动程序</h1><p id="6b3d" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">大多数开发人员与Vonage视频API交互的方式是通过OpenTok SDK。SDK最激动人心的功能之一是可以创建视频驱动程序，将任何类型的视频内容发送到您的发布者所连接的OpenTok会话。</p><p id="9fa9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">视频驱动程序可以分为两个不同的部分:</p><ul class=""><li id="2e98" class="mr ms iq ka b kb kc kf kg kj mt kn mu kr mv kv mw mx my mz bi translated">一方面，我们有视频捕获器，顾名思义，它必须从任何来源捕获视频(或以某种方式生成自己的视频)，并向SDK提供将要发送的视频帧。</li><li id="8912" class="mr ms iq ka b kb na kf nb kj nc kn nd kr ne kv mw mx my mz bi translated">另一方面，我们有视频渲染器，您可能已经猜到了，它负责渲染通过订阅者从OpenTok会话远程传来的视频帧。</li></ul><p id="1be0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您使用过OpenTok SDK，您可能会注意到，在典型的场景中，您不需要处理这些类。这是因为OpenTok SDK附带了默认的捕获器和渲染器，可以从默认的系统摄像头捕获视频，并使用每个平台的图形技术(无论是Metal、OpenGL还是DirectX)渲染远程视频帧。</p><p id="851b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">视频驱动非常强大。您可以创建视频驱动程序来发送任何视频内容。例如，您可以捕获任何游戏引擎的输出，并将这些视频帧发送到OpenTok SDK，并且您可以将游戏内容流式传输给OpenTok会话的任何参与者。您还可以应用任何渲染过滤器，如B&amp;W或边缘检测，使任何订户看起来都不一样。</p><h1 id="a30c" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">创建OpenTok自定义视频捕获器</h1><p id="8a89" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">虽然创建一个自定义的视频捕获器看起来是一项复杂的任务，但它可能比您所能想象的更简单。</p><p id="ca2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据您选择的语言和平台，您只需要实现一个接口、扩展一个类或者符合一个协议。虽然，正如你在上面看到的，我们在这篇文章的源代码示例中使用了swift和iOS。</p><p id="f818" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在该类中，您需要实现或覆盖一些控制捕获器生命周期的基本方法。您需要实现<code class="fe nh ni nj nk b">init</code>、<code class="fe nh ni nj nk b">startCapture</code>、<code class="fe nh ni nj nk b">stopCapture</code>、<code class="fe nh ni nj nk b">isCaptureStarted</code>方法，以及一个指定捕获器设置的方法，包括指定捕获器将要发送的视频帧的高度、宽度、帧速率和像素格式。</p><p id="c92d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">实现这些方法后，您需要向捕获器提供帧。根据平台的不同，可能会有所不同，但通常情况下，您只需要用视频帧数据调用一个方法。</p><p id="5622" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看看最初的实现是什么样子的，</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="29bb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以上是自定义捕获器的必要代码。正如您所看到的，您需要实现OTVideoCapture协议，该协议具有在需要初始化、启动或停止视频捕获器时调用的方法。有一个内部标志来知道捕获开始的时间是很常见的，这就是我们在这个基本实现中所做的。我们将在开始和停止方法中更新该标志。</p><p id="690b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你会注意到这个类继承自<code class="fe nh ni nj nk b">NSObject</code>，因为这在swift类中并不常见，swift类只是继承自Object。背后的原因是OTVideoCapture协议继承了NSObject协议，主要是因为OpenTok SDK代码库大多是Objective-C而在这种语言中，所有的东西都继承了NSObject。</p><p id="da11" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦有了这样的类，下一步就是告诉发布者，不要使用默认的捕获器，它需要使用我们的类。代码看起来像这样:</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="478e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在上面的代码片段中构建的捕获器还没有发送任何东西，因为它只包含初始化代码。如果你尝试使用我们现在拥有的，它应该工作，但你将只发布黑色框架。下一步是知道如何发送帧。</p><p id="5122" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是我们将延迟这一部分，直到我们有一些帧要发送，那些帧将是我们DeepAR视图的输出。</p><h1 id="9af4" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">集成DeepAR和OpenTok</h1><p id="c8ef" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">我们已经到了博文的最后一部分，在第一部分我们看到了如何构建一个基本的视频捕捉器，在第二部分，我们看到了如何用一个狮子的AR滤镜代替你的脸。我们的定制视频捕获器中唯一缺少的是发送视频帧，所以看起来我们已经为最后一步做好了一切准备。</p><p id="c2be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这一步，我们将把两个世界连接在一起；我们需要做几件事。首先，我们需要获得ARView正在制作的视频帧，其次，我们需要使用我们构建的视频捕获器发送它们。</p><p id="102c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一步包括从视图中获取图像。这可以通过设置ARView委托或侦听器来快速完成。当框架可用时，实现侦听器接口的委托或类将收到对其方法的调用。一旦发生这种情况，我们需要将该帧发送到OpenTok世界。</p><p id="16c3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你还记得，我们已经设置了ARView的委托，并且在它的<code class="fe nh ni nj nk b">didInitialize</code>方法中我们启用了lion过滤器，现在我们需要告诉ARView我们想要获取它的视频帧。这是使用下面的代码完成的:</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="bc9e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一行是我们在博文的第一部分中看到的，第二行是我们请求ARView用AR视图的内容调用另一个委托方法。</p><p id="653d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">做完这些之后，我们需要实现<code class="fe nh ni nj nk b">frameAvailable</code>方法，每次视频帧准备好的时候都会调用这个方法。在该方法中，我们将把它的内容发送给OpenTok capturer。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="6d04" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可能已经注意到了最后一行中的<code class="fe nh ni nj nk b">pushFrame</code>方法，这个方法是我们连接DeepAR视图和OpenTok视频捕获器的地方，如果你还记得的话，deepARCapturer是我们使用的自定义视频捕获器的名字。</p><p id="696e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这可能是博文中最复杂的部分。我们正在处理CoreImage和CoreVideo iOS框架，以获取帧的RGB信息。</p><p id="0bca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了让一切正常工作，我们需要在DeepARVideoCapturer中实现pushFrame，它看起来像这样:</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="175b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们一步一步来理解这个方法。正如你所看到的，我们把它分成了三个部分。</p><p id="9882" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第1部分(上面用//1标记)，我们构建了一个新的<code class="fe nh ni nj nk b">OTVideoFrame</code>实例。这个<code class="fe nh ni nj nk b">OTVideoFrame</code>类充当视频帧信息的容器，我们将在稍后将该帧提供给OpenTok SDK时使用它。典型地，视频帧将包含其格式信息(RGB、YUV等。)、尺寸、视频帧中的一行所占的字节数，显然还有视频帧信息。</p><p id="0306" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可能想知道为什么我们需要行字节大小，视频帧格式解释起来很复杂，并且有不同的格式，但通常，它有助于了解视频帧信息缓冲区有多大。你可以用每行的字节数乘以图像高度，你就有了。有时，行的大小不是通过将像素的大小乘以图像的宽度来计算的，因为有时会添加额外的空像素作为填充。</p><p id="dc97" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果这最后一段让你感到困惑，不要担心，你不知道在那里放什么，因为CoreVideo functions会为你返回它。</p><p id="2815" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">回到我们的核心，如你所见，我们将在每一帧中重用“容器”,考虑到这个函数每秒将被调用30次。</p><p id="9c7f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在创建了OTVideoFrame的一个实例并用所有视频帧中不变的信息填充它之后，我们可以移到第2部分(// 2)。</p><p id="4d4d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本节中，在做任何事情之前，我们锁定视频帧的内容，以防止任何其他线程在我们访问它时可以更改或删除它，我们可以通过调用<code class="fe nh ni nj nk b">CVPixelBufferLockBaseAddress</code>来做到这一点，之后，我们通过调用CVPixelBufferGetBaseAddress来获得指向帧信息的指针，最后，我们将该信息添加到我们的OTVideoFrame实例中。</p><p id="b754" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如前所述，有不同类型的视频帧格式。这里我们将使用ARGB，它通常被称为像素图像格式。这意味着缓冲区由一系列用四个值编码的像素信息组成，一个值用于Alpha通道，其他值用于R、G和b。</p><p id="988d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还有其他格式，如YUV422，它被称为平面图像格式，这是因为图像被分成不同的平面。</p><p id="66bb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们解释这个是为了理解<code class="fe nh ni nj nk b">frame.planes?.addPointer()</code>调用，因为我们使用的是ARGB，它有一个平面，这就是我们如何用帧数据填充OTVideoFrame实例。</p><p id="5244" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，有了包含视频帧信息和内容的完整OTVideoFrame实例，我们只需将其发送到OpenTok SDK，以便通过网络发送给其余的会话参与者。你可以在第3部分用<code class="fe nh ni nj nk b">videoCaptureConsumer?.consumeFrame(frame)</code>调用来实现。</p><p id="80b5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe nh ni nj nk b">videoCaptureConsumer</code>是OTVideoCapture协议的成员，该成员是在OpenTok SDK初始化<code class="fe nh ni nj nk b">OTVideoCapturer</code>时设置的，每当你想向会话发送一个视频帧时，就需要调用这个方法。</p><p id="8668" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在所有这些步骤之后，您已经为构建一个Publisher做好了一切准备，该Publisher将把DeepAR magic的内容发送到OpenTok会话，并以您狮子般的外观给每个人留下深刻印象。</p><h1 id="a4e6" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">刚刚开始</h1><p id="6420" class="pw-post-body-paragraph jy jz iq ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">Vonage Video API团队致力于在AR产品功能和合作伙伴关系方面进行投资，以确保我们的客户在构建和改善交互式视频体验时能够获得最佳的技术和专业知识。</p><p id="dbd8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随着Vonage发展我们的视频API，我们看到了对合作伙伴提供的服务的需求增加。我们很高兴能与全球优秀的技术、应用和集成公司合作。有了Vonage，您可以充分利用我们的通信API，获得更大的份额。</p><p id="9075" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可能还喜欢:</p><ul class=""><li id="acad" class="mr ms iq ka b kb kc kf kg kj mt kn mu kr mv kv mw mx my mz bi translated">教程:<a class="ae kw" href="https://www.nexmo.com/blog/2020/06/24/use-a-green-screen-in-javascript-with-vonage-video" rel="noopener ugc nofollow" target="_blank">在带有Vonage视频的Javascript中使用绿色屏幕</a></li><li id="b95e" class="mr ms iq ka b kb na kf nb kj nc kn nd kr ne kv mw mx my mz bi translated">教程:<a class="ae kw" href="https://www.nexmo.com/blog/2020/06/25/create-a-photobooth-with-vue-js-and-flask-part-1-dr" rel="noopener ugc nofollow" target="_blank">用Vue.js和Flask </a>创建一个照相亭</li><li id="3e9d" class="mr ms iq ka b kb na kf nb kj nc kn nd kr ne kv mw mx my mz bi translated">寻找合作伙伴:<a class="ae kw" href="https://www.vonage.com/partners/find-partner/#c=communications-apis&amp;g=1" rel="noopener ugc nofollow" target="_blank">通信API合作伙伴</a></li></ul></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><p id="e4b6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nw">最初发布于</em><a class="ae kw" href="https://www.nexmo.com/blog/2020/09/01/create-custom-ar-filters-with-vonage-video-api-and-deepar" rel="noopener ugc nofollow" target="_blank"><em class="nw">https://www . NEX mo . com/blog/2020/09/01/create-custom-ar-filters-with-vonage-video-API-and-deepar</em></a></p></div></div>    
</body>
</html>