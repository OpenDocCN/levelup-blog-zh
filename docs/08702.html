<html>
<head>
<title>Naive Bayes Classification in NLP Tasks from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理任务中的朴素贝叶斯分类</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/naive-bayes-classification-in-nlp-tasks-from-scratch-17ac3aab1254?source=collection_archive---------5-----------------------#2021-05-26">https://levelup.gitconnected.com/naive-bayes-classification-in-nlp-tasks-from-scratch-17ac3aab1254?source=collection_archive---------5-----------------------#2021-05-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f110" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何从头开始在NLP任务中应用朴素贝叶斯分类的演练教程。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1a79da9e96569eac6c37942f03766a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wSP1pkuA_06oLvX9.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Unsplash上的图像</figcaption></figure><p id="5d45" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本教程中，我们将获得每个单词以及整个主题行的贝叶斯得分。分数将指示主题行和/或令牌是“垃圾邮件”的可能性。你可以在这里找到数据集<a class="ae lu" href="https://drive.google.com/file/d/1A9k6fF8a1ND1v6cTIWQuPWSQMAhIZwMV/view" rel="noopener ugc nofollow" target="_blank">。我们在</a><a class="ae lu" href="https://predictivehacks.com/example-of-a-machine-learning-algorithm-to-predict-spam-emails-in-python/" rel="noopener ugc nofollow" target="_blank">垃圾邮件检测器教程</a>中使用了相同的数据集，因此可以随意比较贝叶斯方法和逻辑回归。</p><p id="5a9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">加载库</strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c651" class="ma mb it lw b gy mc md l me mf">import pandas as pd <br/>import numpy as np <br/>import re from collections <br/>import Counter import string</span></pre><h1 id="c5b6" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">理论和公式</h1><p id="7e2c" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">那么，如何训练一个朴素贝叶斯分类器呢？</p><ul class=""><li id="402e" class="nc nd it la b lb lc le lf lh ne ll nf lp ng lt nh ni nj nk bi translated">训练朴素贝叶斯分类器的第一部分是确定您拥有的类的数量。</li><li id="84d6" class="nc nd it la b lb nl le nm lh nn ll no lp np lt nh ni nj nk bi translated">您将为每个类创建一个概率。P(Dpos)是文档为正的概率。P(Dneg)是文档为负的概率。<br/>使用如下公式，并将值存储在字典中:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0eb8dd7fa1111526fb6ca1f1a8e61d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*LhgGv9KWiv7NRDCVr9nh6w.png"/></div></figure><p id="4da7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中D是文档或主题行的总数，Dpos是正SL的总数，Dneg是负SL的总数。</p><p id="a5d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">先验和对数先验</strong></p><p id="d79d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">先验概率代表目标人群中SL为正对负的潜在概率。换句话说，如果我们没有具体的信息，盲目地从人群中挑选出一个SL，那么它是正的还是负的概率是多少？那就是“先验”。</p><p id="d90b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">先验是概率的比率</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0acb62858b3f298c5b698c9a15f29df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*qLVlSJ-CICe6r1Y5wI0N4A.png"/></div></figure><p id="e9e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以用先验的对数来重新标度，我们称之为对数先验</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0e00bf80828acab235262db76fb54935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*ttb70S7wkiXhTsesOHYRYA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/668532443d485a996d1678b3756a32b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IsJI8eQt4QnTmM5phVWRsw.png"/></div></div></figure><p id="ef8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">一个词的正负概率</strong></p><p id="2aff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了计算词汇表中特定单词的正概率和负概率，我们将使用以下输入:</p><ul class=""><li id="1e3d" class="nc nd it la b lb lc le lf lh ne ll nf lp ng lt nh ni nj nk bi translated"><strong class="la iu"> freq_pos </strong>和<strong class="la iu"> freq_neg </strong>是该特定单词在正类或负类中的频率。换句话说，一个词的正频率就是这个词被计数的次数，其标签为1。</li><li id="626b" class="nc nd it la b lb nl le nm lh nn ll no lp np lt nh ni nj nk bi translated"><strong class="la iu"> N_pos </strong>和<strong class="la iu"> N_neg </strong>分别是所有文档(所有SLs)的正、负词总数。</li><li id="e5d4" class="nc nd it la b lb nl le nm lh nn ll no lp np lt nh ni nj nk bi translated"><strong class="la iu"> V </strong>是整组文档中唯一单词的数量，对于所有的类，无论是正面的还是负面的。</li></ul><p id="5c37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用这些来计算特定单词的正概率和负概率，公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/469ee6375256c56ce4ffa5125edb16b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*i632AKCc_FJmLIWyKAa0qg.png"/></div></figure><p id="04bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们在加法平滑的分子中添加了“+1”。这篇<a class="ae lu" href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener ugc nofollow" target="_blank">维基文章</a>解释了更多关于加法平滑的内容。</p><p id="79ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">对数可能性</strong></p><p id="d74b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了计算同一个单词的对数似然性，我们可以实现以下等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/728ef3cf95d5e9f4e64c2a5fe9f96be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*sD5ANoPayQPz4v3GAc5MxA.png"/></div></figure><p id="1035" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个SL的总概率为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/929407161883395f72f4c94bddd95fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*tQLr8Ot7wZ3ycqZd0HxEFA.png"/></div></figure><h1 id="983d" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">编码</h1><p id="772b" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">让我们通过构建上面的公式来弄脏我们的手。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f878" class="ma mb it lw b gy mc md l me mf"># load the data and set the spam=1 and ham=0<br/># convert the text into lower case and remove the puncuations<br/>df = pd.read_csv("spam.csv")<br/>df['target'] = df.target.map({'spam':1, 'ham':0})<br/>df['text'] = df.text.apply(lambda x:x.lower())<br/>df['text'] = df.text.apply(lambda x:x.translate(str.maketrans('', '', string.punctuation)))<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e43d4dc6a258c721f6a1b51d6a0ae0ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*R1faZtIrtBLydpPr.png"/></div></figure><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7a07" class="ma mb it lw b gy mc md l me mf"># Get the V Freq<br/>V_freq = Counter(" ".join(df['text'].values.tolist()).split(" "))</span><span id="824a" class="ma mb it lw b gy nx md l me mf"># Get the V<br/>V = len(V_freq.keys())</span><span id="e007" class="ma mb it lw b gy nx md l me mf"># get the freq_pos<br/>freq_pos = Counter(" ".join(df.loc[df.target==1]['text'].values.tolist()).split(" "))</span><span id="2e7e" class="ma mb it lw b gy nx md l me mf"># get the freq_neg<br/>freq_neg = Counter(" ".join(df.loc[df.target==0]['text'].values.tolist()).split(" "))</span><span id="9f9b" class="ma mb it lw b gy nx md l me mf"># get the number of positive and negative documents<br/>D_pos = sum(df.target==1)<br/>D_neg = sum(df.target==0)</span><span id="6da7" class="ma mb it lw b gy nx md l me mf"># get the number of unique positive and negative words<br/>N_pos = len(freq_pos.keys()) <br/>N_neg = len(freq_neg.keys())</span><span id="3032" class="ma mb it lw b gy nx md l me mf">logprior = np.log(D_pos/D_neg)</span><span id="16d5" class="ma mb it lw b gy nx md l me mf">def word_loglikelihood(w):<br/>    w = w.lower()<br/>    if w in V_freq:<br/>        p_w_pos = (freq_pos.get(w,0)+1 / (N_pos+V))<br/>        p_w_neg = (freq_neg.get(w,0)+1 / (N_neg+V)) <br/>        return np.log(p_w_pos/p_w_neg)<br/>    else:<br/>        return(0)</span></pre><p id="52b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来看看一些词的得分，比如“<strong class="la iu">讨喜</strong>”、<strong class="la iu">自由</strong>”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/4e5ca7e6b3f3c0fa9a9fd5ce661e5829.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/0*zAsPZvA0W35eLpeZ.png"/></div></figure><p id="7329" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，“免费”这个词的得分很高(&gt; 0)，这意味着这个词与垃圾邮件更相关。相反，“讨喜”这个词得分很低(&lt;0) which means that this word is not related to spam emails.</p><p id="f0e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Let’s create a function that returns the score of the whole subject line by adding up the word likelihood of each word plus the logprior.</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="bdf4" class="ma mb it lw b gy mc md l me mf">def text_loglikelihood(mytxt):<br/>    mytxt = mytxt.lower().split(" ")<br/>    score = logprior<br/>    for w in mytxt:<br/>        score+= word_loglikelihood(w)<br/>        # print(w,word_loglikelihood(w))<br/>    return(score)</span></pre><p id="10d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Get the score of the first SL from our data frame:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="251d" class="ma mb it lw b gy mc md l me mf">text_loglikelihood(df.iloc[0]['text'])</span></pre><p id="20fb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">We get:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d9c9" class="ma mb it lw b gy mc md l me mf">-107.49288547485799</span></pre><p id="ec32" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">which implies that this SL is more likely to be Ham.</p><h1 id="31c5" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">Make Predictions</h1><p id="dcef" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">Let’s say that we want to make predictions for all the SL. We will add two columns. The  【T0】  and the label of the prediction by taking values 0 and 1, where 1 is when the score is positive and 0 otherwise.</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d62a" class="ma mb it lw b gy mc md l me mf">df['score'] = df.text.apply(lambda x:text_loglikelihood(x)) df['prediction'] = df.score.apply(lambda x:int(x&gt;0)) </span><span id="2f8d" class="ma mb it lw b gy nx md l me mf"># confusion matrix df.groupby(['target','prediction']).size().reset_index()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/8daf60f90c1c47858adc479239d4aa19.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/0*9z1_DtW8iXYyAD9P.png"/></div></figure><p id="b3f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Finally, the accuracy on the train dataset is 99.5%</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="375b" class="ma mb it lw b gy mc md l me mf">np.mean(df.target==df.prediction)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f79af2d64257a0476e152c35140c0216.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*1REAF6QpnBj99LTp.png"/></div></figure></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="ca14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ol">原载</em><a class="ae lu" href="https://predictivehacks.com/naive-bayes-classification-in-nlp-tasks-from-scratch/" rel="noopener ugc nofollow" target="_blank"><em class="ol">https://predictivehacks.com</em></a><em class="ol">)。</em></p></div></div>    
</body>
</html>