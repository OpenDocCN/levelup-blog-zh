<html>
<head>
<title>Image Extraction via Python and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Python和深度学习的图像提取</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/image-extraction-via-python-and-deep-learning-27b928830955?source=collection_archive---------9-----------------------#2020-01-23">https://levelup.gitconnected.com/image-extraction-via-python-and-deep-learning-27b928830955?source=collection_archive---------9-----------------------#2020-01-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e917" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你想要一种简单的方法从照片中提取感兴趣的区域吗？例如，给出这样一张照片:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/7c8f098d326d345908e8170e14dee863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/0*pIYAdOAFAX1wwCGD.png"/></div></figure><p id="f54a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果有一个人工智能工具可以自动识别鹦鹉和灵长类动物，然后提取如下的个人图像，那就太酷了:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kw"><img src="../Images/4493494314341be80abb929ead3d1406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vuqFMWyNQ20NLgTX.png"/></div></div></figure><p id="af53" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果这个工具能够以合理的速度批量提取，那就更酷了。有了这样的技术，浏览大型图像目录、自动对其内容进行分类并提取匹配的图像片段将是一件简单的事情。</p><p id="7291" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个简短的教程中，我将演示如何构建这个工具。它会在一幅图像中找到多达80个不同类别的对象，然后有选择地提取它们。</p><p id="3721" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">几年前，这还只是科幻小说。但是，正如您将看到的，现在构建它非常简单。鉴于现代人工智能的能力，我们需要的只是一个合适的深度神经网络和100行python代码。</p><p id="fa7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们将在该工具上添加一个基本的web UI，这样任何人都可以尝试一下。</p><p id="1770" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们开始吧！</p></div><div class="ab cl lb lc hx ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="im in io ip iq"><p id="ff9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于这个项目，我们将应用<a class="ae li" href="https://www.analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/%22" rel="noopener ugc nofollow" target="_blank"> Mask R-CNN </a>神经网络。截至2020年初，该网络在图像识别和分类方面达到了最先进的水平。我们的特定网络是针对<a class="ae li" href="http://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO数据集</a>进行预训练的，这是人工智能图像研究的标准参考数据库。</p><p id="50cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用技术术语来说，<em class="lj">片段</em>指的是图像内部感兴趣的区域。Mask R-CNN被训练来标记它在给定图片中发现的所有片段。例如，在我们的例子中，它返回“鸟”和“人”。然而，我们也可以把网络转过来，提取这个标签中的像素位图。通过这种方式，我们检索了原始图像中组成“鸟”和“人”的所有像素的坐标。</p><p id="8c72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们这样做时，这是我们实际上从网络中获取的内容:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lk"><img src="../Images/3519a5b3075b6f40817871737924db18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ufyRL7Q9ts9ffrdq.png"/></div></div></figure><p id="9bf7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些图像是区域遮罩。在这种情况下，它们定义了一个人和一只鸟的区域。有了这些数据，提取实际的图像片段本身就很简单了。</p><p id="f637" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看这是如何工作的。这个项目中有两个文件:<em class="lj"> segment.py </em>和<em class="lj"> extract.py </em>。Segment.py使用遮罩R-CNN来生成遮罩位图。然后Extract.py使用这些位图实际提取线段。这两个程序可以结合起来，从任意数量的图像中自动提取任意数量的对象。</p><p id="d42f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Segment.py将遮罩位图存储到临时转换目录中。每个位图有一个文件。每个文件名被格式化以显示根文件名、置信度得分、标签和边界矩形。</p><p id="2751" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，下面是编码我们的鹦鹉的掩码文件:</p><p id="547a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">testimage . 99 . bird . 144 _ 28 _ 172 _ 258 . png</strong></p><p id="4901" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这意味着对于一只鸟来说，这是一个高置信度(99%)的区域，其边界矩形的原点为144，28，尺寸为172x258。</p><p id="c9f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们来看看细节。首先，下面是segment.py的主要代码部分:</p><pre class="kp kq kr ks gt ll lm ln lo aw lp bi"><span id="ab9f" class="lq lr it lm b gy ls lt l lu lv">PATH_INPUT_IMAGE = sys.argv[1]<br/>PATH_CONVERSION_DIR = sys.argv[2]<br/><br/>file_name =  os.path.basename(PATH_INPUT_IMAGE).split('.')[0]<br/><br/># create an inference instance of DNN config object<br/>class InferenceConfig(coco.CocoConfig):<br/>    GPU_COUNT = 1<br/>    IMAGES_PER_GPU = 1<br/>config = InferenceConfig()<br/><br/># Create model object in inference mode, fold in weights<br/>model=modellib.MaskRCNN(mode="inference", model_dir=MODEL_PATH, config=config)<br/>model.load_weights(MODEL_WEIGHTS_PATH, by_name=True)<br/><br/># run the model on the input image<br/>image_input = skimage.io.imread(PATH_INPUT_IMAGE)<br/>results = model.detect([image_input], verbose=1)<br/><br/># unpack all results<br/>result = results[0]<br/>class_ids = result['class_ids']<br/>masks = result['masks'].astype(np.uint8)<br/>scores = result['scores']<br/>rois = result['rois']<br/><br/>#<br/># for each region identified, <br/># get the score, label, dimensions and mask<br/># modify the mask so that all active pixels are <br/># white, with background black<br/># save the bitmap to the conversion directory<br/>#<br/>regionFileList = []<br/>for index, class_id in enumerate(class_ids):<br/><br/>    region_label=COCO_CLASS_NAMES[class_id].replace(' ', '_')<br/>    score=int(scores[index] * 100)<br/>    (y1, x1, y2, x2)=rois[index] # bounding box for the max<br/>    width=x2 - x1<br/>    height=y2 - y1<br/><br/>    # slice off the bitmap for this object<br/>    bitmap=masks[:,:,index]   <br/><br/>    # make positive mask pixels white<br/>    bitmap[bitmap &gt; 0]=255    <br/><br/>    path_output_image=f'{PATH_CONVERSION_DIR}/m{file_name}.{score}.{region_label}.{x1}_{y1}_{width}_{height}.png'<br/>    image_region=Image.fromarray(bitmap, 'L')<br/>    image_region.save(path_output_image, 'PNG')<br/>    print(path_output_image)<br/><br/>    regionFileList.append(path_output_image)<br/><br/># lastly, compute the background region <br/># (negative of all other regions)<br/>computeBackgroundRegion(file_name, regionFileList)</span></pre><p id="860b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">算法很简单。首先，我们用COCO权重初始化我们的神经网络。然后，我们将输入图像位图以推理模式输入到网络中。结果是一个标签及其对应位图的数组。这些可以识别神经网络看到的每个图像片段。根据这些数据，我们生成掩模图像并将它们作为文件存储到转换目录中。</p><p id="8ca7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦这个过程完成，extract.py就拥有了进行段提取所需的所有信息。</p><p id="8c5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是extract.py的完整代码清单:</p><pre class="kp kq kr ks gt ll lm ln lo aw lp bi"><span id="bb9b" class="lq lr it lm b gy ls lt l lu lv">import os<br/>import sys<br/>import numpy as np<br/>import cv2</span><span id="34ea" class="lq lr it lm b gy lw lt l lu lv"># return origin coordinates and dimensions of image <br/># (these are encoded in image name)<br/>def getRegionAttributes(image_region):</span><span id="08f3" class="lq lr it lm b gy lw lt l lu lv">    image_region = os.path.basename(image_region)<br/>    (x, y, w, h)  = image_region.split('.')[3].split('_');<br/>    return (int(x), int(y), int(w), int(h))<br/></span><span id="6e9f" class="lq lr it lm b gy lw lt l lu lv"># extract specified region within image<br/>def extract_region(image, region):</span><span id="0e72" class="lq lr it lm b gy lw lt l lu lv">    extracted_image = np.copy(region)<br/>    rows = region.shape[0]<br/>    cols = region.shape[1]<br/>    for row in range(rows):<br/>        for col in range(cols):<br/>            if region[row, col][0] == 255:<br/>                extracted_image[row, col] = image[row, col]<br/>            else:<br/>                extracted_image[row, col] = (255, 255, 255)</span><span id="4403" class="lq lr it lm b gy lw lt l lu lv">    return extracted_image<br/></span><span id="67a0" class="lq lr it lm b gy lw lt l lu lv">if __name__ == '__main__':</span><span id="f386" class="lq lr it lm b gy lw lt l lu lv">    if len(sys.argv) != 4:<br/>        print('usage: python extract.py path_input_image path_region path_output_image')<br/>        exit()</span><span id="b3bb" class="lq lr it lm b gy lw lt l lu lv">    PATH_INPUT_IMAGE = sys.argv[1]<br/>    PATH_REGION = sys.argv[2]<br/>    PATH_OUTPUT_IMAGE = sys.argv[3]</span><span id="b601" class="lq lr it lm b gy lw lt l lu lv">    image_input = cv2.imread(PATH_INPUT_IMAGE)<br/>    region_input = cv2.imread(PATH_REGION)</span><span id="544a" class="lq lr it lm b gy lw lt l lu lv">    # extract region, crop it match the region mask <br/>    (x, y, w, h)=getRegionAttributes(PATH_REGION)<br/>    extracted_image=extract_region(image_input, region_input)<br/>    cropped_extracted_image=extracted_image[y:y+h, x:x+w]<br/>    result_image= v2.resize(cropped_extracted_image, (w, h))</span><span id="63dc" class="lq lr it lm b gy lw lt l lu lv">    cv2.imwrite(PATH_OUTPUT_IMAGE, result_image);<br/>    print(PATH_OUTPUT_IMAGE)</span></pre><p id="4c96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将图像的路径、我们希望提取的片段的掩码文件以及输出的路径作为输入。接下来，我们收集掩码的所有属性(如前所述，编码在掩码文件名中)。然后，我们通过有效地将其位图与原始图像进行and运算来提取分割区域。最后，我们使用数组切片来裁剪结果，并将其调整到原始尺寸。</p><p id="d15d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是:一个提取的图像。</p><p id="1f82" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">点击这里查看完整的github列表<a class="ae li" href="https://github.com/cminson/public/tree/master/extract" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="f554" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这段代码即使在速度很慢的计算机上也能很好地运行。例如，在低端市场的t2.large ec2实例(无GPU)上，从典型图像中识别和提取所有片段大约需要20秒。</p><p id="4ded" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为最后的奖励，我为这个工具构建了一个简单的web UI。你可以在这里找到<a class="ae li" href="https://www.christopherminson.com/demos/segment/extract.html" rel="noopener ugc nofollow" target="_blank"/>。现在，经过很少的工作，我们有了一个通用的网站，它可以神奇地从图像中识别和提取片段。这就是机器学习的奇迹！</p></div></div>    
</body>
</html>