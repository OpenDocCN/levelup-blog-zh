<html>
<head>
<title>An Introduction to Logistic Regression in Python with statsmodels and scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用statsmodels和scikit-learn介绍Python中的逻辑回归</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/an-introduction-to-logistic-regression-in-python-with-statsmodels-and-scikit-learn-1a1fb5ce1c13?source=collection_archive---------5-----------------------#2020-06-16">https://levelup.gitconnected.com/an-introduction-to-logistic-regression-in-python-with-statsmodels-and-scikit-learn-1a1fb5ce1c13?source=collection_archive---------5-----------------------#2020-06-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/42b6e242919bdef18ced67d68ab92e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ssjIB7mg-91NbRCE"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Antonio Grosz 在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><h1 id="44f1" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="d3c8" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">数据科学家、统计学家和其他数据从业者遇到的许多问题都需要确定感兴趣的观察值是否可能属于某个结果的某个类别。例子包括信用评估(例如，潜在的借款人是否会拖欠债务？)、对可能欺诈的信用卡购买的标记以及对象分类(例如，这种植物是否是鸢尾？).虽然在技术上可以使用线性回归模型计算属于一个类别相对于另一个类别的概率，但更合适的回归技术是逻辑回归。这篇文章是上一篇<a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/an-introduction-to-regression-in-python-with-statsmodels-and-scikit-learn-9f75c748f56e">文章</a>的后续文章，标题为“使用statsmodels和scikit-learn介绍Python中的回归”,文章介绍了使用<em class="lz"> statsmodels </em>和<em class="lz"> scikit-learn </em>预测糖尿病诊断的简单逻辑回归模型的例子。</p><h1 id="15e7" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为什么不用线性回归呢？</h1><p id="5e12" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">假设我们有一个感兴趣的结果，<em class="lz"> Y </em>，它有两个离散值:0和1。我们希望使用预测变量<em class="lz"> x </em>来预测一个数据点在<em class="lz"> Y </em>上的值为1与值为0的概率。下面是假设数据的散点图，Y值与<em class="lz"> x </em>相对应。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/29885dc397757d42f3747ab038d00541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*MXl3auf8Cvn-BV-72Naatw.png"/></div></figure><p id="b68b" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">如该图所示，给定数据点在<em class="lz"> Y </em>上可以取值0或1。如果我们对此数据运行线性回归模型，这就是我们获得的预测线。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/3f1a965b78c0eff8debd0c81a82113e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Ak4EP85UXB4WAsVbE6jkhg.png"/></div></figure><p id="b985" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">在这里，我们看到了将线性回归模型拟合到只有两个离散可能值的结果的主要问题之一。线性回归模型会将直线预测线拟合到数据点，但是当感兴趣的结果只有两个可能的值时，预测线将超出结果的下限和上限。</p><p id="ae9a" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">在这种情况下，我们希望我们的预测限制在0和1之间。我们想要一条曲线，s形的线来拟合数据，而不是直线。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/bb9f4c4462d561e7cb087d4bbefb5684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*R6JrNNsUILKDbu8FwDh1Vw.png"/></div></figure><p id="0d96" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">上图中的这条曲线显示了给定数据点在<em class="lz"> x </em>上的值，每个数据点在<em class="lz"> Y </em>上的值为1的概率。这些预测的概率是通过将逻辑回归模型与数据拟合而产生的。现在让我们更详细地研究逻辑回归模型的细节。</p><h1 id="602d" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">逻辑回归模型剖析</h1><p id="3912" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">线性回归模型旨在预测实际结果值<em class="lz"> Y </em>，而逻辑回归模型预测<em class="lz"> Y </em>的<strong class="ld jh">对数优势</strong>。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/430a3adb312f9909860b482e3ffe3d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*FpJYlrJPOLgApphc97T3SA.png"/></div></figure><p id="031e" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">那么，对数几率是多少呢？我们来分解一下这个概念。<strong class="ld jh">赔率</strong>是通过将一个事件的概率(这里用<em class="lz"> p </em>表示)除以它的补数(1- <em class="lz"> p </em>)来计算的。上面等式左边括号内的项表示赔率。要获得对数赔率，我们只需取赔率的自然对数(<em class="lz"> Ln) </em>(赔率的对数也称为<strong class="ld jh"> logit </strong>)。注意，上面等式中的下标<em class="lz"> i </em>仅表示单个数据点。例如，在我们的等式中，下标为<em class="lz"> i </em>的字母<em class="lz"> p </em>代表在<em class="lz"> Y </em>上值为1的单个数据点的概率。在等号的右边，𝛼代表截距(当预测变量<em class="lz"> x </em>的值为0时的对数优势),𝛽是回归权重(单位增加的对数优势在<em class="lz"> x </em>中的变化)。</p><p id="d517" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">现在，你可能会问，对数概率有什么特别之处？难道我们对概率不感兴趣，或者更好的是，预测一个数据点的离散0或1值？答案最终是肯定的，但是对数几率的预测是我们必须首先采取的重要步骤。与概率或二分变量的离散值不同，对数优势是在连续的尺度上测量的。为了说明，下面是<em class="lz"> x </em>和<em class="lz"> Y </em> =1的对数几率之间的关系图。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/699af0af238b887139ebab72dcf7f8d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*oDf1TSKH61BM8l57zkVwzA.png"/></div></figure><p id="3f82" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">来自逻辑回归模型的预测对数优势可以很容易地转换为概率，公式如下，其中<em class="lz"> e </em>表示相关上标数量的指数(上标公式在这种情况下是预测对数优势的公式)。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/5c6039a5de8af17f18793e877dbbc8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*KNN762yrJpIbMCK_TjuGiQ.png"/></div></figure><p id="b382" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">当我们绘制预测概率与值<em class="lz"> x </em>的关系时，我们最终会得到一条在值为0和1时变平的s形曲线，但是我们如何理解这样的曲线呢？考虑下图，该图对之前的预测概率与<em class="lz"> x </em>的关系图进行了一些修改。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/a2a8b6fbecf7d9ab63d06a637491ddc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*k6zk4pQMEPO3jKAbqM2FHg.png"/></div></figure><p id="3a7e" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">在这个例子中，水平虚线表示预测概率值0.5，即<em class="lz"> Y </em>等于1。预测概率曲线在1.95的<em class="lz"> x </em>值处穿过这条水平线；垂直虚线标记了这一点。因此，在这个具有单个预测器的简单例子中，任何具有等于或大于1.95的<em class="lz"> x </em>值的数据点将具有大于或等于0.5的预测概率，并被分类为1。相反，<em class="lz"> x </em>值低于1.95的数据点将被归类为0。</p><p id="bd4e" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">如上图所示，大多数<em class="lz"> x </em>值等于或大于1.95的数据点的实际<em class="lz"> Y </em>值为1，因此被正确分类。这些实际和预测的<em class="lz"> Y </em>值都为1的数据点被称为<strong class="ld jh">真阳性</strong>。同样，大多数具有小于1.95的<em class="lz"> x </em>值的数据点具有0的实际<em class="lz"> Y </em>值，并且也被正确分类；这些都是<strong class="ld jh">真否定</strong>。当然，我们也会得到一些实际的<em class="lz"> Y </em>值与预测的<em class="lz"> Y </em>值不匹配的数据点；这些数据点由红色“+”<em class="lz"/>标记表示。被错误归类为0的实际值为1的数据点为<strong class="ld jh"> </strong>称为<strong class="ld jh">假阴性</strong>、<strong class="ld jh"> </strong>，而被错误归类为1的实际值为0的数据点为<strong class="ld jh">假阳性</strong>。现在让我们看看如何在Python中实际实现和利用逻辑回归。</p><h1 id="0d8e" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">数据</h1><p id="e06c" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在下面的例子中，我们将使用<a class="ae jd" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank">皮马印第安人糖尿病</a>数据。数据和相关信息可在<a class="ae jd" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank">这里</a>找到。让我们从导入<em class="lz"> pandas </em>开始，加载数据，并取这个练习感兴趣的列:<code class="fe mm mn mo mp b">Glucose</code>(血糖浓度)和<code class="fe mm mn mo mp b">Outcome</code>(1 =糖尿病，0 =其他)。葡萄糖列中值为0的观察值被视为缺失数据，并从分析中排除(由于葡萄糖值为0，少于1%的观察值被丢弃)。一旦数据集被加载并限制到所需的列和行，我们就可以用<code class="fe mm mn mo mp b">diab.describe()</code>来看看我们的列的分布。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ebf710acd90ee48a6ed33d68e63bccc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*niiFo7rRgLZ7DoDtqeJxcw.png"/></div></figure><p id="7f93" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">请注意，我们的预测指标葡萄糖的最小值是44。回想一下，当预测值为0时，逻辑回归模型中的截距项代表预测的对数优势。因此，我建议通过从葡萄糖列中的每个值中减去最小值44来“最小居中”葡萄糖。</p><pre class="mb mc md me gt mt mp mu mv aw mw bi"><span id="210c" class="mx ke jg mp b gy my mz l na nb">diab["Glucose"] = diab["Glucose"] - diab["Glucose"].min()</span></pre><p id="abaf" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">请注意，无论我们是否将预测值居中，我们的回归权重和模型拟合指标都是相同的。但是，居中会对截距值产生影响。如果没有最小居中，截距将代表当数据点的葡萄糖值为0时的预测对数优势，而数据集中的最小葡萄糖值为44。通过使葡萄糖最小居中，截距将替代地表示在数据集中观察到的最低葡萄糖浓度水平的观察结果的糖尿病的预测对数优势。好了，现在我们的数据已经设置好了，让我们继续进行一些分析，从<em class="lz"> statsmodels </em>中的逻辑回归建模开始。</p><h1 id="96b9" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">统计模型</h1><p id="64fd" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><em class="lz"> statsmodels </em>是“一个Python模块，它为许多不同的统计模型的估计，以及进行统计测试和统计数据探索提供了类和函数。”如果我们有兴趣对葡萄糖对糖尿病的影响进行正式的统计假设检验，<em class="lz"> statsmodels </em>非常有用，因为它可以轻松地为回归权重提供推断统计指标，如标准误差、置信区间和p值。</p><p id="ba39" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">现在，实际上有两种不同“口味”的<em class="lz"> statsmodels </em>。第一个是在前一篇文章“使用statsmodels和scikit-learn介绍Python中的回归”中使用的标准<em class="lz"> statsmodels </em>包。第二个是<em class="lz"> statsmodels.formula </em>，它允许用户使用字符串中包含的R样式公式来指定模型。除了能够在字符串中为自己的模型编写公式之外，使用<em class="lz"> statsmodels.formula </em>的另一个优点是默认包含截距。因此，让我们在下面的练习中使用<em class="lz"> statsmodels.formula </em>。</p><p id="8537" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">下面的代码导入<em class="lz"> statsmodels.formula，</em>将使用葡萄糖浓度预测糖尿病的逻辑回归模型赋给<code class="fe mm mn mo mp b">m1</code>，然后使用<code class="fe mm mn mo mp b">m1.summary()</code>输出模型结果的表格。要知道<code class="fe mm mn mo mp b">m1</code>是用户自定义的名字。我选择了<code class="fe mm mn mo mp b">m1</code>,因为在我看来，它代表“模型1 ”,但是您不需要受限于我的命名约定。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="7599" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">让我们更详细地分解模型的代码(第3-6行)。第3行从<em class="lz"> statsmodels.formula </em>调用<code class="fe mm mn mo mp b">logit</code>，开始将逻辑回归模型拟合到数据的过程。第4行用字符串<code class="fe mm mn mo mp b">Outcome ~ Glucose</code>指定了型号。<code class="fe mm mn mo mp b">~</code>左侧的列名是结果，右侧的列是预测值(如果您想要包括多个预测值，需要在每个预测值的列名之间放置一个<code class="fe mm mn mo mp b">+</code>)。第5行确定模型拟合的数据，第6行指定实际拟合的逻辑回归模型。</p><p id="b405" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">这是来自<code class="fe mm mn mo mp b">m1</code>的结果表。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/b2a5ca203345d9f25672dedfc017ab49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*rD7lPVQpr3qjqnrkZNgNiA.png"/></div></div></figure><p id="b058" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">请注意，输出左上角的<code class="fe mm mn mo mp b">Method</code>值是<code class="fe mm mn mo mp b">MLE</code>。这代表<strong class="ld jh">最大似然估计；请记住这个术语。现在，请注意输出的右边部分，即<code class="fe mm mn mo mp b">Log-Likelihood</code>。似然函数可概括为观察数据由一组给定的<em class="lz">参数值</em>(即截距和回归权重值)生成的概率。这里，我们的目标是找到使<em class="lz">最大化</em>似然函数的参数值，因此，术语“最大似然”。</strong></p><p id="28f5" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">实际上，最好计算可能性的自然对数，即<strong class="ld jh">对数似然性</strong>，因为与可能性相比，对数似然性<a class="ae jd" href="https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1#:~:text=The%20log%20likelihood&amp;text=This%20is%20important%20because%20it,instead%20of%20the%20original%20likelihood." rel="noopener" target="_blank">更容易区分</a>。其中<em class="lz"> p </em>表示对于给定的观察值，属于二分结果中感兴趣的类别(例如，类别1相对于0)的预测概率，<em class="lz"> i </em>，以及<em class="lz"> y </em>表示观察值在二分结果中的实际值，对数似然可以写成如下。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/2a1d83cf6a034d9b6ff11431e1af192b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VIfyH6_yfT4JJSS8TPbVoQ.png"/></div></div></figure><p id="8566" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">我们模型的对数似然是-393.28。“空”模型，即截距作为唯一参数的模型，其对数似然为-493.35，如<code class="fe mm mn mo mp b">Log-Likelihood</code>下方的<code class="fe mm mn mo mp b">LL-Null</code>输出所示。<code class="fe mm mn mo mp b">LLR p-value</code>提供手头模型与零模型的<a class="ae jd" href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-are-the-likelihood-ratio-wald-and-lagrange-multiplier-score-tests-different-andor-similar/" rel="noopener ugc nofollow" target="_blank"> <strong class="ld jh">似然比测试</strong> </a> <strong class="ld jh"> </strong>的p值。如果这个p值满足统计显著性的既定阈值，那么我们可以得出结论，模型比零模型更适合数据。在这种情况下，似然比测试的结果表明，与只有截距的零模型相比，将葡萄糖浓度作为糖尿病的预测因子显著提高了模型拟合度。</p><p id="dda0" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">现在，让我们关注表格底部的截距和回归权重结果。在<code class="fe mm mn mo mp b">coef</code>列下，<code class="fe mm mn mo mp b">Intercept</code>行的值为-3.9272。这意味着当葡萄糖值为0时，结果值为1(即被分类为糖尿病)的预测对数优势为-3.9272。<code class="fe mm mn mo mp b">coef</code>列中<code class="fe mm mn mo mp b">Glucose</code>的值0.0406意味着葡萄糖值每增加一个单位，被归类为糖尿病的对数优势增加0.0406。对数概率并不十分直观，这就是为什么我们希望最终从我们的模型中预测概率。尽管如此，我们可以将该模型的结果解释为显示较高的葡萄糖浓度与糖尿病的诊断正相关，这是有意义的。</p><p id="efd3" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated"><code class="fe mm mn mo mp b">Glucose</code>行的<code class="fe mm mn mo mp b">P&gt;|z|</code>列中的值0.000表明葡萄糖对结果的影响在常规显著性水平上具有统计学显著性。因此，我们可以确信，在该数据集中观察到的葡萄糖浓度和糖尿病之间的关系不可能仅仅归因于<a class="ae jd" href="https://methods.sagepub.com/Reference/encyc-of-research-design/n401.xml" rel="noopener ugc nofollow" target="_blank">采样误差</a>。<code class="fe mm mn mo mp b">[0.025</code>和<code class="fe mm mn mo mp b">0.075]</code>列中的值分别提供了葡萄糖截距和回归权重的95% <a class="ae jd" href="https://stattrek.com/estimation/confidence-interval.aspx" rel="noopener ugc nofollow" target="_blank">置信区间</a>。葡萄糖的95%置信区间不超过0的事实也告诉我们，在该模型中，葡萄糖对糖尿病的影响在统计学上<a class="ae jd" href="https://blog.minitab.com/blog/adventures-in-statistics-2/understanding-hypothesis-tests-confidence-intervals-and-confidence-levels" rel="noopener ugc nofollow" target="_blank">显著</a>，显著性水平为5%。</p><p id="2a64" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">现在，让我们看一个例子，从我们的结果中获得给定葡萄糖值的预测概率。假设我们想要获得被归类为糖尿病的预测概率，给定葡萄糖浓度值为54。基于上述模型的结果，我们得到预测的对数优势值为-3.5212</p><p id="adaa" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated"><strong class="ld jh">-3.5212 =-3.9272+(54–44)* 0.0406</strong></p><p id="f3be" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">要将预测的对数优势转换为预测的概率，我们需要对对数优势取指数(exp ),然后除以1加上取指数的对数优势。</p><p id="c8a7" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated"><strong class="ld jh">exp(-3.5212)/(1+exp(-3.5212))= 0.0296/1.0296 = 0.0287</strong></p><p id="1b6b" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">因此，假定葡萄糖浓度为54，被归类为糖尿病的概率约为0.03，或转换为百分比时为3%。</p><p id="f9aa" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">如果我们想要使用我们的<em class="lz"> statsmodels </em>模型获得一个单个值的预测，我们首先必须创建一个具有一列一行的<em class="lz">熊猫</em>数据框。在下面的代码中，我创建了一个数据框<code class="fe mm mn mo mp b">newpred</code>，它有一个名为<code class="fe mm mn mo mp b">Glucose</code>的列和一个值为10的单行。然后我就把<code class="fe mm mn mo mp b">m1.predict</code>套用到了<code class="fe mm mn mo mp b">newpred</code>上。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a4959a0447f29dbc202b0b2b473c4829.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*UFyK1cOcZzEpuNrS2FdYiA.png"/></div></figure><p id="ff9b" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">我们可以更好地评估葡萄糖和被归类为糖尿病的概率之间的关系，方法是根据我们的模型对葡萄糖值绘制预测概率。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/fe6a29e3c13c453afd38b3c64caa715d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*LwU_Zf7r-ttWl9dvnShpvw.png"/></div></figure><p id="9eca" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">在上面的图中，我们可以清楚地看到糖尿病的概率随着葡萄糖浓度的增加而增加。</p><h1 id="9484" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">sci kit-学习</h1><p id="ddf4" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><em class="lz"> scikit-learn </em>旨在为预测建模提供方便而有用的工具。逻辑回归就是这样一个工具，可以用<code class="fe mm mn mo mp b">LogisticRegression</code>类来实现。然而，在进入<code class="fe mm mn mo mp b">LogisticRegression</code>之前，让我们创建一个重塑的<em class="lz"> NumPy </em>数组来包含葡萄糖浓度值，以确保我们的预测值数组既有行又有列。</p><pre class="mb mc md me gt mt mp mu mv aw mw bi"><span id="e340" class="mx ke jg mp b gy my mz l na nb">X = diab["Glucose"].to_numpy().reshape(-1, 1)</span></pre><p id="7583" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">为什么上面的代码是必要的？<code class="fe mm mn mo mp b">LogisticRegression</code>类期望预测值数组既有行又有列。然而，我们数据中的<em class="lz">熊猫</em>系列<code class="fe mm mn mo mp b">diab["Glucose"]</code>的尺寸有763行，没有列。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c23474b705dde51fb4b56cc3cf51ee2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*rDwBCUah-K0sLT53fHqPrA.png"/></div></figure><p id="ff44" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">相反，我们新创建的数组<code class="fe mm mn mo mp b">X</code>的维度有763行和一列。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4afb0b1e2ae2112cdae7b73e333099fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*RCkoxkGuMe852-qc4ETu-w.png"/></div></figure><p id="a07b" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">现在我们准备好拟合我们的逻辑回归模型，它在下面的代码中被分配给第3行的<code class="fe mm mn mo mp b">m2</code>。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="123e" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">现在让我们讨论一下<em class="lz"> scikit的基本结构——学习</em>的模型拟合语法。我们从第3行开始，调用<code class="fe mm mn mo mp b">LogisticRegression</code>类，然后将<code class="fe mm mn mo mp b">fit</code>方法附加到这个类上。然后，预测值(即本例中的<code class="fe mm mn mo mp b">X</code>)和结果值(即此处的<code class="fe mm mn mo mp b">diab["Outcome"]</code>)作为参数传递给<code class="fe mm mn mo mp b">fit</code>。使用逻辑回归模型拟合数据并分配给<code class="fe mm mn mo mp b">m2</code>，我们可以分别使用<code class="fe mm mn mo mp b">m2.intercept_</code>和<code class="fe mm mn mo mp b">m2.coef_</code>查看截距和回归权重。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/2e4b96bd75e427f512c016bcbe8c07c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*7yZ0XrJyHz1gBiZbqMRkMw.png"/></div></figure><p id="2a42" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">这些结果与<em class="lz"> statsmodels </em>输出一致。尽管如此，幕后发生的一些事情使得<em class="lz"> scikit-learn </em>的逻辑回归实现与<em class="lz"> statsmodels的</em>略有不同，这里值得一提。首先，<em class="lz"> scikit-learn </em>的<code class="fe mm mn mo mp b">LogisticRegression</code>使<strong class="ld jh">对数损失</strong>最小化，这与最大化似然函数相反。然而，对数损失函数只是对数似然乘以-1，所以这与<em class="lz"> statsmodels </em>中的情况并没有太大的不同。我们可以使用<code class="fe mm mn mo mp b">sklearn.metrics</code>中的<code class="fe mm mn mo mp b">log_loss</code>输出日志损失。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="0258" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">请注意，<code class="fe mm mn mo mp b">log_loss</code>中的<code class="fe mm mn mo mp b">normalize</code>被设置为<code class="fe mm mn mo mp b">False</code>，它输出我们数据中所有观察值的损失总和(与<em class="lz"> statsmodels </em>中对数似然的计算一致)，而不是平均损失。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/4e3403314cb02bbd719af51ed04da40b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYLdF7hKw68hNNkbFCb_0A.png"/></div></div></figure><p id="931c" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">对数损失是393.28，等于来自<em class="lz"> statsmodels </em>的对数似然值乘以-1。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5014405dfc6dd1ba28ae000d25f08cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*xWa2ZgjhqHMJAKp7yWNTJg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来自<em class="nk">统计模型的对数似然和其他结果。</em></figcaption></figure><p id="57e7" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">除了最小化log-loss，<code class="fe mm mn mo mp b">LogisticRegression</code>默认执行正则化(使用l2惩罚作为默认惩罚)。虽然对正则化的全面讨论超出了本文的范围，但还是有必要做一个简单的描述。简而言之，正则化有助于通过惩罚回归权重来防止多重共线性和过度拟合。因此，更强的正则化将减少回归权重的大小。<a class="ae jd" href="https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261" rel="noopener" target="_blank">这里的</a>是对正则化更详细的解释，如果你不熟悉这个概念，可能会觉得有帮助。</p><p id="c813" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">当然，如果您对使用逻辑回归进行预测感兴趣，您可能还想将数据分成<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank">训练集和</a>测试集。这样做将允许您在训练集上拟合模型，然后在测试集中看不见的数据点上评估结果模型。我也鼓励你使用<a class="ae jd" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank">交叉验证</a>技术来防止过度拟合。</p><h1 id="d19c" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">混淆矩阵</h1><p id="79aa" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们还应该看看我们的模型在分类糖尿病和非糖尿病数据点方面表现如何。当我们的主要目标是预测时，这是一项特别重要的任务。完成这一任务的一个简单方法是输出一个<strong class="ld jh">混淆矩阵</strong>，它描绘了实际和预测的肯定/否定分类的分布。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/e53bbecd7d7cd9299194dbc99d0e0d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*0kRl4FlAt5lVItkxOp19bQ.png"/></div></figure><p id="544a" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">这里，如果来自逻辑回归模型的观察的预测概率大于或等于0.5，则该观察的结果(即，诊断的糖尿病)被分类为值1。基于我们的模型，我们使用<em class="lz"> scikit-learn </em>运行，当以最小为中心的葡萄糖值约为96.66或更高时，观察结果将被分类为1(糖尿病)和0(非糖尿病)。这个值96.66是通过下面显示的代码获得的。请注意，从<em class="lz"> scikit-learn </em>获取预测概率，我们必须使用<code class="fe mm mn mo mp b">predict_proba</code>，因为<code class="fe mm mn mo mp b">predict</code>将返回一组观察值的预测分类(即本例中的1或0)。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/2ddcd61c6a4e9ea80df8c6a903c39dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YeiymDa2oxWvmQfFB7bzA.png"/></div></div></figure><p id="f135" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">输出混淆矩阵的代码如下所示。<code class="fe mm mn mo mp b">confusion_matrix</code>中的第一个参数<code class="fe mm mn mo mp b">diab["Outcome"]</code>，指定感兴趣的结果的实际标签。第二个参数，<code class="fe mm mn mo mp b">m2.predict(X)</code>，提供了我们的逻辑回归模型的预测标签。</p><figure class="mb mc md me gt is"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="12d4" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">这是输出的混淆矩阵。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/b19aec5207fc761e8cbce3ccec8cd05f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rB6h8RtUG8oLfH1JeNNLtA.png"/></div></div></figure><p id="002a" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">在左上角我们可以看到我们的观察中有437个是真正的否定；这些观察结果的实际值和预测值为0。移到右下角，我们有132个真阳性；这些观察结果的实际值和预测值为1。因此，我们的模型对763个观察值中的569个进行了正确分类，准确率达到75%。</p><p id="5471" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">现在是错误分类的观察。在右上角，我们可以看到我们有60个实际值为0、预测值为1的观察结果。这些都是我们的误报。最后，在左下方，我们有134个假阴性——实际值为1而预测值为0的观察值。</p><p id="f856" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">值得一提的是，我们还可以使用<code class="fe mm mn mo mp b">pred_table</code>输出一个带有<em class="lz"> statsmodels </em>的混淆矩阵。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/959e597b3899f68c54c86aa73ba0a5b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*XF_tAXr4MtvCDdbOY4D-8w.png"/></div></figure><p id="2c02" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">如你所见，<code class="fe mm mn mo mp b">pred_table</code>的输出与<em class="lz"> scikit-learn </em>的<code class="fe mm mn mo mp b">confusion_matrix</code>的输出相匹配。</p><h1 id="8277" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h1><p id="18de" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在这篇文章和之前的相关文章“用statsmodels和scikit-learn介绍Python中的回归”中，我花了很多精力在回归的基础上，原因有二。首先，线性和逻辑回归都是有用的建模技术。适当处理相关的预测因素，线性和逻辑回归是强大的建模工具，也可以提供可解释的结果。第二，对线性和逻辑回归的扎实理解对于理解许多其他回归和分类技术是必不可少的。</p><p id="7071" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">感谢您花时间阅读这篇文章。我希望你发现了它——或者至少是这篇文章的一部分——信息丰富。如果你感兴趣，这篇文章的全部材料可以在GitHub <a class="ae jd" href="https://github.com/scottadams26/introLogit" rel="noopener ugc nofollow" target="_blank">这里</a>找到。像往常一样，请随时提出建设性的批评、您发现的错误和/或您可能希望在未来的帖子封面中看到的其他主题的建议。下次见，牛逼一点。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><p id="9a96" class="pw-post-body-paragraph lb lc jg ld b le mf lg lh li mg lk ll lm mh lo lp lq mi ls lt lu mj lw lx ly ij bi translated">简介-stats模型。(未注明)。2020年3月1日检索，https://www.statsmodels.org/stable/index.html<a class="ae jd" href="https://www.statsmodels.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>