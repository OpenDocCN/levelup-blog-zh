<html>
<head>
<title>Killer Combo: Softmax and Cross Entropy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">黑仔组合:软最大值和交叉熵</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba?source=collection_archive---------0-----------------------#2020-06-27">https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba?source=collection_archive---------0-----------------------#2020-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="44f4" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated"><em class="km">soft max的导数和交叉熵损失，逐步解释。</em></p><p id="ba7d" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">看一看典型的神经网络——特别是它的最后一层。最有可能的是，您会看到类似这样的内容:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/65109d48cde7137f788446ae7318a2fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WePYX1teg2OgyCSo8mn0WQ.png"/></div></div></figure><p id="d20d" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">softmax和交叉熵损失就像面包和黄油一样合在一起。原因如下:要用反向传播训练网络，需要计算损失的导数。在一般情况下，导数会变得复杂。但是如果你使用软最大值和交叉熵损失，复杂性就会消失。你得到的不是一个又长又笨的公式，而是这个简洁、易于计算的东西:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kz"><img src="../Images/948659899d9fc01a2b11836fae7d4b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyzkak3mPNKmoxs0n-il4Q.png"/></div></div></figure><p id="13c6" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">有几个<a class="ae la" href="https://pragprog.com/titles/pplearn/" rel="noopener ugc nofollow" target="_blank">编程机器学习</a>的读者问我这个导数从何而来。这篇文章就是关于这个计算的:它向你展示了如何一步一步地推导出上面的公式。</p><p id="be36" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">声明:这篇文章包含了很多公式，但是在数学方面我有点无能。铁杆数学迷会觉得我的解释非常罗嗦。如果你在寻找一个遵循严格数学符号的简洁的形式证明，你会在<a class="ae la" href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a>中找到。</p><p id="3de0" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">在我们开始计算之前，让我们先列出我们需要的数学工具。</p><h1 id="2b31" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">我们需要一些工具</h1><p id="6f99" class="pw-post-body-paragraph jo jp iq jq b jr lz jt ju jv ma jx jy jz mb kb kc kd mc kf kg kh md kj kk kl ij bi translated">在这篇文章的其余部分，我们将需要一些派生规则。我将在这里概述其中的四个。</p><p id="71e1" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">先说我们有两个函数:<em class="km"> f(x) </em>和<em class="km"> g(x) </em>。<a class="ae la" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction" rel="noopener ugc nofollow" target="_blank"> <em class="km">链式法则</em> </a>告诉我们如何计算它们合成的导数，<em class="km"> f(g(x)) </em>:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi me"><img src="../Images/24e33dae6503dba45ee13d6c39608f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GjTYGfWIwdr6kbv0097WhA.png"/></div></div></figure><p id="26a0" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">我们还将使用<a class="ae la" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-9/v/quotient-rule" rel="noopener ugc nofollow" target="_blank"> <em class="km">商规则</em> </a>来计算分数的导数:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi mf"><img src="../Images/5266fdfdc162bd8b820ff5e447e6cca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WL_elZioJ50rM9U3_udOww.png"/></div></div></figure><p id="006f" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">最后，我们需要一些基本的导数——那种你通过死记硬背学习的导数。对数函数的<a class="ae la" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-7/v/derivative-of-lnx" rel="noopener ugc nofollow" target="_blank">导数，以及指数函数</a>的<a class="ae la" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-7/v/derivative-of-ex" rel="noopener ugc nofollow" target="_blank">导数:</a></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi mg"><img src="../Images/7ebd86889a582bad49d0a27c4a83c52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJQqN4a_hsmfLQi2QYwJGQ.png"/></div></div></figure><p id="dcc2" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">有了这些工具，让我们计算应用于softmax的交叉熵损失的导数。让我们从softmax的导数开始，稍后我们将回到交叉熵损失。</p><h1 id="aee4" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">导出Softmax</h1><p id="35fb" class="pw-post-body-paragraph jo jp iq jq b jr lz jt ju jv ma jx jy jz mb kb kc kd mc kf kg kh md kj kk kl ij bi translated">我们来看看softmax函数。提醒一下，softmax的输出也是整个神经网络的输出，我称之为<em class="km"> ŷ.它的输入是最后一个隐藏层的节点。他们通常被称为<em class="km">逻辑</em>，所以在这个公式中我把他们命名为<em class="km"> l </em>:</em></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi mh"><img src="../Images/0616dd2e76a232c6aa53ab55236b0bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpFlL1SHUiydR4CU5GZRuw.png"/></div></div></figure><p id="547b" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">softmax的输入和输出数量相等，因此ŷ<em class="km">和l </em>的数量相同。我用字母<em class="km"> k </em>表示输入和输出的数量。</p><p id="f6eb" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">分母中的和有点难读，我们会看到很多。为了便于阅读，我将省略它的索引:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/8c0e2cb3206e678835e629808468c900.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*T0JN0hctGKi-f6g_SLTycA@2x.png"/></div></figure><p id="ca55" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">现在我们要计算softmax对其输入的导数:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/62a3315ef80d0a075cce8ae42b1af8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/1*z3g9paclcT41fZ3YSLkEqw@2x.png"/></div></figure><p id="bf6e" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">两个索引<em class="km"> i </em>和<em class="km"> n </em>可以取1到<em class="km"> k </em>之间的任意值。</p><p id="96aa" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">这个导数起初可能看起来令人困惑。这里有一个更容易处理的技巧:不要试图计算任何一个<em class="km"> n </em>和<em class="km"> i </em>值的一般导数。取而代之的是，针对两种不同的情况进行计算:I<em class="km">I</em>和n<em class="km">n</em>相同的情况，以及它们不同的情况。就这么办吧。</p><h2 id="8734" class="mk lc iq bd ld ml mm dn lh mn mo dp ll jz mp mq lp kd mr ms lt kh mt mu lx mv bi translated">i=n的情况</h2><p id="cfec" class="pw-post-body-paragraph jo jp iq jq b jr lz jt ju jv ma jx jy jz mb kb kc kd mc kf kg kh md kj kk kl ij bi translated">先说<em class="km"> i </em>和<em class="km"> n </em>相同的情况。这意味着我们正在计算softmax输出相对于其匹配输入的导数。</p><p id="605c" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">你还记得我们前几段提到的商法则吗？很好！让我们好好利用它:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi mw"><img src="../Images/954958bbdb164d70041b58f4aab906ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*00ehb_SzIzlw_yVboJphmw.png"/></div></div></figure><h2 id="3001" class="mk lc iq bd ld ml mm dn lh mn mo dp ll jz mp mq lp kd mr ms lt kh mt mu lx mv bi translated">我≠n的情况</h2><p id="6345" class="pw-post-body-paragraph jo jp iq jq b jr lz jt ju jv ma jx jy jz mb kb kc kd mc kf kg kh md kj kk kl ij bi translated">现在我们来看看<em class="km"> i </em>和<em class="km"> n </em>不一样的情况。因此，我们计算输出相对于任何非匹配输入的导数:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi mx"><img src="../Images/24a02599807c97fecdfa202aad1f5acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nFK7ZWZUF4iCDfRv1FL9A.png"/></div></div></figure><h2 id="a4dc" class="mk lc iq bd ld ml mm dn lh mn mo dp ll jz mp mq lp kd mr ms lt kh mt mu lx mv bi translated">总结Softmax的衍生产品</h2><p id="d41f" class="pw-post-body-paragraph jo jp iq jq b jr lz jt ju jv ma jx jy jz mb kb kc kd mc kf kg kh md kj kk kl ij bi translated">让我们把发现的导数压缩成一个公式:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi my"><img src="../Images/429576c9d92c6458f68a2cef3573a140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wRYph37510RySpWO9Fu1vg.png"/></div></div></figure><p id="2762" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">数学家们有自己的符号来表示:“只有当这个条件为真时，才应用公式的这一部分”。我用了颜色来代替。公式中绿色方框适用于<em class="km"> i=n </em>时，蓝色标签适用于<em class="km"> i≠n </em>时。</p><p id="99a4" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">这样，我们就有了softmax的导数。交叉熵损失的导数！</p><h1 id="f078" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">推导交叉熵损失</strong></h1><p id="3497" class="pw-post-body-paragraph jo jp iq jq b jr lz jt ju jv ma jx jy jz mb kb kc kd mc kf kg kh md kj kk kl ij bi translated">以下是神经网络单个输出的交叉熵损失公式:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi mz"><img src="../Images/089fbd766cdfdb01a114bdd35e590e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJNju1dOt_E5_8lGL54vqw.png"/></div></div></figure><p id="32f6" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">要获得所有网络输出的总损耗，我们可以对每个输出的损耗求和:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi na"><img src="../Images/9680db9ec732681cb4ef8ae1ea45f60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oF5BTFfI9iMn1rtv9IHtMw.png"/></div></div></figure></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="8e5b" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">为了避免混淆，我不得不离题了。在<a class="ae la" href="https://pragprog.com/titles/pplearn/" rel="noopener ugc nofollow" target="_blank">书</a>和<a class="ae la" rel="noopener ugc nofollow" target="_blank" href="/grokking-the-cross-entropy-loss-cda6eb9ec307">的另一篇文章</a>中，我向你展示了交叉熵损失的一个微妙不同的公式。这两个公式都有效，但它们的含义不同。上面的例子是单个例子的丢失——通过网络传输的一段数据。在这个公式中，<em class="km"> yᵢ </em>的意思是:“标签的第<em class="km"> i </em>个成分。”相比之下，书中的公式是整个数据集的总损失。在那里，<em class="km"> yᵢ </em>的意思是:“第<em class="km">个标签。”为了得到总损失，你可以使用上面的公式计算每个例子的损失，然后平均所有这些损失。</em></p><p id="e54b" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">换句话说:这和你从书中了解到的，并且希望爱上的，是同一个损失。然而，在书中，我们着眼于大局，而在这篇文章中，我们着眼于细节。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="15f7" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">好了，题外话说完了。让我们对它的一个输入取交叉熵损失的导数。提示:在深入研究之前，请记住链式法则。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ni"><img src="../Images/19880c79f1afb83d07259fb036014731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t5-oLcqcrM2XZBccLwTh5w.png"/></div></div></figure><p id="826c" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">之前，我们计算了softmax的导数。现在我们有了交叉熵损失的导数，用softmax的导数表示。让我们把这两个公式混合起来，看看我们会得到什么。</p><h1 id="dae9" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">把所有的放在一起</h1><p id="ac3c" class="pw-post-body-paragraph jo jp iq jq b jr lz jt ju jv ma jx jy jz mb kb kc kd mc kf kg kh md kj kk kl ij bi translated">我保证我们就快到了。我们还有最后一个计算要做——但这是一个棘手的计算。让我们将softmax的导数替换为交叉熵损失的导数:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi me"><img src="../Images/86a97a365b28287db5a815aec5a2c83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dn8T_H6ccY_xz4lWJMV7Vw.png"/></div></div></figure><p id="0faa" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated">现在我们有了:应用于softmax的交叉熵损失的导数。计算这个东西花了一些时间——但是现在我们有了它，我们可以放松下来，欣赏它的简单。更好的是:我们再也不用计算这个导数了！</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi nj"><img src="../Images/51cd52c93786d2dc6dfa3debd1cd0908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96g8PQjTFSJPBv6KCz5hzQ.jpeg"/></div></div></figure></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ae815976d39b730ca0b1dc526066865d.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*CLGEpC-MfhtV-7XY.png"/></div></figure><p id="6838" class="pw-post-body-paragraph jo jp iq jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ij bi translated"><em class="km">本帖是</em> <a class="ae la" href="http://www.progml.com/" rel="noopener ugc nofollow" target="_blank">编程机器学习</a> <em class="km">的衍生，程序员零到英雄入门，从基础到深度学习。去</em> <a class="ae la" href="http://www.pragprog.com/titles/pplearn" rel="noopener ugc nofollow" target="_blank"> <em class="km">这里</em> </a> <em class="km">找电子书，</em> <a class="ae la" href="https://www.amazon.com/gp/product/1680506609/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=ductyp-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1680506609&amp;linkId=21357a11b4a7bc9be95476540d1d3a09" rel="noopener ugc nofollow" target="_blank"> <em class="km">这里</em> </a> <em class="km">找纸质书，或者来</em> <a class="ae la" href="https://forum.devtalk.com/tag/book-programming-machine-learning" rel="noopener ugc nofollow" target="_blank"> <em class="km">论坛</em> </a> <em class="km">如果你有问题和评论！</em></p></div></div>    
</body>
</html>