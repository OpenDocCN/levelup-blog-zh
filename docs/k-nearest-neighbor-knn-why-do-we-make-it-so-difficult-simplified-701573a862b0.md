# k 近邻(KNN):为什么我们让它变得如此困难？简化了的

> 原文：<https://levelup.gitconnected.com/k-nearest-neighbor-knn-why-do-we-make-it-so-difficult-simplified-701573a862b0>

了解它的变体，以及它们的例子，所有这些都基于简单的描述

![](img/ae5e8c90a869f7a3c136c8849a494a41.png)

从[作者](https://aniltilbe.medium.com)，爱因斯坦与阿尔·帕西诺和马龙·白兰度

k-最近邻算法(KNN)是一种机器学习方法，它分析数据点的最近邻，以了解它们之间的相关性。该算法通过计算每对数据点之间的最近距离来查找相似数据的组或簇，然后利用该知识来查找相似数据的组或簇。

KNN 可用于一系列任务，如分类任务(经常作为经典线性回归方法的替代方法)和超参数优化问题。当处理庞大的数据集时，KNN 往往比其他算法更准确；它的简单性使它适用于许多问题设置。

![](img/a8401779131f209295561de01c55a8c4.png)

来自[作者](https://aniltilbe.medium.com)

## **简单解释，理论**

顾名思义，当处理坐标平面上相互靠近的数据时，最适合使用 KNN。考虑到利用 KNN 的一个目的是定位相似数据的集群或组，这是合乎逻辑的；对于那些出现的集群，数据集应该彼此足够接近。此外，因为 k-最近邻通常是基于距离而不是许多其他类型的质量(如权重)来挑选的，所以在决定算法中应该包括哪些邻居时应该给予注意。

我们在 KNN 有数据点，并正在寻找类似的。我们可以使用[欧几里德](https://link.springer.com/chapter/10.1007/978-3-540-39964-3_62)距离或者其他度量，比如余弦相似度，来找到最近的邻居。接下来，每个点根据其相邻点被分配一个标签或组。我们还可以计算这些点的平均值，并将其用作我们的新点。重复该过程，直到没有发现类似的数据点。KNN 易于构建，并且通过允许我们更高效地对数据进行分组，它对性能有重大影响。

![](img/17e42ee2e30cad8f6c184edf757e55c9.png)

来自[作者](https://aniltilbe.medium.com)

## **一些例子**

1.聚类:KNN 是聚类的理想选择，因为它考虑了数据点之间的距离，并提供了一种更有效的方法来将它们聚类在一起。当您希望定位可比较项目或行为的分组，或者只是以可理解的方式组织数据时，这种方法非常方便。例如，您可以利用 KNN 根据位置(城市)和年龄范围以及其他标准对用户进行分组。

2.时间序列分析:当处理时间序列数据时，例如价格和股票价格，经常需要发现一些模式和见解，而这些模式和见解很难单独使用其他技术快速或容易地识别。有时候，你所需要的只是对事情如何随时间变化的快速概述；群集提供这种类型的信息，而不需要花费大量的时间来搜索无数的数字和值。这种类型的模式发现对于每个点都有大量最近邻的大型数据集特别有效。对于较小的数据集，您仍然可以使用 KNN，但是在查找连接之前，您需要过滤掉不必要的数据点。

3.图像分类:当涉及到图像识别时，年龄、性别和外貌是需要考虑的一些最重要的标准。事实上，出于诊断原因，许多医疗信息是利用诸如 X 射线或 MRI 扫描之类的图像来记录的。尽管对这些图像进行了所有的数据挖掘，但对对象进行分类仍然是机器学习中最困难的挑战之一。KNN 简化了图像分类，考虑了我们在单一图像中看不到的模式(即不同类型儿童照片的年龄范围)。

![](img/0928984d28db377ac4162ccceadaf2b6.png)

来自[作者](https://aniltilbe.medium.com)

## **一些变种**

—监督 KNN:这种方法通过使用训练数据集来学习数据点之间的关系。接下来，it 可以利用这些联系将它们聚集在一起，而无需手动干预。

—无监督 KNN:因为在这种方法中我们没有任何教学或反馈数据，所以我们必须使用替代方法(如计数或计算相似性度量)来手动定位可比数据组。即使有所有这些额外的步骤，无监督 KNN 也比许多其他方法更快更准确，因为它不需要事先假设数据应该如何排序。

—半监督 KNN:监督和非监督学习的结合。我们使用一些训练数据来理解关系，但不是全部。

总的来说，KNN 是机器学习的一项重要技术，因为它帮助我们快速有效地定位可比数据组。当您想要定位与其他数据集群不同的数据集群时，经常在诸如集群或异常检测的过程中使用它。

![](img/9685af44100f4731d6d99c7a0d81df78.png)

来自[作者](https://aniltilbe.medium.com)

## **挑战**

尽管其应用广泛且精确度高，但 KNN 仍有一些局限性。例如，当坐标平面上的数据点不靠近时，它不能有效地执行；如果为太多的远点分配了最近邻值，这可能会导致标注错误的问题。此外，虽然 KNN 提出的大多数潜在的集群解决方案并不强烈依赖于输入因素，使其对变化具有鲁棒性，但也有少数例外情况，即解决方案对这些输入特别敏感。因此，在就最终的 KNN 配置达成一致之前，尝试不同的输入数据集至关重要。

## **离别的思念**

总的来说，当处理大型数据集时，k-最近邻方法产生简单性、鲁棒性和准确性；此外，它的广泛适用性使它成为所有经验水平的机器学习实践者的一个有吸引力的工具。

如果你对这篇文章有任何建议或拓宽主题的建议，我将非常感谢你的来信。只要在这个帖子的任何地方给我发一条私信。

还有，这是我的时事通讯；我希望 [**你能考虑订阅。**](https://predictiveventures.substack.com/)

如果你喜欢阅读这样的故事，并想支持我成为一名作家，可以考虑注册成为 Medium 会员，并获得无限制访问 Medium 上所有故事的权利。

此外，我还写了以下帖子，你可能会感兴趣:

[](/top-7-deep-learning-methods-each-explained-in-less-than-10-seconds-3683120de455) [## 7 大深度学习方法，每种方法用不到 10 秒钟的时间解释

### 对 7 个最重要的深度学习算法的简单解释，每个都在 10 秒内完成。

levelup.gitconnected.com](/top-7-deep-learning-methods-each-explained-in-less-than-10-seconds-3683120de455) [](/top-20-machine-learning-algorithms-explained-in-less-than-10-seconds-each-8fd728f70b19) [## 前 20 个机器学习算法，每个用不到 10 秒钟解释

### 对 20 个最重要的机器学习算法的简单解释，每个都在 10 秒内完成。

levelup.gitconnected.com](/top-20-machine-learning-algorithms-explained-in-less-than-10-seconds-each-8fd728f70b19) 

参考:这个帖子包含文本嵌入的引用。

阿尼尔·蒂尔贝

# 分级编码

感谢您成为我们社区的一员！在你离开之前:

*   👏为故事鼓掌，跟着作者走👉
*   📰查看[升级编码出版物](https://levelup.gitconnected.com/?utm_source=pub&utm_medium=post)中的更多内容
*   🔔关注我们:[Twitter](https://twitter.com/gitconnected)|[LinkedIn](https://www.linkedin.com/company/gitconnected)|[时事通讯](https://newsletter.levelup.dev)

🚀👉 [**将像你这样的开发人员安置在顶级创业公司和科技公司**](https://jobs.levelup.dev/talent/welcome?referral=true)