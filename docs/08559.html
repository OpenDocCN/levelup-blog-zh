<html>
<head>
<title>Fundamentals of Reinforcement Learning: Value Iteration and Policy Iteration with Tutorials</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的基础:价值迭代和政策迭代教程</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/fundamentals-of-reinforcement-learning-value-iteration-and-policy-iteration-with-tutorials-a7ad0049c84f?source=collection_archive---------2-----------------------#2021-05-12">https://levelup.gitconnected.com/fundamentals-of-reinforcement-learning-value-iteration-and-policy-iteration-with-tutorials-a7ad0049c84f?source=collection_archive---------2-----------------------#2021-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7661" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第二部分:解释用于解决MDP问题的价值迭代和策略迭代的概念。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b2e1872b610e1489863213926063a78e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUMexU3zpd10SkvMzt64oA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank"> Pexel </a>上的<a class="ae ky" href="https://www.pexels.com/@pixabay" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄的照片</figcaption></figure><p id="e422" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" href="https://chaodeyu.medium.com/fundamental-of-reinforcement-learning-markov-decision-process-8ba98fa66060" rel="noopener">之前的文章</a>中，我已经通过一个简单的例子和Bellman方程的推导介绍了MDP，Bellman方程是许多强化学习算法的主要组成部分之一。在本文中，我将通过一个简单的例子和如何用Python编写这些算法的教程来展示<strong class="lb iu">值迭代</strong>和<strong class="lb iu">策略迭代</strong>方法。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f287" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">设置MDP问题</h1><p id="4ec2" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">现在开始<strong class="lb iu">教程</strong>。</p><p id="15ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义:状态转换由<strong class="lb iu">p(s′| s，a) </strong>定义——如果你从状态𝑠采取行动<strong class="lb iu"> a </strong>，你有多大可能在状态<strong class="lb iu">s′</strong>结束，为了方便起见，我将使用<strong class="lb iu"> r(s，a，s′)</strong>作为奖励函数。</p><p id="bb65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程的灵感来自Berkeley的牛逼<a class="ae ky" href="https://github.com/berkeleydeeprlcourse/homework" rel="noopener ugc nofollow" target="_blank"> CS294 </a>和Yandex数据分析学院的<a class="ae ky" href="https://github.com/yandexdataschool/Practical_RL" rel="noopener ugc nofollow" target="_blank">实用RL </a>。</p><p id="30a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们定义一个与<a class="ae ky" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> Gym Env </a>兼容的MDP。它是一个开发和比较强化学习算法的工具包。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="7483" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我将使用转移概率和奖励来定义一个MDP问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/9757f9b2963574ad68613404e1ac31da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UY0GWCxS1lecZkXFny0qwg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">MDP问题根据上面的代码。</figcaption></figure><p id="90b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以像使用其他健身房环境一样使用MDP。此外，它还具有可用于值迭代和策略迭代的其他方法，例如:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8ee0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">一、价值迭代</h1><p id="b52c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">现在让我们用<strong class="lb iu">V</strong>value<strong class="lb iu">I</strong>迭代来解决上述MDP问题。</p><p id="edad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是<strong class="lb iu"> VI </strong>算法的伪代码:<br/> 1。对于每个状态，将值函数V(s)初始化为0。<br/> 2。迭代直到<strong class="lb iu">收敛</strong> <br/> *在每第I次迭代时，使用贝尔曼方程更新每个状态的值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/e737c8caedb7764f11ef65fcaf7ebb9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4UZ0wECtN1hncHjVD0W1GA.png"/></div></div></figure><p id="fc52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">收敛</strong>是指当前迭代(I)和上一次迭代(i-1)中每个状态的值之差接近于0。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="4763" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们编写一个函数来计算状态-动作值函数Q(s，a):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="e22a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Q(s，a ),我们现在可以为值迭代定义“下一个”V(s ):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="78ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们把所有的东西组合起来，写成一个有效的<strong class="lb iu"> VI </strong>算法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="d4ca" class="ni md it ne b gy nj nk l nl nm">iter    0   |   V(s0) = 0.000   V(s1) = 0.000   V(s2) = 0.000<br/><br/>iter    1   |   V(s0) = 0.000   V(s1) = 3.500   V(s2) = 0.000<br/><br/>iter    2   |   V(s0) = 0.000   V(s1) = 3.815   V(s2) = 1.890<br/><br/>iter    3   |   V(s0) = 1.701   V(s1) = 4.184   V(s2) = 2.060<br/><br/>。。。 。。。 。。。 。。。 。。。 。。。 。。。 。。。 。。。 。。。<br/><br/>iter   62   |   V(s0) = 8.019   V(s1) = 11.159   V(s2) = 8.911<br/><br/>iter   63   |   V(s0) = 8.020   V(s1) = 11.160   V(s2) = 8.912<br/><br/>iter   64   |   V(s0) = 8.021   V(s1) = 11.161   V(s2) = 8.913<br/><br/>iter   65   |   V(s0) = 8.022   V(s1) = 11.162   V(s2) = 8.915</span><span id="f37d" class="ni md it ne b gy nn nk l nl nm">Terminated</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/5ca7e7e512518a9f0198644b4b333f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eDNHZYF6MY9-iUUPyM169A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">用值迭代求解MDP问题。</figcaption></figure><p id="2f9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从迭代过程中可以看出，算法在迭代65次时缓慢收敛并终止，达到最优状态。s0、s1、s2最终状态值V*(s)分别为8.02、11.1、8.91。</p><p id="8f0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们使用这些V*(s)通过下面提到的等式来找出每个状态下的最佳行动。与V(s)的唯一区别是，这里我们采用的不是max而是arg max:find action with maximum Q(s，a)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/741848d06d53c1f87c283c7d12a2ca9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50UWXlTNuVKCSgecIL_IdA.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="bed9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个MDP问题，最优策略是在状态s0采取行动a1，在状态s1采取行动a0，在状态s2采取行动a0。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e5f0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">二。策略迭代</h1><p id="477e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">现在让我们用<strong class="lb iu"> P </strong>策略<strong class="lb iu"> I </strong>迭代来解决上面提到的同一个MDP问题。</p><p id="28eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是<strong class="lb iu"> PI </strong>算法的伪代码:<br/>从某个初始策略<strong class="lb iu"> 𝛑0 </strong>开始，在以下步骤之间交替:<br/> 1 .策略评估:为每个状态s计算<strong class="lb iu"> V^𝛑i(s) </strong>，如VI，直到收敛<br/> 2。策略改进:根据更新规则计算新的策略<strong class="lb iu"> 𝛑(i+1) </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/176fe61a78f77852d0c1c7d4f6378430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*voGS6bUbQdvSjCa-Qwqz4Q.png"/></div></div></figure><p id="6513" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与VI不同，PI必须维护一个策略，并基于该策略<strong class="lb iu">评估</strong>来估计值<strong class="lb iu"> V^𝛑i </strong>。它只在价值观趋于一致时改变政策— <strong class="lb iu">改进</strong>。评估和改进的过程重复进行，直到策略没有改变或达到最优策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/d800aae01826d9169023cfb97c259ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*du4SSAPaj5LguOnEzF6Bow.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">策略迭代的简要概述[1]</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/1a41c98132633e1c2c4438a1a89e6dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ZrWvBDOAXhqnsSgK1PWDg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">策略迭代算法:更详细的步骤[1]</figcaption></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="70f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们为<strong class="lb iu">评估</strong>编写一个名为<code class="fe nt nu nv ne b">compute_vpi</code>的函数，它计算任意策略的状态值函数<strong class="lb iu">v^𝛑</strong>𝛑。一开始，<strong class="lb iu"> 𝛑 </strong>会被随机初始化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="3072" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们获得了新的状态值，我们就可以更新我们的策略— <strong class="lb iu">改进</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="c8af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们把所有的东西组合起来，写成一个有效的<strong class="lb iu"> PI </strong>算法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="5c84" class="ni md it ne b gy nj nk l nl nm">====================== Policy Evaluation ======================<br/>iter: 0, diff: 0.0<br/>====================== Policy Improvement ======================<br/>Policy is not stable yet, go back to Policy Evaluation<br/>====================== Policy Evaluation ======================<br/>iter: 0, diff: 9.267726745376368<br/>iter: 1, diff: 0.0<br/>====================== Policy Improvement ======================<br/>Policy is not stable yet, go back to Policy Evaluation<br/>====================== Policy Evaluation ======================<br/>iter: 0, diff: 2.227647754197843<br/>iter: 1, diff: 0.0<br/>====================== Policy Improvement ======================<br/>Policy is stable :D<br/>Optimal Q value: {'s0': 8.031919916894896, 's1': 11.171970913211828, 's2': 8.924355463216552}<br/>Optimal Policy: {'s0': 'a1', 's1': 'a0', 's2': 'a0'}</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/5dd19693184f950329fd342fc8301953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EF9BwLR-rmzAGJmX7RCpxQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">利用策略迭代求解MDP问题。</figcaption></figure><p id="86f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到PI的解(最优值和策略)与VI的解几乎相同。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1861" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="0345" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们首先设置了一个与体育馆环境兼容的MDP问题，可以通过一个简单的Python字典轻松实现。在此之后，问题可以通过简单地遵循贝尔曼方程更新和最优行动方程来解决。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="139f" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">推荐阅读</h1><div class="nx ny gp gr nz oa"><a rel="noopener  ugc nofollow" target="_blank" href="/fundamental-of-reinforcement-learning-markov-decision-process-8ba98fa66060"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">强化学习的基础:马尔可夫决策过程</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">第1部分:解释马尔可夫决策过程和贝尔曼方程的概念</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener  ugc nofollow" target="_blank" href="/fundamental-of-reinforcement-learning-monte-carlo-algorithm-85428dc77f76"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">强化学习的基础:蒙特卡罗算法</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">第三部分:阐述了无模型RL算法的基本原理:蒙特卡罗算法</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="oj l"><div class="op l ol om on oj oo ks oa"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5f44" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><p id="10cd" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">[1]萨顿和巴尔托(2017年)。强化学习:导论。剑桥，麻省理工学院出版社</p></div></div>    
</body>
</html>