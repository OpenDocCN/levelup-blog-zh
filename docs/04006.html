<html>
<head>
<title>Fake News Detector: NLP Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">假新闻检测器:NLP项目</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/fake-news-detector-nlp-project-9d67e0177075?source=collection_archive---------6-----------------------#2020-06-05">https://levelup.gitconnected.com/fake-news-detector-nlp-project-9d67e0177075?source=collection_archive---------6-----------------------#2020-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f67e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下面的帖子中，我们将讨论如何创建一个NLP分类器来检测新闻是真是假。</p><p id="e783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如今，假新闻已经成为一种普遍趋势。即使是可信的媒体机构也因传播假新闻而闻名，并正在失去其可信度。那么，我们怎么能相信任何新闻是真的还是假的呢？</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/17e770d33cafc51977f339acbc41807e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RGVPc-MT0q_DCHCavFRHvA.jpeg"/></div></div></figure><p id="dd0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个项目中，我建立了一个分类器模型，可以识别新闻的真假。为此，我使用了Kaggle的数据，但是您可以使用任何数据按照相同的方法来构建这个模型。</p><h2 id="4e59" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">资料组</h2><p id="3519" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated"><a class="ae lv" href="https://www.kaggle.com/c/fake-news/data" rel="noopener ugc nofollow" target="_blank"> Kaggle Data </a> <br/> train.csv:一个完整的训练数据集，具有以下属性:</p><ul class=""><li id="e917" class="lw lx iq jp b jq jr ju jv jy ly kc lz kg ma kk mb mc md me bi translated">id:新闻文章的唯一id</li><li id="70a7" class="lw lx iq jp b jq mf ju mg jy mh kc mi kg mj kk mb mc md me bi translated">标题:新闻文章的标题</li><li id="1432" class="lw lx iq jp b jq mf ju mg jy mh kc mi kg mj kk mb mc md me bi translated">作者:新闻文章的作者</li><li id="b2d1" class="lw lx iq jp b jq mf ju mg jy mh kc mi kg mj kk mb mc md me bi translated">正文:文章的正文；可能不完整</li><li id="6ea8" class="lw lx iq jp b jq mf ju mg jy mh kc mi kg mj kk mb mc md me bi translated">标签:将文章标记为潜在不可靠的标签。<br/>其中1:不可靠，0:可靠。</li></ul><h2 id="6827" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">读取数据</h2><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="ea0d" class="kx ky iq ml b gy mp mq l mr ms">import pandas as pd<br/>train = pd.read_csv('train.csv')</span><span id="77e2" class="kx ky iq ml b gy mt mq l mr ms">train.head()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mu"><img src="../Images/a1f08fd1b09af169da56eb4fa2442f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Mvh15yIOhV7ipUdmBP6OA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">这是训练数据的样子</figcaption></figure><p id="151e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到“标题”、“作者”和“文本”这些特征都很重要，而且都是文本形式的。因此，我们可以将这些特征组合成一个最终特征，用于训练模型。让我们称这个特征为“总的”。</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="5fef" class="kx ky iq ml b gy mp mq l mr ms"># Firstly, fill all the null spaces with a space<br/>train = train.fillna(' ')<br/>train['total'] = train['title'] + ' ' + train['author'] + ' ' +<br/>                 train['text']</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mz"><img src="../Images/b27dc0a858b31e0d20efcc31300ff325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qM8waIZmNNbVFHv_MLN5Fg.jpeg"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">添加“总计”列后，数据如下所示</figcaption></figure><h2 id="207f" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">预处理/清理数据</h2><p id="2852" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">为了预处理数据，我们需要一些库。</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="e771" class="kx ky iq ml b gy mp mq l mr ms">import ntlk<br/>from ntlk.corpus import stopwords<br/>from ntlk.stem import WordNetLemmatizer</span></pre><p id="ce43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面解释了所有这些库的用途。</p><p id="b639" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">停用词:</strong>停用词是那些在文本中多次出现且无助于机器理解文本的常用词。我们不希望这些词出现在我们的数据中。所以，我们去掉这些词。</p><p id="3daa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有这些停用词都以不同的语言存储在ntlk库中。</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="c229" class="kx ky iq ml b gy mp mq l mr ms">stop_words = stopwords.words('english')</span></pre><p id="85ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">分词:</strong>单词分词是将大样本文本拆分成单词的过程。<br/>例如:</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="d320" class="kx ky iq ml b gy mp mq l mr ms">word_data = "It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms"</span><span id="c626" class="kx ky iq ml b gy mt mq l mr ms">nltk_tokens = nltk.word_tokenize(word_data)<br/>print(ntlk_tokens)</span></pre><p id="d34f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它会将字符串word_data转换成这个:<br/> ['It '，' originated '，' from '，' the '，' idea '，' that '，' there '，' are '，' readers '，' who '，' prefer '，' learning '，' new '，' skills '，' from '，' comforts '，' of '，' thes '，' drawing '，' rooms']</p><p id="89bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">词汇化:</strong>词汇化是将同一个词根词的不同词形变化组合在一起的过程，这样它们就可以作为一个单项进行分析。<br/>词汇化的例子:<br/>游泳→游<br/>岩石→岩石<br/>更好→好</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="47b2" class="kx ky iq ml b gy mp mq l mr ms">lemmatizer = WordNetLemmatizer()</span></pre><p id="9a4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的代码是为了我们的测试数据的词汇化，同时排除了停用词。</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="abad" class="kx ky iq ml b gy mp mq l mr ms">for index, row in train.iterrows():<br/>    filter_sentence = ''<br/>    sentence = row['total']</span><span id="7535" class="kx ky iq ml b gy mt mq l mr ms">    # Cleaning the sentence with regex<br/>    sentence = re.sub(r'[^\w\s]', '', sentence)</span><span id="7896" class="kx ky iq ml b gy mt mq l mr ms">    # Tokenization<br/>    words = nltk.word_tokenize(sentence)</span><span id="c842" class="kx ky iq ml b gy mt mq l mr ms">    # Stopwords removal<br/>    words = [w for w in words if not w in stop_words]</span><span id="ef6b" class="kx ky iq ml b gy mt mq l mr ms">    # Lemmatization<br/>    for words in words:<br/>        filter_sentence = filter_sentence  + ' ' +<br/>                         str(lemmatizer.lemmatize(words)).lower()</span><span id="70bd" class="kx ky iq ml b gy mt mq l mr ms">    train.loc[index, 'total'] = filter_sentence</span><span id="5bf4" class="kx ky iq ml b gy mt mq l mr ms">train = train[['total', 'label']]</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2855e530a1a746dcb47f6cb37a7c3e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*hZLk9Z4zrqN0d5ykDW7l2g.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">这是预处理后的数据</figcaption></figure><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="8d9d" class="kx ky iq ml b gy mp mq l mr ms">X_train = train['total']</span><span id="9085" class="kx ky iq ml b gy mt mq l mr ms">Y_train = train['label']</span></pre><p id="b2cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们已经对数据进行了预处理，但它仍然是文本形式的，我们无法将其作为输入提供给我们的机器学习模型。为此我们需要数字。怎么才能解决这个问题？答案是矢量器。</p><h2 id="c09e" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">矢量器</h2><p id="11e4" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">为了将文本数据转换成数字数据，我们将使用两个矢量器。</p><ol class=""><li id="4323" class="lw lx iq jp b jq jr ju jv jy ly kc lz kg ma kk nb mc md me bi translated"><strong class="jp ir">计数矢量器<br/> </strong>为了使用文本数据进行预测建模，必须对文本进行解析以删除某些单词——这个过程被称为<strong class="jp ir">标记化</strong>。这些单词需要被编码为整数或浮点值，作为机器学习算法的输入。这个过程叫做<strong class="jp ir">特征提取(或矢量化)</strong>。</li><li id="6151" class="lw lx iq jp b jq mf ju mg jy mh kc mi kg mj kk nb mc md me bi translated"><strong class="jp ir"> TF-IDF矢量器<br/> </strong> TF-IDF代表<strong class="jp ir">词频—逆文档频率</strong>。它是用于信息检索的最重要的技术之一，用来表示特定的单词或短语对给定的文档有多重要。<br/>点击阅读更多关于此<a class="ae lv" href="https://www.geeksforgeeks.org/sklearn-feature-extraction-with-tf-idf/" rel="noopener ugc nofollow" target="_blank">的信息。</a></li></ol><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="1f29" class="kx ky iq ml b gy mp mq l mr ms">from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="6ab0" class="kx ky iq ml b gy mt mq l mr ms">count_vectorizer = CountVectorizer()<br/>count_vectorizer.fit_transform(X_train)<br/>freq_term_matrix = count_vectorizer.transform(X_train)<br/>tfidf = TfidfTransformer(norm = "l2")<br/>tfidf.fit(freq_term_matrix)<br/>tf_idf_matrix = tfidf.fit_transform(freq_term_matrix)</span></pre><p id="e964" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面写的代码将为你提供一个代表你的文本的矩阵。它将是一个稀疏矩阵，具有大量压缩稀疏行格式的元素。</p><h2 id="6b2f" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">系统模型化</h2><p id="d635" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">现在，我们必须决定哪种分类模型最适合我们的问题。<br/>首先，我们将拆分数据，然后训练模型，以预测我们的模型有多准确。</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="d0f3" class="kx ky iq ml b gy mp mq l mr ms">from sklearn.model_selection import train_test_split</span><span id="7928" class="kx ky iq ml b gy mt mq l mr ms">X_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix,<br/>                                   Y_train, random_state=0)</span></pre><p id="038f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将在这里实现三个模型，并比较它们的性能。</p><ol class=""><li id="de5b" class="lw lx iq jp b jq jr ju jv jy ly kc lz kg ma kk nb mc md me bi translated">逻辑回归</li></ol><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="0047" class="kx ky iq ml b gy mp mq l mr ms">from sklearn.linear_model import LogisticRegression</span><span id="d21f" class="kx ky iq ml b gy mt mq l mr ms">logreg = LogisticRegression()<br/>logreg.fit(X_train, y_train)</span><span id="9dec" class="kx ky iq ml b gy mt mq l mr ms">Accuracy = logreg.score(X_test, y_test)</span></pre><p id="9da3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.朴素贝叶斯</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="f508" class="kx ky iq ml b gy mp mq l mr ms">from sklearn.naive_bayes import MultinomialNB</span><span id="98bc" class="kx ky iq ml b gy mt mq l mr ms">NB = MultinomialNB()<br/>NB.fit(X_train, y_train)</span><span id="39dd" class="kx ky iq ml b gy mt mq l mr ms">Accuracy = NB.score(X_test, Y_test)</span></pre><p id="c632" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.决策树分类器</p><pre class="km kn ko kp gt mk ml mm mn aw mo bi"><span id="1204" class="kx ky iq ml b gy mp mq l mr ms">from sklearn.tree import DecisionTreeClassifier</span><span id="43e1" class="kx ky iq ml b gy mt mq l mr ms">clf = DecisionTreeClassifier()<br/>clf.fit(X_train, y_train)</span><span id="cffe" class="kx ky iq ml b gy mt mq l mr ms">Accuracy = clf.score(X_test, Y_test)</span></pre><h2 id="e5f7" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">表演</h2><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/7506b4668e8e9e3a7ae7ecf7adf165b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*dl72pk-h-Mw0nOiypELeLw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">模型在测试集上的性能</figcaption></figure><p id="5832" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，决策树分类器在训练集上表现最好，准确率为97%。</p><p id="c474" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我还在不属于训练集的不同测试集上测试了该模型，它给出了96.98%的准确率，这是相当不错的。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="ec11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读。我希望你喜欢我的文章，并发现它是有帮助的。如果你有任何问题或建议，请随时写在评论区。你可以在这里和我连线:<a class="ae lv" href="https://www.linkedin.com/in/juyalishant/" rel="noopener ugc nofollow" target="_blank">伊桑·朱亚尔</a></p></div></div>    
</body>
</html>