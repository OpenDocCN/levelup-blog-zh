<html>
<head>
<title>Data Manipulation with Pyspark in 10 Steps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pyspark在10个步骤中进行数据操作</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/data-manipulation-with-pyspark-in-10-steps-ac9d4a0f96f9?source=collection_archive---------4-----------------------#2022-08-29">https://levelup.gitconnected.com/data-manipulation-with-pyspark-in-10-steps-ac9d4a0f96f9?source=collection_archive---------4-----------------------#2022-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f4a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我将逐步指导您如何使用PySpark对真实世界的数据进行数据操作。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4b27670b5f363a63820bbc7b04076029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y9D68t-vnUTL8Lvz"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@_visalliart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安东尼诺·维萨利</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="4f1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark是一个多语言引擎，用于在单节点机器或集群上实现数据工程、数据科学和机器学习项目。您可以使用它来处理批处理和实时数据处理工作负载。它还允许您通过分布式系统以高速和健壮的机制处理大量数据集。分布式系统帮助你使用一组独立的计算机。在本文中，我将通过10个步骤向您展示如何使用PySpark执行数据操作，并涵盖以下主题:</p><ul class=""><li id="384a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">PySpark是什么？</li><li id="3901" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">阅读和理解数据</li><li id="8a41" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">选择列和数据过滤</li><li id="afa7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">添加新列</li><li id="cabc" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将数据分组并应用用户定义的函数</li><li id="984f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">删除和写入数据</li></ul><p id="bd26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始吧！</p><h1 id="de7e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">PySpark是什么？</h1><p id="04bd" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">Spark主要是用Scala编写的，但是你可以用Python来使用Spark。Python是一种动态的通用语言。Pyspark是一个用于spark的Python API，Spark是一个强大的数据处理和分析工具。它允许您使用Python的灵活性和Spark的分布式处理能力。在PySpark发布后，Spark的受欢迎程度在最近几年有了显著提高。</p><p id="3b1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是PySpark的一些优点:</p><ul class=""><li id="8c57" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">PySpark易于使用，因此您可以将其与基于Python的数据科学库集成。</li><li id="d250" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">它是可扩展的，所以你可以用它来处理大量的数据。</li><li id="6d3a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">它是如此的高效，以至于你可以用它在分布式处理系统中处理数据。</li><li id="8ab6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">它非常灵活，因此您可以使用它来处理来自各种来源的数据。</li></ul><p id="2741" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以在jupyter笔记本中安装以下命令。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="3714" class="nj mh iq nf b gy nk nl l nm nn">!pip install pyspark</span></pre><p id="e82d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看如何用PySpark的10个步骤进行数据操作。</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="da62" class="mg mh iq bd mi mj nv ml mm mn nw mp mq jw nx jx ms jz ny ka mu kc nz kd mw mx bi translated">1.创建SparkSession</h1><p id="ff62" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">要使用PySpark，首先需要创建SparkSession。SparkSession是PySpark功能的入口点。运行Spark应用程序时，会启动一个具有主要功能的驱动程序。之后，这个驱动程序在worker节点上运行执行器内部的操作。让我们实例化SparkSession来使用PySpark。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="f41a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PySpark使用带有SparkSession.builder对象的构建器模式，该模式提供了一组方法。您可以使用<code class="fe oc od oe nf b">appName</code>方法给应用程序命名。<code class="fe oc od oe nf b">getOrCreate</code>方法用于在交互和批处理模式下工作，如果已经存在SparkSession，则避免创建新的spark session。</p><h1 id="a5f9" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">2.读取数据</h1><p id="2187" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">本文中我将使用的数据是CSV格式的糖尿病数据集。数据集的目的是基于数据集中包含的某些诊断测量结果，诊断性地预测患者是否患有糖尿病。数据集由几个医学预测变量和一个目标变量<code class="fe oc od oe nf b">Outcome</code>组成。预测变量包括患者的怀孕次数、身体质量指数、胰岛素水平、年龄等。你可以在这里找到这个数据集<a class="ae kv" href="https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="b87c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PySpark在执行操作时提供了两种主要的数据存储结构:RDD和数据帧。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/69fe50f7ca7d0cfd3ce4c64fe4ab8fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i1KH_f7odYN2_EPfJtdyvg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">RNN vs数据框</figcaption></figure><p id="1c67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以将RDD视为对象(或行)的分布式集合。你可以把DataFrame想象成一个表格。请注意，数据帧按列组织记录。让我们以DataFrame的形式读取数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="5767" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将inferSchema参数设置为True。所以后台的Spark会自己推断数据集中值的数据类型。</p><h1 id="aeca" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">3.理解数据</h1><p id="f3ae" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">理解数据是数据分析中最重要的步骤之一。让我们用<code class="fe oc od oe nf b">show</code>方法来看看数据集的前十行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="d190" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以用<code class="fe oc od oe nf b">columns</code>方法打印数据集的列名。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="f8be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用<code class="fe oc od oe nf b">count</code>方法获得数据帧中的记录总数。<code class="fe oc od oe nf b">len</code>方法允许你查看数据帧中的列数。让我们用<code class="fe oc od oe nf b">count</code>和<code class="fe oc od oe nf b">len</code>方法来看看数据集的形状。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="c460" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要获取数据集的架构信息，可以使用printSchema方法，该方法通常用于通过数据分析中的show方法来理解数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="cd57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用<code class="fe oc od oe nf b">describe().show()</code>来查看数据集的描述性统计数据。我将使用<code class="fe oc od oe nf b">truncate</code>参数只看到8个字符。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="1128" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">4.选择列</h1><p id="bb94" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">您可以使用<code class="fe oc od oe nf b">select</code>方法来选择特定的列。让我们用<code class="fe oc od oe nf b">select</code>方法从数据集中提取怀孕和年龄列。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="9e5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还可以使用pyspark.sql.functions模块中的<code class="fe oc od oe nf b">col</code>函数来选择列。让我展示给你看。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="c099" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">5.数据过滤</h1><p id="a32b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">若要清理数据集并仅保留所需的记录，可以执行基于条件筛选记录。过滤数据有两种方法:<code class="fe oc od oe nf b">filter()</code>和<code class="fe oc od oe nf b">where()</code>。让我们用<code class="fe oc od oe nf b">filter</code>方法过滤年龄列的值小于40的数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="6784" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用<code class="fe oc od oe nf b">select</code>方法执行进一步的过滤，只查看特定的列。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="d721" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您还可以应用基于条件的过滤记录。让我们找到60岁以上的人和生病的人的记录。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="e4bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用像<code class="fe oc od oe nf b">&amp;</code>和<code class="fe oc od oe nf b">|</code>符号这样的操作符来应用多个过滤条件。让我们用<code class="fe oc od oe nf b">&amp;</code>来筛选生病和怀孕10次或以上的人。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="2bba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要计算过滤后的记录数，可以使用<code class="fe oc od oe nf b">count</code>方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="9f04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您也可以像使用<code class="fe oc od oe nf b">filter</code>方法一样使用<code class="fe oc od oe nf b">where</code>方法过滤数据。让我向您展示如何使用前面的命令。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="2d10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您也可以将<code class="fe oc od oe nf b">where</code>方法与<code class="fe oc od oe nf b">count</code>方法一起使用。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="b2cb" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">6.添加新列</h1><p id="1ed7" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">您可以使用<code class="fe oc od oe nf b">withColumn</code>方法添加一个新列。让我们使用age列创建一个新列。为此，我将把年龄值添加到十个值中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="24d1" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">7.分组数据</h1><p id="089f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">当处理大量数据时，我们通常会使用<code class="fe oc od oe nf b">groupBy</code>方法来汇总数据。将数据分组后，可以对每个数据应用聚合函数。让我们看看结果列中每个分类值的总和。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="f5f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您还可以使用<code class="fe oc od oe nf b">distinct</code>和<code class="fe oc od oe nf b">count</code>方法来查找列中的不同值。让我们看看怀孕一栏中的地区值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="1a98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用其他聚合函数，如<code class="fe oc od oe nf b">sum</code>、<code class="fe oc od oe nf b">mean</code>或<code class="fe oc od oe nf b">min</code>。让我们在对结果列进行分组后，找出年龄的平均值。注意，<code class="fe oc od oe nf b">alias</code>方法用于命名新列。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="9049" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">8.应用用户定义的函数</h1><p id="4f91" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">您也可以在pyspark.sql.functions模块中使用UDF(用户定义函数)将自己的函数应用于分组数据。为了说明这一点，让我们首先创建一个名为diabete的函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="fdf6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们声明UDF及其返回类型(本例中为StringType)。之后，我将使用withColumn创建一个新列，然后传递相关的Dataframe列(output):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="7b58" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">9.删除数据</h1><p id="efb4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">要删除一列或多列，可以使用PySpark中的<code class="fe oc od oe nf b">drop</code>方法。让我们用<code class="fe oc od oe nf b">drop</code>方法删除<code class="fe oc od oe nf b">Insulin</code>列。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="6e64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要从数据帧中删除重复记录，可以使用<code class="fe oc od oe nf b">dropDuplicates</code>方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="55fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，数据集中没有重复的记录。</p><h1 id="efed" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">10.写入数据</h1><p id="40b3" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在执行数据操作之后，您通常会想要导出您的结果。您可以使用<code class="fe oc od oe nf b">write</code>方法将干净的数据帧以所需的格式写入所需的位置。让我们将结果写在CSV文件夹中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="e11e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看这个文件夹的前五行。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="35f3" class="nj mh iq nf b gy nk nl l nm nn">$ls data/my_dataset.csv | head -5</span><span id="7c1d" class="nj mh iq nf b gy og nl l nm nn">Output:<br/>_SUCCESS <br/>part-00000-2b5e0da4-fab1-4141-ae6f-1f89eea9818f-c000.csv part-00001-2b5e0da4-fab1-4141-ae6f-1f89eea9818f-c000.csv part-00002-2b5e0da4-fab1-4141-ae6f-1f89eea9818f-c000.csv part-00003-2b5e0da4-fab1-4141-ae6f-1f89eea9818f-c000.csv</span></pre><p id="98f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，这个文件夹包括许多分区。为了减少分区的数量，您可以使用具有所需分区数量的<code class="fe oc od oe nf b">coalesce</code>方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="8ce7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们再看一下这个文件夹</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="0128" class="nj mh iq nf b gy nk nl l nm nn">$ls data/my_dataset.csv | head</span><span id="88e5" class="nj mh iq nf b gy og nl l nm nn">Output:<br/>_SUCCESS<br/>part-00000-53da5401-5d0a-4fad-a7e3-0114833b68d8-c000.csv</span></pre><p id="e591" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，这个文件夹中只有一个CSV文件。</p><h1 id="45e5" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="6ba9" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">Pyspark是一个与spark一起工作的Python API，Spark是一个强大的数据处理和分析工具<em class="nd">。</em>它允许您使用Python的灵活性和Spark的分布式处理能力。在这本笔记本中，我谈到了使用PySpark从读取数据到导出数据的数据操作。你可以在这里找到这本笔记本<a class="ae kv" href="https://www.kaggle.com/code/tirendazacademy/data-manipulation-with-pyspark" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="370f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读。我希望你喜欢它。别忘了关注我们的<a class="ae kv" href="https://jovian.ai/outlink?url=http%3A%2F%2Fyoutube.com%2Ftirendazacademy" rel="noopener ugc nofollow" target="_blank">YouTube</a>|<a class="ae kv" href="https://jovian.ai/outlink?url=http%3A%2F%2Ftirendazacademy.medium.com" rel="noopener ugc nofollow" target="_blank">Medium</a>|<a class="ae kv" href="https://jovian.ai/outlink?url=http%3A%2F%2Ftwitter.com%2Ftirendazacademy" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae kv" href="https://jovian.ai/outlink?url=http%3A%2F%2Fgithub.com%2Ftirendazacademy" rel="noopener ugc nofollow" target="_blank">GitHub</a>|<a class="ae kv" href="https://jovian.ai/outlink?url=https%3A%2F%2Fwww.linkedin.com%2Fin%2Ftirendaz-academy" rel="noopener ugc nofollow" target="_blank">Linkedin</a>|<a class="ae kv" href="https://jovian.ai/outlink?url=https%3A%2F%2Fwww.kaggle.com%2Ftirendazacademy" rel="noopener ugc nofollow" target="_blank">ka ggle</a>😎</p><div class="oh oi gp gr oj ok"><a href="https://heartbeat.comet.ml/three-tree-based-machine-learning-models-b69504af12d6" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">三种基于树的机器学习模型</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">关于如何使用Scikit实现决策树、随机森林和XGBoost的指南-学习和优化超参数…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">heartbeat.comet.ml</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy kp ok"/></div></div></a></div><h1 id="f711" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">资源</h1><ul class=""><li id="0759" class="ls lt iq ky b kz my lc mz lf oz lj pa ln pb lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.amazon.com/Analysis-Python-PySpark-Jonathan-Rioux/dp/1617297208" rel="noopener ugc nofollow" target="_blank">使用Python和PySpark进行数据分析</a></li><li id="e3db" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.amazon.com/Machine-Learning-PySpark-Processing-Recommender/dp/1484277767" rel="noopener ugc nofollow" target="_blank">用PySpark进行机器学习</a></li></ul><p id="dcad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果这篇文章有帮助，请点击拍手👏按钮几下，以示支持👇</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="93fd" class="mg mh iq bd mi mj nv ml mm mn nw mp mq jw nx jx ms jz ny ka mu kc nz kd mw mx bi translated">分级编码</h1><p id="1ad4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">感谢您成为我们社区的一员！在你离开之前:</p><ul class=""><li id="1c00" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">👏为故事鼓掌，跟着作者走👉</li><li id="082d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">📰查看<a class="ae kv" href="https://levelup.gitconnected.com/?utm_source=pub&amp;utm_medium=post" rel="noopener ugc nofollow" target="_blank">级编码出版物</a>中的更多内容</li><li id="a3d0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">🔔关注我们:<a class="ae kv" href="https://twitter.com/gitconnected" rel="noopener ugc nofollow" target="_blank">推特</a> | <a class="ae kv" href="https://www.linkedin.com/company/gitconnected" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a> | <a class="ae kv" href="https://newsletter.levelup.dev" rel="noopener ugc nofollow" target="_blank">时事通讯</a></li></ul><p id="1ae1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">🚀👉<a class="ae kv" href="https://jobs.levelup.dev/talent/welcome?referral=true" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">加入升级人才集体，找到一份惊艳的工作</strong> </a></p></div></div>    
</body>
</html>