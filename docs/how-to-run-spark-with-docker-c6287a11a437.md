# å¦‚ä½•ç”¨ Docker è¿è¡Œ Spark

> åŸæ–‡ï¼š<https://levelup.gitconnected.com/how-to-run-spark-with-docker-c6287a11a437>

## Pyspak æ•™ç¨‹

![](img/015131458bc57e0adf54cf59834c78cb.png)

[é˜¿å¸•å¥‡ Spark](https://spark.apache.org/) çš„å®˜æ–¹ logo

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†æŒ‡å¯¼æ‚¨åœ¨ Docker å®¹å™¨ä¸­å®‰è£…å’Œè¿è¡Œ Apache Spark å’Œ PySparkã€‚

å¯¹äºä¸ç†Ÿæ‚‰ Spark çš„äººæ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼è®¡ç®—ç³»ç»Ÿï¼Œå¯ä»¥å¿«é€Ÿå¤„ç†å¤§é‡æ•°æ®ã€‚PySpark æ˜¯ Spark çš„ Python æ¥å£ï¼Œå…è®¸æ‚¨ä½¿ç”¨ Spark çš„å¼ºå¤§åŠŸèƒ½å’Œ Python çš„ç®€å•æ€§ã€‚

æœ¬æ–‡åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†:

*   Spark å’Œ Docker çš„ä»‹ç»ï¼Œè§£é‡Šä½¿ç”¨å®ƒä»¬è¿›è¡Œæ•°æ®å¤„ç†çš„å¥½å¤„ï¼›
*   åœ¨ Docker å®¹å™¨ä¸­å®‰è£…å’Œè¿è¡Œ Spark å’Œ PySpark çš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬è®¾ç½®å¿…è¦çš„ä¾èµ–é¡¹å’Œé…ç½®ã€‚
*   æœ€åï¼Œåœ¨ç¬¬ä¸‰éƒ¨åˆ†ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•è¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ª PySpark è„šæœ¬ã€‚

åœ¨æœ¬æ•™ç¨‹ç»“æŸæ—¶ï¼Œæ‚¨å°†æ‹¥æœ‰ä¸€ä¸ªåœ¨ Docker å®¹å™¨ä¸­è¿è¡Œçš„ Spark å’Œ PySpark çš„å·¥ä½œå®‰è£…ï¼Œå¹¶å‡†å¤‡å¥½å¼€å§‹ä½¿ç”¨å®ƒä»¬æ¥å®Œæˆæ‚¨è‡ªå·±çš„æ•°æ®å¤„ç†ä»»åŠ¡ã€‚

æœ¬æ–‡å°†ä½¿ç”¨æœ€å®Œæ•´çš„é…ç½®ï¼Œè€Œä¸æ˜¯ä½¿ç”¨â€œjupyter/pyspark-notebookâ€Docker æ˜ åƒçš„é…ç½®ã€‚

æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼ğŸ˜

# 1.Spark å’Œ Docker ç®€ä»‹

# ç«èŠ±/ Pyspark

Apache Spark æ˜¯ä¸€ä¸ªæµè¡Œçš„å¼€æºæ•°æ®å¤„ç†å¼•æ“ï¼Œå¹¿æ³›åº”ç”¨äºå¤§æ•°æ®é¢†åŸŸã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œå®ƒèƒ½å¤Ÿå¿«é€Ÿé«˜æ•ˆåœ°å¤„ç†å¤§é‡æ•°æ®ï¼Œæ˜¯æ•°æ®ç§‘å­¦å®¶å’Œåˆ†æå¸ˆçš„å®è´µå·¥å…·ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºäº† Apache Spark çš„ä¸‰å¤§ç‰¹å¾ã€‚

*   **åˆ†å¸ƒå¼å¤„ç†â€”** Apache Spark å¯ä»¥è·¨å¤šä¸ªåˆ†å¸ƒå¼è®¡ç®—é›†ç¾¤å¤„ç†å¤§é‡æ•°æ®ï¼Œä½¿å…¶é«˜æ•ˆä¸”å¯æ‰©å±•ã€‚
*   **å†…å­˜è®¡ç®—â€”** Apache Spark å°†æ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œä»è€Œå®ç°æ›´å¿«çš„å¤„ç†å’Œå®æ—¶åˆ†æ
*   **æ”¯æŒå¤šç§è¯­è¨€** â€” Apache Spark æ”¯æŒå¤šç§è¯­è¨€ç¼–ç¨‹ï¼ŒåŒ…æ‹¬ Pythonã€Java å’Œ Scalaã€‚

# ç å¤´å·¥äºº

ä¸€å¹´å‰ï¼Œæˆ‘å¼€å§‹ä»äº‹ **Docker** çš„å·¥ä½œï¼Œè¿™æ˜¯ä¸€å¥—å¥‡å¦™çš„å¹³å°è½¯ä»¶(PAAS)äº§å“ï¼Œæˆ‘ç°åœ¨æ˜¯å®ƒçš„è¶…çº§ç²‰ä¸ã€‚

è¯¥å·¥å…·ä½¿ç”¨æ“ä½œç³»ç»Ÿçº§è™šæ‹ŸåŒ–ï¼Œå…è®¸å¯¹å®¹å™¨ä¸­çš„è½¯ä»¶è¿›è¡Œå¥‡å¦™çš„å®šåˆ¶ï¼Œå¯ä»¥è½»æ¾åœ°ä¸æ‚¨çš„åŒäº‹æˆ–å¼€å‘ç¯å¢ƒä¹‹é—´å…±äº«ï¼Œâ€œ*å¹¶ç¡®ä¿æ‚¨å…±äº«çš„æ¯ä¸ªäººéƒ½è·å¾—ä»¥ç›¸åŒæ–¹å¼å·¥ä½œçš„ç›¸åŒå®¹å™¨â€*(ç”±[å®˜æ–¹ Docker ç½‘ç«™](https://docs.docker.com/get-started/overview/#:~:text=Docker%20provides%20the%20ability%20to,simultaneously%20on%20a%20given%20host.))ã€‚

Docker çš„å·¥ä½œæ–¹å¼éå¸¸ç®€å•ï¼Œå› ä¸ºå®ƒä½¿ç”¨äº†å®¢æˆ·æœº-æœåŠ¡å™¨æ¶æ„ã€‚"*Docker å®¢æˆ·ç«¯ä¸* ***Docker å®ˆæŠ¤è¿›ç¨‹*** *å¯¹è¯ï¼Œåè€…è´Ÿè´£æ„å»ºã€è¿è¡Œå’Œåˆ†å‘ Docker å®¹å™¨ã€‚Docker å®¢æˆ·æœºå’Œå®ˆæŠ¤è¿›ç¨‹å¯ä»¥åœ¨åŒä¸€ä¸ªç³»ç»Ÿä¸Šè¿è¡Œï¼Œæˆ–è€…æ‚¨å¯ä»¥å°† Docker å®¢æˆ·æœºè¿æ¥åˆ°è¿œç¨‹ Docker å®ˆæŠ¤è¿›ç¨‹ã€‚Docker å®¢æˆ·æœºå’Œå®ˆæŠ¤ç¨‹åºä½¿ç”¨ REST API é€šè¿‡ UNIX å¥—æ¥å­—æˆ–ç½‘ç»œæ¥å£è¿›è¡Œé€šä¿¡ã€‚å¦ä¸€ä¸ª Docker å®¢æˆ·ç«¯æ˜¯ Docker Composeï¼Œå®ƒè®©æ‚¨å¯ä»¥ä½¿ç”¨ç”±ä¸€ç»„å®¹å™¨ç»„æˆçš„åº”ç”¨ç¨‹åºã€‚*(æ–‡å­—æ‘˜è‡ª Docker å®˜æ–¹ç½‘ç«™)ã€‚

![](img/3252c2faaedbef84b250893a410f4011.png)

Docker æ¶æ„(æ¥è‡ª Docker å®˜æ–¹ç½‘ç«™)

ä½¿ç”¨ Docker æœ€å¸¸è§çš„æ–¹å¼æ˜¯åœ¨ä¸€ä¸ªåä¸º **Dockerfile** çš„æ–‡æœ¬æ–‡ä»¶ä¸­è®¾ç½®å‡ ä¸ªæŒ‡ä»¤ã€‚è¯¥æ–‡ä»¶é¦–å…ˆä»å…¬å…± Docker å­˜å‚¨åº“ä¸­â€œè°ƒç”¨â€ä¸€ä¸ªå›¾åƒ(ä¾‹å¦‚ Python å›¾åƒã€æ°”æµå›¾åƒç­‰)æ¥è®¾ç½®åŸºæœ¬å›¾åƒã€‚ç„¶åï¼Œå®ƒå°†è¿è¡Œå‡ ä¸ªç”¨æˆ·å®šä¹‰çš„å‘½ä»¤æ¥å®šåˆ¶æ‚¨çš„æ–°æ˜ åƒã€‚

æœ€åï¼Œåœ¨è¿è¡Œâ€œdocker buildâ€å‘½ä»¤åï¼Œä¸€ä¸ªæ–°çš„æ˜ åƒè¢«åˆ›å»ºï¼Œæ•´ä¸ªä¸Šä¸‹æ–‡(é€’å½’åœ°)è¢«å‘é€åˆ°å®ˆæŠ¤è¿›ç¨‹ã€‚

# 3.ä½¿ç”¨ Docker è¿è¡Œ Spark/PySpark

ä¸ºäº†åœ¨ Docker å®¹å™¨ä¸­è¿è¡Œ Spark å’Œ Pysparkï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘ä¸€ä¸ª Docker æ–‡ä»¶æ¥è¿è¡Œå®šåˆ¶çš„æ˜ åƒã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä» Docker Hub è°ƒç”¨ Python 3.9.1 é•œåƒ:

```
FROM python:3.9.1
```

å¯¹äºæ¥ä¸‹æ¥çš„æ­¥éª¤ï¼Œæ‚¨éœ€è¦ä¸‹è½½æ–‡ä»¶â€œfhvhv _ tripdata _ 2021â€“01 . CSV . gz â€,æ‚¨å¯ä»¥åœ¨æ­¤[é“¾æ¥ä¸­è·å¾—è¯¥æ–‡ä»¶ã€‚](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhv)è¯¥æ–‡ä»¶ä¹Ÿå¯ç”¨äºå…¶ä»–é¡¹ç›®ã€‚

æ¥ä¸‹æ¥çš„æ­¥éª¤åŒ…æ‹¬å®‰è£…â€œcurlâ€ã€â€œwgetâ€å’Œâ€œpandasâ€ã€‚æˆ‘ä»¬è¿˜å°†ä¹‹å‰ä¸‹è½½çš„æ–‡ä»¶å¤åˆ¶åˆ°å®¹å™¨ä¸­ã€‚

```
RUN apt-get install curl wget
RUN pip install pandas

COPY fhvhv_tripdata_2021-01.csv.gz .
```

ä¸ºäº†è·å¾—æ›´æœ‰æ¡ç†çš„é…ç½®ï¼Œæˆ‘ä»¬å°†è®¾ç½®è¦ä½¿ç”¨çš„ Sparkã€Hadoop å’Œ Java ç‰ˆæœ¬:

```
# VERSIONS
ENV SPARK_VERSION=3.3.1 \
HADOOP_VERSION=3 \
JAVA_VERSION=11
```

ç„¶åæˆ‘ä»¬éœ€è¦è®¾ç½® Java ç¯å¢ƒå˜é‡ï¼Œä¸‹è½½ JDK 11 å¹¶ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…å®ƒ:

```
# SET JAVA ENV VARIABLES
ENV JAVA_HOME="/home/jdk-${JAVA_VERSION}.0.2"
ENV PATH="${JAVA_HOME}/bin/:${PATH}"

# DOWNLOAD JDk 11 AND INSTALL
RUN DOWNLOAD_URL="https://download.java.net/java/GA/jdk${JAVA_VERSION}/9/GPL/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \
    && TMP_DIR="$(mktemp -d)" \
    && curl -fL "${DOWNLOAD_URL}" --output "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \
    && mkdir -p "${JAVA_HOME}" \
    && tar xzf "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" -C "${JAVA_HOME}" --strip-components=1 \
    && rm -rf "${TMP_DIR}" \
    && java --version
```

ç„¶åæˆ‘ä»¬éœ€è¦ä» https://dlcdn.apache.org ä¸‹è½½å¹¶å®‰è£… Spark:

```
# DOWNLOAD SPARK AND INSTALL
RUN DOWNLOAD_URL_SPARK="https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && wget --no-verbose -O apache-spark.tgz  "${DOWNLOAD_URL_SPARK}"\
    && mkdir -p /home/spark \
    && tar -xf apache-spark.tgz -C /home/spark --strip-components=1 \
    && rm apache-spark.tgz
```

æœ€åï¼Œæˆ‘ä»¬è®¾ç½®ä¸€äº›é¢å¤–çš„ç¯å¢ƒå˜é‡:

```
# SET SPARK ENV VARIABLES
ENV SPARK_HOME="/home/spark"
ENV PATH="${SPARK_HOME}/bin/:${PATH}"

# SET PYSPARK VARIABLES
ENV PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"
ENV PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH"
```

æœ€åï¼Œæˆ‘ä»¬å°†å…¥å£ç‚¹è®¾ç½®ä¸º Python å‘½ä»¤è¡Œ:

```
ENTRYPOINT ["python" ]
```

**å®Œæ•´çš„ Dockerfile æ–‡ä»¶é…ç½®å¦‚ä¸‹:**

```
FROM python:3.9.1

RUN apt-get install curl wget
RUN pip install pandas

COPY fhvhv_tripdata_2021-01.csv.gz .

# VERSIONS
ENV SPARK_VERSION=3.3.1 \
HADOOP_VERSION=3 \
JAVA_VERSION=11

# SET JAVA ENV VARIABLES
ENV JAVA_HOME="/home/jdk-${JAVA_VERSION}.0.2"
ENV PATH="${JAVA_HOME}/bin/:${PATH}"

# DOWNLOAD JAVA 11 AND INSTALL
RUN DOWNLOAD_URL="https://download.java.net/java/GA/jdk${JAVA_VERSION}/9/GPL/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \
    && TMP_DIR="$(mktemp -d)" \
    && curl -fL "${DOWNLOAD_URL}" --output "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" \
    && mkdir -p "${JAVA_HOME}" \
    && tar xzf "${TMP_DIR}/openjdk-${JAVA_VERSION}.0.2_linux-x64_bin.tar.gz" -C "${JAVA_HOME}" --strip-components=1 \
    && rm -rf "${TMP_DIR}" \
    && java --version

# DOWNLOAD SPARK AND INSTALL
RUN DOWNLOAD_URL_SPARK="https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && wget --no-verbose -O apache-spark.tgz  "${DOWNLOAD_URL_SPARK}"\
    && mkdir -p /home/spark \
    && tar -xf apache-spark.tgz -C /home/spark --strip-components=1 \
    && rm apache-spark.tgz

# SET SPARK ENV VARIABLES
ENV SPARK_HOME="/home/spark"
ENV PATH="${SPARK_HOME}/bin/:${PATH}"

# SET PYSPARK VARIABLES
ENV PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"
ENV PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH"

# Let's change to  "$NB_USER" command so the image runs as a non root user by default
USER $NB_UID

ENTRYPOINT ["python" ]
```

ä¸ºäº†æ„å»ºæˆ‘ä»¬å®šåˆ¶çš„ Docker æ˜ åƒï¼Œæˆ‘ä»¬éœ€è¦è¿è¡Œ docker build å‘½ä»¤:

```
docker build -t spark_docker_v1 .
```

# 3.Pyspark çš„é¦–æ¬¡è¿è¡Œ

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ª Docker æ˜ åƒï¼Œå¯ä»¥è¿è¡Œ Spark äº†ï¼Œå› æ­¤ä¹Ÿå¯ä»¥è¿è¡Œ Pyspark äº†ã€‚

è¦è®¾ç½®å®¹å™¨è¿è¡Œï¼Œæˆ‘ä»¬è°ƒç”¨ä»¥ä¸‹å‘½ä»¤:

```
docker run --rm -it -p 4040:4040 spark_docker_v1
```

å°†å®¹å™¨è®¾ç½®ä¸ºè¿è¡Œåï¼Œæ‚¨å°†è·å¾— Python å‘½ä»¤è¡Œ:

```
Python 3.9.1 (default, Feb  9 2021, 07:42:03)
[GCC 8.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
```

åœ¨å‘½ä»¤è¡Œä¸Šï¼Œæ‚¨å¯ä»¥é€šè¿‡å¯¼å…¥æ‰€éœ€çš„åŒ…ã€å¯åŠ¨ Spark ä¼šè¯ã€è¯»å–å¯¼å…¥çš„æ–‡ä»¶ï¼Œç„¶åæ˜¾ç¤º Spark æ•°æ®å¸§çš„å‰ 20 è¡Œæ¥æµ‹è¯• Spark:

```
import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master("local[*]") \
    .appName('tepst') \
    .getOrCreate()

df = spark.read \
    .option("header", "true") \
    .csv('fhvhv_tripdata_2021-01.csv.gz')

df.show()
```

å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæ‚¨å°†è·å¾—ä»¥ä¸‹ç»“æœ:

![](img/4c2bc6f02e0c34102487bd16643310b6.png)

åœ¨ *localhost:4040* ä¸Šï¼Œæ‚¨å°†çœ‹åˆ°è¿™å¹…å›¾åƒï¼Œè¡¨ç¤º Spark æ­£åœ¨å·¥ä½œ:

![](img/3d7a837752c6158cdc1475b1d7432240.png)

# æœ€åçš„æƒ³æ³•

å¦‚æœæ‚¨æƒ³ä¸º Spark åº”ç”¨ç¨‹åºä½¿ç”¨ä¸€è‡´ä¸”éš”ç¦»çš„ç¯å¢ƒï¼Œåœ¨ Docker ä¸Šè¿è¡Œ Spark å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

ç„¶è€Œï¼Œåœ¨ Docker ä¸Šè¿è¡Œ Spark å¯èƒ½å¹¶ä¸æ€»æ˜¯æœ€ä½³é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªå¤§å‹é›†ç¾¤ï¼Œæ‹¥æœ‰è¿è¡Œ Spark çš„ä¸“ç”¨èµ„æºï¼Œé‚£ä¹ˆç›´æ¥åœ¨é›†ç¾¤ä¸Šè¿è¡Œ Spark å¯èƒ½æ¯”ä½¿ç”¨ Docker å®¹å™¨æ›´æœ‰æ•ˆã€‚

ä¸€ä¸ªå¥½çš„æ›¿ä»£æ–¹æ³•å¯èƒ½æ˜¯é€šè¿‡åœ¨ Kubernetes é›†ç¾¤ä¸Šéƒ¨ç½² Sparkï¼Œä½¿ç”¨ Kubernetes ä¸º Spark æä¾›æ›´å¤šçš„èµ„æºã€‚è¿™å…è®¸æ‚¨é€šè¿‡å‘é›†ç¾¤æ·»åŠ æ›´å¤šèŠ‚ç‚¹æ¥æ°´å¹³æ‰©å±• Spark åº”ç”¨ç¨‹åºã€‚

ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿå…³æ³¨æˆ‘æ›´å¤šå…³äº[åª’ä½“](https://medium.com/@lgsoliveira)çš„æ–‡ç« ã€‚

[é˜…è¯»è·¯æ˜“æ–¯Â·å¥¥åˆ©ç»´æ‹‰(ä»¥åŠåª’ä½“ä¸Šæˆåƒä¸Šä¸‡çš„å…¶ä»–ä½œå®¶)çš„æ¯ä¸€ä¸ªæ•…äº‹](https://medium.com/@lgsoliveira/membership)

# åˆ†çº§ç¼–ç 

æ„Ÿè°¢æ‚¨æˆä¸ºæˆ‘ä»¬ç¤¾åŒºçš„ä¸€å‘˜ï¼åœ¨ä½ ç¦»å¼€ä¹‹å‰:

*   ğŸ‘ä¸ºæ•…äº‹é¼“æŒï¼Œè·Ÿç€ä½œè€…èµ°ğŸ‘‰
*   ğŸ“°æŸ¥çœ‹[å‡çº§ç¼–ç å‡ºç‰ˆç‰©](https://levelup.gitconnected.com/?utm_source=pub&utm_medium=post)ä¸­çš„æ›´å¤šå†…å®¹
*   ğŸ””å…³æ³¨æˆ‘ä»¬:[Twitter](https://twitter.com/gitconnected)|[LinkedIn](https://www.linkedin.com/company/gitconnected)|[æ—¶äº‹é€šè®¯](https://newsletter.levelup.dev)

ğŸš€ğŸ‘‰ [**åŠ å…¥å‡çº§äººæ‰é›†ä½“ï¼Œæ‰¾åˆ°ä¸€ä»½ç¥å¥‡çš„å·¥ä½œ**](https://jobs.levelup.dev/talent/welcome?referral=true)