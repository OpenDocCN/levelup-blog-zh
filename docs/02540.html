<html>
<head>
<title>Stylistic differences between R and Python in modelling data through decision trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R和Python在通过决策树建模数据方面的风格差异</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/stylistic-differences-between-r-and-python-in-modelling-data-through-decision-trees-ea6f7c98e6e8?source=collection_archive---------6-----------------------#2020-03-20">https://levelup.gitconnected.com/stylistic-differences-between-r-and-python-in-modelling-data-through-decision-trees-ea6f7c98e6e8?source=collection_archive---------6-----------------------#2020-03-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1538" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">数据科学文体学</h2><div class=""/><div class=""><h2 id="ee79" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何用R和Python <strong class="ak">开发一个决策树，为</strong>的每个内部节点表示一个属性“测试”，为每个分支表示测试结果，为每个叶节点表示一个类标签。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/88c3a98a29357731d48c377b610bde21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ejt7fKYK_zu7PRn9rqQQ8w.jpeg"/></div></div></figure><p id="cc04" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在<a class="ae lw" href="https://medium.com/swlh/stylistic-differences-between-r-and-python-for-data-preparation-bfda6ebd15aa" rel="noopener"> <strong class="lc ja">准备好数据框架</strong></a><a class="ae lw" href="https://medium.com/analytics-vidhya/stylistic-differences-between-r-and-python-for-exploratory-data-analysis-1c5195162b8a" rel="noopener"><strong class="lc ja">探索了一些关键关系</strong></a><strong class="lc ja"/><strong class="lc ja"/><a class="ae lw" rel="noopener ugc nofollow" target="_blank" href="/stylistic-differences-between-r-and-python-in-setting-up-the-dataframe-pre-modelling-6c5ec0195901"><strong class="lc ja">建立数据框架</strong></a><strong class="lc ja"/>下一步就是数据建模。这可能是数据科学方法中最具决定性的方面，因为有各种各样的方法和算法来为大型数据集建模。</p><blockquote class="lx ly lz"><p id="3a28" class="la lb ma lc b ld le ka lf lg lh kd li mb lk ll lm mc lo lp lq md ls lt lu lv ij bi translated"><strong class="lc ja">算法</strong>:在有限的步骤中解决一个数学问题(如寻找最大公约数)的过程，经常需要重复运算</p></blockquote><p id="8b32" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最简单的数据建模方法之一是决策树，它由一组连接到向下延伸的分支的决策节点组成。起点是根节点，终点是叶节点。该算法在决策节点测试变量，分析每个可能的结果，并导致最佳分裂，产生新的分支。该分支可能通向另一个决策节点或终止于叶节点。</p><p id="cd77" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">决策树的最终目的是创建一组尽可能精确的叶节点，其中特定叶节点中的每条记录都具有相同的分类。通过<em class="ma">决策</em>的序列，该树提供了一组具有最高可能置信度的分类分配。然而，选择一种在各种属性之间产生尽可能多的分类一致性的算法是至关重要的。重要的是，算法应该在训练数据集上进行测试，因此下面的部分将隐含地提到这一点。</p><h1 id="643d" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">1.分类和回归树</h1><p id="330a" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">数据科学中最常用的决策树算法是分类和回归树(CART)。这是由一系列的问题和答案组成的。这些问题的结果是一个树状结构，其中末端是终端节点，在这一点上没有更多的问题。其主要特点是:</p><p id="c185" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> 1。</strong>根据基尼系数<br/> <strong class="lc ja"> 2，基于一个变量的值在节点上分割数据的规则。</strong>决定分支何时终止于叶节点<br/> <strong class="lc ja">的停止规则3。</strong>对每个终端叶节点中的目标变量的预测</p><p id="bb31" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">CART方法生成的决策树是<strong class="lc ja">二进制</strong>，这意味着对于每个决策节点，它们正好包含两个分支。CART是一种递归算法，它将训练数据集划分为具有相似目标属性值的子集。CART算法通过对每个决策节点的所有变量和所有可能的分裂值进行综合搜索来增长树，目的是根据<strong class="lc ja"> GINI指数</strong>选择最优的一个。</p><blockquote class="lx ly lz"><p id="a49b" class="la lb ma lc b ld le ka lf lg lh kd li mb lk ll lm mc lo lp lq md ls lt lu lv ij bi translated"><strong class="lc ja">基尼指数</strong>衡量随机选择时，特定变量在特定节点内被分类的程度或概率。如果所有的元素都属于一个类，那么可以定义为<strong class="lc ja">纯</strong>。基尼指数的程度在0和1之间变化，其中0表示所有元素都属于某一个类别，或者只存在一个类别，1表示元素随机分布在各个类别中。</p></blockquote><p id="6cf5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">GINI指数的计算方法是从1中减去每类概率的平方和。它倾向于较大的分区，并选择具有较低基尼指数的特征进行分割。当所有元素都属于某个类或者值接近零时，1表示元素随机分布在各个类中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f18d1db087aad4f2db9ce9608fabb18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*mYpBdGCTh8abNZ1FJ2RPWA.jpeg"/></div></figure><p id="eaba" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">此外，CART还可能需要选择一种修剪技术，如称为<a class="ae lw" href="http://mlwiki.org/index.php/Cost-Complexity_Pruning" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja">成本复杂性修剪</strong> </a>的技术，以从决策树中删除冗余分支，从而提高其准确性。Python和R都允许限制节点的数量，因此提供了一种直接的方法来确定输出中的分支和节点的数量。</p><blockquote class="lx ly lz"><p id="0f9a" class="la lb ma lc b ld le ka lf lg lh kd li mb lk ll lm mc lo lp lq md ls lt lu lv ij bi translated"><strong class="lc ja">修剪</strong>是机器学习中的一种技术，它通过删除树中对实例分类没有什么帮助的部分来减少决策树的大小。修剪的双重目标是降低最终分类器的复杂性，以及通过减少过度拟合和移除可能基于错误数据的分类器部分来获得更好的预测准确性。</p></blockquote><h1 id="e24b" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">2.Python中的CART算法</h1><p id="fb34" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">在<strong class="lc ja"> Python中，</strong>创建决策树的起点是上传相关的库，这些库允许按照迄今为止所阐述的概念进行计算。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="cd9f" class="nh mf iq nd b gy ni nj l nk nl">import pandas as pd<br/>import numpy as np<br/>from sklearn import preprocessing<br/>import cv2<br/>import matplotlib.pyplot as plt<br/>import statsmodels.tools.tools as stattools<br/>from sklearn.tree import DecisionTreeClassifier, export_graphviz</span></pre><p id="aa14" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">其次，可以通过混合了分类预测值和数值预测值以及分类目标变量的数据集进行分析。在将在<strong class="lc ja"> Python </strong>中展示的例子中，数据集的创建遵循了与前一篇博客中类似的方法。对于这个模型，我们将使用随机生成的变量来模拟分析，以描述一个数据集，该数据集记录了人道主义危机在财务方面的影响。预测因素将是休克的类型及其持续时间，而目标变量将是给予的援助数量。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="2131" class="nh mf iq nd b gy ni nj l nk nl">df = pd.DataFrame(np.random.randint(0,1000,size=(1000,1)), columns= ['IncomeLoss'])</span><span id="603e" class="nh mf iq nd b gy nm nj l nk nl">df["DurationMonths"] = np.random.randint(0, 12, df.shape[0])</span><span id="c4f9" class="nh mf iq nd b gy nm nj l nk nl">Type_Shock = (['Draught','Floods','Famine'])<br/>df["Type_Shock"] = np.random.choice(Type_Shock, size=len(df))</span><span id="9d29" class="nh mf iq nd b gy nm nj l nk nl">df["AidMillion"] = np.random.randint(0, 1000, df.shape[0])<br/>category = pd.cut(df.AidMillion,bins=[0,500,1000],labels=['Below_500_Mill','Above_500_Mill'])</span><span id="b494" class="nh mf iq nd b gy nm nj l nk nl">df.insert(2,'Aid_Given',category)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f4c8674ee3e380a96bb95150870e7312.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*rp1ejDhEStpULL-tQJqLsA.jpeg"/></div></figure><p id="3349" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一旦在<strong class="lc ja"> Python </strong>中定义了数据帧，我们将不得不隔离相关的变量。在这种情况下，y是分为两个属性的目标变量(高于5亿的aid和低于5亿的aid ),需要一个<code class="fe no np nq nd b">LabelEncoder</code>命令用于决策树。然后，需要将分类属性转换为数字形式的数组，并与数字预测值连接，在本例中，数字预测值是电击的持续时间。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="f42b" class="nh mf iq nd b gy ni nj l nk nl">y=df[['Aid_Given']]<br/>le = preprocessing.LabelEncoder()<br/>y= le.fit_transform(y.astype(str))</span><span id="b283" class="nh mf iq nd b gy nm nj l nk nl">type_disaster= np.array(df['TypeShock'])</span><span id="e68d" class="nh mf iq nd b gy nm nj l nk nl">(type_category, type_category_dict)= stattools.categorical(type_disaster, drop=True, dictnames=True)</span><span id="3813" class="nh mf iq nd b gy nm nj l nk nl">type_disaster_pd=pd.DataFrame(type_category)</span><span id="b801" class="nh mf iq nd b gy nm nj l nk nl">X= pd.concat((df[['DurationMonths']], type_disaster_pd), axis=1)</span></pre><p id="5ec3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然后通过<code class="fe no np nq nd b">DecisionTreeClassifier</code>命令在<strong class="lc ja"> Python </strong>中创建CART决策树，该命令根据Gini标准组织所有数据，并根据预测值(本例中为4个)确定适合目标变量分类的叶节点数量。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="e1ad" class="nh mf iq nd b gy ni nj l nk nl">X_names=[“DurationMonths”, “Draught”, “Floods”, “Famine”]<br/>y_names=[“Below_500_Mill”, “Above_500_Mill”]</span><span id="cc79" class="nh mf iq nd b gy nm nj l nk nl">cart01=DecisionTreeClassifier(criterion=”gini”, max_leaf_nodes=7).fit(X,y)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/cf6f58dfc407ffedd6e468e25ab0de2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XetGfrVwaZq9ZbI1yOeqCw.jpeg"/></div></div></figure><p id="f218" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一旦模型最终确定，Python中的以下片段可以帮助在<em class="ma"> Jupyter </em>笔记本中可视化决策树。graphviz的默认导出功能在。点和转换成png需要一个特定的命令:<em class="ma">！dot-Tpng</em><strong class="lc ja"><em class="ma">nametree . dot</em></strong><em class="ma">-o</em><strong class="lc ja"><em class="ma">nametree.png。</em> </strong></p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="ad6e" class="nh mf iq nd b gy ni nj l nk nl">%matplotlib inline<br/>export_graphviz(cart01, out_file= "decision_tree_CART.dot", feature_names=X_names, class_names=y_names)</span><span id="82bb" class="nh mf iq nd b gy nm nj l nk nl">! dot -Tpng decision_tree_CART.dot -o decision_tree_CART.png<br/>img = cv2.imread('decision_tree_CART.png')<br/>plt.figure(figsize = (15, 15))<br/>plt.imshow(img)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/200200904b5e4441d5210999690abb91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*insY4NUA3QBkGOFroRBZsg.jpeg"/></div></div></figure><p id="212c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">从输出中可以看到的输出可以通过以下方式读取。各种<strong class="lc ja">预测器</strong>(分类和数字)是每个决策节点中的第一个概念，分界点分为二进制真和假。基尼值提供了要在节点中分类特定目标值的概率的度量，在这种情况下，大多数<strong class="lc ja">基尼系数</strong>大约为0.5，因为数据集是随机生成的，但是随着决策向下进行，基尼系数下降。<strong class="lc ja">值</strong>范围显示了值的分割如何在两个类别(高于和低于5亿)之间进行，并进一步指定了哪个<strong class="lc ja">类别</strong>是该节点中相对于目标变量的主导类别。</p><p id="1f8f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">作为最后一步，我们的目标是根据通过CART获得的分类模型，获得数据集中每个变量的aid分类。<code class="fe no np nq nd b">predict</code>命令在<strong class="lc ja"> Python </strong>中以数组的形式生成相关输出。该值列表可在以后用于查看它是否明显偏离测试数据帧。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="8191" class="nh mf iq nd b gy ni nj l nk nl">predAidCART= cart01.predict(X)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d0453ce49de3c7670ab73c62e14bc9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*Xzyzwxq8xSUSIDF4AVxi1w.jpeg"/></div></figure><h1 id="1ccd" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">3.R中的CART算法</h1><p id="e972" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">在<strong class="lc ja"> R </strong>中，相同的起点是上传我们想要生成的决策树的相关库，在本例中是<code class="fe no np nq nd b">rpart</code>。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="96d4" class="nh mf iq nd b gy ni nj l nk nl">library(rpart)<br/>library(rpart.plot)</span></pre><p id="0bfb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了在<strong class="lc ja"> R </strong>中进行类似的分析，我们还需要依赖具有相同结构(数值/分类预测值)和目标变量的数据集。在这种情况下，建议的数据集是对之前使用的数据集的精确重建，尽管通过随机化我们会得到不同的结果。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="6b4a" class="nh mf iq nd b gy ni nj l nk nl">df &lt;- data.frame(replicate(1,sample(0:1000, 1000, rep=TRUE)))<br/>colnames(df) &lt;- c("IncomeLoss")<br/>df$DurationMonths &lt;- sample(0:12, dim(df), rep=TRUE)<br/>df$Type &lt;- sample(c('Draught','Floods','Famine'), size = nrow(df), replace = TRUE)<br/>df$Location &lt;- sample(c('Urban','Rural'), size = nrow(df), replace = TRUE)<br/>df$AidMillion &lt;- sample(0:1000, dim(df), rep=TRUE)<br/>df$Aid_Given &lt;- ifelse(df$AidMillion &lt;= 500, "Above_500_Mill", "Below_500_Mill")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/83fd427ea91cbc5c7d91a799fe3a432a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*CQK442IhSN3TTxi9J_m0hQ.jpeg"/></div></figure><p id="d98c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> R </strong>中的第二步是分解分类变量(预测变量和目标变量),然后使用<code class="fe no np nq nd b">rpart</code>算法(递归分割和回归树)根据指定的方法<code class="fe no np nq nd b">class</code>建立决策序列，该序列将根据目标变量的性质(在本例中为分类变量)而变化。该方法旨在评估分类中的分割是否是基于基尼标准的最佳分割。与前一种情况不同，绘图是一个简单的片段。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="d036" class="nh mf iq nd b gy ni nj l nk nl">df$Type &lt;- factor(df$Type)<br/>df$Aid_Given &lt;- factor(df$Aid_Given)</span><span id="07d9" class="nh mf iq nd b gy nm nj l nk nl">cart01 &lt;- rpart(formula= Aid_Given ~ Type + DurationMonths, data=df, method='class')</span><span id="c95d" class="nh mf iq nd b gy nm nj l nk nl">rpart.plot(cart01, type=4, extra=3)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ebf17f27f567225e2aa0298589c5d7a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*x30FCYhJgB1fR43-rZKmuA.jpeg"/></div></figure><p id="292d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">同样，分支的左侧指示分类是否正确，而右侧指示相反的情况。尽管在<strong class="lc ja"> R </strong>中的可视化输出比Python中的版本包含的信息更少，但在试图解释决策之间的逻辑时，它似乎更容易理解。更清晰的分类和数值预测提供了一个更容易解释的分类。根据目标变量的性质，还有其他选项，因此代码版本只适用于分类变量。</p><p id="7549" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在生成具有实际值的决策树之后，<strong class="lc ja"> R </strong>还允许开发预测值列表。在生成包含用于决策树的预测变量的数据帧后，需要命令<code class="fe no np nq nd b">predict</code>来生成根据CART模型分类的目标值列表。稍后可以将它们与测试数据集进行匹配。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="b5b7" class="nh mf iq nd b gy ni nj l nk nl">X&lt;-data.frame(Type=df$Type, DurationMonths= df$DurationMonths)</span><span id="fc57" class="nh mf iq nd b gy nm nj l nk nl">predAidCART &lt;- predict(object= cart01, newdata=X, type="class")<br/>head(predAidCART)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f388cb0dd545f544911fbfdb31de4db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*76cIAc5C_hI8SevqEorLxg.jpeg"/></div></figure><h1 id="06a3" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">4.决策树的C5.0算法</h1><p id="a158" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">另一种生成决策树的算法是C5.0，与CART不同，它不局限于二叉分裂。该算法使用信息增益或熵减少的概念来选择最佳分裂。</p><blockquote class="lx ly lz"><p id="8ebc" class="la lb ma lc b ld le ka lf lg lh kd li mb lk ll lm mc lo lp lq md ls lt lu lv ij bi translated">信息增益通过根据随机变量的给定值分割数据集来测量<strong class="lc ja">熵</strong>的减少。更大的信息增益意味着更低的熵，因此更少的惊奇。低概率事件有更多的信息，高概率事件有更少的信息。熵对随机变量中的<strong class="lc ja">信息位</strong>进行量化，换句话说就是它的概率分布。</p></blockquote><p id="6d46" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">C5.0使用<strong class="lc ja">熵</strong>的概念来测量纯度。数据样本的熵表示类别值的混合程度；最小值0表示样本完全同质，而1表示最大无序量。熵的定义可以在下面的等式中指定，对于给定的数据段，术语<em class="ma"> c </em>指的是不同类级别的数量，而p指的是落入cass级别I的值的比例</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/badacff8c068adb65547c9e0b8391b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*6mNd-CSXo-0S3weaCrbvgw.jpeg"/></div></figure><p id="3511" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，C.50算法被用于属性选择测量，该测量可以通过信息增益来构建决策树，该信息增益通过增加根据候选分裂对数据集进行划分所产生的信息来减少熵。在这种情况下，修剪技术将是二项式置信限，以在不损失准确性的情况下减小树的大小。</p><blockquote class="lx ly lz"><p id="cf1c" class="la lb ma lc b ld le ka lf lg lh kd li mb lk ll lm mc lo lp lq md ls lt lu lv ij bi translated">当数据是二分的(例如0或1，是或否)时，使用二项式置信区间。二项式置信区间提供了具有特定置信水平的特定结果比例(例如成功率)的区间。</p></blockquote><p id="1d99" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">转到<strong class="lc ja"> Python </strong>中的代码，通过在<code class="fe no np nq nd b">DecisionTreeClassifier</code>命令中进行快速更改，将标准从基尼转换为熵，同时使用CART中使用的相同代码片段，可以轻松生成C5.0决策树。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="bfa1" class="nh mf iq nd b gy ni nj l nk nl">cart02=DecisionTreeClassifier(criterion=”entropy”, max_leaf_nodes=4).fit(X,y)</span><span id="39d4" class="nh mf iq nd b gy nm nj l nk nl">%matplotlib inline<br/>export_graphviz(cart02, out_file= "decision_tree_C50.dot", feature_names=X_names, class_names=y_names)</span><span id="8dae" class="nh mf iq nd b gy nm nj l nk nl">! dot -Tpng decision_tree_C50.dot -o decision_tree_C50.png<br/>img = cv2.imread('decision_tree_CART.png')<br/>plt.figure(figsize = (10, 10))<br/>plt.imshow(img)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/38ec0f42691c5d009dcb6082b4cfff47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxIIbybJNpe7AXuuDXX8mQ.jpeg"/></div></div></figure><p id="ca82" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">视觉输出的结果遵循与基尼系数生成的树相似的路径，尽管分界点的分布有所变化(例如，在第3级，数值预测值的分界点明显不同)。一般来说，基尼系数和信息增益熵是一样的，它们可以互换使用。如果有选择的话，基尼系数会更低，因为它不需要对数计算，而对数计算可能会更复杂。</p><p id="1a0e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> R </strong>中的C5.0树虽然代表相同的逻辑，但产生了非常不同的视觉输出。公式中的不同之处在于使用了不同的命令<code class="fe no np nq nd b">C.50</code>和叶节点中最小记录数的定义(在这种情况下，值为50)。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="a5a5" class="nh mf iq nd b gy ni nj l nk nl">C50 &lt;- C5.0(formula= Aid_Given ~ DurationMonths + Type, data=df, control= C5.0Control(minCases=50))</span><span id="7b61" class="nh mf iq nd b gy nm nj l nk nl">plot (C50)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/2fe2dbce58d0cb64ce675fed24e1ad99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNfEfL7nvGQ7W8QNswK5OQ.jpeg"/></div></div></figure><p id="e719" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个版本的视觉输出很有见地。例如，该树表明，在节点7，如果危机持续时间小于6个月，则分类目标变量(给予的援助)的大部分类别条目在5亿以上，而当持续时间大于10个月时则相反，如节点8所述。条目以某种一致性分布在各个叶节点上，直到节点7，而在某些情况下，第一个叶节点可能包含更多的条目。这种解释与现实生活中的案例场景无关，但可以帮助你适应这种视觉输出。</p><h1 id="4c62" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">4.随机森林</h1><p id="65fa" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">CART和C.50对所有记录生成一个决策树。然而，还有另一种使用多棵树的方法，其中在确定每个记录的最终分类时考虑每个树的输出。<strong class="lc ja">随机森林</strong>是一种监督学习算法，它建立在一系列决策树的基础上，并将每个记录的树分类组合成一个最终分类，这是集成方法的一个例子。换句话说，这是一种建模技术，它将多个模型的输出考虑在内，以得出最终答案。</p><p id="340f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">随机森林算法首先通过从初始数据集中随机抽取样本来构建每个决策树，以便每个树可以基于不同的记录集进行构建。对于树的每个节点，选择预测变量的子集进行考虑。一旦生成了不同的树，数据集中的每条记录都会被每棵树分类并投票。具有最流行类别(或<strong class="lc ja">票数</strong>)的值被认为是用于预测的最终分类。</p><p id="8603" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在<strong class="lc ja"> Python </strong>中，随机森林的构建需要一个转换成一维数组的响应变量。<code class="fe no np nq nd b">ravel</code>命令可以创建这样的格式。然后，<code class="fe no np nq nd b">RandomForestClassifier</code>可用于指定算法的参数，n_estimators输入指定在标准下要考虑的树的数量(可以是<code class="fe no np nq nd b">gini</code>或<code class="fe no np nq nd b">entropy</code>)。要通过算法查看最终分类，我们可以像以前一样使用predict命令。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="9e21" class="nh mf iq nd b gy ni nj l nk nl">from sklearn.ensemble import RandomForestClassifier<br/>import numpy as np</span><span id="d12f" class="nh mf iq nd b gy nm nj l nk nl">rfy= np.ravel(y)</span><span id="00b3" class="nh mf iq nd b gy nm nj l nk nl">rf01= RandomForestClassifier (n_estimators=100, criterion="gini").fit(X, rfy)</span><span id="c28e" class="nh mf iq nd b gy nm nj l nk nl">rf01.predict(X)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/917e6341a0b72cfe2f7649c93aeca573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*G6zgpHew8x2jOKDnHvG_8w.jpeg"/></div></figure><p id="320f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在R中，库<code class="fe no np nq nd b">randomForest</code>允许我们建立一个模型，遵循与之前用来创建决策树的代码相似的代码。ntree的规范是在我们的模型中校准精度水平与数据集大小之间的平衡的重要规范。输出保存在rf01中，标签为预测，为数据集中的每个条目存储一条记录。</p><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="2c2a" class="nh mf iq nd b gy ni nj l nk nl">library(randomForest)</span><span id="2574" class="nh mf iq nd b gy nm nj l nk nl">rf01 &lt;- randomForest(formula= Aid_Given ~ DurationMonths+Type, data=df, ntree=100, type="classification")</span><span id="6d14" class="nh mf iq nd b gy nm nj l nk nl">head(rf01$predicted)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/32d25b9bdecc98fbf09172d775708b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*ZP3iwedPA9Klz4kDcegS-A.jpeg"/></div></figure><h1 id="28db" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">在这种建模方法之后，下一篇博客将解释建模数据的其他方法。这将是下一篇博客的特色！敬请期待</h1></div></div>    
</body>
</html>