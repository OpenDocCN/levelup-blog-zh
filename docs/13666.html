<html>
<head>
<title>Building a Web Scraper for Three Common Use Cases</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为三种常见用例构建一个Web Scraper</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/building-a-web-scraper-for-three-common-use-cases-7a5ffc88284f?source=collection_archive---------19-----------------------#2022-09-26">https://levelup.gitconnected.com/building-a-web-scraper-for-three-common-use-cases-7a5ffc88284f?source=collection_archive---------19-----------------------#2022-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4858" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">分页搜索、无限滚动和图像搜索——使用Python的实用分步指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/02d83b6aecac4dc4dd3ca9079d5706f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Sv5GtbdLJCs-fM1b"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@charlesdeluvio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> charlesdeluvio </a>拍摄的照片</figcaption></figure><h1 id="af88" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">动机</h1><p id="4c72" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最近，我在当地零售商的<a class="ae ky" href="https://www.harveynorman.com.au/computers-tablets/computers/laptops" rel="noopener ugc nofollow" target="_blank">网站</a>上浏览新笔记本电脑，发现自己不得不手动记下多台笔记本电脑的品牌、规格和价格(用于比较)，同时浏览多页搜索结果。我向我的合作伙伴表达了我对手工操作的失望，他也表达了对一个电子商务网站的类似失望。</p><p id="7042" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我探索了一个潜在的案例，使用web scraper将网站上的内容手动转换为结构化(例如表格)数据。本文针对以下三(3)种常见使用情形，提供了此类web刮刀的实用分步指南:</p><ol class=""><li id="fdee" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">搜索跨多个页面返回搜索结果的网站(“分页搜索”)</li><li id="ef20" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">通过手动向下滚动(“无限滚动”)搜索返回更多搜索结果的网站</li><li id="fae5" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">下载网站上托管的图像(“图像抓取”)</li></ol><h1 id="0599" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">网页抓取的非技术性介绍</h1><p id="180c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">用户在网站上遇到的大多数内容实际上是一些HTML代码的输出。这些代码通常由所有网站都遵循的一些通用规则编译而成。这意味着在网站“前端”对用户可见的内容被存储在HTML代码的相关层中，并且可以通过查询HTML代码的相关层来下载。</p><p id="1abc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">用于本文目的的web抓取器主要使用<a class="ae ky" href="https://beautiful-soup-4.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>和<a class="ae ky" href="https://selenium-python.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"> Selenium </a>库在Python中构建。特别是，BeautifulSoup库提供了从网站背后的HTML代码中搜索、查询和返回数据的功能，而Selenium库支持浏览器自动化(例如，在Google Chrome或Firefox浏览器中执行某些操作)。</p><h1 id="17fe" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用例1:用于分页搜索的Web Scraper</h1><p id="f854" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了演示这个用例，我们将从<a class="ae ky" href="https://www.sephora.com.au/search?q=foundation&amp;page=1" rel="noopener ugc nofollow" target="_blank"> Sephora </a>网站上搜集与搜索“基金会”相关的数据(不要评判)。我们将从导入Python库开始，如下所示。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a95d" class="nl la it nh b gy nm nn l no np">from bs4 import BeautifulSoup<br/>from selenium import webdriver<br/>import pandas as pd # for storing data in a dataframe</span></pre><p id="f5f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们将尝试使用Google Chrome来抓取搜索的“第一页”上感兴趣的数据，然后在此基础上对所有其他页面进行For循环。为了分别抓取化妆品的<strong class="lt iu">品牌</strong>、<strong class="lt iu">描述</strong>和<strong class="lt iu">价格</strong>，我们首先将网站的HTML代码解析为一个名为<em class="nq"> soup_sephora </em>的Python对象，如下所示。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="ffc3" class="nl la it nh b gy nm nn l no np">## url of page 1 of a search for "foundation" on the Sephora website<br/>url = "<a class="ae ky" href="https://www.sephora.com.au/search?q=foundation&amp;page=" rel="noopener ugc nofollow" target="_blank">https://www.sephora.com.au/search?q=foundation&amp;page=</a>1"</span><span id="3292" class="nl la it nh b gy nr nn l no np">## The option below prevents Chrome from physically opening <br/>options = webdriver.ChromeOptions()<br/>options.add_argument('--headless')</span><span id="851e" class="nl la it nh b gy nr nn l no np">## Download and specify the location of the chromedriver<br/>driver = webdriver.Chrome(<br/>                          executable_path = r'C:\Users\Jin\Documents\Webscraping\Drivers\chromedriver.exe',<br/>                          chrome_options = options<br/>                           )<br/>driver.get(url)</span><span id="9307" class="nl la it nh b gy nr nn l no np">## Return the HTML codes<br/>soup_sephora = BeautifulSoup(driver.page_source, 'lxml')</span></pre><p id="d2f0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我建议将HTML代码(存储在<em class="nq"> soup_sephora </em>对象中)粘贴到可搜索的文档类型中，例如Microsoft Word或Notepad，因为它有助于识别我们需要查询以返回感兴趣的数据的属性。举例来说，页面1上每个产品的价格属性都存储在HTML结构中的“product-price”类中，这可以通过对给定产品的价格执行“CTRL+F”来识别。在YSL foundation定价为$95的实例中，如下面的第一张图片所示，这是作为“product-price”类下的文本字符串嵌入到<em class="nq"> soup_sephora </em>对象中的，如下面的第二张图片所示，可以通过在Microsoft Word文档中搜索“$95”来找到它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/85c36de6304c55462920904623dfb8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvCZkaU3Lrwxw8xJXSA4yQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图1:丝芙兰<a class="ae ky" href="https://www.sephora.com.au/search?q=foundation&amp;page=1" rel="noopener ugc nofollow" target="_blank">网站</a>截图</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/b6d12882dab0b36ceea9a7a6b081a26b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6aqt4dWsTRn4cQtXW4pnrQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图2: HTML代码片段。作者图片</figcaption></figure><p id="768a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面的代码返回Python列表中第1页上所有产品的价格。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="39f1" class="nl la it nh b gy nm nn l no np">soup_sephora.findAll(class_ = "product-price")</span></pre><p id="0678" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如上所述，搜索结果的第1页上的所有产品的品牌、描述和价格可以存储在熊猫数据框架中，如下所示。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="7452" class="nl la it nh b gy nm nn l no np">brand = []<br/>description = []<br/>price = []</span><span id="e277" class="nl la it nh b gy nr nn l no np">url = "<a class="ae ky" href="https://www.sephora.com.au/search?q=foundation&amp;page=" rel="noopener ugc nofollow" target="_blank">https://www.sephora.com.au/search?q=foundation&amp;page=</a>1"</span><span id="b03a" class="nl la it nh b gy nr nn l no np">options = webdriver.ChromeOptions()<br/>options.add_argument('--headless')</span><span id="ec0b" class="nl la it nh b gy nr nn l no np">driver = webdriver.Chrome(<br/>                                 executable_path = r'C:\Users\Jin\Documents\Webscraping\Drivers\chromedriver.exe',<br/>                                 chrome_options = options<br/>                                )<br/>driver.get(url)<br/>soup_sephora = BeautifulSoup(driver.page_source, 'lxml')</span><span id="8c0e" class="nl la it nh b gy nr nn l no np">i = 0</span><span id="5a74" class="nl la it nh b gy nr nn l no np">for item in soup_sephora.findAll(class_ = "product-card-brand"):<br/>    i = i + 1<br/>    brand.append(item.get_text(strip = True))</span><span id="586a" class="nl la it nh b gy nr nn l no np">for item in soup_sephora.findAll(class_ = "product-card-product"):<br/>    i = i + 1<br/>    description.append(item.get_text(strip = True))<br/>            <br/>for item in soup_sephora.findAll(class_ = "product-price"):<br/>    i = i + 1<br/>    price.append(item.get_text(strip = True))</span><span id="4914" class="nl la it nh b gy nr nn l no np">driver.close()<br/></span><span id="1f19" class="nl la it nh b gy nr nn l no np">## Converting into a pandas dataframe</span><span id="4b46" class="nl la it nh b gy nr nn l no np">df_sephora = pd.DataFrame(<br/>                        {'Brand': brand,<br/>                         'Description': description,<br/>                         'Price': price<br/>                        })</span><span id="0721" class="nl la it nh b gy nr nn l no np">df_sephora.head(10)</span></pre><p id="04e6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上面熊猫数据帧的输出如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/beb1091d5a1a635f4270cc21a8ecb6e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAsxuJE-xEfJlehV9jDcoQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图3:从网站上抓取的结构化内容——分页滚动。作者图片</figcaption></figure><p id="3ac4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">请注意,“价格”列可能保证对与看似销售折扣相关的产品进行进一步的清理。</p><p id="c237" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在要抓取后续页面，只需用for循环包装上面的代码，并如下软编码<em class="nq"> url </em>对象(在将列绑定到dataframe之前)。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="14a3" class="nl la it nh b gy nm nn l no np">## Specify number of pages for the For-Loop</span><span id="d117" class="nl la it nh b gy nr nn l no np"><strong class="nh iu">for j in range(1, number of pages):</strong></span><span id="55fa" class="nl la it nh b gy nr nn l no np"><strong class="nh iu">    url = "</strong><a class="ae ky" href="https://www.sephora.com.au/search?q=foundation&amp;page=" rel="noopener ugc nofollow" target="_blank"><strong class="nh iu">https://www.sephora.com.au/search?q=foundation&amp;page=</strong></a><strong class="nh iu">" +       str(j)</strong></span><span id="595c" class="nl la it nh b gy nr nn l no np">    **INSERT CODE ABOVE per Page 1 Scrape **<br/>             .<br/>             .<br/>             .</span></pre><h1 id="5998" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用例2:无限滚动的Web Scraper</h1><p id="5b2f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">无限滚动是一种网站设计，当用户向下滚动一页时，它会加载更多的内容，不再需要分页(用户需要单击才能看到后续页面)。这使得为用例1构建的For循环用处不大。</p><p id="7e1a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">也就是说，无限滚动也可以通过模仿向下滚动的web scraper实现自动化。这可以通过告诉scraper在将HTML代码解析为Python对象之前，首先滚动网页的整个长度，等待几秒钟(以便加载内容)，然后再次滚动网页的整个长度，直到它到达无限滚动的结尾来实现。</p><p id="b1d4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了演示这个用例，我们将从<a class="ae ky" href="https://www.acnestudios.com/au/en/search?q=handbag" rel="noopener ugc nofollow" target="_blank"> Acne Studio </a>网站上收集与搜索“眼袋”相关的数据。</p><p id="71f3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们需要的Python库在很大程度上符合用例1，并增加了以下内容:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="b18b" class="nl la it nh b gy nm nn l no np">import time ## for setting a wait time<br/>from urllib.parse import urljoin ## for joining strings for urls</span></pre><p id="cbc5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将从启动Google Chrome开始，根据用例1设置URL，并指定“滚动，等待几秒钟，然后再次滚动”的动作顺序，如下所示。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="d6ba" class="nl la it nh b gy nm nn l no np">options = webdriver.ChromeOptions()<br/>options.add_argument('--headless')</span><span id="5f75" class="nl la it nh b gy nr nn l no np">driver = webdriver.Chrome(executable_path = r'C:\Users\Jin\Documents\Webscraping\Drivers\chromedriver.exe'<br/>                         , chrome_options = options)<br/>url_acne = "<a class="ae ky" href="https://www.acnestudios.com/au/en/search?q=handbag" rel="noopener ugc nofollow" target="_blank">https://www.acnestudios.com/au/en/search?q=bag</a>s"<br/>print(url_acne)</span><span id="30fc" class="nl la it nh b gy nr nn l no np">driver.get(url_acne)</span><span id="64b7" class="nl la it nh b gy nr nn l no np">time.sleep(2)  # Allow 2 seconds for the web page to open<br/>scroll_pause_time = 2 # wait 2 seconds for web page to load<br/> <br/>screen_height = driver.execute_script("return window.screen.height;")   # get the screen height of the web</span><span id="1d4c" class="nl la it nh b gy nr nn l no np">i = 1</span><span id="b7ad" class="nl la it nh b gy nr nn l no np">while True:<br/>    # scroll one screen height each time<br/>    driver.execute_script("window.scrollTo(0, {screen_height}*{i});".format(screen_height = screen_height, i = i))  <br/>    i += 1<br/>    time.sleep(scroll_pause_time)<br/>    # update scroll height after each scroll<br/>    scroll_height = driver.execute_script("return document.body.scrollHeight;")  <br/>    # Break the loop at the very end of the web page<br/>    if (screen_height) * i &gt; scroll_height:<br/>        break</span></pre><p id="2cd5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一旦我们到达无限滚动的物理末端，我们就可以存储HTML代码并使用与用例1相同的方法下载感兴趣的数据，如下所示。对于这个用例，我们将分别为每个手提包下载<strong class="lt iu">描述</strong>、<strong class="lt iu">价格</strong>和<strong class="lt iu">图片URL </strong>。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c01e" class="nl la it nh b gy nm nn l no np">description = []<br/>price = []<br/>url = []<br/>soup = BeautifulSoup(driver.page_source, "html.parser")</span><span id="1f79" class="nl la it nh b gy nr nn l no np">for desc in soup.find_all(class_ = "product-tile__name"):<br/>    <br/>    description.append(desc.get_text(strip = True))<br/>    <br/>for desc in soup.find_all(class_ = "product-tile__price font--monospace"):<br/>    <br/>    price.append(desc.get_text(strip = True))<br/>    <br/>for item in soup.find_all(class_ = "tile__link"):<br/>    <br/>    base = "<a class="ae ky" href="https://www.acnestudios.com/" rel="noopener ugc nofollow" target="_blank">https://www.acnestudios.com/</a>"<br/>    link = item.attrs["href"]<br/>    url_join = urljoin(base, link)<br/>    <br/>    url.append(url_join)</span><span id="4946" class="nl la it nh b gy nr nn l no np">## Converting into a pandas dataframe<br/>df_acne = pd.DataFrame(<br/>                        {<br/>                         'Description': description,<br/>                         'Price': price,<br/>                         'URL': url<br/>                        })</span><span id="8595" class="nl la it nh b gy nr nn l no np">df_acne</span></pre><p id="6b6e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">dataframe输出的示例如下所示。如果没有无限滚动，数据帧可能只包含10多条记录。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/96d274a4ded0d6ed897188e26e252ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPexAUQdoDZ2uoqXaTkrtw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图4:从网站上抓取的结构化内容——无限滚动。作者图片</figcaption></figure><h1 id="83ea" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用例3:图像刮擦</h1><p id="efa1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">图像抓取指的是一种自动化，其中网站托管的图像被下载并系统地保存在本地计算机中。由于以下原因，这可能很有用:</p><ul class=""><li id="91ce" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm nw my mz na bi translated">网站发布的图片有可能在日后被删除</li><li id="b934" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nw my mz na bi translated">下载的图像可以离线重新访问(即没有互联网)</li><li id="594c" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nw my mz na bi translated">可以以更全面的方式查看下载的图像(例如，按特定顺序，或按命名约定)</li></ul><p id="21e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了演示这个用例，我们将从<a class="ae ky" href="https://v2.mangapark.net/manga/h2/i294558/c1" rel="noopener ugc nofollow" target="_blank"> MangaPark </a>网站抓取一个漫画系列下的所有图像。随着整个漫画保存在个人电脑本地，人们可以轻松地浏览漫画页面，而不是在网上点击每一章的“下一页”按钮，并在中间遇到大量的在线广告。</p><p id="752f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用与用例1和2相同的Python库，我们(类似地)从尝试抓取单个章节下的图像开始，并使用for循环在这个抓取的基础上构建其他章节下的图像。对于日本漫画系列《H2》的<a class="ae ky" href="https://v2.mangapark.net/manga/h2/i294558/c1" rel="noopener ugc nofollow" target="_blank">第一章</a>:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="68f4" class="nl la it nh b gy nm nn l no np">url = "<a class="ae ky" href="https://v2.mangapark.net/manga/h2/i294558/c1" rel="noopener ugc nofollow" target="_blank">https://v2.mangapark.net/manga/h2/i294558/c1</a>"<br/>print(url)</span><span id="4b78" class="nl la it nh b gy nr nn l no np">options = webdriver.ChromeOptions()<br/>options.add_argument('--headless')<br/>driver = webdriver.Chrome( <br/>                          executable_path = r'C:\Users\Jin\Documents\Webscraping\Drivers\chromedriver.exe',<br/>                          chrome_options = options<br/>                         )<br/>driver.get(url)</span><span id="061d" class="nl la it nh b gy nr nn l no np">soup = BeautifulSoup(driver.page_source, 'html')</span><span id="bd8d" class="nl la it nh b gy nr nn l no np">## The number of images under a chapter varies, but the error skip<br/>## lets us download up to 50 without an out-of-index error<br/>## Image is saved and named in chronological order, in the H2 folder<br/>## in the path where the Python script is saved</span><span id="5c79" class="nl la it nh b gy nr nn l no np">try:</span><span id="7d8b" class="nl la it nh b gy nr nn l no np">for i in range(50):    <br/>        urllib.request.urlretrieve(str(soup.find_all(class_ = "img")[i].attrs["src"]),<br/>                               "H2/" + str(i) + ".jpg".format(i))</span><span id="a439" class="nl la it nh b gy nr nn l no np">except:<br/>    pass</span></pre><p id="85e1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一旦我们完成了上面的工作，就变成了定位“下一个”页面的URL并重复上面的抓取。这可以如下进行。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="89fa" class="nl la it nh b gy nm nn l no np">url = "<a class="ae ky" href="https://v2.mangapark.net/manga/h2/i294558/c1" rel="noopener ugc nofollow" target="_blank">https://v2.mangapark.net/manga/h2/i294558/c1</a>"<br/>chapter_number = 338</span><span id="cc92" class="nl la it nh b gy nr nn l no np">for j in range(1, upper_limit + 1):<br/>      <br/>    print(url)<br/>    options = webdriver.ChromeOptions()<br/>    options.add_argument('--headless')<br/>    driver = webdriver.Chrome(<br/>                              executable_path = r'C:\Users\Jin\Documents\Webscraping\Drivers\chromedriver.exe',<br/>                              chrome_options = options<br/>                             )<br/>    driver.get(url)<br/>    soup = BeautifulSoup(driver.page_source, 'html')</span><span id="92f1" class="nl la it nh b gy nr nn l no np">try:</span><span id="f56a" class="nl la it nh b gy nr nn l no np">for i in range(50):    <br/>            urllib.request.urlretrieve(str(soup.find_all(class_ = "img")[i].attrs["src"]),<br/>                                   "H2/" + str(j) + "_" + str(i) + ".jpg".format(i))</span><span id="6d9b" class="nl la it nh b gy nr nn l no np">except: ## number of pictures may vary on a webpage<br/>        pass</span><span id="6f22" class="nl la it nh b gy nr nn l no np">## Below locates the URL for the "Next" page<br/>base_url = "<a class="ae ky" href="https://v2.mangapark.net/" rel="noopener ugc nofollow" target="_blank">https://v2.mangapark.net/</a>"<br/>    next_page = soup.find_all("a", href = True)[12].get("href")</span><span id="0704" class="nl la it nh b gy nr nn l no np">url = urljoin(base_url, next_page)</span></pre><p id="4e97" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在坐下来，一边喝茶，一边享受阅读H2漫画的乐趣(我读了很多)。</p><h1 id="8427" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="921d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我展示了三(3)个带有现成代码的web抓取用例。这些用例可以很容易地扩展，以包括HTML代码在网站上拥有的附加属性——一个供读者探索的属性！</p><p id="12aa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为下一步，戴上我的精算师/数据科学家的帽子，我将探索商业机会，例如监控竞争对手网站的变化或收集评论网站上客户的潜在正面/负面评论——请关注此空间！</p></div></div>    
</body>
</html>