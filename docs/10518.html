<html>
<head>
<title>Quantum-Inspired MNIST</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量子激发的MNIST</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/quantum-inspired-mnist-6e33466d991b?source=collection_archive---------5-----------------------#2021-12-17">https://levelup.gitconnected.com/quantum-inspired-mnist-6e33466d991b?source=collection_archive---------5-----------------------#2021-12-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/e7620493a3de60425cfd89c7d99be1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*Ft2rLuO82eItlvJn5HOi9A.png"/></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated"><a class="ae jc" href="https://commons.wikimedia.org/wiki/File:MnistExamples.png" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:MnistExamples.png</a></figcaption></figure><div class=""/><h1 id="5e0b" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">没有比这更容易的了…</h1><p id="5ce8" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">如果你可以在不建立和训练模型的情况下进行机器学习，会怎么样？如果你能忘记权重、激活函数和优化器会怎么样？如果你能把归一化和导数换成加法和减法会怎么样？</p><p id="76b2" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">你可以。</p><p id="60e4" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我将使用<a class="ae jc" href="https://medium.com/swlh/quantum-classification-cecbc7831be" rel="noopener">分类</a>作为例子，使用<a class="ae jc" href="https://towardsdatascience.com/quantum-mnist-f2c765bdd478" rel="noopener" target="_blank"> MNIST数据集</a>。这应该也适用于<a class="ae jc" rel="noopener ugc nofollow" target="_blank" href="/quantum-clustering-c498b089b88e">聚类</a>，因为启发这个实验的<a class="ae jc" rel="noopener ugc nofollow" target="_blank" href="/comparing-quantum-states-c6445e1e46fd">量子算法对两者都适用。</a></p><p id="84bb" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">对于那些不知道的人来说，MNIST是一个流行的手写数字数据集，数字0到9。MNIST练习的目标是获取一个手写的一位数，并准确地识别出它想要的数字。而且，不遇到机器学习大概是不可能进入的，因为这是一个干净的数据集。您可以专注于构建和训练模型，而不是清理数据。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi md"><img src="../Images/e954dbb0a137eccb68ad02db2d0d5919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MRdh6dZbBro4SzjXizJK_Q.png"/></div></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">量子分类</figcaption></figure><h2 id="716c" class="mm kd jf bd ke mn mo dn ki mp mq dp km ll mr ms kq lp mt mu ku lt mv mw ky mx bi translated">经典分类是如何工作的？</h2><p id="a920" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在最基本的层面上，分类从输入和输出开始，我们确定“权重”,当乘以输入时，产生已知的输出。然后，我们将这些权重乘以测试输入，并获得建议每个输入最可能分类的输出。</p><pre class="me mf mg mh gt my mz na nb aw nc bi"><span id="70c3" class="mm kd jf mz b gy nd ne l nf ng">inputs * weights = outputs</span></pre><p id="4dfd" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">嗯，这看起来并不难，对吧？这有什么挑战性？</p><p id="dd88" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">挑战在于有效地找到重量。我们从将输入乘以随机权重开始，确定我们离已知输出有多远，并以小增量调整权重。我们这样做很多很多次，直到小的权重调整将加权输入和输出之间的距离减小到它们的最低可能值。</p><p id="dbf1" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">但是，这些调整应该有多大呢？如果我们太自由了，超出了我们的目标怎么办？或者，如果我们太保守了，训练要花很长时间呢？那么，我们需要另一个模型来调整我们的超参数？</p><p id="f15f" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">因此，基本分类在概念上很简单，但也可能变得复杂。我们正在寻求优化运行时间和准确性。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nh"><img src="../Images/a71d894ba5935a4401f26a0d767017ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7HpliE0XE3CrlzFe0UU3Ow.png"/></div></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">量子MNIST</figcaption></figure><h2 id="64b5" class="mm kd jf bd ke mn mo dn ki mp mq dp km ll mr ms kq lp mt mu ku lt mv mw ky mx bi translated">量子分类是如何工作的？</h2><p id="c62c" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于经典分类，我们使用加权来减少输入和输出之间的“距离”。“距离”这个词很重要，因为有一种称为交换测试的量子算法，也称为内积、核方法，以及对本讨论最重要的距离度量。如果我们将经典数据映射到量子态，<a class="ae jc" rel="noopener ugc nofollow" target="_blank" href="/comparing-quantum-states-c6445e1e46fd">交换测试决定了两个量子态有多近或多远</a>。</p><p id="1364" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">经典分类和量子分类的区别在于，我们不需要加权，也不需要做任何调整。我们只是测量。如果我们的测试数据测量最接近分类A，那么这就是我们的测试数据的可能分类。另一方面，如果我们的数据点测量值最接近分类B，那么这就是可能的分类。就这么简单。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f42b2277c9c83e2214c44c0d07353b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*xvCUOpP6kMeHjEPUloQcRQ.png"/></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">规范互换测试</figcaption></figure><h2 id="20d9" class="mm kd jf bd ke mn mo dn ki mp mq dp km ll mr ms kq lp mt mu ku lt mv mw ky mx bi translated">什么是量子启发分类？</h2><p id="631f" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">如果我们经典地做同样的事情，就像我们定量地做一样，只是确定值之间的距离，会怎么样？</p><pre class="me mf mg mh gt my mz na nb aw nc bi"><span id="ddad" class="mm kd jf mz b gy nd ne l nf ng">distance += absolute_value(MNIST_value - test_data_value)</span></pre><p id="e42f" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">MNIST数据集包含28x28像素或总共784像素的图像。如果我们一个像素一个像素地循环，确定一个测试数字和零、同一个测试数字和一、同一个测试数字和二之间的总距离，等等，会怎么样呢？</p><pre class="me mf mg mh gt my mz na nb aw nc bi"><span id="304d" class="mm kd jf mz b gy nd ne l nf ng">.<br/>.<br/>.</span><span id="11e1" class="mm kd jf mz b gy nj ne l nf ng">Target: 9<br/>Actual: 9</span><span id="beb3" class="mm kd jf mz b gy nj ne l nf ng">Correct: 72%</span></pre><p id="ce7a" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">训练数据集包含大约6000个每个数字的记录，所以我喜欢使用平均数字。对于所有的0、1等等，每个像素的平均亮度值是多少？然后我取了前100个测试数字，完全按照我刚才描述的那样做。我一个像素一个像素地计算测试数字和平均值0之间的距离，然后是相同的数字和平均值1，以此类推。最低总和代表测试数据和某一特定分类之间的最短距离，这就是可能的分类。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nh"><img src="../Images/176e34ce80550e4b57e78ac4800622f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oD2o80NROQS8FjVM3mD2Pw.png"/></div></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">比较多维数据</figcaption></figure><h2 id="8495" class="mm kd jf bd ke mn mo dn ki mp mq dp km ll mr ms kq lp mt mu ku lt mv mw ky mx bi translated">结果如何比较？</h2><p id="4d4c" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在100个测试数字的小样本中，准确率为72%。随机猜测只有10%，所以72%并不可怕。而且，考虑到我们只做加法和减法，它<a class="ae jc" href="https://www.researchgate.net/figure/Accuracy-versus-the-average-number-of-operations-for-MNIST-dataset_fig2_329701816" rel="noopener ugc nofollow" target="_blank">与传统上执行230，000次运算</a>相比。在低端，我发现有人在MNIST准确率<a class="ae jc" href="https://datascience.stackexchange.com/questions/38604/too-low-accuracy-on-mnist-dataset-using-a-neural-network" rel="noopener ugc nofollow" target="_blank">低至11% </a>的情况下苦苦挣扎，我发现Kaggle比赛准确率在99%以上。事实上，这两个极端的原因是我们可以增加的复杂程度。一方面，这种方法击败了正在努力学习图像分类的学生，另一方面，同时保留了相当多的改进空间。</p><p id="4c01" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我还必须说，量子启发的分类速度很快。我已经训练了非常简单的分类模型，最简单的通常仍然需要至少一分钟。相比之下，7840轮加减运算几乎是瞬间完成的。它实际上需要一秒钟，但这可能是因为在Jupyter笔记本上运行它。这不是最优的。</p><h2 id="84bc" class="mm kd jf bd ke mn mo dn ki mp mq dp km ll mr ms kq lp mt mu ku lt mv mw ky mx bi translated">下一步是什么？</h2><p id="6c34" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我的起点是72%的准确率。除了加法和减法什么都没有。再简单不过了。然而，我有办法让这种方法保持概念上的简单，同时有望提高准确性。</p><p id="1ae2" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">首先，我一直关注偏离平均数字，因为我一直关注训练数字相互重叠。我这里指的当然是量子分类。此外，我还担心维数减少和有限的量子位可用性。然而，如果按照传统方式来做，这些真的不是问题。因此，我有另一种“编码”来尝试，我们将看到它如何影响准确性。</p><p id="c73b" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">第二，经典MNIST使用卷积神经网络(CNN)，而不是我在这里描述的简单方法。我们不做单个像素和单个像素的比较，也不会只做一个比较。因此，我可以扩展它，将小像素组与小像素组进行比较，就像CNN一样，看看会发生什么。</p><p id="ad45" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">第三，我可以分析Kaggle提交的内容。我实际上从来没有这样做过，因为直到现在我只和MNIST·量子合作过。但是，也许有一些想法可以在不增加太多复杂性的情况下实现。量子启发的MNIST的要点是简单，因为量子MNIST的简单。</p><p id="53a4" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated"><em class="nk">一如既往的可以在</em> <a class="ae jc" href="https://github.com/bsiegelwax/Quantum-Inspired-MNIST" rel="noopener ugc nofollow" target="_blank"> <em class="nk"> GitHub </em> </a> <em class="nk">上找到代码。</em></p></div></div>    
</body>
</html>