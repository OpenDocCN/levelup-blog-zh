<html>
<head>
<title>A Decent Introduction to Gradient Descent in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中梯度下降的恰当介绍</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/a-decent-introduction-to-gradient-descent-in-python-846be2e41592?source=collection_archive---------16-----------------------#2021-02-22">https://levelup.gitconnected.com/a-decent-introduction-to-gradient-descent-in-python-846be2e41592?source=collection_archive---------16-----------------------#2021-02-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ea89632e6257babdbd550cd9ffbb7888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yOyhs0f9mL9CokGriekGbQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://pixabay.com/images/id-3501528/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/images/id-3501528/</a></figcaption></figure><div class=""/><p id="e31b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> G </span>梯度下降是当今机器学习算法中的一个基本元素。我们使用G <em class="lk">梯度下降</em>来更新机器学习模型的参数，并试图通过它来优化它。线索是模型会自己更新这些参数。这导致模型做出更好的预测。</p><p id="1c2a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的文章中，我们将深入探讨<em class="lk">梯度下降</em>，<em class="lk"> </em>，我将借助一个示例场景来解释它，我们将使用<em class="lk">监督学习</em>和Python来解决它。我们不会为此使用任何ML框架，如PyTorch或TensorFlow。完整的源代码可从<a class="ae jd" href="https://gist.github.com/larswaechter/c2df0bc9b0f15c64220eb5699b25ddf1" rel="noopener ugc nofollow" target="_blank"> GitHub </a>获得。</p><h1 id="69cb" class="ll lm jg bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">监督学习</h1><p id="5bd8" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">让我们看看<a class="ae jd" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">维基百科对监督学习的</a>定义。</p><blockquote class="mo mp mq"><p id="f74c" class="kd ke lk kf b kg kh ki kj kk kl km kn mr kp kq kr ms kt ku kv mt kx ky kz la ij bi translated">监督学习是基于示例输入-输出对学习将输入映射到输出的函数的机器学习任务。</p></blockquote><p id="420a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">听起来令人困惑，但实际上很简单。假设你想让电脑告诉你一张照片是不是猫的。</p><p id="d8b8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在计算机能够告诉你它是不是一只猫之前，你必须教会计算机猫的照片是什么样子的。为此，你向计算机输入许多照片(输入)和照片是否是猫的信息(输出)。这个过程称为<em class="lk">训练阶段</em>。</p><p id="8b67" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当<em class="lk">训练阶段</em>完成后，你可以向你的电脑展示一张它从未见过的猫的照片，它会告诉你这是不是猫的照片。太神奇了！为此，您的计算机使用它之前刚刚学习的<em class="lk">功能</em>。</p><p id="6924" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个猫的例子是一个分类问题。<em class="lk">分类</em>的任务是预测一个离散的类标签(“猫”和“没有猫”)。另一种类型的问题称为<em class="lk">回归，</em>这是预测数值或连续值的任务。下面的例子就是这样一个<em class="lk">回归问题</em>。</p><h1 id="70ff" class="ll lm jg bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">场景</h1><p id="3bbb" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">想象我们是一个游乐园的主人。出于组织的原因，我们想知道或者至少预测第二天的访客数量。幸运的是，我们根据上个季节的天气记录了一些游客数量。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/e89df4bd503ef6a06d27ec8b706b7589.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*8dkpqsna7v5W47e5YOzTpg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">基于天气的游客数量</figcaption></figure><p id="c825" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于维基百科对监督学习的定义，这些值就是所谓的“示例输入输出对”。温度是“输入”，游客数量是“输出”。</p><p id="ddfa" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，当室外温度为30摄氏度时，我们的公园有620名游客。这些笔记是我们ML模型的训练数据。左栏中的值称为<em class="lk">特性。</em>右栏中的标签称为<em class="lk">标签</em>。</p><p id="fabf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后但并非最不重要的是，注明的数据放在一个美丽的图表。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/e1021b68ecf55448b8f955c44d928536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpF5Wv-GU_Juo0ckDtyfmg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">基于天气的游客数量</figcaption></figure><p id="a621" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回到我们的问题。当我们知道明天将会有多少游客在我们的公园里的时候，我们还能期待看到多少游客呢？如你所见，我们的表格中没有关于33 C的访客数量的信息。我们必须预测价值。我们开始吧！</p><h1 id="5016" class="ll lm jg bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">线性回归</h1><p id="51b2" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">线性回归帮助我们找到“最佳拟合直线”,我们可以将它放入图表的数据中。当我们发现这条线时，我们可以用它来预测不属于我们记录的其他值，比如当室外温度为33℃时，明天的访问者数量。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/e3b461319967d97d9674c0a553694f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fRXp6EFld0Z5Hpaiq_lig.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">描述我们模型的线性函数</figcaption></figure><p id="d624" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能还记得在学校时，一个线性函数被描述为:</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5fdd713452713aeff1fbabdbbf5a45aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*19zZxVqADpwe8viOjPByNw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">线性函数</figcaption></figure><ul class=""><li id="6fcd" class="nb nc jg kf b kg kh kk kl ko nd ks ne kw nf la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">y</code>输出(访客)也称为<code class="fe nk nl nm nn b">f(x)</code></li><li id="e626" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">m</code>是斜坡</li><li id="b93e" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">x</code>是解释变量(天气)</li><li id="c708" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">b</code>是偏见</li></ul><p id="33d1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能也很熟悉这个方程的其他一些变量名。使用这个代表我们的线的等式，我们可以预测以后任何给定输入(<code class="fe nk nl nm nn b">x</code>)的输出(<code class="fe nk nl nm nn b">y</code>)。</p><p id="3db2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的场景中，当室外温度为33摄氏度(<code class="fe nk nl nm nn b">x</code>)时，我们希望预测访客数量(<code class="fe nk nl nm nn b">y</code>)，因此是<code class="fe nk nl nm nn b">f(33) = y = m * 33 + b</code>。</p><p id="36b1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们必须明智地选择<code class="fe nk nl nm nn b">m</code>和<code class="fe nk nl nm nn b">b</code>的值。幸运的是，计算机将为我们做这件事。它们是模型内部的可学习参数。</p><p id="471d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们知道这个方程是用来预测值的。让我们用Python实现这个:</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/fe809e51551d3120bee9455f9428b756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMKweaqwDMnFjyrt9Ig36g.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">相当于:y = mx + b</figcaption></figure><p id="7ffc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意我们把变量<code class="fe nk nl nm nn b">x</code>的名字改成了<code class="fe nk nl nm nn b">w</code>，意思是<em class="lk">权重。</em>我们的目标是这个<code class="fe nk nl nm nn b">predict()</code>函数告诉我们任何给定输入变量<code class="fe nk nl nm nn b">X</code>的输出。为此，我们需要给<code class="fe nk nl nm nn b">w</code>和<code class="fe nk nl nm nn b">b</code>赋值。</p><p id="7595" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们找到这些值之前，让我们寻找一种方法来衡量我们新的<code class="fe nk nl nm nn b">predict()</code>函数的质量。一个所谓的<em class="lk">损失函数</em>可以为我们做到这一点。</p><h1 id="3a54" class="ll lm jg bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">损失函数</h1><p id="441b" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated"><em class="lk">损失函数</em>测量模型的预测与其标签的距离。换句话说:它决定了我们的模型有多好。</p><p id="c0a8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们使用的是<em class="lk">线性回归</em>、<em class="lk">、</em>，我们就不必发明自己的<em class="lk">损失函数</em>。已经有一个我们可以用了。这个叫做<em class="lk">均方误差</em> (MSE)。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/be6382aa9f9ffe74a76f290a66871328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3xbY6tuoGSuMXZRsPQ-jCg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">均方误差</figcaption></figure><p id="2b4e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数包括四个变量<code class="fe nk nl nm nn b">n</code>、<code class="fe nk nl nm nn b">i</code>、<code class="fe nk nl nm nn b">y'</code>和<code class="fe nk nl nm nn b">y</code>:</p><ul class=""><li id="64d2" class="nb nc jg kf b kg kh kk kl ko nd ks ne kw nf la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">n</code>是我们拥有的输入输出对的数量(14)</li><li id="9e24" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">i</code>是我们正在查看的训练数据中当前对的索引</li><li id="dfbf" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">y'</code>是从我们的模型中得到的预测值，我们将从一开始声明的<code class="fe nk nl nm nn b">predict()</code>中得到</li><li id="675a" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">y</code>是我们从训练数据标签中获得的实际输出值</li></ul><p id="c822" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于来自我们训练数据的每个值，该函数计算标签和来自<code class="fe nk nl nm nn b">predict()</code>的预测值之间的<strong class="kf jh">差</strong>，并对结果求平方。所有这些结果相加，最后取平均值。</p><p id="8307" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以使用<em class="lk">损失函数</em>来确定我们的模型是好是坏。函数的输出值越小，我们的模型就越好，因为在这种情况下只有很少的错误。另一方面，高值意味着更差的模型。所以，我们的目标是<strong class="kf jh">最小化</strong>的输出，因为我们想要尽可能少的错误。</p><p id="cb72" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用Python实现MSE。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/ca1f49ae012e24b0bc723a802194e97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVllIukKElV3yyZQmsuEWQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Python中的均方误差</figcaption></figure><p id="2ee7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们有四个论点:</p><ul class=""><li id="11f2" class="nb nc jg kf b kg kh kk kl ko nd ks ne kw nf la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">X</code>是一个包含温度的Numpy数组(<em class="lk">特性</em>)</li><li id="c6b7" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la ng nh ni nj bi translated"><code class="fe nk nl nm nn b">Y</code>是一个包含访问者数量的Numpy数组(<em class="lk">标签</em>)</li></ul><p id="002a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nk nl nm nn b">w</code>和<code class="fe nk nl nm nn b">b</code>用于<code class="fe nk nl nm nn b">predict()</code>函数，其输出相当于MSE方程中的<code class="fe nk nl nm nn b">y</code>。因为<code class="fe nk nl nm nn b">X</code>和<code class="fe nk nl nm nn b">Y</code>都是Numpy数组，所以对每个数组元素执行<code class="fe nk nl nm nn b">(predict(X, w, b) - Y) ** 2</code>部分。</p><p id="23bf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们能够衡量我们的模型是好是坏。请记住，我们试图为<code class="fe nk nl nm nn b">predict()</code>中的<code class="fe nk nl nm nn b">w</code>和<code class="fe nk nl nm nn b">b</code>找到合适的值。</p><p id="1c70" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们使用<code class="fe nk nl nm nn b">loss()</code>函数来测量我们的模型的<em class="lk">损失</em>，用一些随机数来表示权重<code class="fe nk nl nm nn b">w</code>。为了简单起见，我们将设置<code class="fe nk nl nm nn b">b = 0</code>，因此预测只取决于<code class="fe nk nl nm nn b">predict()</code>中的<code class="fe nk nl nm nn b">w</code>。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/565580531b15d922c41ad73d2d088c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PMF6nb7zKG9S2PPwB78Krg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">不同重量的损失</figcaption></figure><p id="f556" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以看到，当我们设置<code class="fe nk nl nm nn b">w = ~14</code>时，模型的<em class="lk">损失</em>最小。这个位置也被称为最小<em class="lk">位置</em>。当人们看到图表时，他们可以很容易地识别这个点。另一方面，计算机不能这样做。此外，您并不总是知道重量<code class="fe nk nl nm nn b">w</code>可能接近哪个数字。如果<em class="lk">最小值</em>在<code class="fe nk nl nm nn b">w = 3000</code>会怎么样？x轴上的刻度太小，无法直观地识别最小<em class="lk">值</em>。</p><p id="e2ec" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们需要找到一种方法来告诉计算机如何找到这个点。为此，<em class="lk">梯度下降</em>是正确的选择。</p><h1 id="456b" class="ll lm jg bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">梯度下降</h1><p id="9fa9" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated"><em class="lk">梯度下降</em>是寻找函数局部<em class="lk">最小值</em>的算法。在这种情况下，我们试图找到我们的<em class="lk">损失函数</em>的最小值<em class="lk">，因为在这个位置，模型做出了<strong class="kf jh">最佳预测</strong>。</em></p><p id="38a0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<em class="lk">梯度下降</em>中，我们在图表中选择一个随机起点。从这个位置开始，我们将朝着最小值<em class="lk">迈出许多步</em>。但是我们怎么知道该往哪个方向走(向左还是向右)以及一步应该走多远呢？</p><h2 id="3652" class="nx lm jg bd ln ny nz dn lr oa ob dp lv ko oc od lz ks oe of md kw og oh mh oi bi translated">梯度</h2><p id="dea2" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">为此，我们使用所谓的<em class="lk">渐变。</em><em class="lk">梯度</em>实际上可以有两种含义，这取决于函数参数的数量。</p><p id="268a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这两种情况下，你可以说一个函数的<em class="lk">梯度</em>是在它的所有<strong class="kf jh">偏导数</strong>的帮助下计算的。不过先说一下<em class="lk">渐变</em>是什么。</p><p id="42d8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">一个参数:f(x) </strong></p><p id="d5f3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们有一个类似于<code class="fe nk nl nm nn b">f(x) = 2x + 1</code>的函数，它只依赖于一个参数(<code class="fe nk nl nm nn b">x</code>)，那么梯度就是图在任意给定点的<strong class="kf jh">陡度</strong>，也称为<em class="lk">斜率</em>。举个例子吧。</p><p id="ba9d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是随机选择函数<code class="fe nk nl nm nn b">f(x) = 2x + 1</code>的图形。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/eac19e89f1edad5439cb28d26af5017d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*iZVx0_qbZ9teJoi1_cHs5A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">f(x) = 2x + 1</figcaption></figure><p id="19a2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们计算给定点的<em class="lk">梯度</em>。为此，我们首先计算关于<code class="fe nk nl nm nn b">x</code>的偏导数，因为这是我们唯一的参数。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/63969b553e9018e5edd79fdf2a7638d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fxtst8ullRqsQgR6BDynIg.png"/></div></div></figure><p id="edc2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果是<code class="fe nk nl nm nn b">2</code>，函数<code class="fe nk nl nm nn b">f(x)</code>的<em class="lk">斜率</em>到底是多少。因为它是一个线性函数，所以在任意给定点<code class="fe nk nl nm nn b">x</code>处<em class="lk">梯度</em>等于<code class="fe nk nl nm nn b">2</code>。</p><p id="a8ac" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们有像<code class="fe nk nl nm nn b">f(x) = (x - 3)^2 + 2</code>这样的非线性函数呢？在这种情况下，它的图表如下所示。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/797040bf9eef0cedfb52434db8681c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*i7rQ6ORuD8n-DjNsu0MN5w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">(x - 3) + 2</figcaption></figure><p id="30bf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能会注意到这个图表类似于我们上面的<em class="lk">损失函数</em>。现在，我们计算图中任意一点的<em class="lk">梯度</em>。同样，我们首先计算关于<code class="fe nk nl nm nn b">x</code>的偏导数，因为这是我们唯一的参数。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/ddb93ba02985d8dfe2b9f23c7a560bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_3PP9oyXnuR_v_5-OCEyg.png"/></div></div></figure><p id="9867" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，每个点的<em class="lk">斜率</em>并不相同。这里就看<code class="fe nk nl nm nn b">x</code>了。假设我们站在图表中的<code class="fe nk nl nm nn b">x = 6</code>。该点的<em class="lk">坡度</em>又名<em class="lk">坡度</em>是多少？在这种情况下是<code class="fe nk nl nm nn b">2*(6–3) = 6</code>。红线是在给定点<code class="fe nk nl nm nn b">x</code>处<code class="fe nk nl nm nn b">6</code>的<em class="lk">斜率</em>的切线。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ac115e0abd2cc729681b4b1dd6fd5a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*PuEn1XqEsPyv3phzno_Kyw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">x = 6时的梯度</figcaption></figure><p id="1d97" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再比如。假设我们在同一张图中的<code class="fe nk nl nm nn b">x = 4</code>处。这里的<em class="lk">坡度</em>又名<em class="lk">坡度</em>为<code class="fe nk nl nm nn b">2*(4-3) = 2</code>。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/856fe09731df0752b53a0aaa2aa7142c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HdkIkTxTBg9mPOdJZ4Y1Gw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">x = 4时的梯度</figcaption></figure><p id="2869" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里重要的是<em class="lk">梯度</em> <strong class="kf jh">越来越减少</strong>越来越接近图表的<em class="lk">最小值</em>。在我们的示例点比较上面两幅图的<em class="lk">斜率</em>。第二个具有较小的<em class="lk">斜率</em>，因为该点更接近最小<em class="lk">斜率</em>。</p><p id="f508" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图形<em class="lk">最小值</em>处，其中<code class="fe nk nl nm nn b">x = 3</code>处，G <em class="lk">半径</em>为<code class="fe nk nl nm nn b">0</code>。在这个位置根本没有<em class="lk">斜率</em>，如红色切线所示。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/4f3028854f89b178943ae8995806a1c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fdHRlqzv1GzbPzdVD4j_3w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">x = 3处的坡度= &gt;无坡度</figcaption></figure><p id="216b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，一般来说，我们可以说<em class="lk">梯度</em>告诉我们是否远离图表的最小值<em class="lk">或</em>。<em class="lk">坡度</em>越小，我们就越接近<em class="lk">最小值</em>。</p><p id="0c3f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧，但这对我们有什么帮助？借助于<em class="lk">梯度</em>，我们可以<strong class="kf jh">最小化</strong>我们的<em class="lk">损失函数</em>的输出。这就是我们的目标，我们希望将损失降到最低。</p><p id="e039" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设上面的图表是我们损失函数的过程，其中<code class="fe nk nl nm nn b">y</code>是<em class="lk">损失</em>和<code class="fe nk nl nm nn b">x</code>是<em class="lk">权重。</em>我们想要找到<code class="fe nk nl nm nn b">x</code>的值，其中<code class="fe nk nl nm nn b">y</code>是最小的，因为<code class="fe nk nl nm nn b">y</code>是<em class="lk">损失</em>、<em class="lk">、</em>，并且我们想要在我们的模型中有一个<em class="lk">最小损失</em>来做出更好的预测。在上面的场景中，那将是在位置<code class="fe nk nl nm nn b">x = 3</code>。</p><p id="7e50" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是我们如何找到这个点<code class="fe nk nl nm nn b">x</code>？这就是<em class="lk">梯度下降</em>算法发挥作用的地方。</p><h2 id="c07e" class="nx lm jg bd ln ny nz dn lr oa ob dp lv ko oc od lz ks oe of md kw og oh mh oi bi translated">梯度下降(续)</h2><p id="2268" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在<em class="lk">梯度下降</em>中，我们将<code class="fe nk nl nm nn b">x</code>(权重)设置为随机初始值。假设我们设置了<code class="fe nk nl nm nn b">x = 6</code>。那是我们的起点。实际上，这个值可能是<code class="fe nk nl nm nn b">0</code>。我们的功能同上。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0ac10f966e4870ac480c4b1cd5fffacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uIGMe60MrUvTdQJqQnuxLw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">起点x = 6</figcaption></figure><p id="48a1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从那里开始，我们计算给定起始位置<code class="fe nk nl nm nn b">x</code>的<em class="lk">梯度</em>。之后，我们根据<em class="lk">渐变的</em>大小向左迈一小步(因为我们想达到最小值)。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c07b76a251b879bbbb6faaa7e5d360ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JqPLeNy4Qg5xjHh0UeaGDQ.jpeg"/></div></figure><p id="1098" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们如何知道我们是需要向左移动还是向右移动？在这种情况下，<em class="lk">坡度</em>为<strong class="kf jh">正</strong>，因为在这个位置有一个正斜率。所以，我们知道我们必须向左走。如果我们在一个更靠左的地方，我们必须向右走，因为那里的<em class="lk">斜率</em>是<strong class="kf jh">负</strong>。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c9bac850860ed79a67552e85a88ea756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1EK5c0UYlJi-sD2POTWvfA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">正斜率与负斜率</figcaption></figure><p id="365e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们回到下降。现在我们到达了图表上的一个新的点，我们再次计算<em class="lk">的<em class="lk">梯度</em>。然后，我们又向左走了一步。然而，这一次，我们采取的步骤比前一步稍微<strong class="kf jh">小</strong>，因为<em class="lk">梯度</em>随着我们接近<em class="lk">最小值</em>而减小。</em></p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9fd8cb98037d315615b8cd6bd06f1ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bQDgQ67fv5hdZK7IQLSCwA.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">我们更接近最小值</figcaption></figure><p id="f850" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们重复这个过程多次，直到我们达到或者至少接近图表的最小值<em class="lk"/>。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/aa49fcb9a2e5ee844428138a903dd78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*GypddB2FoTqe-ugKF-wLUw.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">我们几乎达到了最小值</figcaption></figure><p id="04fa" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里重要的是，每走一步，步长就变得越来越小。这是因为我们的步长取决于给定位置的<em class="lk">梯度</em>(斜率)的大小，我们越接近图表的<em class="lk">最小值</em>，梯度就越小。正如你在上面看到的。</p><p id="226f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，步长不仅仅取决于<em class="lk">渐变的</em>大小。在我们的起始位置(<code class="fe nk nl nm nn b">x = 6</code>)处，<em class="lk">斜率</em>也是<code class="fe nk nl nm nn b">6</code>。现在想象我们向左走<code class="fe nk nl nm nn b">6</code>步。在这种情况下，我们会错过图表的最小值<em class="lk">和结束时太左。这就是为什么我们必须走一定比例的<em class="lk">坡度</em>。</em></p><p id="9c26" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个比例也叫<em class="lk">学习率。</em>在我们的<em class="lk">梯度下降</em>的每次迭代期间(当我们采取单步时)，算法将<em class="lk">学习率</em>乘以<em class="lk">梯度</em>。由此产生的产品称为<strong class="kf jh"> <em class="lk"> </em> </strong> <em class="lk">渐变步长:</em> <code class="fe nk nl nm nn b">gradient_step = lr * gradient</code>。这是我们向左迈出的实际一步。</p><p id="81fd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lk">学习率</em>是我们自己设定的值。我们可以用<code class="fe nk nl nm nn b">0.001</code>作为例子。过高的<em class="lk">学习率</em>可能会导致迈出过大的步伐，我们可能会错过<em class="lk">最小值</em>。如果<em class="lk">学习率</em>太小，我们往往无法在给定的迭代次数内达到最小<em class="lk">学习率</em>。</p><p id="a488" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，从我们当前的<em class="lk">权重</em>中减去每次迭代中得到的<em class="lk">梯度步长</em>，即<code class="fe nk nl nm nn b">x</code>值:<code class="fe nk nl nm nn b">x = x - gradient_step</code>。这样我们的<em class="lk">重量</em>就一步步接近目标值:即<em class="lk">损失</em> ( <code class="fe nk nl nm nn b">y</code>)最小的值。</p><p id="f3da" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<code class="fe nk nl nm nn b">gradient_step</code>为负是因为<em class="lk">斜率</em>为负，我们<strong class="kf jh">将</strong>加上<code class="fe nk nl nm nn b">gradient_step</code>，因为减和减都是正，并相应向右移动。</p><p id="00f3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们把这些都放在一起，用Python实现它。请记住，图形的<code class="fe nk nl nm nn b">x</code>值在我们的代码库中表示为<em class="lk">权重</em> <code class="fe nk nl nm nn b">w</code>。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/462427d547a56de2fee86cb4b365eab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ysDNsckRmP-PEhmTs7uIBA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Python中的梯度下降</figcaption></figure><p id="3192" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我前面提到的，梯度<em class="lk">是利用函数对每个参数的偏导数计算出来的。在这种情况下，我们的函数是<em class="lk">损失函数</em> (MSE)。目前我们只处理一个参数<code class="fe nk nl nm nn b">w</code>。所以，我们计算关于<code class="fe nk nl nm nn b">w</code>的偏导数。你可以在<code class="fe nk nl nm nn b">gradient()</code>中看到结果。这篇文章可能会让你更好地理解MSE的偏导数是如何计算的。</em></p><p id="bd5b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为到目前为止我们只处理一个参数，所以当我们在<code class="fe nk nl nm nn b">gradient()</code>函数中调用<code class="fe nk nl nm nn b">predict()</code>时，我们为<code class="fe nk nl nm nn b">b</code>传递<code class="fe nk nl nm nn b">0</code>。我们现在就解决这个问题。</p><p id="49d4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">两个参数:f(x，y) </strong></p><p id="84d5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，我们假设我们的模型只依赖于一个参数<code class="fe nk nl nm nn b">x</code>，也称为<em class="lk">权重</em> <code class="fe nk nl nm nn b">w</code> <em class="lk">。但事实并非如此。如果我们看一下<code class="fe nk nl nm nn b">predict()</code>函数，我们可以看到它对于给定值<code class="fe nk nl nm nn b">X</code>的输出取决于权重<code class="fe nk nl nm nn b">w</code>和偏差<code class="fe nk nl nm nn b">b</code>。所以我们有两个参数。</em></p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/6fd2f9118a58b32ef37f3f856e191a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iBmm2Mdhnx4Uqs5amX5XrQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">“X”的输出实际上取决于“w”和“b”</figcaption></figure><p id="f6e4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着，上面的图表不再正确。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/aa49fcb9a2e5ee844428138a903dd78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*GypddB2FoTqe-ugKF-wLUw.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">这里的输出仅取决于一个参数= &gt;错误</figcaption></figure><p id="de56" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的<code class="fe nk nl nm nn b">y</code>是我们的<em class="lk">损失</em>和<code class="fe nk nl nm nn b">x</code>重量(<code class="fe nk nl nm nn b">w</code>)。现在，我们需要偏差<code class="fe nk nl nm nn b">b</code>的另一个维度，因为我们模型的<em class="lk">损失</em>取决于我们将为<code class="fe nk nl nm nn b">w</code> <strong class="kf jh">和</strong> <code class="fe nk nl nm nn b">b</code>设置的值。</p><p id="2d79" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着我们的<em class="lk">损失函数</em>有<strong class="kf jh">三个轴</strong>，并且可能看起来像下面这样，带有一些<em class="lk">权重</em>和<em class="lk">偏差</em>的随机值。绿色十字表示图表的最小值<em class="lk"/>。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/6e3f1e4218936853cd2bea561f07c74f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JtdBWYG7ytzimcfQ0mFbtg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">损失函数包括“w”和“b”</figcaption></figure><p id="ca58" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的场景中，我们有一个这样的函数，它取决于两个输入参数<code class="fe nk nl nm nn b">x,y</code>也就是<code class="fe nk nl nm nn b">w,b</code>，梯度<em class="lk">是一个<strong class="kf jh">向量</strong>，它可以被解释为“最快增长的方向和速率”。换句话说:它显示了从我们图表中的给定位置开始的最陡上升方向。</em></p><p id="53ed" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象你站在一座山的山谷里，用手指着山顶。在这种情况下，你的手指是<em class="lk">渐变。</em>它指向最陡的斜坡。</p><p id="d006" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与这个方向相反的是最陡下降的方向。如果我们沿着这个方向走，会发生什么？我们到达图的最小值<em class="lk"/>。对于给定的<code class="fe nk nl nm nn b">w</code>和<code class="fe nk nl nm nn b">b</code>，我们的<em class="lk">损失</em>最小的点。使用这个位置的坐标作为<em class="lk">重量</em>和<em class="lk">偏差</em>，我们的模型将做出最佳预测。</p><p id="6bea" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当函数有两个参数时我们如何计算<em class="lk">梯度</em>？就像上面一样，我们计算<strong class="kf jh">相对于<code class="fe nk nl nm nn b">x</code>(重量)的偏导数</strong>。此外，我们对<code class="fe nk nl nm nn b">y</code>(偏置)也做了同样的处理。</p><p id="2655" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用Python来实现它。我们可以考虑上面对<code class="fe nk nl nm nn b">w</code>计算的偏导数。而且，我们在调用<code class="fe nk nl nm nn b">predict()</code>时不再传递<code class="fe nk nl nm nn b">0</code>作为参数，而是传递<code class="fe nk nl nm nn b">b</code>。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/8b03d39e3cc2fac9c339f8bfd2fedcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KGWOQIH3_xfv8BFc9qeZLQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">“loss()”相对于“w”和“b”的偏导数</figcaption></figure><p id="30e3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请记住，<em class="lk">渐变</em>是一个<strong class="kf jh">矢量</strong>。在这种情况下，向量有两个元素。这就是为什么我们从<code class="fe nk nl nm nn b">gradient()</code>返回两个值。</p><p id="c991" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于从现在开始我们有两个<em class="lk">渐变</em>，我们必须改变我们的<code class="fe nk nl nm nn b">train()</code>功能。值<code class="fe nk nl nm nn b">w</code>和<code class="fe nk nl nm nn b">b</code>都必须在每次迭代后更新。</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/9d0bd93f4234d468c270263d66edb307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJ7zpDpX_N544vg9_0HsRg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Python中的梯度下降</figcaption></figure><h1 id="0b5a" class="ll lm jg bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">预测</h1><p id="1855" class="pw-post-body-paragraph kd ke jg kf b kg mj ki kj kk mk km kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">最后，我们有了从头回答问题所需的一切。这是我们的代码库:</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/369c5e0f61566783d33d2c9780d0e673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQjPq-Wy286iFrzKtCIPFw.png"/></div></div></figure><p id="3a12" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嗯，问题是什么来着？</p><blockquote class="mo mp mq"><p id="5f94" class="kd ke lk kf b kg kh ki kj kk kl km kn mr kp kq kr ms kt ku kv mt kx ky kz la ij bi translated">当我们知道明天会有33摄氏度的时候，我们的公园里会有多少游客呢？</p></blockquote><p id="8380" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们来回答这个。为此，我们采取以下措施:</p><ol class=""><li id="7534" class="nb nc jg kf b kg kh kk kl ko nd ks ne kw nf la on nh ni nj bi translated">读取训练数据</li><li id="df8e" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la on nh ni nj bi translated">训练模型</li><li id="22eb" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la on nh ni nj bi translated">做出预测</li><li id="2988" class="nb nc jg kf b kg no kk np ko nq ks nr kw ns la on nh ni nj bi translated">输出</li></ol><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/0a9a143c511ef057cd1174f639fa55f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UHvdQpZvhdv-DmG_GNZXXg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">做出预测</figcaption></figure><p id="cac3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们的模型完成训练后，它将为<code class="fe nk nl nm nn b">w</code>和<code class="fe nk nl nm nn b">b</code>设置适当的值。现在我们可以使用这两个参数来预测<code class="fe nk nl nm nn b">x = 33</code>的输出。我们简单地调用<code class="fe nk nl nm nn b">predict()</code>并传入相应的参数。</p><p id="e43a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们运行Python程序并预测值时，我们将获得以下关于<em class="lk">权重</em>、<em class="lk">偏差</em>和预测的输出:</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/433b444a28e19753d200c3079b709fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*IjozNkAIM5uTy5E_NDQ3fQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">33摄氏度的预测</figcaption></figure><p id="4456" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，我们预计第二天会有<strong class="kf jh"> ~543名访客</strong>。我们还可以预测任何一天的访客数量。我们只需改变调用<code class="fe nk nl nm nn b">predict()</code>时传入的<code class="fe nk nl nm nn b">X</code>参数(度)。</p><p id="3e8b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看在训练阶段的每次迭代之后，模型是如何改进的。到底有没有改善？</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/2df828e675f2c634c32aa88794972fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-fObGukQR0bk77_hbC89ag.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">损失曲线= &gt;每次迭代后的损失</figcaption></figure><p id="58f9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，开始时<em class="lk">损失</em>非常高。这就是模型最差但改进最快的地方。后来每一次迭代后改善越来越少。最后，由于<em class="lk">梯度</em>下降，损失<em class="lk">下降较慢。</em></p><p id="4175" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还记得一开始描述我们模型的线性函数吗？这个函数现在看起来如下:</p><figure class="mv mw mx my gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/7d65a822e46957b932468e5c0c405e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQuulCw77HyhRmtavH57-Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">代表我们模型的线性函数</figcaption></figure><p id="698b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它用于预测值。这是我们机器学习模型的核心。这里很酷的一点是，在训练数据的帮助下，机器从一开始就学会了为<code class="fe nk nl nm nn b">x</code>和<code class="fe nk nl nm nn b">y</code>设置适当的值。神奇！</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><p id="9b0e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！我希望这篇文章能帮助你更好地理解<em class="lk">梯度下降</em>。您可以在<a class="ae jd" href="https://gist.github.com/larswaechter/c2df0bc9b0f15c64220eb5699b25ddf1" rel="noopener ugc nofollow" target="_blank"> GitHub </a>找到完整的源代码，以及一些额外的函数来创建本文中的图表。</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><p id="5645" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章最初发表在我的博客上。看一看。</p><div class="ip iq gp gr ir oy"><a href="https://larswaechter.dev/blog/gradient-descent-introduction-python/" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jh gy z fp pd fr fs pe fu fw jf bi translated">Python中梯度下降的恰当介绍</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">2021年2月22日梯度下降是当今机器学习算法中的一个基本元素。我们使用渐变…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">拉斯瓦切特.德夫</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm ix oy"/></div></div></a></div></div></div>    
</body>
</html>