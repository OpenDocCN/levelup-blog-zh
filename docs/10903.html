<html>
<head>
<title>How to Efficiently Transform a CSV File and Upload it in Compressed Form to AWS S3 (Python, Boto3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何有效地转换CSV文件并以压缩形式上传到AWS S3 (Python，Boto3)</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/efficiently-transforming-compressing-in-memory-and-ingesting-csv-files-to-aws-s3-using-python-da7bcec5f8f?source=collection_archive---------1-----------------------#2022-01-24">https://levelup.gitconnected.com/efficiently-transforming-compressing-in-memory-and-ingesting-csv-files-to-aws-s3-using-python-da7bcec5f8f?source=collection_archive---------1-----------------------#2022-01-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/3313d097f4a68c4b9bc597630349cf27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KMvGFeeXz6upzPQR"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">凯文·Ku在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="fda7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您一直从事数据工程领域的工作，那么您很可能已经参与了CSV文件的处理。尽管它不是最有效的分析格式，但CSV格式仍然在当前的数据格局中占有相当大的份额。这是一种受到广泛支持的格式，通常在源系统(如数据库)以文件形式提供数据时会遇到，这些数据将被接收到数据湖(随后被接收到专门的存储区，如服务于特定用例的数据仓库)。如果您是一家AWS商店，并且需要处理CSV文件并将它们上传到S3，那么本文将介绍一种使用Python和AWS SDK (boto3)的方法。虽然人们可以用许多方法来解决这个问题，但有几个考虑因素值得强调，使它成为一个相当可行的解决方案:</p><ul class=""><li id="a26b" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">接收前压缩—网络通常是任何系统架构中的主要瓶颈。另一方面，CSV格式在压缩方面效率不高，因此解压缩时通常体积较大。因此，将庞大的CSV文件接收到AWS S3可能是一项成本相当高的操作。总是建议在接收数据之前进行压缩。现在，在选择正确的压缩格式方面，可以有多种考虑因素，具体取决于下游处理将如何完成，例如，如果您使用Spark等大数据工具，建议使用可拆分的压缩格式。为了简单起见，我使用gzip作为压缩格式。</li><li id="381a" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">在ETL中避免I/O——只要有可能，建议在ETL过程中避免磁盘I/O。磁盘I/O是相当昂贵/缓慢的操作，因此，如果您可以在内存中做任何事情，它最终会有助于您的方法的整体优化。</li></ul><p id="6145" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了这些基本的考虑，让我们看一个例子，看看如何转换CSV文件，压缩它(在运行中或在内存中)并通过Python的本地库和AWS SDK (boto3)将它上传到S3。</p><h2 id="7b07" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">要求:</h2><ul class=""><li id="2294" class="le lf it ki b kj ml kn mm kr mn kv mo kz mp ld lj lk ll lm bi translated">AWS帐户</li><li id="a07a" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">IAM用户</li><li id="f4fb" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">S3水桶</li><li id="5b83" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">计算机编程语言</li><li id="67ed" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">Boto3</li></ul><p id="9a6b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设您有一个简单的CSV文件，如下所示:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="d739" class="ls lt it mv b gy mz na l nb nc">sensor_code,parameter_name,start_timestamp,finish_timestamp,sensor_value</span><span id="cb8e" class="ls lt it mv b gy nd na l nb nc">DEX123,OCO_1,30/09/2021 5:00:00 PM,1/10/2021 1:00:00 AM,0.3008914</span><span id="d1fe" class="ls lt it mv b gy nd na l nb nc">DEX123,OCO_1,30/09/2021 6:00:00 PM,1/10/2021 2:00:00 AM,0.2821953</span><span id="043e" class="ls lt it mv b gy nd na l nb nc">DEX123,OCO_1,30/09/2021 7:00:00 PM,1/10/2021 3:00:00 AM,0.2513988</span><span id="236f" class="ls lt it mv b gy nd na l nb nc">DEX123,OCO_1,30/09/2021 8:00:00 PM,1/10/2021 4:00:00 AM,0.2153951</span><span id="fc1d" class="ls lt it mv b gy nd na l nb nc">DEX129,OCO_1,30/09/2021 9:00:00 PM,1/10/2021 5:00:00 AM,0.1723991</span><span id="040a" class="ls lt it mv b gy nd na l nb nc">DEX129,OCO_2,30/09/2021 10:00:00 PM,1/10/2021 6:00:00 AM,0.1423465</span><span id="bf27" class="ls lt it mv b gy nd na l nb nc">DEX129,OCO_2,30/09/2021 11:00:00 PM,1/10/2021 7:00:00 AM,0.1424952</span><span id="6d18" class="ls lt it mv b gy nd na l nb nc">DEX129,OCO_2,1/10/2021 12:00:00 AM,1/10/2021 8:00:00 AM,0.1455519</span><span id="725d" class="ls lt it mv b gy nd na l nb nc">DEX129,OCO_2,1/10/2021 1:00:00 AM,1/10/2021 9:00:00 AM,0.1682339</span></pre><p id="cbbd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，这可以被认为是传感器度量的时间序列数据。您可能已经注意到时间戳格式不是“标准”格式，即yyyy-mm-dd HH:MM:SS。如果您正在将数据加载到数据仓库中，它们在解析非标准时间格式时可能会有一些限制。例如，红移可以识别许多时间戳格式，但不是全部。上例中的格式是Redshift在向其加载数据时无法识别的(例如，通过来自S3的COPY命令)。因此，如果目标是将这些数据加载到Redshift中，那么就需要进行一些基本的转换，以便源文件中的时间戳格式是标准化的，从而可以在复制操作期间被Redshift识别。</p><p id="e623" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个示例代码，看看如何在Python中以最佳方式完成这项任务:</p><figure class="mq mr ms mt gt ju"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="0591" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以在上面的片段中发生了很多事情。为了更好地理解，我们来分解一下:</p><ul class=""><li id="faa1" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">第7行:我们通过<em class="ng"> boto3.client() </em>方法创建一个S3客户端。建议使用<em class="ng"> boto3。Session() </em>然后用它创建boto3.client(本文给出了一个<a class="ae kf" href="https://ben11kehoe.medium.com/boto3-sessions-and-why-you-should-use-them-9b094eb5ca8e" rel="noopener">很好的解释</a>)。为了简单起见，我只使用了<em class="ng"> boto3.client() </em></li><li id="4092" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第9行:我们使用内存中的字节缓冲区来存储字节，从而创建一个二进制流。简单地说，可以把它看作是将字节写入内存文件而不是磁盘上的实际文件的一种方式。这将用于存储数据的压缩(gzipped)对象将被上传到S3。</li><li id="143e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第10行到第11行:我们通过python的<em class="ng"> open() </em>函数打开我们的CSV源文件，使用基于“with”上下文管理器的方法来确保文件在结束时正确关闭，即使在异常的情况下。然后我们利用<em class="ng"> csv.reader() </em>并将打开的文件对象传递给它。我们还指定了CSV文件的分隔符。</li><li id="fb38" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第12行:因为我们的CSV文件包含头，所以我们使用python的next函数来获取第一个项目/行。(在这种情况下是header，它将是list类型)。</li><li id="5504" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第13到15行:这是我们进行实际处理的地方，即将时间戳格式解析为标准格式。我们遍历csv_rdr对象，每次迭代都会产生一行。我们使用<em class="ng">datetime . datetime . strptime(x[2]，" %d/%m/%Y %I:%M:%S %p") </em>按照时间戳格式(<em class="ng"> %d/%m/%Y %I:%M:%S %p </em>)解析第3列中的时间戳值(由于x[2])，将其转换为字符串，然后存储它。这为我们提供了标准yyyy-mm-dd HH:MM:SS格式的时间戳值。</li><li id="cfaa" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第16行:我们将转换后的行添加到<em class="ng"> transformed_rows </em>列表中。请注意，对于大文件的内存利用率来说，这不是一个优化的方法。为了有效处理大文件，请考虑使用生成器。</li><li id="1876" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第18行:我们创建一个新的列表<em class="ng"> transformed_rows_header </em>，它是两个列表的串联，即<em class="ng"> header </em>和<em class="ng"> transformed_rows。</em>因此，<em class="ng"> transformed_rows_header </em>是一个列表的列表。这个<em class="ng">transformed _ rows _ header</em>的第一个元素是header。每个后续元素都是经过解析的时间戳格式的转换行。在这一阶段，我们将数据转换为我们想要的状态，但是它存在于Python的对象(即列表)中。</li><li id="e6de" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第19行:类似于我们打开文件的方式，我们通过<em class="ng"> gzip初始化一个上下文管理器。GzipFile </em>来指定我们想要写一个gzip文件。我们将在第8行初始化的<em class="ng"> mem_file </em> BytesIO缓冲区指定为我们的目标，即gzip操作的输出将被写入的位置。我们将“wb”指定为模式，以指定我们要写入的字节。对于<em class="ng"> compresslevel </em>，我们使用6，这也是默认设置，可以很好地平衡速度和压缩比。因此，在一个坚果壳中，我们准备将gzip文件的输出写到哪里(在本例中，写到内存缓冲区)</li><li id="aac0" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第20到22行:我们需要将转换后的数据(当前在列表的<em class="ng">transformed _ rows _ header</em>列表中)刷新为CSV格式，然后可以压缩并上传为文件。为此，我们初始化一个名为<em class="ng"> buff </em>的内存中的字符串缓冲区(类似于内存中的字节IO缓冲区)，并使用“csv”模块将我们的列表的结果，即<em class="ng">transformed _ rows _ header</em>写入这个内存中的文件(buff)即字符串缓冲区。您可以认为它是将CSV数据写入文件，但在这种情况下，我们执行的是内存操作。由于前面讨论的原因，我们没有将数据写出到磁盘。因此，在这个阶段，我们的<em class="ng">“buff”</em>现在包含了纯CSV格式的转换数据。</li><li id="f28e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第23行:我们获取内存中转换的CSV文件的内容(<em class="ng"> buff </em>)，压缩它并写入内存字节缓冲区(<em class="ng"> mem_file </em>)。此外，还要注意字符编码(即UTF-8)。最后，我们在<em class="ng"> mem_file </em> bytes buffer中将CSV数据转换为gzip压缩格式的字节。</li><li id="aad2" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第24行:我们将流的位置放在内存缓冲区的开始。把它想象成一个指向文件开始的光标。这样做是为了当我们上传到S3时，整个文件从头开始读取。</li><li id="6e8f" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">第25行:我们使用<em class="ng"> s3.put_object() </em>方法将数据上传到指定的桶和前缀。在这种情况下，对于Body参数，我们指定保存压缩和转换后的CSV数据的<em class="ng"> mem_file </em>(内存字节缓冲区)</li></ul><p id="3f40" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有维奥拉。如果您已经处理了AWS方面的事情，例如，您有一个帐户、一个存储桶、一个有权限写入存储桶的IAM用户、一个配置的AWS CLI配置文件(或者一个角色，如果您已经在AWS中)，那么它应该读取文件、转换、压缩并将其上传到S3存储桶！</p><p id="cf21" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">差不多就是这样了。本文用基本的转换逻辑演示了一个非常简单的场景。您也可以使用相同的逻辑，同时使用Pandas之类的库进行稍微高级一些的转换。编码快乐！</p></div></div>    
</body>
</html>