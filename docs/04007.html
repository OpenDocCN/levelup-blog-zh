<html>
<head>
<title>Vanishing and Exploding Gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消失和爆炸渐变</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/vanishing-and-exploding-gradients-ae7fb88f3b66?source=collection_archive---------7-----------------------#2020-06-05">https://levelup.gitconnected.com/vanishing-and-exploding-gradients-ae7fb88f3b66?source=collection_archive---------7-----------------------#2020-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fd71" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">带乙状结肠激活(功能)</h2></div><div class="ki kj kk kl gt ab cb"><figure class="km kn ko kp kq kr ks paragraph-image"><img src="../Images/8361e291fbe2eecc6e99e608ca86e621.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*vXrh0y4n4MdxYhVHdVT7qA.jpeg"/></figure><figure class="km kn kv kp kq kr ks paragraph-image"><img src="../Images/fd8b752f02a628d388ae5e79e54453f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*8JKnKVEBtVzrD-iIQrn9Yw.png"/><figcaption class="kw kx gj gh gi ky kz bd b be z dk la di lb lc translated">图— 1: <a class="ae ld" href="https://www.pixtastock.com/illustration/14535028" rel="noopener ugc nofollow" target="_blank">爆炸</a>和<a class="ae ld" href="https://harrypotter.fandom.com/wiki/Vanishing_Spell" rel="noopener ugc nofollow" target="_blank">消失</a>图像</figcaption></figure></div><p id="655f" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">在这篇博客中，我将解释为什么sigmoid激活会有消失和爆炸梯度问题。消失和爆炸梯度是神经网络面临的最大问题之一。消失梯度导致收敛缓慢，而爆炸梯度导致权重变化太大，而这种太大的变化通常是不希望的。由于消失梯度和爆炸梯度，神经网络不收敛。</p><p id="0bae" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">这个博客是关于乙状结肠激活中的消失和爆炸梯度。所以，我们先讨论一下什么是sigmoid函数。</p><h1 id="7323" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">Sigmoid函数</h1><p id="4bf4" class="pw-post-body-paragraph le lf it lg b lh ms ju lj lk mt jx lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">这是一个数学函数，它有一条<strong class="lg iu">-</strong>形状的曲线(或称sigmoid曲线)。该功能可以定义为:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/bb9f2928f121e631def6d656f94e8867.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*VdGXvd-cJJGAvntpAVQ36Q.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图2: Sigmoid函数</figcaption></figure><p id="c9fa" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">这个函数的导数将是—</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/a66327e985e06e5efc159e24de419447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*H6OUJtxEwq88-V5tpT8DaQ.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图Sigmoid函数的导数</figcaption></figure><p id="9445" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">这就是这个函数的美妙之处，因为它的导数是用it来表示的。这个导数关于y轴对称，导数的最大值是<strong class="lg iu"> 0.25。</strong>所以乙状结肠的导数在于[0，0.25]。对于导数最大值<strong class="lg iu"> <em class="nd"> x=0。</em> </strong>我们来看看乙状结肠和它的导数的图。</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/42e6b9206e0f6c7006fc9b1170301123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*54WxyNRIoO6OX8Jwa-caeQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图4</figcaption></figure><h1 id="a68d" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">消失梯度</h1><p id="e36c" class="pw-post-body-paragraph le lf it lg b lh ms ju lj lk mt jx lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">在消失渐变中，渐变变得非常小(接近<strong class="lg iu"> <em class="nd"> 0 </em> </strong>)，这导致权重的变化非常小(或者几乎没有变化)。权重无变化(即<strong class="lg iu"> <em class="nd"> gradient=0 </em> </strong>)是终止的条件，但这不是收敛的解决方案。让我们看看这个消失的梯度是如何发生的，为什么会发生。</p><blockquote class="nf ng nh"><p id="e7c4" class="le lf nd lg b lh li ju lj lk ll jx lm ni lo lp lq nj ls lt lu nk lw lx ly lz im bi translated">让我们看看数据是如何在神经元中流动的—</p></blockquote><p id="eb75" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg iu"> <em class="nd"> x </em> </strong>:是对神经元的输入，<strong class="lg iu"> <em class="nd"> O: </em> </strong>是神经元的输出。类似地，我们可以使用这个概念定义许多层的架构。下图显示了神经网络架构。</p><div class="ki kj kk kl gt ab cb"><figure class="km kn nl kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><img src="../Images/54e95f240d30fae76a57489f07342ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*PGCwQ3g-fUqqgU12-7MftA.jpeg"/></div></figure><figure class="km kn nm kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><img src="../Images/3d384ab534d66cb3be23016c064d3aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1508/format:webp/1*c5Q_ETF5uIs5KFS3zb38tQ.jpeg"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk nn di no lc translated">图— 5</figcaption></figure></div><p id="82b4" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">让我们计算第一层(输入层，即w1和b1)的梯度。</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/063acba5ab02c531b669e1f9c15b9d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*2APd4FyVmXyniz4GaK8OTw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图6</figcaption></figure><p id="d698" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">sigmoid的导数的最大值是0.25，最小值是0。最大值小于1。较小的数多次相乘得到的数非常小(接近<strong class="lg iu"> <em class="nd"> 0 </em> </strong>)。这就是在具有sigmoid激活的神经网络中消失梯度是如何发生的。如果NN很深，所有激活都是sigmoid，那么消失梯度的几率非常高。</p><p id="c013" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg iu">注:</strong>权重小(不要太小)。</p><h1 id="4a6e" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">爆炸梯度</h1><p id="aaa3" class="pw-post-body-paragraph le lf it lg b lh ms ju lj lk mt jx lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">在爆炸梯度中，梯度值变得非常大，这导致更新的权重变化太大(不希望的)。在上面的例子中，我们看到渐变消失是因为sigmoid的最大导数值小于1。</p><blockquote class="nq"><p id="ccab" class="nr ns it bd nt nu nv nw nx ny nz lz dk translated">那么问题来了，sigmoid激活如何导致爆炸梯度？</p></blockquote><p id="82ba" class="pw-post-body-paragraph le lf it lg b lh oa ju lj lk ob jx lm ln oc lp lq lr od lt lu lv oe lx ly lz im bi translated">在图-6中，如果我们可以证明一个大于1的项，那么，以同样的方式，我们可以证明每个大于1的项，我们可以说，sigmoid激活可以导致爆炸梯度。</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi of"><img src="../Images/8be972f89592f4a6365f1bc7442137b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*afb_Aci-nrKbc69b5EJnxA.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图— 7</figcaption></figure><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/338c520e62c4c0e01d2325d05b84ba42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*ajfGCGj4bIo3BjQiGuLJ0w.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图— 8</figcaption></figure><p id="e563" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">以上偏导值如果<strong class="lg iu"> <em class="nd"> w_2 </em> </strong>大可以大于1(设<strong class="lg iu"> <em class="nd"> w_2=100，</em> </strong>那么这个值就变成<strong class="lg iu"> <em class="nd"> 25) </em> </strong>。同样，如果所有权重都是大值，则其他值可以大于1 <strong class="lg iu"> <em class="nd">。</em>T15】</strong></p><p id="399b" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">如果初始化的权重非常大，则sigmoid激活可以具有爆炸梯度。所以在初始化权重时要小心。</p></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><div class="ki kj kk kl gt oo"><a href="https://skilled.dev" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">编写面试问题</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一个完整的平台，在这里我会教你找到下一份工作所需的一切，以及…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">技术开发</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc kt oo"/></div></div></a></div></div></div>    
</body>
</html>