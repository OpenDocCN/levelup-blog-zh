<html>
<head>
<title>Getting useful data from the internet: A quickstart guide to Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从互联网上获取有用的数据:Scrapy快速入门指南</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/getting-useful-data-from-the-internet-a-quickstart-guide-to-scrapy-4a284a61501a?source=collection_archive---------17-----------------------#2022-08-02">https://levelup.gitconnected.com/getting-useful-data-from-the-internet-a-quickstart-guide-to-scrapy-4a284a61501a?source=collection_archive---------17-----------------------#2022-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eb6a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">收集亚马逊产品数据的用例</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ccdc0d13a8e35e8820719dbbde10ebd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fZVNEZ-gHq2epg86"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@umeshsonii?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乌梅什·索尼</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="9d60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大多数有用的数据并没有以一种特别便于计算机或人类使用的方式收集。而是以各种无法使用的形式生活在互联网上。</p><p id="99df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的演示中，我们将从Amazon收集产品数据。请注意，这不是一个特别强大的解决方案，因为我没有实现任何类似代理轮换的东西(如果您试图抓取像Amazon这样的大型网站，该网站对机器人和爬虫特别有抵抗力，则需要代理轮换)，我也没有介绍如何大规模运行您的蜘蛛(例如在托管的云服务器上)。</p><p id="5fb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">至此，您应该已经了解了HTML和CSS选择器，以及Python <code class="fe ls lt lu lv b">requests</code>模块的基础知识。</p><h1 id="7534" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">设置Scrapy</h1><p id="c61e" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated"><a class="ae kv" href="https://scrapy.org/download/" rel="noopener ugc nofollow" target="_blank">使用你选择的包管理器下载Scrapy </a>(我用的是conda，但是你也可以用<code class="fe ls lt lu lv b">pip</code>轻松下载)。然后，<code class="fe ls lt lu lv b">cd</code>进入一个名为<code class="fe ls lt lu lv b">scrapy</code>的文件夹，然后运行<code class="fe ls lt lu lv b">scrapy startproject amazon</code>。将创建一个名为<strong class="ky ir"> amazon </strong>的文件夹。然后<code class="fe ls lt lu lv b">cd amazon</code>。</p><p id="ea8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要创建你的第一个蜘蛛，运行<code class="fe ls lt lu lv b">scrapy genspider amazon amazon.com</code>。</p><h2 id="9fad" class="mt lx iq bd ly mu mv dn mc mw mx dp mg lf my mz mi lj na nb mk ln nc nd mm ne bi translated">文件系统概述</h2><p id="8436" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated">在您的<code class="fe ls lt lu lv b">amazon</code>文件夹中，您会看到一个<em class="nf"> scrapy.cfg </em>文件以及另一个名为<code class="fe ls lt lu lv b">amazon</code>的文件夹，其中包含更多文件:</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="06fa" class="mt lx iq lv b gy nk nl l nm nn">amazon/<br/>├─ spiders/<br/>│  ├─ __init__.py<br/>│  ├─ amazon.py<br/>├─ __init__.py<br/>├─ pipelines.py<br/>├─ settings.py<br/>├─ middlewares.py<br/>├─ items.py<br/>scrapy.cfg</span></pre><p id="0c41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们来看一下<em class="nf"> settings.py </em>文件，里面有<code class="fe ls lt lu lv b">BOT_NAME</code>等基本信息。对于我们的项目，确保您启用了以下设置:</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="b94b" class="mt lx iq lv b gy nk nl l nm nn">DEPTH_LIMIT = 3<br/>DEPTH_PRIORITY = 1<br/>SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'<br/>SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'<br/>ROBOTSTXT_OBEY = True<br/>CONCURRENT_REQUESTS = 1<br/>DOWNLOAD_DELAY = 0.05<br/>DOWNLOAD_TIMEOUT = 15</span></pre><p id="00bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中大部分是不言自明的，但是文件顶部的注释也应该引导您到解释它们是什么的文档链接。调度程序只是描述Scrapy应该如何访问提供给它的链接。在我们的例子中，我们需要一个先进先出(FIFO)链路优先级队列。</p><p id="bac9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还限制了并发请求的数量，这样Amazon就不会阻止我们，特别是因为我们还没有使用旋转代理。Scrapy还可以自动为我们读取网站的<code class="fe ls lt lu lv b">robots.txt</code>文件，大多数网站都必须制定规则，规定哪些机器人应该能够访问网站的哪些部分。当然，一个网站没有办法在没有机器人检测和阻止的情况下执行这些规则，这并不总是万无一失的，但不管怎样，遵守规则总是好的。</p><h1 id="a007" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">创造一只蜘蛛</h1><p id="8506" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated">在<em class="nf"> items.py </em>你可以把你喜欢的任何字段放在这里。</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="c3ef" class="mt lx iq lv b gy nk nl l nm nn">class AmazonItem(scrapy.Item):<br/>    # define the fields for your item here like:<br/>    name = scrapy.Field()<br/>    price = scrapy.Field()<br/>    rating = scrapy.Field()<br/>    num_reviews = scrapy.Field()<br/>    model_num = scrapy.Field()<br/>    department = scrapy.Field()<br/>    manufacturer = scrapy.Field()<br/>    sizes = scrapy.Field()<br/>    colors = scrapy.Field()<br/>    asin = scrapy.Field()<br/>    image_url = scrapy.Field()</span></pre><p id="373c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的<em class="nf"> pipelines.py </em>文件应该是非常基本的，我们不需要从这里做任何修改。它定义了一个管道类，使我们能够在收集数据后对项目进行转换。如果我们想清理数据，可以在这里进行。让我们暂时让它保持原样(只是返回传递给它的项)。</p><p id="b77d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们编辑<strong class="ky ir">蜘蛛<em class="nf"> </em> </strong>目录中名为<em class="nf"> amazon.py </em>的文件。它定义了一个类(在我的例子中，我称它为<code class="fe ls lt lu lv b">AmazonProductSpider</code>，但你的可能不同)。在<code class="fe ls lt lu lv b">name</code>属性中，真正重要的是你如何称呼你的蜘蛛。这对于每只蜘蛛来说一定是不同的。我们还想指定另外两个属性:</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="6e7b" class="mt lx iq lv b gy nk nl l nm nn">name = 'AmazonProductSpider'<br/>allowed_domains = ['amazon.com']<br/>start_urls = ['http://amazon.com/']</span></pre><p id="cd9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个蜘蛛都是从调用<code class="fe ls lt lu lv b">start_requests()</code>方法开始的。默认情况下，这个方法将调用<code class="fe ls lt lu lv b">parse()</code>方法，但是我们想稍微改变一下设置。首先，我们将定义我们的<code class="fe ls lt lu lv b">start_requests()</code>方法。记住<code class="fe ls lt lu lv b">from urllib.parse import urlencode, urljoin</code>。</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="1754" class="mt lx iq lv b gy nk nl l nm nn">def start_requests(self):<br/>    queries = ["dress"]<br/>    for query in queries:<br/>        yield scrapy.Request(<br/>            'https://www.amazon.com/s?' + urlencode({'k': query}),<br/>            callback=self.parse_keyword_response<br/>        )</span></pre><p id="0672" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个蜘蛛方法应该<code class="fe ls lt lu lv b">yield</code>一个响应或项目，而不是<code class="fe ls lt lu lv b">return</code>。这是因为Scrapy是异步的，所以<code class="fe ls lt lu lv b">yield</code>关键字将返回一个指向函数的生成器，而不是数据本身，Scrapy可以在调度作业时调用函数。在这个方法中，我们告诉Scrapy通过产生一个带有URL和回调函数(如何处理响应)的<code class="fe ls lt lu lv b">scrapy.Request</code>对象来访问每个产品查询的搜索URL。</p><p id="7248" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们实际上可以定义这个回调函数了。创建一个名为<code class="fe ls lt lu lv b">parse_keyword_response()</code>的新方法。</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="f888" class="mt lx iq lv b gy nk nl l nm nn">def parse_keyword_response(self, response):<br/>    products = response.xpath("//*[@data-asin]")<br/>    for product in products:<br/>        asin = product.xpath("@data-asin").get()<br/>        yield scrapy.Request(url='https://www.amazon.com/dp/' +<br/>            asin, callback=self.parse_product_response, meta={'asin': asin}<br/>        )</span><span id="f62a" class="mt lx iq lv b gy no nl l nm nn">    next_page = response.xpath("//*[@class='s-pagination-strip']/a[contains(concat(' ',normalize-space(@class),' '),' s-pagination-next ')]/@href").get()<br/>    if next_page:<br/>        yield scrapy.Request(url=urljoin("http://amazon.com/", next_page), callback=self.parse_keyword_response)</span></pre><p id="58ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Scrapy允许我们方便地使用xpath选择器从响应文本中选择项目和数据。</p><h2 id="7fa2" class="mt lx iq bd ly mu mv dn mc mw mx dp mg lf my mz mi lj na nb mk ln nc nd mm ne bi translated">xpath选择器简介</h2><p id="12df" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated">Xpath选择器是可以在HTML文档主体上执行某种搜索的字符串。例如，要选择任何id为“cat”的HTML元素，我们可以在Scrapy中使用<code class="fe ls lt lu lv b">response.xpath("//*[@id='cat']/text()").get()</code>。<code class="fe ls lt lu lv b">//*</code>表示搜索<code class="fe ls lt lu lv b">:root</code>类的任何子元素(不仅仅是直接子元素)。(我们也可以使用类似于<code class="fe ls lt lu lv b">//div</code>或<code class="fe ls lt lu lv b">//h1</code>的东西来分别表示div或h1元素。)在括号中，我们可以标注要搜索的元素属性。<code class="fe ls lt lu lv b">/text()</code>选择器直接在前一个元素中获取文本<em class="nf">。(使用<code class="fe ls lt lu lv b">/</code>和<code class="fe ls lt lu lv b">//</code>选择器分别指示直接子女或子女。)</em></p><p id="4caa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b">.get()</code>方法告诉Scrapy只获取匹配我们搜索的第一个元素，但是我们也可以使用<code class="fe ls lt lu lv b">.getall()</code>来获取匹配我们搜索的所有元素。</p><p id="9d02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也许我们想在<code class="fe ls lt lu lv b">data-asin</code>属性中获得属性值本身，就像我们上面做的那样。我们将获得所有产品选择器的列表，然后对每个选择器调用<code class="fe ls lt lu lv b">asin = product.xpath("@data-asin").get()</code>。</p><p id="a202" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Chrome和Firefox开发控制台支持测试xpath选择器的方法。您可以运行<code class="fe ls lt lu lv b">$x("&lt;path here&gt;")</code>来获取搜索返回的元素数组。</p><p id="f432" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://devhints.io/xpath#class-check" rel="noopener ugc nofollow" target="_blank">这个xpaths备忘单</a>非常有用。如果想按类搜索，请注意构造选择器的独特方式，因为xpath选择器没有内置的方式来检查列表中是否存在类，而是直接搜索类字符串。</p><p id="50ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到我们的<code class="fe ls lt lu lv b">parse_keyword_response()</code>函数。对于在该页面上找到的每个产品，我们会根据产品的ASIN号向该产品页面发送一个请求项。有许多方法可以做到这一点(例如，您可以简单地让Scrapy跟踪href属性中的链接)，但是这种方法允许我们在进行过程中收集每个产品的ASIN。</p><p id="be23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来我们可以定义一个名为<code class="fe ls lt lu lv b">parse_product_response()</code>的方法，该方法定义当Scrapy到达产品页面时该做什么。</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="cfad" class="mt lx iq lv b gy nk nl l nm nn">def parse_product_response(self, response):<br/>    asin = response.meta['asin']<br/>    name = response.xpath("//*[@id='productTitle']/text()").get()<br/>    price = response.xpath("//*[contains(concat(' ',normalize-space(@class),' '),' a-price ')]//text()").get()<br/>    rating = response.xpath("//*[@id='acrPopover']/@title").get()<br/>    num_reviews = response.xpath("//[@id='acrCustomerReviewText']/text().get()</span><span id="d366" class="mt lx iq lv b gy no nl l nm nn">    details = response.xpath("//*[@id='detailBulletsWrapper_feature_div']//li//text()").getall()<br/>    if details:<br/>        model_num = details[8]<br/>        department = details[13]<br/>        manufacturer = details[23]<br/>    else:<br/>        model_num = None<br/>        department = None<br/>        manufacturer = None<br/><br/>    colors = response.xpath("//*[@id='variation_color_name']/ul/li/@title").getall()<br/>    if colors:<br/>        colors = [color[16:] for color in colors]<br/><br/>    sizes = response.xpath("//*[@name='dropdown_selected_size_name']/option/@data-a-html-content").getall()<br/>    image_url = response.xpath("//*[@class='imgTagWrapper']/img/@src").get()</span><span id="bfec" class="mt lx iq lv b gy no nl l nm nn">    yield {<br/>        'asin': asin,<br/>        'name': name,<br/>        'price': price,<br/>        'rating': rating,<br/>        'num_reviews': num_reviews,<br/>        'model_num': model_num,<br/>        'department': department,<br/>        'manufacturer': manufacturer,<br/>        'sizes': sizes,<br/>        'colors': colors,<br/>        'image_url': image_url<br/>    }</span></pre><p id="c4fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个方法定义了从页面中获取数据所需的xpath选择器。Amazon产品页面确实很笨重(比您可能想要解析的大多数网站都笨重)，而且经常更改，因此您可能需要在开发控制台中进行大量测试，以获得您想要的结果。我建议在多个页面上进行测试，并避免高度特定的选择器(例如，在许多产品页面上访问价格的选择器是高度可变的，所以我简单地制作了一个选择器，它获取它在价格块中找到的第<em class="nf">个</em>文本段，包括范围内的文本段——这可能并不总是有效，但对我的用例来说是可行的)。</p><p id="b196" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能也注意到了，这次我们返回了一个字典，而不是一个<code class="fe ls lt lu lv b">scrapy.Request</code>。如果我们生成一个字典而不是一个请求，Scrapy将自动创建一个<code class="fe ls lt lu lv b">AmazonItem</code>实例，用我们在<em class="nf"> items.py </em> <strong class="ky ir"> </strong>文件中定义的正确字段填充。</p><p id="8485" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！现在我们可以在我们的终端运行<code class="fe ls lt lu lv b">scrapy crawl AmazonProductSpider -o data.csv</code>来运行蜘蛛。注意，您应该将命令中的<code class="fe ls lt lu lv b">AmazonProductSpider</code>替换为您在蜘蛛中设置的<code class="fe ls lt lu lv b">name</code>属性。</p><p id="5bcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">把蜘蛛停在中间的某个地方。您的输出应该如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/390f17ecd2e154e289333337816ef971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qL9wtNJwZjC26i_PKdQtzw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">输出部分I .作者提供的图像</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/60a7597e3622c03dc80922d232d57509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6sOlUq0FAJd0D-5YZiUT3w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">产出第二部分。作者图片</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/ef3eb4ecb18490a83d451fb8133b0406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EjNrlO9T-oQvQpVNT-bu4Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">产出第三部分。作者图片</figcaption></figure><p id="ec4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以看出<code class="fe ls lt lu lv b">model_num</code>、<code class="fe ls lt lu lv b">department</code>和<code class="fe ls lt lu lv b">manufacturer</code>列给出的结果不一致。这是因为我使用了超级特定的选择器，在未来的迭代中，我可能会希望调整这些选择器，以更好地适应其他产品页面。我们可能还想运行<em class="nf"> pipelines.py </em>文件中的一些脚本或其他一些数据管道脚本，以将<code class="fe ls lt lu lv b">num_reviews</code>和<code class="fe ls lt lu lv b">rating</code>转换成浮点数或整数，并将<code class="fe ls lt lu lv b">sizes</code>列转换成一定范围或数量的大小，以便我们的数据更加可用。</p><h1 id="0464" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">扩展Scrapy</h1><p id="0a54" class="pw-post-body-paragraph kw kx iq ky b kz mo jr lb lc mp ju le lf mq lh li lj mr ll lm ln ms lp lq lr ij bi translated">我们还没有实现代理轮换，这很重要，因为如果你一次打太多电话，亚马逊会开始阻止你的IP地址。你可以从像GimmeProxy或者T21免费代理列表这样的网站上找到免费代理，然后设置一个脚本来测试这些代理</p><pre class="kg kh ki kj gt ng lv nh ni aw nj bi"><span id="15d9" class="mt lx iq lv b gy nk nl l nm nn">import requests</span><span id="fac7" class="mt lx iq lv b gy no nl l nm nn">for i in range(10):<br/>    p = requests.get("https://gimmeproxy.com/api/getProxy").json().get('ipPort')<br/>    try:<br/>        if requests.get("https://httpbin.org/ip", proxies={'http': p, 'https': p}).status_code == 200:<br/>            print(p)<br/>            with open('proxies.txt', 'a') as f:<br/>                f.write(p + '\n')<br/>    except requests.exceptions.ConnectionError as e:<br/>        print("Connection error with proxy:", p)<br/>        continue</span></pre><p id="c7fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">调用GimmeProxy API，并通过向<a class="ae kv" href="https://httpbin.org/" rel="noopener ugc nofollow" target="_blank"> HTTPBin </a>发出请求来测试返回的随机IP地址是否可以实际用作代理(Python <code class="fe ls lt lu lv b">requests</code>模块有一个<code class="fe ls lt lu lv b">proxies</code>参数，可以更改该参数以包含IP代理)。</p><p id="777a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了在Scrapy中实现，我们需要在<em class="nf">middleware . py</em>文件中建立一个定制的中间件，或者使用一个专门用于IP轮换的第三方中间件，比如<code class="fe ls lt lu lv b"><a class="ae kv" href="https://github.com/TeamHG-Memex/scrapy-rotating-proxies" rel="noopener ugc nofollow" target="_blank">scrapy-rotating-proxies</a></code>。还有许多(付费的)API提供可靠的代理轮换。</p><p id="4f5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Scrapy的其他好处我们还没有谈到，那就是动态的自动调节设置、更多的并发请求等等，所以我鼓励你在你的用例中利用这些功能。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="199a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Neha Desaraju是德克萨斯大学奥斯汀分校学习计算机科学的学生。你可以在网上的<a class="ae kv" href="https://estaudere.github.io/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">estau dere . github . io</strong></a><strong class="ky ir">找到她。</strong></p></div></div>    
</body>
</html>