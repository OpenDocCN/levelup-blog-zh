<html>
<head>
<title>Use Computer Vision for Product Detection on a Grocery Shelf</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用计算机视觉检测食品货架上的产品</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/product-detection-from-grocery-shelf-9db031e0ddc1?source=collection_archive---------1-----------------------#2021-04-22">https://levelup.gitconnected.com/product-detection-from-grocery-shelf-9db031e0ddc1?source=collection_archive---------1-----------------------#2021-04-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a8298f1c8389c77d873b58ae3a44b461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhbOmuyucLDD1qFmWjVp9g.jpeg"/></div></div></figure><h1 id="6663" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">索引</h1><ol class=""><li id="11c8" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">介绍</li><li id="04a2" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">商业问题</li><li id="e3dc" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">映射到机器学习问题</li><li id="bfba" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">数据概述</li><li id="79b3" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">电子设计自动化(Electronic Design Automation)</li><li id="346f" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">建模</li><li id="80dd" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">模型比较</li><li id="37d7" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">最终管道和部署</li><li id="5f58" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">未来的工作</li><li id="801f" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">参考</li><li id="8525" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">轮廓</li></ol><h1 id="1cca" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> 1。)简介</strong></h1><p id="6674" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">问题围绕着检测放在杂货店中的产品，我们正在构建一个模型，它可以检测产品，并以二维(x，y)坐标的形式在图像周围给我们一个矩形边界框</p><h1 id="01e8" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> 2。)业务问题</strong></h1><p id="b453" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">由于这种检测问题以后可能会扩展到许多问题，如产品盗窃警报和产品计数问题，因此为该问题提出解决方案变得非常重要</p><h1 id="7752" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">3.)映射到机器学习问题</h1><p id="983a" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">在解决这个问题后，我们可以观察到，我们可以通过应用计算机视觉对象检测技术来解决这个问题，通过该技术我们可以获得图像中存在的产品的坐标，因此我们将使用最先进的对象检测技术，这些技术构建在卷积神经网络和深度神经网络层之上，以构建一个鲁棒的高性能模型。</p><h1 id="66ed" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">4.)数据集概述:</h1><p id="d5cf" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">数据由杂货店货架的图像和文本文件形式的边界框注释组成。</p><ul class=""><li id="8cbc" class="kw kx iq ky b kz mi lb mj ld mk lf ml lh mm lj mn ll lm ln bi translated">使用来自https://github.com/gulvarol/grocerydataset#shelfimages<a class="ae mo" href="https://github.com/gulvarol/grocerydataset#shelfimages" rel="noopener ugc nofollow" target="_blank">的数据</a></li><li id="d32c" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">数据表单:JPG图像文件</li><li id="6740" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">特征-杂货店货架的图像</li><li id="e100" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">地面真实:图像的注释</li></ul><p id="74d9" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">以下是图像属性和命名:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="f687" class="nb jz iq mx b gy nc nd l ne nf">"C&lt;c&gt;_P&lt;p&gt;_N&lt;n&gt;_S&lt;s&gt;_&lt;i&gt;.JPG"<br/>        where<br/>            &lt;c&gt; := camera id (1: iPhone5S, 2: iPhone4, 3: Sony Cybershot, 4: Nikon Coolpix)<br/>            &lt;p&gt; := planogram id<br/>            &lt;n&gt; := the rank of the top shelf on the image according to the planogram<br/>            &lt;s&gt; := number of shelves on the image<br/>            &lt;i&gt; := copy number</span></pre><h1 id="bcbd" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">5.)探索性数据分析</h1><p id="125d" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">我们将进行一些探索性的数据分析，以获得一些见解，并更好地理解数据</p><p id="61af" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated"><strong class="ky ir">可用图像数量</strong></p><p id="b25b" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">获取数据集中可用的数据点数</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a495" class="nb jz iq mx b gy nc nd l ne nf">Number of Data Points: 354</span></pre><p id="4512" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated"><strong class="ky ir">从数据中打印随机图像</strong></p><p id="33b4" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">让我们从数据集中打印一张随机图像</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/eb361b4aeb03cc6bdf0f3b0656be054f.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*-5h1Ynd0u7udvM-JICNORw.png"/></div></div></figure><p id="cf86" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated"><strong class="ky ir">在随机图像上绘制包围盒</strong></p><p id="e227" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">所以让我们用我们的注释来想象一个带注释的图像是什么样子</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/8c0fa750c24647c2ef5f0f110e7fb514.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*hET7d8pQ5xif5Eu0i0ej4g.png"/></div></div></figure><h2 id="b2ec" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">图像中的平均架数</h2><p id="8209" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">由于这些图像来自杂货店，并且每个图像都包含货架上的产品，因此每个图像都可以有多个货架，所以让我们来看看数据集拥有的平均货架数量</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="e998" class="nb jz iq mx b gy nc nd l ne nf">Average Number of Shelves in Image 3.2720848056537104</span></pre><h2 id="47b8" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">货架数量直方图</h2><p id="98a6" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">在上面的代码片段中，我们看到一个图像中的平均货架数约为3，但是为了更好地可视化，我们将绘制货架数的直方图</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7abbfbb3f87fdac74c9bf94c38c3f33e.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*1jjWuazWZEu0wtRWBGysZw.png"/></div></figure><h2 id="d2e1" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">图像中产品的平均数量</h2><p id="1712" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">让我们看看一个图像可以包含的平均产品数量是多少</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="3227" class="nb jz iq mx b gy nc nd l ne nf">Average Number of Products in Image 37.24293785310734</span></pre><h2 id="4fab" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated"><strong class="ak">产品数量直方图</strong></h2><p id="b687" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">为了更直观，我们将再次绘制几种产品的直方图</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/814697d2237119a5c4fca087b7fdf116.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*YNsxk6Tek1fS5punLvaoIQ.png"/></div></figure><h2 id="50fb" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">使用每台相机拍摄的图像数量</h2><p id="e962" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">由于图像是使用不同类型的相机拍摄的，所以让我们绘制一个条形图，告诉我们从每个相机点击的图像数量</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ede7f816f573cd6dec81817a397182ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*YLSCdNbyx1pXl_qsHa1egA.png"/></div></figure><h2 id="158b" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">观察结果:</h2><ul class=""><li id="1147" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj mn ll lm ln bi translated">货架平均数量约为3个</li><li id="0068" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">不到90%的映像具有少于或等于4个托架</li><li id="8d73" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">图片中产品的平均数量37 . 4836383836363637</li><li id="78fe" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">不到99%的图片中的产品少于80种</li><li id="c7b1" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">iPhone 5是所有设备中图像捕捉数量最多的</li></ul><blockquote class="nx ny nz"><p id="851b" class="lt lu oa ky b kz mi lv lw lb mj lx ly ob mp ma mb oc mq md me od mr mg mh lj ij bi translated">因此，我们将从这里开始构建我们的模型，我们将尝试不同的模型，并选择更适合数据的模型。</p></blockquote><h1 id="171d" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">准备输入和模型</h1><p id="e250" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated"><strong class="ky ir">克隆Github模型</strong></p><p id="1e9b" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">在下面的命令片段中，我们将把工作目录更改到我们的工作区文件夹中，在我们的例子中是“tf2”。然后我们将克隆张量流模型。然后，我们可以将工作目录更改为object_detection，它位于“models/research/”中</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="3cb0" class="nb jz iq mx b gy nc nd l ne nf">%cd /content/drive/MyDrive/tf2/<br/>!git clone <a class="ae mo" href="https://github.com/tensorflow/models/" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/models/</a><br/>%cd /content/drive/My Drive/tf2/models/research/object_detection</span></pre><h2 id="71a9" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">创建输入管道</h2><ul class=""><li id="6999" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj mn ll lm ln bi translated">首先，我们将创建一个字典，其中键是图像，值是注释</li><li id="b9a2" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">创建包含以下列的train_labels.csv和test _ labels.csv】文件名'，'宽度'，'类别'，'高度'，' xmin '，' xmax '，' ymin '，' ymax '</li></ul><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h2 id="f8df" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">从图像和注释创建张量流记录文件</h2><ul class=""><li id="4bdb" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj mn ll lm ln bi translated">我们将创造一列火车。记录并测试。记录在下面</li></ul><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h2 id="1c82" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">创建标签地图</h2><p id="1220" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">在下面的代码片段中，我们只是将包含标签信息的文件labelmap.pbtxt写入training文件夹</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="262e" class="nb jz iq mx b gy nc nd l ne nf">%%writefile models/research/object_detection/training/labelmap.pbtxt<br/>item {<br/> id: 1<br/> name: ‘Product’<br/>}</span></pre><h1 id="94c9" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">6.)建模</h1><h1 id="4511" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">高效Det建模</h1><h2 id="f8d0" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">从Tensorflow Zoo下载efficient_det模型</h2><p id="7e13" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">TensorFlow zoo中有许多可用的预训练模型，我们将挑选一些模型，并在我们的数据集上对它们进行微调</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ba9a" class="nb jz iq mx b gy nc nd l ne nf">%cd /content/drive/MyDrive/tf2/model/research/object_detection/</span><span id="0630" class="nb jz iq mx b gy oe nd l ne nf">!wget <a class="ae mo" href="http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz" rel="noopener ugc nofollow" target="_blank">http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz</a></span><span id="7844" class="nb jz iq mx b gy oe nd l ne nf">!tar -xvzf efficientdet_d0_coco17_tpu-32.tar.gz</span><span id="614e" class="nb jz iq mx b gy oe nd l ne nf">!rm -rf efficientdet_d0_coco17_tpu-32.tar.gz</span></pre><h2 id="5855" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">将配置文件保存到Training/effdet.config</h2><p id="b6df" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">在此步骤中，我们将在training文件夹中保存高效det模型的配置文件，下面是创建和保存配置文件的命令</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="4b54" class="nb jz iq mx b gy nc nd l ne nf">%%writefile training/effdet.config<br/>model {<br/> ssd {<br/> num_classes: 1<br/> image_resizer {<br/> keep_aspect_ratio_resizer {<br/> min_dimension: 512<br/> max_dimension: 512<br/> pad_to_max_dimension: true<br/> }<br/> }<br/> feature_extractor {<br/> type: “ssd_efficientnet-b0_bifpn_keras”<br/> conv_hyperparams {<br/> regularizer {<br/> l2_regularizer {<br/> weight: 3.9999998989515007e-05<br/> }<br/> }<br/> initializer {<br/> truncated_normal_initializer {<br/> mean: 0.0<br/> stddev: 0.029999999329447746<br/> }<br/> }<br/> activation: SWISH<br/> batch_norm {<br/> decay: 0.9900000095367432<br/> scale: true<br/> epsilon: 0.0010000000474974513<br/> }<br/> force_use_bias: true<br/> }<br/> bifpn {<br/> min_level: 3<br/> max_level: 7<br/> num_iterations: 3<br/> num_filters: 64<br/> }<br/> }<br/> box_coder {<br/> faster_rcnn_box_coder {<br/> y_scale: 1.0<br/> x_scale: 1.0<br/> height_scale: 1.0<br/> width_scale: 1.0<br/> }<br/> }<br/> matcher {<br/> argmax_matcher {<br/> matched_threshold: 0.5<br/> unmatched_threshold: 0.5<br/> ignore_thresholds: false<br/> negatives_lower_than_unmatched: true<br/> force_match_for_each_row: true<br/> use_matmul_gather: true<br/> }<br/> }<br/> similarity_calculator {<br/> iou_similarity {<br/> }<br/> }<br/> box_predictor {<br/> weight_shared_convolutional_box_predictor {<br/> conv_hyperparams {<br/> regularizer {<br/> l2_regularizer {<br/> weight: 3.9999998989515007e-05<br/> }<br/> }<br/> initializer {<br/> random_normal_initializer {<br/> mean: 0.0<br/> stddev: 0.009999999776482582<br/> }<br/> }<br/> activation: SWISH<br/> batch_norm {<br/> decay: 0.9900000095367432<br/> scale: true<br/> epsilon: 0.0010000000474974513<br/> }<br/> force_use_bias: true<br/> }<br/> depth: 64<br/> num_layers_before_predictor: 3<br/> kernel_size: 3<br/> class_prediction_bias_init: -4.599999904632568<br/> use_depthwise: true<br/> }<br/> }<br/> anchor_generator {<br/> multiscale_anchor_generator {<br/> min_level: 3<br/> max_level: 7<br/> anchor_scale: 4.0<br/> aspect_ratios: 1.0<br/> aspect_ratios: 2.0<br/> aspect_ratios: 0.5<br/> scales_per_octave: 3<br/> }<br/> }<br/> post_processing {<br/> batch_non_max_suppression {<br/> score_threshold: 9.99999993922529e-09<br/> iou_threshold: 0.5<br/> max_detections_per_class: 100<br/> max_total_detections: 100<br/> }<br/> score_converter: SIGMOID<br/> }<br/> normalize_loss_by_num_matches: true<br/> loss {<br/> localization_loss {<br/> weighted_smooth_l1 {<br/> }<br/> }<br/> classification_loss {<br/> weighted_sigmoid_focal {<br/> gamma: 1.5<br/> alpha: 0.25<br/> }<br/> }<br/> classification_weight: 1.0<br/> localization_weight: 1.0<br/> }<br/> encode_background_as_zeros: true<br/> normalize_loc_loss_by_codesize: true<br/> inplace_batchnorm_update: true<br/> freeze_batchnorm: false<br/> add_background_class: false<br/> }<br/>}<br/>train_config {<br/> batch_size: 12<br/> data_augmentation_options {<br/> random_horizontal_flip {<br/> }<br/> }<br/> data_augmentation_options {<br/> random_scale_crop_and_pad_to_square {<br/> output_size: 512<br/> scale_min: 0.10000000149011612<br/> scale_max: 2.0<br/> } <br/> }<br/> <br/> sync_replicas: true<br/> optimizer {<br/> momentum_optimizer {<br/> learning_rate {<br/> cosine_decay_learning_rate {<br/> learning_rate_base: 0.07999999821186066<br/> total_steps: 300000<br/> warmup_learning_rate: 0.0010000000474974513<br/> warmup_steps: 2500<br/> }<br/> }<br/> momentum_optimizer_value: 0.8999999761581421<br/> }<br/> use_moving_average: false<br/> }<br/> fine_tune_checkpoint: “efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0”<br/> num_steps: 300000<br/> startup_delay_steps: 0.0<br/> replicas_to_aggregate: 8<br/> max_number_of_boxes: 100<br/> unpad_groundtruth_tensors: false<br/> fine_tune_checkpoint_type: “detection”<br/> use_bfloat16: true<br/> fine_tune_checkpoint_version: V2<br/>}<br/>train_input_reader: {<br/> label_map_path: “training/labelmap.pbtxt”<br/> tf_record_input_reader {<br/> input_path: “train.record”<br/> }<br/>}</span><span id="dba4" class="nb jz iq mx b gy oe nd l ne nf">eval_config: {<br/> metrics_set: “coco_detection_metrics”<br/> use_moving_averages: false<br/> batch_size: 1;<br/>}</span><span id="9a17" class="nb jz iq mx b gy oe nd l ne nf">eval_input_reader: {<br/> label_map_path: “training/labelmap.pbtxt”<br/> shuffle: false<br/> num_epochs: 1<br/> tf_record_input_reader {<br/> input_path: “test.record”<br/> }<br/>}</span></pre><h2 id="0252" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">训练模型</h2><p id="f50b" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">在下面的命令片段中，我们正在训练我们的effdet模型</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="78f9" class="nb jz iq mx b gy nc nd l ne nf">#Changing directory to inside our object detecton model<br/>%cd /content/drive/My Drive/tf2/models/research/object_detection</span><span id="eab9" class="nb jz iq mx b gy oe nd l ne nf">#Configuring the protos<br/>!protoc object_detection/protos/*.proto --python_out=.</span><span id="995a" class="nb jz iq mx b gy oe nd l ne nf"># training the model<br/>!python model_main_tf2.py --pipeline_config_path=training/effdet.config --model_dir=training/effdet/ --alsologtostderr --num_train_steps=2000</span></pre><h2 id="d846" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">高效Det的评估</h2><p id="a5a2" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">因为我们已经训练了我们的effdet模型，所以我们将评估该模型并得到结果</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="76df" class="nb jz iq mx b gy nc nd l ne nf">!python model_main_tf2.py — model_dir=training/effdet/ — pipeline_config_path=training/effdet.config — checkpoint_dir=training/effdet/</span></pre><h2 id="95a2" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">训练后绘制并标记分数</h2><p id="5d95" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">以下是我们对模型进行评估后得到的分数</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8100" class="nb jz iq mx b gy nc nd l ne nf">Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.641<br/> Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.959<br/> Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834<br/> Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000<br/> Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000<br/> Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.641<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.020<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.200<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.699<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.699</span></pre><h2 id="6669" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">张量板的视觉损失</h2><p id="2e8f" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">让我们设想effdet模型的损失</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/615c8d0769cd8ee6fc48ad7fcc96d652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kQ_qhQdlitpEncHbVTQjw.png"/></div></div></figure><h1 id="b88a" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">用中心网建模</h1><h2 id="658c" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">从Tensorflow动物园下载中心网络模型</h2><p id="f5c4" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">从Tensorflow Zoo下载Centernet模型</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="0020" class="nb jz iq mx b gy nc nd l ne nf">%cd /content/drive/MyDrive/tf2/model/research/object_detection/</span><span id="0511" class="nb jz iq mx b gy oe nd l ne nf">!wget <a class="ae mo" href="http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_512x512_coco17_tpu-8.tar.gz" rel="noopener ugc nofollow" target="_blank">http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_512x512_coco17_tpu-8.tar.gz</a></span><span id="81cf" class="nb jz iq mx b gy oe nd l ne nf">!tar -xvzf centernet_hg104_512x512_coco17_tpu-8.tar.gz</span><span id="5ac8" class="nb jz iq mx b gy oe nd l ne nf">!rm -rf centernet_hg104_512x512_coco17_tpu-8.tar.gz</span></pre><h2 id="e4b9" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">将以下配置文件保存到Training/centernet</h2><p id="4147" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">在此步骤中，我们将把中心网络模型的配置文件保存在training文件夹中，下面是创建和保存配置文件的命令</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="cd0d" class="nb jz iq mx b gy nc nd l ne nf">%%writefile training/effdet.config</span><span id="209d" class="nb jz iq mx b gy oe nd l ne nf">%%writefile training/centernet.config<br/>model {<br/> center_net {<br/> num_classes: 1<br/> feature_extractor {<br/> type: “hourglass_104”<br/> channel_means: 104.01361846923828<br/> channel_means: 114.03422546386719<br/> channel_means: 119.91659545898438<br/> channel_stds: 73.60276794433594<br/> channel_stds: 69.89082336425781<br/> channel_stds: 70.91507720947266<br/> bgr_ordering: true<br/> }<br/> image_resizer {<br/> keep_aspect_ratio_resizer {<br/> min_dimension: 512<br/> max_dimension: 512<br/> pad_to_max_dimension: true<br/> }<br/> }<br/> object_detection_task {<br/> task_loss_weight: 1.0<br/> offset_loss_weight: 1.0<br/> scale_loss_weight: 0.10000000149011612<br/> localization_loss {<br/> l1_localization_loss {<br/> }<br/> }<br/> }<br/> object_center_params {<br/> object_center_loss_weight: 1.0<br/> classification_loss {<br/> penalty_reduced_logistic_focal_loss {<br/> alpha: 2.0<br/> beta: 4.0<br/> }<br/> }<br/> min_box_overlap_iou: 0.699999988079071<br/> max_box_predictions: 100<br/> }<br/> }<br/>}<br/>train_config {<br/> batch_size: 12<br/> data_augmentation_options {<br/> random_horizontal_flip {<br/> }<br/> }<br/> data_augmentation_options {<br/> random_crop_image {<br/> min_aspect_ratio: 0.5<br/> max_aspect_ratio: 1.7000000476837158<br/> random_coef: 0.25<br/> }<br/> }<br/> data_augmentation_options {<br/> random_adjust_hue {<br/> }<br/> }<br/> data_augmentation_options {<br/> random_adjust_contrast {<br/> }<br/> }<br/> data_augmentation_options {<br/> random_adjust_saturation {<br/> }<br/> }<br/> data_augmentation_options {<br/> random_adjust_brightness {<br/> }<br/> }<br/> data_augmentation_options {<br/> random_absolute_pad_image {<br/> max_height_padding: 200<br/> max_width_padding: 200<br/> pad_color: 0.0<br/> pad_color: 0.0<br/> pad_color: 0.0<br/> }<br/> }<br/> optimizer {<br/> adam_optimizer {<br/> learning_rate {<br/> manual_step_learning_rate {<br/> initial_learning_rate: 0.0010000000474974513<br/> schedule {<br/> step: 90000<br/> learning_rate: 9.999999747378752e-05<br/> }<br/> schedule {<br/> step: 120000<br/> learning_rate: 9.999999747378752e-06<br/> }<br/> }<br/> }<br/> epsilon: 1.0000000116860974e-07<br/> }<br/> use_moving_average: false<br/> }<br/> fine_tune_checkpoint: “centernet_hg104_512x512_coco17_tpu-8/checkpoint/ckpt-0”<br/> num_steps: 140000<br/> max_number_of_boxes: 100<br/> unpad_groundtruth_tensors: false<br/> fine_tune_checkpoint_type: “fine_tune”<br/> fine_tune_checkpoint_version: V2<br/>}<br/>train_input_reader {<br/> label_map_path: “training/labelmap.pbtxt”<br/> tf_record_input_reader {<br/> input_path: “train.record”<br/> }<br/>}<br/>eval_config {<br/> metrics_set: “coco_detection_metrics”<br/> use_moving_averages: false<br/> batch_size: 1<br/>}<br/>eval_input_reader {<br/> label_map_path: “training/labelmap.pbtxt”<br/> shuffle: false<br/> num_epochs: 1<br/> tf_record_input_reader {<br/> input_path: “test.record”<br/> }<br/>}</span></pre><h2 id="1576" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">训练模型</h2><p id="00b0" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">在下面的命令片段中，我们正在训练我们的模型</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8262" class="nb jz iq mx b gy nc nd l ne nf">#changing directory to inside object_detection folder<br/>%cd /content/drive/My Drive/tf2/model/research/object_detection</span><span id="c4a9" class="nb jz iq mx b gy oe nd l ne nf">#configuring the protos<br/>!protoc object_detection/protos/*.proto — python_out=.</span><span id="e59b" class="nb jz iq mx b gy oe nd l ne nf"># Training the model<br/>!python model_main_tf2.py — pipeline_config_path=training/centernet.config — model_dir=training/centernet/ — alsologtostderr — num_train_steps=2000</span></pre><h2 id="c758" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">评估模型</h2><p id="4e4d" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">使用下面的命令片段，我们将评估centernet模型</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="39ee" class="nb jz iq mx b gy nc nd l ne nf">!python model_main_tf2.py — model_dir=training/centernet — pipeline_config_path=training/centernet.config — checkpoint_dir=training/centernet/</span></pre><h2 id="83c2" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">训练后绘制并标记分数</h2><p id="f453" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">以下是centernet模型在我们数据集上的得分</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="08d1" class="nb jz iq mx b gy nc nd l ne nf">Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.709<br/> Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.957<br/> Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930<br/> Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000<br/> Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000<br/> Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.022<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.217<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000<br/> Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.762</span></pre><h2 id="57a6" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">张量板</h2><p id="166c" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">使用张量板可视化损失</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/8b65dc0afcd84581740c2d92a9101208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ySFWLAT9luCshK_nY-BcQw.png"/></div></div></figure><h1 id="267a" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">从Tensorflow动物园下载模型</h1><p id="f0bb" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">通过下面的命令片段，我们可以从TensorFlow zoo下载一个更快的R-CNN模型</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="359c" class="nb jz iq mx b gy nc nd l ne nf"><strong class="mx ir">%</strong>cd <strong class="mx ir">/</strong>content<strong class="mx ir">/</strong>drive<strong class="mx ir">/</strong>MyDrive<strong class="mx ir">/</strong>tf2<strong class="mx ir">/</strong>model<strong class="mx ir">/</strong>research<strong class="mx ir">/</strong>object_detection<strong class="mx ir">/</strong></span><span id="bbf0" class="nb jz iq mx b gy oe nd l ne nf"><strong class="mx ir">!</strong>wget <a class="ae mo" href="http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8.tar.gz" rel="noopener ugc nofollow" target="_blank">http:<strong class="mx ir">//</strong>download.tensorflow.org<strong class="mx ir">/</strong>models<strong class="mx ir">/</strong>object_detection<strong class="mx ir">/</strong>tf2<strong class="mx ir">/</strong>20200711<strong class="mx ir">/</strong>faster_rcnn_resnet101_v1_800x1333_coco17_gpu<strong class="mx ir">-</strong>8.tar.gz</a></span><span id="8c4e" class="nb jz iq mx b gy oe nd l ne nf"><strong class="mx ir">!</strong>tar <strong class="mx ir">-</strong>xvzf faster_rcnn_resnet101_v1_800x1333_coco17_gpu<strong class="mx ir">-</strong>8.tar.gz<strong class="mx ir">!</strong>rm <strong class="mx ir">-</strong>rf faster_rcnn_resnet101_v1_800x1333_coco17_gpu<strong class="mx ir">-</strong>8.tar.gz</span></pre><h1 id="30a3" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">将以下配置文件保存在Training/faster_rcnn.config中</h1><p id="48fc" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">将faster _ rcnn配置文件写入磁盘</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="7f6d" class="nb jz iq mx b gy nc nd l ne nf">%%writefile training/faster_rcnn.config<br/># Faster R-CNN with Resnet<br/># Sync-trained on COCO (8 GPUs),<br/># Initialized from Imagenet classification checkpoint<br/># TF2-Compatible, *Not* TPU-Compatible</span><span id="7531" class="nb jz iq mx b gy oe nd l ne nf">model {<br/>  faster_rcnn {<br/>    num_classes: 1<br/>    image_resizer {<br/>      keep_aspect_ratio_resizer {<br/>        min_dimension: 800<br/>        max_dimension: 1333<br/>        pad_to_max_dimension: true<br/>      }<br/>    }<br/>    feature_extractor {<br/>      type: 'faster_rcnn_resnet101_keras'<br/>    }<br/>    first_stage_anchor_generator {<br/>      grid_anchor_generator {<br/>        scales: [0.25, 0.5, 1.0, 2.0]<br/>        aspect_ratios: [0.5, 1.0, 2.0]<br/>        height_stride: 16<br/>        width_stride: 16<br/>      }<br/>    }<br/>    first_stage_box_predictor_conv_hyperparams {<br/>      op: CONV<br/>      regularizer {<br/>        l2_regularizer {<br/>          weight: 0.0<br/>        }<br/>      }<br/>      initializer {<br/>        truncated_normal_initializer {<br/>          stddev: 0.01<br/>        }<br/>      }<br/>    }<br/>    first_stage_nms_score_threshold: 0.0<br/>    first_stage_nms_iou_threshold: 0.7<br/>    first_stage_max_proposals: 300<br/>    first_stage_localization_loss_weight: 2.0<br/>    first_stage_objectness_loss_weight: 1.0<br/>    initial_crop_size: 14<br/>    maxpool_kernel_size: 2<br/>    maxpool_stride: 2<br/>    second_stage_box_predictor {<br/>      mask_rcnn_box_predictor {<br/>        use_dropout: false<br/>        dropout_keep_probability: 1.0<br/>        fc_hyperparams {<br/>          op: FC<br/>          regularizer {<br/>            l2_regularizer {<br/>              weight: 0.0<br/>            }<br/>          }<br/>          initializer {<br/>            variance_scaling_initializer {<br/>              factor: 1.0<br/>              uniform: true<br/>              mode: FAN_AVG<br/>            }<br/>          }<br/>        }<br/>      }<br/>    }<br/>    second_stage_post_processing {<br/>      batch_non_max_suppression {<br/>        score_threshold: 0.0<br/>        iou_threshold: 0.6<br/>        max_detections_per_class: 100<br/>        max_total_detections: 100<br/>      }<br/>      score_converter: SOFTMAX<br/>    }<br/>    second_stage_localization_loss_weight: 2.0<br/>    second_stage_classification_loss_weight: 1.0<br/>  }<br/>}</span><span id="aa38" class="nb jz iq mx b gy oe nd l ne nf">train_config: {<br/>  batch_size: 2<br/>  num_steps: 200000<br/>  optimizer {<br/>    momentum_optimizer: {<br/>      learning_rate: {<br/>        cosine_decay_learning_rate {<br/>          learning_rate_base: 0.01<br/>          total_steps: 200000<br/>          warmup_learning_rate: 0.0<br/>          warmup_steps: 5000<br/>        }<br/>      }<br/>      momentum_optimizer_value: 0.9<br/>    }<br/>    use_moving_average: false<br/>  }<br/>  gradient_clipping_by_norm: 10.0<br/>  fine_tune_checkpoint_version: V2<br/>  fine_tune_checkpoint: "faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8/checkpoint/ckpt-0"<br/>  fine_tune_checkpoint_type: "detection"<br/>  data_augmentation_options {<br/>    random_horizontal_flip {<br/>    }<br/>  }</span><span id="7098" class="nb jz iq mx b gy oe nd l ne nf">data_augmentation_options {<br/>    random_adjust_hue {<br/>    }<br/>  }</span><span id="2f08" class="nb jz iq mx b gy oe nd l ne nf">data_augmentation_options {<br/>    random_adjust_contrast {<br/>    }<br/>  }</span><span id="c9c0" class="nb jz iq mx b gy oe nd l ne nf">data_augmentation_options {<br/>    random_adjust_saturation {<br/>    }<br/>  }</span><span id="688f" class="nb jz iq mx b gy oe nd l ne nf">data_augmentation_options {<br/>     random_square_crop_by_scale {<br/>      scale_min: 0.6<br/>      scale_max: 1.3<br/>    }<br/>  }<br/>}</span><span id="82a9" class="nb jz iq mx b gy oe nd l ne nf">train_input_reader: {<br/>  label_map_path: "training/labelmap.pbtxt"<br/>  tf_record_input_reader {<br/>    input_path: "train.record"<br/>  }<br/>}</span><span id="61f8" class="nb jz iq mx b gy oe nd l ne nf">eval_config: {<br/>  metrics_set: "coco_detection_metrics"<br/>  use_moving_averages: false<br/>  batch_size: 1;<br/>}</span><span id="ea15" class="nb jz iq mx b gy oe nd l ne nf">eval_input_reader: {<br/>  label_map_path: "training/labelmap.pbtxt"<br/>  shuffle: false<br/>  num_epochs: 1<br/>  tf_record_input_reader {<br/>    input_path: "test.record"<br/>  }<br/>}</span></pre><h2 id="b630" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">训练模型</h2><p id="f53b" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">下面的命令片段在我们的数据集上训练我们更快的R-CNN模型</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="9212" class="nb jz iq mx b gy nc nd l ne nf">%cd /content/drive/My Drive/tf2/models/research/object_detection</span><span id="243f" class="nb jz iq mx b gy oe nd l ne nf">!protoc /protos/*.proto — python_out=.</span><span id="9535" class="nb jz iq mx b gy oe nd l ne nf">!python model_main_tf2.py — pipeline_config_path=training/faster_rcnn.config — model_dir=training/faster_rcnn/ — alsologtostderr — num_train_steps=2000</span></pre><h2 id="5e27" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">评估模型</h2><p id="2a9f" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">使用下面的代码，我们将能够评估我们更快的R-CNN模型</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="2c65" class="nb jz iq mx b gy nc nd l ne nf">!python model_main_tf2.py — model_dir=training/faster_rcnn — pipeline_config_path=training/faster_rcnn.config — checkpoint_dir=training/faster_rcnn/</span></pre><h2 id="83b2" class="nb jz iq bd ka nj nk dn ke nl nm dp ki ld nn no km lf np nq kq lh nr ns ku nt bi translated">张量板</h2><p id="671a" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">可视化损失更快的R-CNN模型我们的数据集。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/88c17c14e8f527eceff4171add7e51ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBUy3O77ReSxn6qnbCeJaQ.png"/></div></div></figure><h1 id="540e" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">7.)对比</h1><p id="8f9a" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">因此，让我们看看数据集上所有模型的mAP(precision)和mAR(recall)</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi gj"><img src="../Images/351047bdba517feb9290353e5fb77992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofBzs-HiWPvo81opy611mQ.png"/></div></div></figure><p id="72f3" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">从上表来看，我们所有的模型都运行良好，但显然，centernet的性能优于所有模型，因此让我们使用centernet来构建我们的最终管道</p><h1 id="fb98" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">8.)最终管道和部署</h1><p id="cf58" class="pw-post-body-paragraph lt lu iq ky b kz la lv lw lb lc lx ly ld lz ma mb lf mc md me lh mf mg mh lj ij bi translated">我们将在colab上本地部署应用程序。但是在colab中，我们无法使用浏览器访问本地端口，因此我们将使用ngrok API将端口上运行的应用托管到ngrok服务器，我们还将使用streamlit来部署我们的应用。</p><p id="bada" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">让我们将完整的部署代码保存在app.py文件中</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="191a" class="nb jz iq mx b gy nc nd l ne nf">%%writefile app.py</span><span id="8e8f" class="nb jz iq mx b gy oe nd l ne nf">import streamlit as st</span><span id="14c5" class="nb jz iq mx b gy oe nd l ne nf">st.markdown(“&lt;h1 style=’text-align: center; color: White;background-color:#e84343'&gt;Welcome to Product Detection WebApp : &lt;/h1&gt;”, unsafe_allow_html=True)</span><span id="102e" class="nb jz iq mx b gy oe nd l ne nf">import numpy as np</span><span id="bdd5" class="nb jz iq mx b gy oe nd l ne nf">import os</span><span id="de3a" class="nb jz iq mx b gy oe nd l ne nf">import six.moves.urllib as urllib<br/>import sys<br/>import tarfile<br/>import tensorflow as tf<br/>import zipfile<br/>from collections import defaultdict<br/>from io import StringIO<br/>from matplotlib import pyplot as plt<br/>from PIL import Image<br/>from IPython.display import display<br/>from object_detection.utils import ops as utils_ops<br/>from object_detection.utils import label_map_util<br/>from object_detection.utils import visualization_utils as vis_util<br/>utils_ops.tf = tf.compat.v1<br/>tf.gfile = tf.io.gfile<br/>category_index = label_map_util.create_category_index_from_labelmap(‘training/labelmap.pbtxt’, use_display_name=True)</span><span id="b127" class="nb jz iq mx b gy oe nd l ne nf">detection_model = tf.saved_model.load(‘inference_graph/saved_model’)</span><span id="c79f" class="nb jz iq mx b gy oe nd l ne nf">def run_inference_for_single_image(model, image):</span><span id="236b" class="nb jz iq mx b gy oe nd l ne nf">  image = np.asarray(image)</span><span id="98b5" class="nb jz iq mx b gy oe nd l ne nf">  input_tensor = tf.convert_to_tensor(image)</span><span id="0b1d" class="nb jz iq mx b gy oe nd l ne nf">  input_tensor = input_tensor[tf.newaxis,…]</span><span id="8d70" class="nb jz iq mx b gy oe nd l ne nf">  model_fn = model.signatures[‘serving_default’]</span><span id="0598" class="nb jz iq mx b gy oe nd l ne nf">  output_dict = model_fn(input_tensor)</span><span id="f329" class="nb jz iq mx b gy oe nd l ne nf">  num_detections = int(output_dict.pop(‘num_detections’))</span><span id="819f" class="nb jz iq mx b gy oe nd l ne nf">  output_dict = {key:value[0, :num_detections].numpy() for key,value in output_dict.items()}</span><span id="5928" class="nb jz iq mx b gy oe nd l ne nf">  output_dict[‘num_detections’] = num_detections</span><span id="1c5a" class="nb jz iq mx b gy oe nd l ne nf">  output_dict[‘detection_classes’] =       output_dict[‘detection_classes’].astype(np.int64)</span><span id="ac1c" class="nb jz iq mx b gy oe nd l ne nf">  if ‘detection_masks’ in output_dict:</span><span id="d351" class="nb jz iq mx b gy oe nd l ne nf">    detection_masks_reframed =   utils_ops.reframe_box_masks_to_image_masks(   output_dict[‘detection_masks’],   output_dict[‘detection_boxes’],image.shape[0], image.shape[1] )</span><span id="5496" class="nb jz iq mx b gy oe nd l ne nf">  detection_masks_reframed = tf.cast(detection_masks_reframed &gt; 0.5,tf.uint8)</span><span id="3ecb" class="nb jz iq mx b gy oe nd l ne nf">  output_dict[‘detection_masks_reframed’] =   detection_masks_reframed.numpy()</span><span id="8e3f" class="nb jz iq mx b gy oe nd l ne nf">  return output_dict</span><span id="6d92" class="nb jz iq mx b gy oe nd l ne nf">def show_inference(model, image_path):</span><span id="9001" class="nb jz iq mx b gy oe nd l ne nf">  image_np = np.array(Image.open(image_path))</span><span id="7b80" class="nb jz iq mx b gy oe nd l ne nf">  output_dict = run_inference_for_single_image(model, image_np)</span><span id="3751" class="nb jz iq mx b gy oe nd l ne nf">  vis_util.visualize_boxes_and_labels_on_image_array(</span><span id="d810" class="nb jz iq mx b gy oe nd l ne nf">  image_np,</span><span id="0f45" class="nb jz iq mx b gy oe nd l ne nf">  output_dict[‘detection_boxes’],</span><span id="503e" class="nb jz iq mx b gy oe nd l ne nf">  output_dict[‘detection_classes’],</span><span id="eac1" class="nb jz iq mx b gy oe nd l ne nf">  output_dict[‘detection_scores’],</span><span id="9983" class="nb jz iq mx b gy oe nd l ne nf">  category_index,</span><span id="1527" class="nb jz iq mx b gy oe nd l ne nf">  instance_masks=output_dict.get(‘detection_masks_reframed’, None),</span><span id="4a95" class="nb jz iq mx b gy oe nd l ne nf">  use_normalized_coordinates=True,</span><span id="faa6" class="nb jz iq mx b gy oe nd l ne nf">  line_thickness=8)</span><span id="2538" class="nb jz iq mx b gy oe nd l ne nf">  return image_np</span><span id="8d1e" class="nb jz iq mx b gy oe nd l ne nf">def main():</span><span id="1295" class="nb jz iq mx b gy oe nd l ne nf">  st.markdown(“&lt;h1 style=’text-align: center; color:     White;background-color:#e84343'&gt;Please Upload the Image — &lt;/h1&gt;”, unsafe_allow_html=True)</span><span id="d4d2" class="nb jz iq mx b gy oe nd l ne nf">  uploaded_path = st.file_uploader(“Choose an Image………” , type = “jpg”)</span><span id="dc65" class="nb jz iq mx b gy oe nd l ne nf">  if uploaded_path != None:</span><span id="0616" class="nb jz iq mx b gy oe nd l ne nf">     st.markdown(“&lt;h1 style=’text-align: center; color: White;background-color:#e84343'&gt;Generating Output …&lt;/h1&gt;”, unsafe_allow_html=True)</span><span id="8374" class="nb jz iq mx b gy oe nd l ne nf">  array = show_inference(detection_model, uploaded_path)</span><span id="54db" class="nb jz iq mx b gy oe nd l ne nf">  im = Image.fromarray(array)</span><span id="2de8" class="nb jz iq mx b gy oe nd l ne nf">  st.image(im , caption=”Output” , use_column_width = True)</span><span id="6729" class="nb jz iq mx b gy oe nd l ne nf">if __name__ ==’__main__’:</span><span id="cf54" class="nb jz iq mx b gy oe nd l ne nf">  main() #calling the main method</span></pre><p id="0b4d" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">让我们使用上面写的app.py文件来部署我们的应用程序</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="7a67" class="pw-post-body-paragraph lt lu iq ky b kz mi lv lw lb mj lx ly ld mp ma mb lf mq md me lh mr mg mh lj ij bi translated">让我们来看看应用程序的运行情况</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/8dcd6416924f71ec76a548a12981f91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*nzpHmIBhsva13Y97MDt5_g.gif"/></div></figure><h1 id="65e0" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">9.)未来工作</h1><ul class=""><li id="bc12" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj mn ll lm ln bi translated">从不同的产品来源收集更多的图片。</li><li id="97d2" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">试验除上述对象检测模型之外的模型。</li><li id="b49c" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">在模型中添加不同的产品类别。</li></ul><h1 id="86f8" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">10.)参考文献</h1><ul class=""><li id="dc45" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj mn ll lm ln bi translated"><a class="ae mo" href="https://towardsdatascience.com/custom-object-detection-for-non-data-scientists-70325fef2dbb" rel="noopener" target="_blank">https://towards data science . com/custom-object-detection-for-non-data-scientists-70325 fef 2 dbb</a></li><li id="ec8d" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated"><a class="ae mo" href="https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html" rel="noopener ugc nofollow" target="_blank">https://tensor flow-object-detection-API-tutorial . readthedocs . io/en/latest/training . html</a></li><li id="8f76" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated">【https://www.appliedaicourse.com/ T4】</li><li id="e61e" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated"><a class="ae mo" href="https://github.com/tensorflow/models" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/models</a></li></ul><h1 id="4d55" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">11.)简介</h1><ul class=""><li id="3153" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj mn ll lm ln bi translated"><strong class="ky ir">GitHub</strong>T10<strong class="ky ir">T12】链接T14】T15】</strong></li><li id="9f16" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj mn ll lm ln bi translated"><strong class="ky ir">LinkedIn P</strong><a class="ae mo" href="https://www.linkedin.com/in/parv-gupta-neo/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">rofile</strong></a><strong class="ky ir">。</strong></li></ul></div></div>    
</body>
</html>