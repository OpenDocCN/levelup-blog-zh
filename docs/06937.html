<html>
<head>
<title>DQN Maze Solver</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DQN迷宫求解器</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/dqn-maze-solver-d01afbd1986b?source=collection_archive---------24-----------------------#2021-01-12">https://levelup.gitconnected.com/dqn-maze-solver-d01afbd1986b?source=collection_archive---------24-----------------------#2021-01-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="79e2" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h1><p id="98ba" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在我之前的文章中，我展示了如何使用基于表格的Q学习方法构建一个解决迷宫的Q学习器。在这篇文章中，我展示了如何使用DQN(深度Q学习)解决相同的迷宫。这个示例程序的代码可以在<a class="ae lj" href="https://github.com/danmcleran/tinymind/tree/master/examples/dqn_maze" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="ac61" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">强化学习</h1><p id="e601" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在强化学习问题中，主体通过评估环境的状态、采取行动和接受奖励来与环境进行交互。我们的目标是了解随着时间的推移，哪些行为能带来最大的回报。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/c908b7cb8309f375316770c34cd4e2da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*S2H0DkcgBLjzbd9EwhwQPQ.png"/></div></div></figure><h1 id="717a" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">迷宫问题</h1><p id="9d2b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">正如在<a class="ae lj" href="https://medium.com/swlh/table-based-q-learning-in-under-1kb-3cc0b5b54b43" rel="noopener">的上一篇文章</a>中，我们将尝试解决迷宫问题。一只老鼠被随机丢进迷宫中6个地方中的一个，它必须学会找到奶酪(奖励)。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/3cf1ffddd315c42b49fc06e56cc2344a.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*zHrvcAky-0R196SgMBr5aw.png"/></div></figure><p id="08a4" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">正如在<a class="ae lj" href="https://medium.com/swlh/table-based-q-learning-in-under-1kb-3cc0b5b54b43" rel="noopener">之前的文章</a>中一样，我们在程序中定义迷宫如下:</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mc md l"/></div></figure><h1 id="9d01" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">DQN建筑</h1><p id="9934" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于这个问题，我们将从<a class="ae lj" href="https://medium.com/swlh/table-based-q-learning-in-under-1kb-3cc0b5b54b43" rel="noopener">之前的解决方案</a>中去掉Q表，用神经网络代替。我们将需要1个输入神经元(状态)和6个输出神经元(动作)。我选择实现一个比输出神经元多2个神经元的隐藏层(感觉不错)。每个输出神经元代表从该状态采取的动作。输入层有1个神经元，代表我们鼠标的当前状态。使用<a class="ae lj" rel="noopener ugc nofollow" target="_blank" href="/qformat-92b4e570235f"> Q-format </a>，我通过获取当前状态并除以最大可能状态(即总共5，6个状态，编号为0-5)来缩放输入。为此，DQN神经网络查询环境，并期望它初始化指针，该指针指向给定状态数值表示的神经网络的输入数组。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mc md l"/></div></figure><p id="5ff2" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">实现这一点的另一种方法是有6个输入神经元。但是，这种方法工作得很好，因为我们有16位分辨率，只有6个状态。</p><h1 id="54f1" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">DQN神经网络</h1><p id="f464" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">使用的神经网络代码也来自<a class="ae lj" href="https://github.com/danmcleran/tinymind" rel="noopener ugc nofollow" target="_blank"> tinymind </a>。为了深入了解这些神经网络是如何工作的，请看我的文章<a class="ae lj" rel="noopener ugc nofollow" target="_blank" href="/a-neural-network-in-under-4kb-41fc2d7d9174">。训练神经网络来预测从给定状态采取的每个动作的Q值。在神经网络被训练之后，我们然后选择包含最高Q值的动作(即输出神经元0 ==动作0，输出神经元1 ==动作1，等等。).</a></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi me"><img src="../Images/0fcd5f099fc0ad868b5104a94c134456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*c94RkiZ60x7dmUp8PfA5Aw.png"/></div></figure><p id="66d9" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">图1:DQN迷宫求解器的神经网络结构</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mf"><img src="../Images/eb97168928301900798adeb4c22b7004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4GtuzBZwQBSkFwvxYS2OdA.png"/></div></div></figure><p id="1107" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">图2:代理使用神经网络来获取/设置Q值</p><p id="8c66" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">DQN在<a class="ae lj" href="https://github.com/danmcleran/tinymind/blob/master/examples/dqn_maze/dqn_mazelearner.h" rel="noopener ugc nofollow" target="_blank"> dqn_mazelearner.h </a>中定义如下:</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mc md l"/></div></figure><h1 id="d2c0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">构建示例</strong></h1><p id="d884" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">将目录更改为tinymind/examples/dqn_maze中的示例代码。我们创建一个目录来保存构建的可执行程序，然后编译这个示例。我们需要编译神经网络LUT代码，因为我们都定义了要编译的lut的类型。在这种情况下，我们只需要带符号的<a class="ae lj" rel="noopener ugc nofollow" target="_blank" href="/qformat-92b4e570235f"> Q-format </a>类型的Q16.16的tanh激活函数。有关<a class="ae lj" href="https://github.com/danmcleran/tinymind" rel="noopener ugc nofollow" target="_blank"> tinymind </a>中神经网络如何工作的详细说明，请参见本文<a class="ae lj" rel="noopener ugc nofollow" target="_blank" href="/a-neural-network-in-under-4kb-41fc2d7d9174">文章</a>。</p><pre class="ll lm ln lo gt mg mh mi mj aw mk bi"><span id="197c" class="ml jo iq mh b gy mm mn l mo mp"># Simple Makefile for the DQN maze example</span><span id="74cb" class="ml jo iq mh b gy mq mn l mo mp"># Tell the code to compile the Q16.16 tanh activation function LUT</span><span id="78a7" class="ml jo iq mh b gy mq mn l mo mp"><strong class="mh ir">default</strong>:</span><span id="747a" class="ml jo iq mh b gy mq mn l mo mp">#   Make an output dir to hold the executable</span><span id="c542" class="ml jo iq mh b gy mq mn l mo mp">mkdir -p ./output</span><span id="0f3a" class="ml jo iq mh b gy mq mn l mo mp">#   Build the example with default build flags</span><span id="b3fa" class="ml jo iq mh b gy mq mn l mo mp">g++ -O3 -Wall -o ./output/dqn_maze dqn_maze.cpp dqn_mazelearner.cpp ../../cpp/lookupTables.cpp -I../../cpp -I../../include/ -DTINYMIND_USE_TANH_16_16=1</span><span id="fc9e" class="ml jo iq mh b gy mq mn l mo mp"><strong class="mh ir">debug</strong>:</span><span id="f568" class="ml jo iq mh b gy mq mn l mo mp">#   Make an output dir to hold the executable</span><span id="8f32" class="ml jo iq mh b gy mq mn l mo mp">mkdir -p ./output</span><span id="88c6" class="ml jo iq mh b gy mq mn l mo mp">#   Build the example with default build flags</span><span id="ede3" class="ml jo iq mh b gy mq mn l mo mp">g++ -g -Wall -o ./output/dqn_maze dqn_maze.cpp dqn_mazelearner.cpp ../../cpp/lookupTables.cpp -I../../cpp -I../../include/ -DTINYMIND_USE_TANH_16_16=1</span><span id="eb2d" class="ml jo iq mh b gy mq mn l mo mp"># Remove all object files</span><span id="294b" class="ml jo iq mh b gy mq mn l mo mp"><strong class="mh ir">clean</strong>:</span><span id="0372" class="ml jo iq mh b gy mq mn l mo mp">rm -f ./output/*</span></pre><p id="f7d7" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">这将构建一个maze leaner示例程序，并将可执行文件放在。/输出。现在，我们可以进入生成可执行文件的目录，并运行示例程序。</p><pre class="ll lm ln lo gt mg mh mi mj aw mk bi"><span id="8092" class="ml jo iq mh b gy mm mn l mo mp">cd ./output<br/>./dqn_maze</span></pre><p id="1314" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">当程序结束运行时，您将看到最后一条输出消息，如下所示:</p><pre class="ll lm ln lo gt mg mh mi mj aw mk bi"><span id="ce60" class="ml jo iq mh b gy mm mn l mo mp">take action 5<br/>*** starting in state 4 ***<br/>take action 5<br/>*** starting in state 5 ***<br/>take action 1<br/>take action 5<br/>*** starting in state 2 ***<br/>take action 3<br/>take action 1<br/>take action 5<br/>*** starting in state 0 ***<br/>take action 4<br/>take action 5<br/>*** starting in state 1 ***<br/>take action 5<br/>*** starting in state 1 ***<br/>take action 5<br/>*** starting in state 3 ***<br/>take action 1<br/>take action 5<br/>*** starting in state 5 ***<br/>take action 1<br/>take action 5<br/>*** starting in state 4 ***<br/>take action 5</span></pre><p id="b8f5" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">您的消息可能会略有不同，因为我们在每次迭代中都会在随机房间中启动鼠标。在示例程序执行期间，我们将所有鼠标活动保存到文件(dqn_maze_training.txt和dqn_maze_test.txt)中。在训练文件中，鼠标在前400集采取随机动作，然后在接下来的100集，随机性从100%随机降低到0%随机。要查看前几次训练迭代，您可以这样做:</p><pre class="ll lm ln lo gt mg mh mi mj aw mk bi"><span id="9fd7" class="ml jo iq mh b gy mm mn l mo mp">head dqn_maze_training.txt</span></pre><p id="308f" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">您应该会看到类似这样的内容:</p><pre class="ll lm ln lo gt mg mh mi mj aw mk bi"><span id="5ad6" class="ml jo iq mh b gy mm mn l mo mp">1,3,4,3,1,5,<br/>1,3,1,5,<br/>1,3,4,3,1,5,<br/>5,4,5,<br/>1,5,<br/>2,3,1,5,<br/>3,2,3,4,0,4,3,1,5,<br/>4,0,4,5,<br/>4,0,4,0,4,0,4,5,<br/>4,0,4,5,</span></pre><p id="9807" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">同样，您的消息看起来会略有不同。第一个数字是开始状态，其后的每个逗号分隔值是鼠标从一个房间到另一个房间的随机移动。示例:在上面的第一行中，我们从1号房间开始，然后移动到3号、4号、1号、5号房间。因为5是我们的目标状态，所以我们停止了。这看起来如此不稳定的原因是，在训练的前400次迭代中，我们从可能的行动中随机做出决定。一旦我们到了状态5，我们得到我们的奖励，并停止。</p><h1 id="4785" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">可视化培训和测试</h1><p id="daad" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我已经包含了一个<a class="ae lj" href="https://github.com/danmcleran/tinymind/blob/master/examples/dqn_maze/dqn_mazeplot.py" rel="noopener ugc nofollow" target="_blank"> Python脚本</a>来绘制训练和测试数据。如果我们绘制起始状态== 2的训练数据(即开始时鼠标被放入房间2):</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/c7e1d98990d3412d73a04a2e8226fe16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*3OIsCF4Xu1co1o_F-KYtGg.png"/></div></figure><p id="2b7b" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">图上的每一条线代表一集，在这一集的开始，我们将老鼠随机放入2号房间。你可以看到，在最坏的情况下，我们采取了18次随机移动来找到目标状态(状态5)。这是因为在每一步，我们只是生成一个随机数，从可用的动作中进行选择(例如，下一步应该移动到哪个房间)。如果我们使用脚本来绘制起始状态== 2的测试数据:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/ac3e94004cf0347ec5fd1772c266aaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*6jAYfpG96Rmmas7H2PpZmA.png"/></div></figure><p id="1d56" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">你可以看到，在我们完成训练后，Q学习者已经学会了一个最佳路径:2-&gt;3-&gt;1-&gt;5。</p><h1 id="c353" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">确定Q学习者的规模</h1><p id="2161" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们使用DQN而不是基于表的Q学习器的原因是，当状态的数量或状态空间的复杂性会使Q表变得不合理地大时。对于这个迷宫问题，这是不成立的。我选择这个问题来展示基于表格的Q-learning和DQN之间的直接比较，不一定是为了节省空间。事实上，DQN实现比基于表的方法占用更多的代码和数据空间。在一个更复杂的状态空间中，我们将能够实现DQN相对于基于表的优势。</p><p id="d107" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">在任何情况下，我们都想测量实现我们的迷宫求解DQN需要多少代码和数据:</p><pre class="ll lm ln lo gt mg mh mi mj aw mk bi"><span id="22a4" class="ml jo iq mh b gy mm mn l mo mp">g++ -c dqn_mazelearner.cpp -O3 -I../../cpp -I../../include/ -DTINYMIND_USE_TANH_16_16=1 &amp;&amp; mv dqn_mazelearner.o ./output/.</span><span id="87b7" class="ml jo iq mh b gy mq mn l mo mp">cd ./output/</span><span id="2f88" class="ml jo iq mh b gy mq mn l mo mp">size dqn_mazelearner.o</span></pre><p id="d31b" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">您应该看到的输出是:</p><pre class="ll lm ln lo gt mg mh mi mj aw mk bi"><span id="558d" class="ml jo iq mh b gy mm mn l mo mp">text data bss dec hex filename<br/>12224 8 3652 15884 3e0c dqn_mazelearner.o</span></pre><p id="79e4" class="pw-post-body-paragraph kl km iq kn b ko lx kq kr ks ly ku kv kw lz ky kz la ma lc ld le mb lg lh li ij bi translated">我们可以看到整个DQN在16KB以内。DQN很小。</p><h1 id="4ae0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="2a01" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">到今天为止，<a class="ae lj" href="https://github.com/danmcleran/tinymind" rel="noopener ugc nofollow" target="_blank"> tinymind </a>支持两种类型的Q-learning:基于表格的Q-Learning和DQN。示例程序以及单元测试存在于存储库中，以展示它们的用途。DQN用一个神经网络代替Q表来学习状态、动作和Q值之间的关系。当状态空间很大并且Q表消耗的内存非常大时，人们会希望使用DQN。通过使用DQN，我们用内存换取了CPU周期。DQN的CPU开销将远远大于Q表。但是，DQN允许我们进行Q学习，同时保持我们的内存足迹对于复杂环境的可管理性。</p></div></div>    
</body>
</html>