# PyTorch ä¸­çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»‹ç»

> åŸæ–‡ï¼š<https://levelup.gitconnected.com/introduction-to-natural-language-processing-nlp-in-pytorch-8b7344c9dfec>

![](img/6452c6a59f75fabb7944ab1cf7e1213b.png)

ç…§ç‰‡ç”± [Pietro Jeng](https://unsplash.com/@pietrozj) åœ¨ [Unsplash](https://unsplash.com/) ä¸Šæ‹æ‘„

# å•è¯åµŒå…¥

å•è¯åµŒå…¥æˆ–å•è¯å‘é‡æä¾›äº†ä¸€ç§å°†å•è¯ä»è¯æ±‡è¡¨æ˜ å°„åˆ°ä½ç»´ç©ºé—´çš„æ–¹æ³•ï¼Œåœ¨ä½ç»´ç©ºé—´ä¸­ï¼Œå…·æœ‰ç›¸ä¼¼å«ä¹‰çš„å•è¯é å¾—å¾ˆè¿‘ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ç»„é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯å‘é‡æ¥ç†Ÿæ‚‰å®ƒä»¬çš„å±æ€§ã€‚å­˜åœ¨å¤šç»„é¢„è®­ç»ƒçš„å•è¯åµŒå…¥ï¼›è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ ConceptNet Numberbatchï¼Œå®ƒä»¥æ˜“äºä½¿ç”¨çš„æ ¼å¼(h5)æä¾›äº†ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ä¸‹è½½ã€‚

```
# Download word vectors
from urllib.request import urlretrieve
import os
if not os.path.isfile('datasets/mini.h5'):
    print("Downloading Conceptnet Numberbatch word embeddings...")
    conceptnet_url = '[http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'](http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5')
    urlretrieve(conceptnet_url, 'datasets/mini.h5')
```

è¦è¯»å– h5 æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`h5py`åŒ…ã€‚å¦‚æœæ‚¨éµå¾ªäº† PyTorch åœ¨ 1A çš„å®‰è£…è¯´æ˜ï¼Œæ‚¨åº”è¯¥å·²ç»ä¸‹è½½äº†å®ƒã€‚å¦åˆ™ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨

```
*# If you environment isn't currently active, activate it:*
*# conda activate pytorch*pip install h5py
```

æ‚¨å¯èƒ½éœ€è¦é‡æ–°æ‰“å¼€æ­¤ç¬”è®°æœ¬ï¼Œå®‰è£…æ‰èƒ½ç”Ÿæ•ˆã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬ç”¨åŒ…æ‰“å¼€åˆšåˆšä¸‹è½½çš„`mini.h5`æ–‡ä»¶ã€‚æˆ‘ä»¬ä»æ–‡ä»¶ä¸­æå–ä¸€ä¸ª utf-8 ç¼–ç çš„å•è¯åˆ—è¡¨ï¼Œä»¥åŠå®ƒä»¬çš„ 300 ç»´å‘é‡ã€‚

```
# Load the file and pull out words and embeddings
import h5pywith h5py.File('datasets/mini.h5', 'r') as f:
    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]
    all_embeddings = f['mat']['block0_values'][:]

print("all_words dimensions: {}".format(len(all_words)))
print("all_embeddings dimensions: {}".format(all_embeddings.shape))print("Random example word: {}".format(all_words[1337]))
```

ç°åœ¨ï¼Œ`all_words`æ˜¯ä¸€åˆ—ğ‘‰å­—ç¬¦ä¸²(æˆ‘ä»¬ç§°ä¹‹ä¸º*è¯æ±‡è¡¨*)ï¼Œè€Œ`all_embeddings`æ˜¯ä¸€ä¸ªğ‘‰Ã—300 çŸ©é˜µã€‚ç´å¼¦çš„å½¢å¼æ˜¯`/c/language_code/word`â€”â€”ä¾‹å¦‚`/c/en/cat`å’Œ`/c/es/gato`ã€‚

æˆ‘ä»¬åªå¯¹è‹±è¯­å•è¯æ„Ÿå…´è¶£ã€‚æˆ‘ä»¬ä½¿ç”¨ Python list comprehensions æ¥æå–è‹±è¯­å•è¯çš„ç´¢å¼•ï¼Œç„¶ååªæå–è‹±è¯­å•è¯(å»æ‰å…­ä¸ªå­—ç¬¦çš„`/c/en/`å‰ç¼€)åŠå…¶åµŒå…¥ã€‚

```
# Restrict our vocabulary to just the English words
english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]
english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]
english_embeddings = all_embeddings[english_word_indices]print("Number of English words in all_words: {0}".format(len(english_words)))
print("english_embeddings dimensions: {0}".format(english_embeddings.shape))print(english_words[1337])
```

å•è¯å‘é‡çš„å¤§å°ä¸å¦‚å®ƒçš„æ–¹å‘é‡è¦ï¼›æ•°é‡å¯ä»¥è¢«è®¤ä¸ºä»£è¡¨ä½¿ç”¨çš„é¢‘ç‡ï¼Œä¸å•è¯çš„è¯­ä¹‰æ— å…³ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å¯¹è¯­ä¹‰æ„Ÿå…´è¶£ï¼Œæ‰€ä»¥æˆ‘ä»¬*å½’ä¸€åŒ–*æˆ‘ä»¬çš„å‘é‡ï¼Œå°†æ¯ä¸ªå‘é‡é™¤ä»¥å®ƒçš„é•¿åº¦ã€‚ç»“æœæ˜¯ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„å•è¯å‘é‡çš„é•¿åº¦éƒ½æ˜¯ 1ï¼Œå› æ­¤ï¼Œä½äºå•ä½åœ†ä¸Šã€‚ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ä¸å®ƒä»¬ä¹‹é—´è§’åº¦çš„ä½™å¼¦æˆæ­£æ¯”ï¼Œå¹¶æä¾›äº†ç›¸ä¼¼æ€§çš„åº¦é‡(ä½™å¼¦è¶Šå¤§ï¼Œè§’åº¦è¶Šå°)ã€‚

![](img/594e93f8496fd204ae3fe7c018e1e351.png)

å›¾ç‰‡æ¥è‡ªä½œè€…

```
import numpy as npnorms = np.linalg.norm(english_embeddings, axis=1)
normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])
```

æˆ‘ä»¬æƒ³æ–¹ä¾¿åœ°æŸ¥æ‰¾å•è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå­—å…¸ï¼Œå®ƒå°†æˆ‘ä»¬ä»ä¸€ä¸ªå•è¯æ˜ å°„åˆ°å®ƒåœ¨å•è¯åµŒå…¥çŸ©é˜µä¸­çš„ç´¢å¼•ã€‚

```
index = {word: i for i, word in enumerate(english_words)}
```

ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½æµ‹é‡å•è¯å¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬ç”¨ NumPy å–ç‚¹ç§¯ã€‚

```
def similarity_score(w1, w2):
    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])
    return score# A word is as similar with itself as possible:
print('cat\tcat\t', similarity_score('cat', 'cat'))# Closely related words still get high scores:
print('cat\tfeline\t', similarity_score('cat', 'feline'))
print('cat\tdog\t', similarity_score('cat', 'dog'))# Unrelated words, not so much
print('cat\tmoo\t', similarity_score('cat', 'moo'))
print('cat\tfreeze\t', similarity_score('cat', 'freeze'))# Antonyms are still considered related, sometimes more so than synonyms
print('antonym\topposite\t', similarity_score('antonym', 'opposite'))
print('antonym\tsynonym\t', similarity_score('antonym', 'synonym'))int('antonym\tsynonym\t', similarity_score('antonym', 'synonym'))
```

ä¾‹å¦‚ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ‰¾åˆ°ä¸ç»™å®šå•è¯æœ€ç›¸ä¼¼çš„å•è¯

```
def closest_to_vector(v, n):
    all_scores = np.dot(normalized_embeddings, v)
    best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))
    return best_words[:n]def most_similar(w, n):
    return closest_to_vector(normalized_embeddings[index[w], :], n)print(most_similar('cat', 10))
print(most_similar('dog', 10))
print(most_similar('duke', 10))
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨`closest_to_vector`æ¥å¯»æ‰¾æˆ‘ä»¬è‡ªå·±åˆ›é€ çš„â€œé™„è¿‘â€çš„å•è¯å‘é‡ã€‚è¿™è®©æˆ‘ä»¬èƒ½å¤Ÿè§£å†³ç±»æ¯”ã€‚æ¯”å¦‚ä¸ºäº†è§£å†³ç±»æ¯”â€œç”·:å¼Ÿ::å¥³:ï¼Ÿâ€ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸€ä¸ªæ–°çš„å‘é‡`brother - man + woman`:å…„å¼Ÿçš„æ„æ€ï¼Œå‡å»ç”·äººçš„æ„æ€ï¼ŒåŠ ä¸Šå¥³äººçš„æ„æ€ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®å“ªäº›å•è¯åœ¨åµŒå…¥ç©ºé—´ä¸­æœ€æ¥è¿‘è¿™ä¸ªæ–°å‘é‡ã€‚

```
def solve_analogy(a1, b1, a2):
    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]
    return closest_to_vector(b2, 1)print(solve_analogy("man", "brother", "woman"))
print(solve_analogy("man", "husband", "woman"))
print(solve_analogy("spain", "madrid", "france"))
```

è¿™ä¸‰ä¸ªç»“æœéƒ½æŒºå¥½çš„ï¼Œä½†æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç±»æ¯”çš„ç»“æœå¯èƒ½ä¼šä»¤äººå¤±æœ›ã€‚å°è¯•å…¶ä»–ç±»æ¯”ï¼Œçœ‹çœ‹ä½ æ˜¯å¦èƒ½æƒ³å‡ºåŠæ³•æ¥è§£å†³ä½ æ³¨æ„åˆ°çš„é—®é¢˜(ä¾‹å¦‚ï¼Œå¯¹`solve_analogy()`ç®—æ³•çš„ä¿®æ”¹)ã€‚

# åœ¨æ·±åº¦æ¨¡å‹ä¸­ä½¿ç”¨å•è¯åµŒå…¥

å•è¯åµŒå…¥å¾ˆæœ‰è¶£ï¼Œä½†å®ƒä»¬çš„ä¸»è¦ç”¨é€”æ˜¯è®©æˆ‘ä»¬è®¤ä¸ºå•è¯å­˜åœ¨äºè¿ç»­çš„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­ï¼›ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°æœ‰çš„å…·æœ‰è¿ç»­æ•°å­—æ•°æ®çš„æœºå™¨å­¦ä¹ æŠ€æœ¯(å¦‚é€»è¾‘å›å½’æˆ–ç¥ç»ç½‘ç»œ)æ¥å¤„ç†æ–‡æœ¬ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªç‰¹åˆ«ç®€å•çš„ç‰ˆæœ¬ã€‚æˆ‘ä»¬å°†å¯¹ä¸€ç»„ç”µå½±è¯„è®ºè¿›è¡Œ*æƒ…æ„Ÿåˆ†æ*:ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†å°è¯•æ ¹æ®æ–‡æœ¬å°†ç”µå½±è¯„è®ºåˆ†ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å•è¯åµŒå…¥æ¨¡å‹æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å°†ä»£è¡¨ä¸€ç¯‡è¯„è®ºï¼Œå› ä¸º*è¡¨ç¤ºå•è¯åœ¨è¯„è®ºä¸­çš„åµŒå…¥*ã€‚ç„¶åæˆ‘ä»¬å°†è®­ç»ƒä¸€ä¸ªä¸¤å±‚ MLP(ä¸€ä¸ªç¥ç»ç½‘ç»œ)æ¥å°†è¯„è®ºåˆ†ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚æ­£å¦‚ä½ å¯èƒ½çŒœåˆ°çš„é‚£æ ·ï¼Œä»…ä»…ä½¿ç”¨åµŒå…¥çš„å¹³å‡å€¼ä¼šä¸¢å¼ƒå¥å­ä¸­çš„å¤§é‡ä¿¡æ¯ï¼Œä½†æ˜¯å¯¹äºæƒ…æ„Ÿåˆ†æè¿™æ ·çš„ä»»åŠ¡ï¼Œå®ƒä¼šæœ‰æƒŠäººçš„æ•ˆæœã€‚

å¦‚æœæ‚¨è¿˜æ²¡æœ‰ï¼Œè¯·ä¸‹è½½`movie-simple.txt`æ–‡ä»¶ã€‚è¯¥æ–‡ä»¶çš„æ¯ä¸€è¡ŒåŒ…å«

1.  æ•°å­— 0(è¡¨ç¤ºè´Ÿæ•°)æˆ–æ•°å­— 1(è¡¨ç¤ºæ­£æ•°)ï¼Œåé¢è·Ÿç€
2.  ä¸€ä¸ªåˆ¶è¡¨ç¬¦(ç©ºç™½å­—ç¬¦)ï¼Œç„¶å
3.  è¯„è®ºæœ¬èº«ã€‚

è®©æˆ‘ä»¬é¦–å…ˆè¯»å–æ•°æ®æ–‡ä»¶ï¼Œå°†æ¯ä¸€è¡Œè§£ææˆä¸€ä¸ªè¾“å…¥è¡¨ç¤ºåŠå…¶å¯¹åº”çš„æ ‡ç­¾ã€‚åŒæ ·ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ SWEMï¼Œæˆ‘ä»¬å°†æŠŠæ‰€æœ‰å•è¯çš„å•è¯åµŒå…¥çš„å¹³å‡å€¼ä½œä¸ºæˆ‘ä»¬çš„è¾“å…¥ã€‚

```
import string
remove_punct=str.maketrans('','',string.punctuation)# This function converts a line of our data file into
# a tuple (x, y), where x is 300-dimensional representation
# of the words in a review, and y is its label.
def convert_line_to_example(line):
    # Pull out the first character: that's our label (0 or 1)
    y = int(line[0])

    # Split the line into words using Python's split() function
    words = line[2:].translate(remove_punct).lower().split()

    # Look up the embeddings of each word, ignoring words not
    # in our pretrained vocabulary.
    embeddings = [normalized_embeddings[index[w]] for w in words
                  if w in index]

    # Take the mean of the embeddings
    x = np.mean(np.vstack(embeddings), axis=0)
    return x, y# Apply the function to each line in the file.
xs = []
ys = []
with open("datasets/movie-simple.txt", "r", encoding='utf-8', errors='ignore') as f:
    for l in f.readlines():
        x, y = convert_line_to_example(l)
        xs.append(x)
        ys.append(y)# Concatenate all examples into a numpy array
xs = np.vstack(xs)
ys = np.vstack(ys)print("Shape of inputs: {}".format(xs.shape))
print("Shape of labels: {}".format(ys.shape))num_examples = xs.shape[0]
```

æ³¨æ„ï¼Œåœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œä½œä¸ºé¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„è¾“å…¥å•è¯å·²ç»è¢«è½¬æ¢ä¸ºå‘é‡ã€‚ä¸å­¦ä¹ å•è¯åµŒå…¥ç›¸åï¼Œè¿™å®è´¨ä¸Šæ˜¯åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å°†æˆ‘ä»¬çš„å•è¯åµŒå…¥é”å®šåœ¨é€‚å½“çš„ä½ç½®ã€‚å­¦ä¹ å•è¯åµŒå…¥ï¼Œæ— è®ºæ˜¯ä»é›¶å¼€å§‹è¿˜æ˜¯ä»ä¸€äº›é¢„å…ˆè®­ç»ƒçš„åˆå§‹åŒ–ä¸­è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸éƒ½æ˜¯å¯å–çš„ï¼Œå› ä¸ºå®ƒä½¿å®ƒä»¬ä¸“é—¨ç”¨äºç‰¹å®šçš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ç›¸å¯¹è¾ƒå°ï¼Œè€Œä¸”æˆ‘ä»¬çš„è®¡ç®—é¢„ç®—ä¹Ÿæœ‰é™ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æ”¾å¼ƒå­¦ä¹ è¿™ä¸ªæ¨¡å‹çš„å•è¯åµŒå…¥ã€‚æˆ‘ä»¬ç¨åå°†å†æ¬¡è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»è§£æäº†æ•°æ®ï¼Œè®©æˆ‘ä»¬ä¿å­˜ 20%çš„æ•°æ®(å››èˆäº”å…¥ä¸ºæ•´æ•°)ç”¨äºæµ‹è¯•ï¼Œå…¶ä½™çš„ç”¨äºè®­ç»ƒã€‚æˆ‘ä»¬åŠ è½½çš„æ–‡ä»¶é¦–å…ˆæœ‰æ‰€æœ‰çš„è´Ÿé¢è¯„ä»·ï¼Œç„¶åæ˜¯æ‰€æœ‰çš„æ­£é¢è¯„ä»·ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨å°†å®ƒæ‹†åˆ†æˆè®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†ä¹‹å‰å¯¹å…¶è¿›è¡Œæ´—ç‰Œã€‚ç„¶åæˆ‘ä»¬å°†æ•°æ®è½¬æ¢æˆ PyTorch å¼ é‡ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬è¾“å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚

```
print("First 20 labels before shuffling: {0}".format(ys[:20, 0]))shuffle_idx = np.random.permutation(num_examples)
xs = xs[shuffle_idx, :]
ys = ys[shuffle_idx, :]print("First 20 labels after shuffling: {0}".format(ys[:20, 0]))import torchnum_train = 4*num_examples // 5x_train = torch.tensor(xs[:num_train])
y_train = torch.tensor(ys[:num_train], dtype=torch.float32)x_test = torch.tensor(xs[num_train:])
y_test = torch.tensor(ys[num_train:], dtype=torch.float32)
```

æˆ‘ä»¬å¯ä»¥åœ¨å°†æ¯ä¸€æ‰¹æ•°æ®è¾“å…¥åˆ°æ¨¡å‹ä¸­æ—¶åˆ†åˆ«å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–ï¼Œä½†æ˜¯ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª TensorDataset å’Œ DataLoaderï¼Œå°±åƒæˆ‘ä»¬è¿‡å»å¯¹ MNIST ä½¿ç”¨çš„é‚£æ ·ã€‚

```
reviews_train = torch.utils.data.TensorDataset(x_train, y_train)
reviews_test = torch.utils.data.TensorDataset(x_test, y_test)train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)
test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)
```

æ˜¯æ—¶å€™åœ¨ PyTorch ä¸­æ„å»ºæ¨¡å‹äº†ã€‚

```
import torch.nn as nn
import torch.nn.functional as F
```

é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºæ¨¡å‹ï¼Œç»„ç»‡æˆä¸€ä¸ª`nn.Module`ã€‚æˆ‘ä»¬å¯ä»¥å°† MLP çš„è¾“å‡ºæ•°è®¾ä¸ºè¯¥æ•°æ®é›†çš„ç±»æ•°(å³ 2)ã€‚ç„¶è€Œï¼Œç”±äºæˆ‘ä»¬è¿™é‡Œåªæœ‰ä¸¤ä¸ªè¾“å‡ºç±»(â€œæ­£â€ä¸â€œè´Ÿâ€)ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªè¾“å‡ºå€¼ï¼Œå°†æ‰€æœ‰å¤§äº 00 çš„ç§°ä¸ºâ€œæ­£â€ï¼Œå°†æ‰€æœ‰å°äº 00 çš„ç§°ä¸ºâ€œè´Ÿâ€ã€‚å¦‚æœæˆ‘ä»¬é€šè¿‡ sigmoid è¿ç®—ä¼ é€’æ­¤è¾“å‡ºï¼Œåˆ™å€¼è¢«æ˜ å°„åˆ°[0ï¼Œ1][0ï¼Œ1]ï¼Œ0.50.5 æ˜¯åˆ†ç±»é˜ˆå€¼ã€‚

```
class SWEM(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(300, 64)
        self.fc2 = nn.Linear(64, 1)def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x
```

ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå› ä¸ºæˆ‘ä»¬åªè¿›è¡ŒäºŒè¿›åˆ¶åˆ†ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨äºŒè¿›åˆ¶äº¤å‰ç†µ(BCE)æŸå¤±ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„äº¤å‰ç†µæŸå¤±ã€‚ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œå¸¦é€»è¾‘â€çš„ç‰ˆæœ¬ã€‚

```
## Training
# Instantiate model
model = SWEM()# Binary cross-entropy (BCE) Loss and Adam Optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)# Iterate through train set minibatchs 
for epoch in range(250):
    correct = 0
    num_examples = 0
    for inputs, labels in train_loader:
        # Zero out the gradients
        optimizer.zero_grad()

        # Forward pass
        y = model(inputs)
        loss = criterion(y, labels)

        # Backward pass
        loss.backward()
        optimizer.step()

        predictions = torch.round(torch.sigmoid(y))
        correct += torch.sum((predictions == labels).float())
        num_examples += len(inputs)

    # Print training progress
    if epoch % 25 == 0:
        acc = correct/num_examples
        print("Epoch: {0} \t Train Loss: {1} \t Train Acc: {2}".format(epoch, loss, acc))## Testing
correct = 0
num_test = 0with torch.no_grad():
    # Iterate through test set minibatchs 
    for inputs, labels in test_loader:
        # Forward pass
        y = model(inputs)

        predictions = torch.round(torch.sigmoid(y))
        correct += torch.sum((predictions == labels).float())
        num_test += len(inputs)

print('Test accuracy: {}'.format(correct/num_test))
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å­¦ä¹ äº†ä»€ä¹ˆï¼Œçœ‹çœ‹å®ƒå¦‚ä½•å“åº”ä¸åŒå•è¯çš„å•è¯å‘é‡:

```
# Check some words
words_to_test = ["exciting", "hated", "boring", "loved"]for word in words_to_test:
    x = torch.tensor(normalized_embeddings[index[word]].reshape(1, 300))
    print("Sentiment of the word '{0}': {1}".format(word, torch.sigmoid(model(x))))
```

è¯•è¯•è‡ªå·±çš„ä¸€äº›è¯å§ï¼ä½ ä¹Ÿå¯ä»¥å°è¯•æ”¹å˜æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒå®ƒï¼Œçœ‹çœ‹ç»“æœå¦‚ä½•å˜åŒ–ã€‚èƒ½å¦ä¿®æ”¹æ¶æ„ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Ÿæˆ–è€…ï¼Œä½ èƒ½åœ¨ä¸ç‰ºç‰²å¤ªå¤šå‡†ç¡®æ€§çš„æƒ…å†µä¸‹ç®€åŒ–æ¨¡å‹å—ï¼Ÿå¦‚æœä½ å°è¯•ç›´æ¥å¯¹å‡å€¼åµŒå…¥è¿›è¡Œåˆ†ç±»å‘¢ï¼Ÿ

# å­¦ä¹ å•è¯åµŒå…¥

åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥ï¼Œä½†æ²¡æœ‰å­¦ä¹ å®ƒä»¬ã€‚å•è¯åµŒå…¥æ˜¯é¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªè®­ç»ƒä¸­ä¿æŒä¸å˜ã€‚å¦‚æœæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å–œæ¬¢å­¦ä¹ å•è¯ embeddings å’Œæˆ‘ä»¬çš„æ¨¡å‹ã€‚é¢„è®­ç»ƒå•è¯åµŒå…¥é€šå¸¸æ˜¯åœ¨å…·æœ‰æ— ç›‘ç£ç›®æ ‡çš„å¤§å‹è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œå¹¶ä¸”é€šå¸¸æ˜¯éç‰¹å®šçš„ã€‚å¦‚æœæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å–œæ¬¢å­¦ä¹ å•è¯åµŒå…¥ï¼Œè¦ä¹ˆä»é›¶å¼€å§‹ï¼Œè¦ä¹ˆé€šè¿‡å¾®è°ƒï¼Œå› ä¸ºä½¿å®ƒä»¬ç‰¹å®šäºä»»åŠ¡å¯èƒ½ä¼šæé«˜æ€§èƒ½ã€‚

æˆ‘ä»¬å¦‚ä½•å­¦ä¹ å•è¯åµŒå…¥ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©å®ƒä»¬æˆä¸ºæˆ‘ä»¬æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯åŠ è½½æ•°æ®çš„ä¸€éƒ¨åˆ†ã€‚åœ¨ PyTorch ä¸­ï¼Œè¿™æ ·åšçš„é¦–é€‰æ–¹å¼æ˜¯ä½¿ç”¨`nn.Embedding`ã€‚åƒæˆ‘ä»¬è§è¿‡çš„å…¶ä»–`nn`å±‚(ä¾‹å¦‚`nn.Linear`)ä¸€æ ·ï¼Œ`nn.Embedding`å¿…é¡»é¦–å…ˆè¢«å®ä¾‹åŒ–ã€‚å®ä¾‹åŒ–æœ‰ä¸¤ä¸ªå¿…éœ€çš„å‚æ•°ï¼Œå³åµŒå…¥çš„æ•°é‡(å³è¯æ±‡å¤§å°ğ‘‰)å’Œå•è¯åµŒå…¥çš„ç»´åº¦(åœ¨å‰é¢çš„ä¾‹å­ä¸­æ˜¯ 300)ã€‚

```
VOCAB_SIZE = 5000
EMBED_DIM = 300embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)embedding **=** nn.Embedding(VOCAB_SIZE, EMBED_DIM)
```

åœ¨å¼•æ“ç›–ä¸‹ï¼Œè¿™åˆ›å»ºäº†ä¸€ä¸ª 5000Ã—300 çš„å•è¯åµŒå…¥çŸ©é˜µã€‚

```
embedding.weight.size()
```

è¯·æ³¨æ„ï¼Œè¿™ä¸ªçŸ©é˜µåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ª 300 ç»´çš„å•è¯åµŒå…¥ 5000 ä¸ªå•è¯ä¸­çš„æ¯ä¸€ä¸ªï¼Œå †å åœ¨å½¼æ­¤çš„é¡¶éƒ¨ã€‚åœ¨è¿™ä¸ªåµŒå…¥çŸ©é˜µä¸­æŸ¥æ‰¾ä¸€ä¸ªå•è¯åµŒå…¥ï¼Œå°±æ˜¯ç®€å•åœ°é€‰æ‹©è¿™ä¸ªçŸ©é˜µçš„ç‰¹å®šè¡Œï¼Œå¯¹åº”äºè¿™ä¸ªå•è¯ã€‚

å½“å­¦ä¹ å•è¯åµŒå…¥æ—¶ï¼Œ`nn.Embedding`æŸ¥æ‰¾é€šå¸¸æ˜¯æ¨¡å‹æ¨¡å—ä¸­çš„ç¬¬ä¸€ä¸ªæ“ä½œã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¦ä¸ºä¹‹å‰çš„ SWEM æ¨¡å‹å­¦ä¹ å•è¯ embeddingsï¼Œè¯¥æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·:

```
class SWEMWithEmbeddings(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.fc1 = nn.Linear(embedding_size, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, num_outputs)def forward(self, x):
        x = self.embedding(x)
        x = torch.mean(x, dim=0)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x
```

è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¨¡å‹å„å±‚çš„å¤§å°æŠ½è±¡ä¸ºæ„é€ å‡½æ•°å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šè¿™äº›è¶…å‚æ•°ã€‚

```
model = SWEMWithEmbeddings(
    vocab_size = 5000,
    embedding_size = 300, 
    hidden_dim = 64, 
    num_outputs = 1,
)
print(model)
```

æ³¨æ„ï¼Œé€šè¿‡ä½¿åµŒå…¥æˆä¸ºæˆ‘ä»¬æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå¯¹`forward()`å‡½æ•°çš„é¢„æœŸè¾“å…¥ç°åœ¨æ˜¯è¾“å…¥å¥å­çš„å•è¯æ ‡è®°ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿå¿…é¡»ä¿®æ”¹æˆ‘ä»¬çš„æ•°æ®è¾“å…¥ç®¡é“ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç¬”è®°æœ¬(4B)ä¸­çœ‹åˆ°å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚

# é€’å½’ç¥ç»ç½‘ç»œ

åœ¨æ·±åº¦å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œåºåˆ—æ•°æ®é€šå¸¸ç”¨é€’å½’ç¥ç»ç½‘ç»œ(RNNs)å»ºæ¨¡ã€‚ç”±äºè‡ªç„¶è¯­è¨€å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ä¸ªå•è¯åºåˆ—ï¼Œè‡ªç„¶ç¥ç»ç½‘ç»œé€šå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„å…¨è¿æ¥å’Œå·ç§¯ç½‘ç»œä¸€æ ·ï¼Œrnn ä½¿ç”¨çº¿æ€§å’Œéçº¿æ€§å˜æ¢çš„ç»„åˆæ¥å°†è¾“å…¥æŠ•å½±åˆ°æ›´é«˜çº§åˆ«çš„è¡¨ç¤ºä¸­ï¼Œè¿™äº›è¡¨ç¤ºå¯ä»¥ä¸å…¶ä»–å±‚å †å åœ¨ä¸€èµ·ã€‚

## ä½œä¸ºåºåˆ—çš„å¥å­

é¡ºåºæ¨¡å‹ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ¨¡å‹ä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºâ€œæ—¶é—´â€ç»´åº¦çš„å­˜åœ¨:å¥å­(æˆ–æ®µè½ã€æ–‡æ¡£)ä¸­çš„å•è¯æœ‰ä¸€ä¸ªä¼ è¾¾æ„ä¹‰çš„é¡ºåº:

![](img/9efa74e720d958748f13114271dc1d36.png)

ä½œè€…å›¾ç‰‡

åœ¨ä¸Šé¢çš„ç¤ºä¾‹åºåˆ—ä¸­ï¼Œå•è¯â€œRecurrentâ€æ˜¯ğ‘¡=1 å•è¯ï¼Œæˆ‘ä»¬ç”¨ğ‘¤1 è¡¨ç¤ºï¼›åŒæ ·ï¼Œâ€œç¥ç»â€æ˜¯ğ‘¤2ï¼Œç­‰ç­‰ã€‚æ­£å¦‚å‰é¢å‡ èŠ‚æ‰€å¸Œæœ›ç»™ä½ ç•™ä¸‹çš„å°è±¡ï¼Œå°†å•è¯å»ºæ¨¡ä¸ºåµŒå…¥å‘é‡ğ‘¥1,â€¦,ğ‘¥ğ‘‡é€šå¸¸æ¯”ä¸€é”®å‘é‡(ğ‘¤1,â€¦ğ‘¤ğ‘‡å¯¹åº”çš„ä»¤ç‰Œ)æ›´æœ‰åˆ©ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥é€šå¸¸æ˜¯ä¸ºæ¯ä¸ªè¾“å…¥å•è¯åšä¸€ä¸ªåµŒå…¥è¡¨æŸ¥æ‰¾ã€‚è®©æˆ‘ä»¬å‡è®¾ 300 ç»´çš„å•è¯åµŒå…¥ï¼Œå¹¶ä¸”ä¸ºäº†ç®€å•èµ·è§ï¼Œå‡è®¾ä¸€ä¸ªå¤§å°ä¸º 1 çš„å°æ‰¹é‡ã€‚

```
mb = 1
x_dim = 300 
sentence = ["recurrent", "neural", "networks", "are", "great"]xs = []
for word in sentence:
    xs.append(torch.tensor(normalized_embeddings[index[word]]).view(1, x_dim))

xs = torch.stack(xs, dim=0)
print("xs shape: {}".format(xs.shape))
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†è¾“å…¥æ ¼å¼åŒ–ä¸º(å•è¯Ã—å°æ‰¹Ã—åµŒå…¥å°ºå¯¸å•è¯Ã—å°æ‰¹Ã—åµŒå…¥å°ºå¯¸)ã€‚è¿™æ˜¯ PyTorch RNNs çš„é¦–é€‰è¾“å…¥é¡ºåºã€‚

å‡è®¾æˆ‘ä»¬æƒ³è¦å¤„ç†è¿™ä¸ªä¾‹å­ã€‚åœ¨æˆ‘ä»¬ä¹‹å‰çš„æƒ…æ„Ÿåˆ†æä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªæ˜¯å–äº†ä¸€æ®µæ—¶é—´å†…çš„å¹³å‡åµŒå…¥ï¼Œå°†è¾“å…¥è§†ä¸ºä¸€ä¸ªâ€œå•è¯åŒ…â€å¯¹äºç®€å•çš„é—®é¢˜ï¼Œè¿™å¯ä»¥å‡ºå¥‡åœ°å¥½ï¼Œä½†æ˜¯æ­£å¦‚æ‚¨å¯èƒ½æƒ³è±¡çš„é‚£æ ·ï¼Œå¥å­ä¸­å•è¯çš„é¡ºåºé€šå¸¸å¾ˆé‡è¦ï¼Œæœ‰æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¯¹è¿™ç§æ—¶é—´æ„ä¹‰è¿›è¡Œå»ºæ¨¡ã€‚è¾“å…¥ RNNsã€‚

## å›é¡¾:å…¨è¿æ¥å±‚

åœ¨æˆ‘ä»¬ä»‹ç» RNN ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å†æ¬¡å›é¡¾ä¸€ä¸‹æˆ‘ä»¬åœ¨é€»è¾‘å›å½’å’Œå¤šå±‚æ„ŸçŸ¥å™¨ç¤ºä¾‹ä¸­ä½¿ç”¨çš„å…¨è¿æ¥å±‚ï¼Œåœ¨ç¬¦å·ä¸Šæœ‰ä¸€äº›å˜åŒ–:

![](img/a508948bc8fe8eed1f6b267e05f9283b.png)

ä½œè€…å›¾ç‰‡

å¯¹äºéšè—çŠ¶æ€ï¼Œæˆ‘ä»¬å°†æŠŠå…¨è¿æ¥å›¾å±‚çš„ç»“æœç§°ä¸ºâ„ï¼Œè€Œä¸æ˜¯ğ‘¦ã€‚å˜é‡ğ‘¦é€šå¸¸ä¿ç•™ç»™ç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚ï¼›å› ä¸ºé€»è¾‘å›å½’æ˜¯å•å±‚çš„ï¼Œæ‰€ä»¥ä½¿ç”¨ğ‘¦å°±å¯ä»¥äº†ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾æœ‰ä¸€ä¸ªä»¥ä¸Šçš„å±‚ï¼Œæ›´å¸¸è§çš„æ˜¯å°†ä¸­é—´è¡¨ç¤ºç§°ä¸ºâ„.æ³¨æ„ï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨ğ‘“(æ¥è¡¨ç¤ºéçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚è¿‡å»ï¼Œæˆ‘ä»¬å°†ğ‘“()è§†ä¸º ReLUï¼Œä½†è¿™ä¹Ÿå¯èƒ½æ˜¯ğœ()æˆ– tanh()éçº¿æ€§ã€‚å¯è§†åŒ–:

![](img/ec7bf025c4d6c71a01bdc6d4e4493ce4.png)

ä½œè€…å›¾ç‰‡

è¿™é‡Œè¦æ³¨æ„çš„å…³é”®æ˜¯ï¼Œæˆ‘ä»¬ç”¨çº¿æ€§å˜æ¢(ç”¨ğ‘ŠW å’Œğ‘b)æŠ•å°„è¾“å…¥ğ‘¥xï¼Œç„¶åå¯¹è¾“å‡ºåº”ç”¨éçº¿æ€§ï¼Œåœ¨è®­ç»ƒæœŸé—´ç»™æˆ‘ä»¬â„h.ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å­¦ä¹ ğ‘ŠW å’Œğ‘b.

## åŸºæœ¬çš„ RNN

ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ä½¿ç”¨å…¨è¿æ¥å›¾å±‚çš„ç¤ºä¾‹ä¸åŒï¼Œé¡ºåºæ•°æ®å…·æœ‰å¤šä¸ªè¾“å…¥ğ‘¥1,â€¦,ğ‘¥ğ‘‡ï¼Œè€Œä¸æ˜¯å•ä¸ªğ‘¥.æˆ‘ä»¬éœ€è¦æ ¹æ® RNN çš„æƒ…å†µè°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹ã€‚è™½ç„¶æœ‰å‡ ç§å˜ä½“ï¼Œä½† RNN çš„ä¸€ç§å¸¸è§åŸºæœ¬é…æ–¹æ˜¯åŸƒå°”æ›¼ RNNï¼Œå¦‚ä¸‹æ‰€ç¤º*:

â„ğ‘¡=tanh((ğ‘¥ğ‘¡ğ‘Šğ‘¥+ğ‘ğ‘¥)+(â„ğ‘¡âˆ’1ğ‘Šâ„+ğ‘â„))

å…¶ä¸­ tanh()t æ˜¯åŒæ›²æ­£åˆ‡ï¼Œä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚RNNs æŒ‰é¡ºåºä¸€æ¬¡å¤„ç†ä¸€ä¸ªå•è¯(ğ‘¥ğ‘¡)ï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥äº§ç”Ÿä¸€ä¸ªéšè—çŠ¶æ€â„ğ‘¡htã€‚ä¸Šé¢ç­‰å¼çš„å‰åŠéƒ¨åˆ†åº”è¯¥çœ‹èµ·æ¥å¾ˆç†Ÿæ‚‰ï¼›ä¸å…¨è¿æ¥å±‚ä¸€æ ·ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªè¾“å…¥ğ‘¥ğ‘¡è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œç„¶ååº”ç”¨éçº¿æ€§ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½åº”ç”¨äº†ç›¸åŒçš„çº¿æ€§å˜æ¢(ğ‘Šğ‘¥ï¼Œğ‘ğ‘¥)ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬è¿˜å¯¹ä¹‹å‰éšè—çš„çŠ¶æ€â„ğ‘¡âˆ’1 åº”ç”¨äº†å•ç‹¬çš„çº¿æ€§å˜æ¢(ğ‘Šâ„ï¼Œğ‘â„),å¹¶å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„æŠ•å½±è¾“å…¥ä¸­ã€‚è¿™ç§åé¦ˆè¢«ç§°ä¸º*å¾ªç¯*è¿æ¥ã€‚

RNN æ¶æ„ä¸­çš„è¿™äº›æœ‰å‘å¾ªç¯èµ‹äºˆäº†å®ƒä»¬å¯¹æ—¶é—´åŠ¨æ€è¿›è¡Œå»ºæ¨¡çš„èƒ½åŠ›ï¼Œä½¿å®ƒä»¬ç‰¹åˆ«é€‚åˆå¯¹åºåˆ—(ä¾‹å¦‚æ–‡æœ¬)è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬å¯ä»¥å°† RNN å›¾å±‚å¯è§†åŒ–å¦‚ä¸‹:

![](img/6b42b1781a910811c4ba8985c02e3da3.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬å¯ä»¥åœ¨æ—¶é—´ä¸­å±•å¼€ä¸€ä¸ª RNNï¼Œè®©å®ƒçš„æ—¶åºæ€§æ›´åŠ æ˜æ˜¾:

![](img/3ede5950ff7ad08d39b33fb52065c7bb.png)

ä½œè€…å›¾ç‰‡

æ‚¨å¯ä»¥å°†è¿™äº›å¾ªç¯è¿æ¥è§†ä¸ºå…è®¸æ¨¡å‹åœ¨è®¡ç®—å½“å‰è¾“å…¥çš„éšè—çŠ¶æ€æ—¶è€ƒè™‘åºåˆ—çš„å…ˆå‰éšè—çŠ¶æ€ã€‚

*æ³¨æ„:æˆ‘ä»¬å®é™…ä¸Šä¸éœ€è¦ä¸¤ä¸ªå•ç‹¬çš„åå·®ğ‘ğ‘¥å’Œğ‘â„ï¼Œå› ä¸ºä½ å¯ä»¥å°†ä¸¤ä¸ªåå·®åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„å¯å­¦ä¹ å‚æ•°ğ‘.ç„¶è€Œï¼Œå•ç‹¬ç¼–å†™å®ƒæœ‰åŠ©äºæ¸…æ¥šåœ°è¡¨æ˜æˆ‘ä»¬æ­£åœ¨å¯¹ğ‘¥ğ‘¡å’Œâ„ğ‘¡âˆ’1.æ‰§è¡Œçº¿æ€§è½¬æ¢è¯´åˆ°ç»„åˆå˜é‡ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡å°†ğ‘¥ğ‘¡xt å’Œâ„ğ‘¡âˆ’1 ä¸²è”æˆå•ä¸ªå‘é‡ğ‘§ğ‘¡ï¼Œç„¶åæ‰§è¡Œå•ä¸ªçŸ©é˜µä¹˜ğ‘§ğ‘¡ğ‘Šğ‘§+ğ‘æ¥è¡¨è¾¾ä¸Šè¿°æ“ä½œï¼Œå…¶ä¸­ğ‘Šğ‘§æœ¬è´¨ä¸Šæ˜¯ğ‘Šğ‘¥å’Œğ‘Šâ„ä¸²è”ã€‚äº‹å®ä¸Šï¼Œè¿™å°±æ˜¯å®ç°çš„â€œå®˜æ–¹â€RNNs æ¨¡å—çš„æ•°é‡ï¼Œå› ä¸ºå•ç‹¬çŸ©é˜µä¹˜æ³•è¿ç®—æ•°é‡çš„å‡å°‘ä½¿å¾—è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚è¿™äº›éƒ½æ˜¯å®ç°ç»†èŠ‚ã€‚

## PyTorch çš„ RNNs

æˆ‘ä»¬å¦‚ä½•åœ¨ PyTorch ä¸­å®ç° RNNï¼Ÿæœ‰ç›¸å½“å¤šçš„æ–¹æ³•ï¼Œä½†è®©æˆ‘ä»¬é¦–å…ˆä»é›¶å¼€å§‹å»ºç«‹åŸƒå°”æ›¼ RNNï¼Œä½¿ç”¨è¾“å…¥åºåˆ—â€œé€’å½’ç¥ç»ç½‘ç»œæ˜¯ä¼Ÿå¤§çš„â€ã€‚

```
# As always, import PyTorch first
import numpy as np
import torch
```

åœ¨ RNN ä¸­ï¼Œæˆ‘ä»¬å°†è¾“å…¥ğ‘¥ğ‘¡å’Œä¹‹å‰éšè—çš„çŠ¶æ€â„ğ‘¡âˆ’1 éƒ½æŠ•å½±åˆ°æŸä¸ªéšè—çš„ç»´åº¦ï¼Œæˆ‘ä»¬å°†é€‰æ‹©è¯¥ç»´åº¦ä¸º 128ã€‚ä¸ºäº†æ‰§è¡Œè¿™äº›æ“ä½œï¼Œæˆ‘ä»¬è¦å®šä¹‰ä¸€äº›æˆ‘ä»¬å°†è¦å­¦ä¹ çš„å˜é‡ã€‚

```
h_dim = 128# For projecting the input
Wx = torch.randn(x_dim, h_dim)/np.sqrt(x_dim)
Wx.requires_grad_()
bx = torch.zeros(h_dim, requires_grad=True)# For projecting the previous state
Wh = torch.randn(h_dim, h_dim)/np.sqrt(h_dim)
Wh.requires_grad_()
bh = torch.zeros(h_dim, requires_grad=True)print(Wx.shape, bx.shape, Wh.shape, bh.shape)
```

ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ä¸º RNN çš„ä¸€ä¸ªæ—¶é—´æ­¥é•¿å®šä¹‰ä¸€ä¸ªå‡½æ•°ã€‚è¯¥å‡½æ•°é‡‡ç”¨å½“å‰è¾“å…¥ğ‘¥ğ‘¡å’Œå…ˆå‰éšè—çš„çŠ¶æ€â„ğ‘¡âˆ’1ï¼Œæ‰§è¡Œçº¿æ€§å˜æ¢ğ‘¥ğ‘Šğ‘¥+ğ‘ğ‘¥å’Œâ„ğ‘Šâ„+ğ‘â„ï¼Œç„¶åæ˜¯åŒæ›²æ­£åˆ‡éçº¿æ€§ã€‚

```
def RNN_step(x, h):
    h_next = torch.tanh((torch.matmul(x, Wx) + bx) + (torch.matmul(h, Wh) + bh))return h_next
```

æˆ‘ä»¬çš„ RNN çš„æ¯ä¸€æ­¥éƒ½éœ€è¦è¾“å…¥(å³å•è¯è¡¨ç¤º)å’Œä¹‹å‰çš„éšè—çŠ¶æ€(ä¹‹å‰åºåˆ—çš„æ€»ç»“)ã€‚æ³¨æ„ï¼Œåœ¨å¥å­çš„å¼€å¤´ï¼Œæˆ‘ä»¬æ²¡æœ‰å…ˆå‰çš„éšè—çŠ¶æ€ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å…¶åˆå§‹åŒ–ä¸ºæŸä¸ªå€¼ï¼Œä¾‹å¦‚ï¼Œå…¨é›¶:

```
# Word embedding for first word
x1 = xs[0, :, :]# Initialize hidden state to 0
h0 = torch.zeros([mb, h_dim])
```

ä¸ºäº†é‡‡å– RNN çš„ä¸€æ¬¡æ€§æ­¥éª¤ï¼Œæˆ‘ä»¬è°ƒç”¨æˆ‘ä»¬ç¼–å†™çš„å‡½æ•°ï¼Œåœ¨ğ‘¥1 å’Œâ„0.ä¼ é€’åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ

```
# Forward pass of one RNN step for time step t=1
h1 = RNN_step(x1, h0)print("Hidden state h1 dimensions: {0}".format(h1.shape))
```

æˆ‘ä»¬å¯ä»¥å†æ¬¡è°ƒç”¨`RNN_step`å‡½æ•°ï¼Œä»æˆ‘ä»¬çš„ RNN ä¸­è·å¾—ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥é•¿è¾“å‡ºã€‚

```
# Word embedding for second word
x2 = xs[1, :, :]# Forward pass of one RNN step for time step t=2
h2 = RNN_step(x2, h1)print("Hidden state h2 dimensions: {0}".format(h2.shape))
```

æˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦ç»§ç»­å±•å¼€ RNNã€‚å¯¹äºæ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬é¦ˆå…¥å½“å‰è¾“å…¥(ğ‘¥ğ‘¡)å’Œå…ˆå‰çš„éšè—çŠ¶æ€(â„ğ‘¡âˆ’1)ä»¥è·å¾—æ–°çš„è¾“å‡ºã€‚

## ä½¿ç”¨`torch.nn`

åœ¨å®è·µä¸­ï¼Œå¾ˆåƒå…¨è¿æ¥å’Œå·ç§¯å±‚ï¼Œæˆ‘ä»¬é€šå¸¸ä¸åƒä¸Šé¢é‚£æ ·ä»å¤´å®ç° RNNsï¼Œè€Œæ˜¯ä¾èµ–äºæ›´é«˜çº§åˆ«çš„ APIã€‚PyTorch åœ¨`torch.nn`åº“ä¸­å®ç°äº† RNNsã€‚

```
import torch.nnrnn = nn.RNN(x_dim, h_dim)
print("RNN parameter shapes: {}".format([p.shape for p in rnn.parameters()]))
```

è¯·æ³¨æ„ï¼Œç”±`torch.nn`åˆ›å»ºçš„ RNN äº§ç”Ÿçš„å‚æ•°ä¸æˆ‘ä»¬ä»ä¸Šé¢çš„ä¾‹å­ä¸­å¾—åˆ°çš„å‚æ•°å…·æœ‰ç›¸åŒçš„ç»´åº¦ã€‚

ä¸ºäº†ä½¿ç”¨ RNN æ‰§è¡Œå‘å‰ä¼ é€’ï¼Œæˆ‘ä»¬å°†æ•´ä¸ªè¾“å…¥åºåˆ—ä¼ é€’ç»™`forward()`å‡½æ•°ï¼Œè¯¥å‡½æ•°è¿”å›æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„éšè—çŠ¶æ€(`hs`)å’Œæœ€ç»ˆçš„éšè—çŠ¶æ€(`h_T`)ã€‚

```
hs, h_T = rnn(xs)print("Hidden states shape: {}".format(hs.shape))
print("Final hidden state shape: {}".format(h_T.shape))
```

æˆ‘ä»¬å¦‚ä½•å¤„ç†è¿™äº›éšè—çŠ¶æ€å‘¢ï¼Ÿè¿™å–å†³äºå‹å·å’Œä»»åŠ¡ã€‚å°±åƒå¤šå±‚æ„ŸçŸ¥å™¨å’Œå·ç§¯ç¥ç»ç½‘ç»œä¸€æ ·ï¼Œrnn ä¹Ÿå¯ä»¥å †å åœ¨å¤šä¸ªå±‚ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å‡ºâ„1,â€¦,â„ğ‘‡æ˜¯ä¸‹ä¸€å±‚çš„é¡ºåºè¾“å…¥ã€‚å¦‚æœ RNN å±‚æ˜¯æœ€ç»ˆå±‚ï¼Œåˆ™â„ğ‘‡æˆ–â„1,â€¦,â„ğ‘‡çš„å¹³å‡å€¼/æœ€å¤§å€¼å¯ä»¥ç”¨ä½œæ•°æ®åºåˆ—çš„æ±‡æ€»ç¼–ç ã€‚é¢„æµ‹çš„ç»“æœä¹Ÿä¼šå¯¹ RNN äº§å‡ºçš„æœ€ç»ˆç”¨é€”äº§ç”Ÿå½±å“ã€‚

## é—¨æ§ RNNs

è™½ç„¶æˆ‘ä»¬åˆšåˆšæ¢ç´¢çš„ rnn å¯ä»¥æˆåŠŸåœ°æ¨¡æ‹Ÿç®€å•çš„åºåˆ—æ•°æ®ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥å¤„ç†è¾ƒé•¿çš„åºåˆ—ï¼Œå…¶ä¸­[æ¶ˆå¤±æ¢¯åº¦](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)æ˜¯ä¸€ä¸ªç‰¹åˆ«å¤§çš„é—®é¢˜ã€‚å¤šå¹´æ¥å·²ç»æå‡ºäº†è®¸å¤š RNN å˜ä½“æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”ç»éªŒè¡¨æ˜è¿™äº›å˜ä½“æ›´åŠ æœ‰æ•ˆã€‚ç‰¹åˆ«æ˜¯ï¼Œé•¿çŸ­æœŸè®°å¿†(LSTM)å’Œé—¨æ§å¾ªç¯å•å…ƒ(GRU)æœ€è¿‘åœ¨æ·±åº¦å­¦ä¹ ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚æˆ‘ä»¬ä¸æ‰“ç®—åœ¨è¿™é‡Œè¯¦ç»†è®¨è®ºå®ƒä»¬ä¸æ™®é€š rnn åœ¨ç»“æ„ä¸Šæœ‰ä»€ä¹ˆä¸åŒï¼›ä¸€ä¸ªå¥‡å¦™çš„æ€»ç»“å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°[ã€‚æ³¨æ„,â€œRNNâ€ä½œä¸ºä¸€ä¸ªåå­—æœ‰ç‚¹è¶…è½½:å®ƒæ—¢å¯ä»¥æŒ‡æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„åŸºæœ¬é€’å½’æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æŒ‡ä¸€èˆ¬çš„é€’å½’æ¨¡å‹(åŒ…æ‹¬ LSTMs å’Œ GRUs)ã€‚](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

åˆ›å»º LSTMs å’Œ GRUs å›¾å±‚çš„æ–¹æ³•ä¸åˆ›å»ºåŸºæœ¬ RNN å›¾å±‚çš„æ–¹æ³•å¤§è‡´ç›¸åŒã€‚åŒæ ·ï¼Œä¸è¦è‡ªå·±å®ç°å®ƒï¼Œå»ºè®®ä½¿ç”¨`torch.nn`å®ç°ï¼Œå°½ç®¡æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨æŸ¥çœ‹æºä»£ç ï¼Œè¿™æ ·æ‚¨å°±èƒ½ç†è§£å¹•åå‘ç”Ÿäº†ä»€ä¹ˆã€‚

```
lstm = nn.LSTM(x_dim, h_dim)
print("LSTM parameters: {}".format([p.shape for p in lstm.parameters()]))gru = nn.GRU(x_dim, h_dim)
print("GRU parameters: {}".format([p.shape for p in gru.parameters()]))
```

# ç«ç‚¬æŠ¥

å°±åƒ PyTorch æ‹¥æœ‰ç”¨äºè®¡ç®—æœºè§†è§‰çš„ [Torchvision](https://pytorch.org/docs/stable/torchvision/index.html) ä¸€æ ·ï¼ŒPyTorch ä¹Ÿæ‹¥æœ‰ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„ [Torchtext](https://torchtext.readthedocs.io/en/latest/) ã€‚ä¸ Torchvision ä¸€æ ·ï¼ŒTorchtext æ‹¥æœ‰å¤§é‡æµè¡Œçš„ NLP åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡(å¦‚æƒ…æ„Ÿåˆ†æã€è¯­è¨€å»ºæ¨¡ã€æœºå™¨ç¿»è¯‘)ã€‚å®ƒä¹Ÿæœ‰ä¸€äº›é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯åµŒå…¥ï¼ŒåŒ…æ‹¬æµè¡Œçš„å•è¯è¡¨ç¤ºå…¨å±€å‘é‡(GloVe)ã€‚å¦‚æœæ‚¨éœ€è¦åŠ è½½è‡ªå·±çš„æ•°æ®é›†ï¼ŒTorchtext æœ‰è®¸å¤šæœ‰ç”¨çš„å®¹å™¨ï¼Œå¯ä»¥ä½¿æ•°æ®ç®¡é“æ›´å®¹æ˜“ã€‚

æ‚¨éœ€è¦å®‰è£… TorchText æ¥ä½¿ç”¨å®ƒ:

```
*# If you environment isn't currently active, activate it:*
*# conda activate pytorch*pip install torchtext
```

ä¸€æ—¦ä½ ç†è§£äº†è¿™å¥è¯ï¼Œä½ å°±å·²ç»å®Œæˆäº† PyTorch ä¸­è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨çš„æ‰€æœ‰æ­¥éª¤

ä»¥ä¸‹æ˜¯æ‚¨ä»Šå¤©çš„æˆå°±æ€»ç»“:

*   **å•è¯åµŒå…¥**
*   **åœ¨æ·±åº¦æ¨¡å‹ä¸­ä½¿ç”¨å•è¯åµŒå…¥**
*   **å­¦ä¹ å•è¯åµŒå…¥**
*   ***é€’å½’ç¥ç»ç½‘ç»œ(RNNs):å¥å­ä½œä¸ºåºåˆ—ï¼Œå¤ä¹ :å…¨è¿æ¥å±‚ï¼ŒåŸºæœ¬ RNNï¼ŒPyTorch ä¸­çš„ RNNsï¼Œä½¿ç”¨ torch.nnï¼Œé—¨æ§ RNNsï¼Œ***
*   **ç«ç‚¬æ–‡æœ¬**