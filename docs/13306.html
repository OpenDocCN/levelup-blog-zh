<html>
<head>
<title>Explaining Each Machine Learning Model In Brief</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简要解释每个机器学习模型</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/explaining-each-machine-learning-model-in-brief-92f82b41ba71?source=collection_archive---------3-----------------------#2022-08-25">https://levelup.gitconnected.com/explaining-each-machine-learning-model-in-brief-92f82b41ba71?source=collection_archive---------3-----------------------#2022-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6cbe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">20种机器学习模型概述从线性回归到XGBoost到DBSCAN聚类直到PCA</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0d61e82e1840c7828b9cc4a5ac726527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DKTvim2HjXkbxz6q"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·温克勒在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="9841" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博客中，我想分享一个资源，它提供了所有机器学习模型的简明解释，从简单的线性回归到XGBoost再到聚类技术。</p><h2 id="0661" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">涵盖的型号</h2><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="a7e1" class="lv lw it mp b gy mt mu l mv mw"><strong class="mp iu">1.  Linear Regression<br/>2.  Polynomial Regression<br/>3.  Ridge Regression<br/>4.  Lasso Regression<br/>5.  Elastic Regression<br/>6.  Logistic Regression<br/>7.  K Nearest-Neighbors<br/>8.  Naive Bayes<br/>9.  Support Vector Machines<br/>10. Decision Trees<br/>11. Random Forest<br/>12. Extra Trees<br/>13. Gradient Boost<br/>14. Ada Boost<br/>15. XGBoost<br/>16. K Means Clustering<br/>17. Hierarchical Clustering<br/>18. DBSCAN Clustering<br/>19. Apriori <br/>20. PCA</strong></span></pre><h1 id="ec64" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated"><span class="l ni nj nk bm nl nm nn no np di"> L </span>线性回归</h1><p id="274b" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">线性回归通过使用最小二乘法找到一条与所有数据点距离最小的“<strong class="lb iu">最佳拟合线</strong>，试图找到自变量和因变量之间的关系。最小二乘法找到最小化残差平方和(SSR)的线性方程。</p><p id="4dc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，下面的绿线比蓝线更适合，因为它与所有数据点的距离都最小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/7461e0c3d554936b1c59f1fc65068e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLpfWH8dzd-sqKV0bCyrQA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图1:作者使用<a class="ae ky" href="http://Canva.com" rel="noopener ugc nofollow" target="_blank">Canva.com</a>创建的图像</figcaption></figure><h1 id="cfcf" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">拉索回归(L1)</h1><p id="b1cc" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">套索回归是一种正则化技术，通过在模型中引入一定量的偏差来减少过度拟合。这是通过最小化残差的平方差并加上惩罚来实现的，其中惩罚等于λ乘以斜率的绝对值。λ指的是惩罚的严厉程度。它作为一个超参数工作，可以改变以减少过度拟合并产生更好的拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/93cc018fc666a367354eb1490bf7e2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vyD2dxwUNdR2dX8XJr4QNQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图2:成本函数套索回归</figcaption></figure><p id="8c8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们有大量要素时，L1正则化是首选，因为它忽略了斜率值非常小的所有变量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/c29b90e4a330b11576abbcba224549f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZCDNTnlTEejTXgi-7ihdA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图3:显示正则化对过度拟合回归线的影响的图表</figcaption></figure><h1 id="cb4f" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">岭回归(L2)</h1><p id="73f5" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">岭回归类似于套索回归。两者唯一的区别是刑期的计算。它增加了一个罚项，等于幅度的平方乘以λ。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/c1ecb760918ff7d7b59f9eb1c9ac86ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-rbgeTNzoep2RvNHgyuFGQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图4:成本函数岭回归</figcaption></figure><p id="a14f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们的数据遭受多重共线性(<em class="ny">独立变量高度相关</em>)时，最好使用L2正则化，因为它将所有系数收缩为零。</p><h1 id="47c4" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">弹性净回归</h1><p id="fc39" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">弹性网回归结合了套索和岭回归的损失，提供了一个更有规律的模型。</p><p id="c9b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它允许两种损失的平衡，与单独使用l1或l2相比，这导致了更好的执行模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/5ccf2070c2a533dccf4f2d6173646834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4Jcyy8nk5sdb1glp.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图5:图片摘自《走向数据科学》文章“<a class="ae ky" href="https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6" rel="noopener" target="_blank">从线性回归到岭回归套索和弹性网</a></figcaption></figure><h1 id="a97b" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">多项式回归</h1><p id="3523" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">它将因变量和自变量之间的关系建模为nᵗʰ次数多项式。多项式是形式为<strong class="lb iu"><em class="ny"/></strong>的项的和，其中<em class="ny"> n </em>为非负整数，<em class="ny"> k </em>为常数，<em class="ny"> x </em>为自变量。它用于非线性数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0244705759c59c4c51d828123ea107e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8WwqKtEFSQaBN2N_nebyw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图6:在非线性数据上拟合简单线性回归和多项式回归直线。</figcaption></figure><h1 id="7baa" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">逻辑回归</h1><p id="29f6" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">逻辑回归是一种分类技术，试图找到数据的最佳拟合曲线。它利用sigmoid函数在范围0和1之间转换输出。与使用最小二乘法找到最佳拟合直线的线性回归不同，逻辑回归使用最大似然估计(MLE) <strong class="lb iu"> </strong>来找到最佳拟合直线(曲线)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b415507796f96da33d33b8dfff4616af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fFU5HSXlaxQyrAwHEFK3Lg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图7:二进制输出的线性回归与逻辑回归</figcaption></figure><h1 id="a5b4" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">k-最近邻(KNN)</h1><p id="1658" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">KNN是一种分类算法，它根据新数据点与最近分类点的距离对其进行分类。它假设彼此非常接近的数据点出口高度相似。</p><p id="be2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">knn算法也被称为懒惰学习器，因为它存储训练数据，直到新的数据点出现时才将其分类到不同的类中进行预测。</p><p id="1d1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，KNN利用欧几里德距离为新数据寻找最近的分类点，采用最近类的<em class="ny">模式</em>为新数据点寻找预测类。</p><p id="92f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果k值设置得很低，那么新的数据点可能被认为是异常值，但是如果它太高，那么它可能会忽略样本很少的类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ac8f8d7efdf4d02f20ff700316c341c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TU6txJJcFB66cB4jvm5Elg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图8:对数据应用KNN之前和之后</figcaption></figure><h1 id="2544" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">朴素贝叶斯</h1><p id="0efe" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">朴素贝叶斯是一种基于贝叶斯定理的分类技术。它主要用于文本分类。</p><p id="cd3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">贝叶斯定理描述了一个事件的概率，它基于可能与该事件相关的条件的先验知识。它陈述了下面的等式—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/4f9c1547b96c5bdcb152476cceeafb83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pb5CtTS6VBfFh1O9"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图9:贝叶斯定理公式</figcaption></figure><p id="30aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">朴素贝叶斯之所以称为朴素贝叶斯，是因为它假设某个特征的出现独立于其他特征的出现。</p><h1 id="47a8" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">支持向量机</h1><p id="483f" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">支持向量机的目标是在n维空间(n个特征)中找到一个超平面，该超平面可以将数据点分成不同的类。它是通过最大化类之间的余量(距离)来找到的。</p><p id="c57c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">支持向量是超平面的封闭数据点，它可以影响超平面的位置和方向，并有助于最大化类之间的间隔。超平面的维数取决于输入特征的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4ae3063abdc299bc7fc1757370690cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YaofmqDcIoMNkF9y_zPvDw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图10:线性可分数据的支持向量机</figcaption></figure><h1 id="fea9" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">决策图表</h1><p id="6fc4" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">决策树是一种基于树的结构化分类器，包含一系列条件语句，用于确定样本到达底部之前的路径。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/001868a6353dd7d4e893b04e3e7822e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Sxg9obN-k4trCzU4lHLAw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图11:一个决策树的例子</figcaption></figure><p id="b48f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树的内部节点代表特征，分支代表决策规则，叶节点代表结果。树的决策节点像if-else条件一样工作，叶节点包含决策节点的输出。</p><p id="7aa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它首先使用属性选择度量(ID3或CART)选择一个属性作为根节点，然后递归地将剩余的属性与其父节点进行比较，以创建子节点，直到树到达其叶节点。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="5a37" class="lv lw it mp b gy mt mu l mv mw"><strong class="mp iu">CART (GINI)<br/></strong>1. Probability Table<br/>2. Calculate the Gini Index for Attribute Values Like Overcast, Sunny, Rain<br/><em class="ny">1 - (P/P+N)² -(N/P+N)²</em></span><span id="c0f7" class="lv lw it mp b gy oc mu l mv mw">3. Calculate Gini Index for Attribute Eg: Outlook<br/><em class="ny">len(sunny) / len(y) *gini(sunny) + ....</em></span><span id="30e3" class="lv lw it mp b gy oc mu l mv mw"><strong class="mp iu">ID3 (INFORMATION GAIN &amp; ENTROPY)<br/></strong>1. Calculate IG For y<br/><em class="ny">IG(Attr) = -[P/P+N] * log[p/P+N] - [N/P+N * log[N/P+N]</em></span><span id="e200" class="lv lw it mp b gy oc mu l mv mw">2. Calculate Entropy for each value of different attributes in by like outlook - overcase,rain,sunny<br/><em class="ny">entropy(Attr=Value) = -[P/P+N] * log[p/P+N] - [N/P+N * log[N/P+N]</em></span><span id="9508" class="lv lw it mp b gy oc mu l mv mw">3. Calculte Gain For Attribute Like outlook<br/><em class="ny">Gain(Outlook) = len(sunny) / len(y) *entropy(sunny) + ....</em></span><span id="26f3" class="lv lw it mp b gy oc mu l mv mw">4. Calculate Total Information Gain<br/><em class="ny">IG(y) - Gain(Attribute)<br/>IG(Outcome) - Gain(Outlook)</em></span></pre><h1 id="0784" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">随机森林</h1><p id="51f2" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">随机森林是由几个决策树组成的集合技术。在构建每棵树时，它使用bagging和特征随机性来创建一个不相关的决策树森林。</p><p id="aa88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林中的每棵树都在不同的数据子集上进行训练，以预测结果，然后选择具有多数投票的结果作为随机森林预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/942611625f9d9b5dde642a3c0291612f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeD0lX7feOFTWHtl4Lxkrw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图12:4个估计量的随机森林分类器。</figcaption></figure><p id="1fe3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果我们只创建了一个决策树，第二个，那么我们的预测将是0类，但依赖于所有四棵树的模式，我们的预测已变为1类，这就是随机森林的力量。</p><h1 id="7ea7" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">额外的树</h1><p id="fb6e" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">Extra Trees非常类似于随机森林分类器，两者之间唯一的区别是它们选择根节点的方式。在随机森林中，最佳特征用于分裂，而在额外的树分类器<strong class="lb iu">中，随机特征被选择用于分裂</strong>，因此额外的树提供了更多的随机性和特征之间非常少的相关性。</p><p id="1782" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两者之间的另一个比较是，随机森林使用bootstrap副本来生成大小为N的子集，用于训练集成成员(决策树)，而额外的树使用整个原始样本。</p><p id="3527" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与随机森林相比，额外树算法在计算上快得多，因为考虑到随机选择分裂点，对于每个决策树，训练直到预测的整个过程是相同的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/3a63919eb14c4666da488a21b79fb229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nijg4NgxkAxM9uHKljfZaQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图13:随机树和额外树的比较</figcaption></figure><h1 id="e6ac" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">ADA增强</h1><p id="4244" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">ADA Boost是一种类似于随机森林的增强算法，但有一些显著的不同</p><ol class=""><li id="ad49" class="od oe it lb b lc ld lf lg li of lm og lq oh lu oi oj ok ol bi translated">ADA Boost不是构建决策树的森林，而是构建决策树桩的森林。(树桩是只有一个节点和两片叶子的决策树)</li><li id="cea6" class="od oe it lb b lc om lf on li oo lm op lq oq lu oi oj ok ol bi translated">在最终决策中，每个决策树桩被赋予不同的权重。</li><li id="5ca7" class="od oe it lb b lc om lf on li oo lm op lq oq lu oi oj ok ol bi translated">它为错误分类的数据点分配更高的权重，以便在构建下一个模型时赋予它们更多的重要性。</li><li id="c0b3" class="od oe it lb b lc om lf on li oo lm op lq oq lu oi oj ok ol bi translated">它有助于将多个“弱分类器”组合成一个强分类器。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/46aa825b04e12286312c57e41e64a350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TFnAG7gS1DxdnnBBaBIfFQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图14:提升集成学习算法的一般过程</figcaption></figure><h1 id="1d2d" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">梯度推进</h1><p id="f41c" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">梯度推进构建了多个决策树，其中每一个树从先前树的错误中学习。它利用残差来提高预测性能。梯度增强的全部目的是尽可能地减少残余误差。</p><p id="1ea0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升类似于ADA提升，两者的区别在于ADA提升构建决策树桩，而梯度提升构建多叶决策树。</p><p id="84c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度推进从建立一个基础决策树开始，并取初始预测值(通常是平均值)。然后，以初始特征和残差为因变量建立新的决策树。对新决策树的预测是通过采用模型的初始预测+样本乘以学习率的残差来进行的，并且该过程一直重复，直到我们达到最小误差。</p><h1 id="0a20" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">XGBoost</h1><p id="279d" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">XGBoost是一种更加<strong class="lb iu">规则化的渐变增强</strong>形式。它使用先进的正则化(L1 &amp; L2)，以提高模型泛化能力。</p><p id="32de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost利用叶子和前一个节点之间的相似性分数来决定哪个节点用作根节点，哪个用作子节点。</p><h1 id="7e93" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">k均值聚类</h1><p id="6ede" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">KMeans聚类是一种无监督的机器学习算法，它将未标记的数据分组为K个不同的簇，其中K是用户定义的整数。</p><p id="1722" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它是一种迭代算法，利用聚类质心将未标记的数据划分为K个聚类，使得具有相似属性的数据点属于同一聚类。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="097d" class="lv lw it mp b gy mt mu l mv mw">1. Define K and Create K Clusters<br/>2. Calculate Euclidean Distance of each data point from K Centroid<br/>3. Assing CLosest Data Point to Centroid and Create a CLuster<br/>4. Recalculate Centroid By Taking mean</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/30133a4a0e861ca391b03787d20a0ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Juf3TO-KwYRtMTFZ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图15:使用不同K值的K均值聚类对未标记数据进行聚类</figcaption></figure><h1 id="1c5c" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">分层聚类</h1><p id="b02a" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">分层聚类是另一种基于聚类的算法，它以树的形式创建聚类的层次结构来划分数据。它自动找到数据之间的关系，并将它们分成n个不同的簇，其中n是数据的大小。</p><p id="8491" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">层次聚类有两种主要方法:凝聚法和分裂法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/ca1bdc3b3f0f91d063c8373189bbe776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wDAim9-LFGSCNASn.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图16:聚集和分裂层次聚类之间的聚类创建过程比较</figcaption></figure><p id="ee0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在凝聚聚类中，我们将每个数据点视为单个聚类，然后组合这些聚类，直到我们只剩下一个组(完整的数据集)。另一方面，分裂式层次聚类从整个数据集(被视为一个单独的聚类)开始，然后将其划分为不太相似的聚类，直到每个单独的数据点成为其自己唯一的聚类。</p><h1 id="cfb9" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">DBSCAN聚类</h1><p id="b4c9" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?highlight=dbscan" rel="noopener ugc nofollow" target="_blank"> DBSCAN </a>(带有噪声的应用的基于密度的空间聚类)基于这样的假设工作:如果一个数据点更接近该聚类的<em class="ny">多个</em>数据点，而不是任何单个点，则该数据点属于该聚类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/babc7c4e4d25401b0487fbbdca1ed1da.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*6ggWk3gxu54XNhwe.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图17:摘自维基百科的DBSCAN聚类示例，minPts = 4。点A和其他红点是核心点，因为在<em class="ot"> ε </em>半径内围绕这些点的区域包含至少4个点(包括点本身)。因为它们都可以互相访问，所以它们形成了一个集群。点B和C不是核心点，但是可以从A(经由其他核心点)到达，因此也属于该集群。点N是一个噪声点，它既不是核心点，也不是可直接到达的点<strong class="bd lx"> [ </strong> <a class="ae ky" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lx">来源</strong> </a> <strong class="bd lx"> ] </strong></figcaption></figure><p id="b703" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ou ov ow mp b">epsilon</code>和<code class="fe ou ov ow mp b">min_points</code>是两个重要的参数，用于将数据划分为小的聚类。<code class="fe ou ov ow mp b">epsilon</code>指定一个点与另一个点的接近程度，以便将其视为聚类的一部分，同时<code class="fe ou ov ow mp b">min_points</code>确定形成聚类所需的最小数据点数。</p><h1 id="0786" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">Apriori算法</h1><p id="6f81" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">Apriori算法是一种关联规则挖掘算法，它根据数据项之间的依赖关系将数据项映射在一起。</p><p id="8542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用apriori算法创建关联规则有一些关键步骤。确定大小为1的每个项目集的支持度，其中支持度是数据集中项目的频率。<br/> 2。倾向于所有低于最小支持度阈值的项目集(由用户决定)<br/> 3。创建大小为n+1的项目集(n是前一个项目集的大小),并重复步骤1和2，直到所有项目集支持度都高于阈值。<br/> 4。使用置信度生成规则(当已经给出x的出现次数时，x &amp; y出现的频率)</p><h1 id="c683" class="mx lw it bd lx my mz na ma nb nc nd md jz ne ka mg kc nf kd mj kf ng kg mm nh bi translated">主成分分析</h1><p id="b9d4" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">PCA是一种线性降维技术，它将一组相关特征转换成更少(<em class="ny"> k &lt; p </em>)数量的不相关特征，称为主成分。</p><p id="2f6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过应用PCA，我们丢失了一些信息，但它提供了许多好处，如提高模型性能，降低硬件要求，并提供了使用可视化来理解数据的更好机会。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/86739619f4935daa191b2ed5818647b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S-A4PsKHtK3dxUAw.png"/></div></div></figure><p id="2ad2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢你读到这里，如果你喜欢我的内容并想支持我—</p><p id="013d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi oy translated"><span class="l ni nj nk bm nl nm nn no np di"> D </span> <em class="ny"> o关注我上</em> <a class="ae ky" href="http://abhayparashar31.medium.com/" rel="noopener"> <strong class="lb iu">中</strong> </a> <strong class="lb iu"> </strong> <em class="ny">和我的</em> <a class="ae ky" href="http://medium.com/pythoneers" rel="noopener"> <strong class="lb iu">惊艳发布</strong> </a> <em class="ny">面向Python开发者和AI爱好者。</em></p><p id="7034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi oy translated"><span class="l ni nj nk bm nl nm nn no np di"> A </span> <em class="ny">在</em><a class="ae ky" href="https://www.linkedin.com/in/abhay-parashar-328488185/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">LinkedIn</strong></a><em class="ny">上与我关联。</em></p><p id="b262" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi oy translated"><span class="l ni nj nk bm nl nm nn no np di"> T </span> <em class="ny">瓮中之鳖</em> <a class="ae ky" href="https://abhayparashar31.medium.com/membership" rel="noopener"> <strong class="lb iu">我的引荐链接</strong> </a> <em class="ny">。你会费的一小部分会归我。</em></p><p id="7f77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi oy translated"><span class="l ni nj nk bm nl nm nn no np di">A</span><em class="ny">ttach yourself to</em><a class="ae ky" href="https://abhayparashar31.medium.com/subscribe" rel="noopener"><strong class="lb iu">我的邮件列表</strong> </a> <em class="ny">千万不要错过阅读我的另一篇文章。<br/> </em>落款— <a class="oz pa ep" href="https://medium.com/u/76f234261155?source=post_page-----92f82b41ba71--------------------------------" rel="noopener" target="_blank">阿沛·帕拉沙尔</a> 🧑‍💻</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/86739619f4935daa191b2ed5818647b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S-A4PsKHtK3dxUAw.png"/></div></div></figure><h2 id="3945" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">一些精选的文章供你接下来阅读</h2><ul class=""><li id="3860" class="od oe it lb b lc nq lf nr li pb lm pc lq pd lu pe oj ok ol bi translated"><a class="ae ky" href="https://medium.com/pythoneers/10-facts-you-didnt-know-about-python-b18d87529c23" rel="noopener"> <strong class="lb iu">关于Python你不知道的10个事实</strong> </a></li><li id="68be" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated"><a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/10-advance-python-concepts-to-level-up-your-python-skills-da3d6284ad53"> <strong class="lb iu">提升你的Python技能的10个高级Python概念</strong> </a></li><li id="3852" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated"><a class="ae ky" href="https://medium.com/pythoneers/10-useful-automation-scripts-you-need-to-try-using-python-de9c993f1f5" rel="noopener"> <strong class="lb iu"> 10个有用的自动化脚本你需要尝试使用Python </strong> </a></li><li id="d89b" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated"><a class="ae ky" href="https://medium.com/pythoneers/35-most-valuable-github-repositories-for-developers-45ab9df1af81" rel="noopener"> <strong class="lb iu"> 35个对开发者最有价值的GitHub库</strong> </a></li><li id="0cb9" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated"><a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/22-python-code-snippets-for-everyday-problems-4c6a216c33ae"> <strong class="lb iu">针对日常问题的22个Python代码片段</strong> </a></li><li id="e9fc" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated"><a class="ae ky" href="https://medium.com/pythoneers/15-python-packages-you-probably-dont-know-exits-aef0525a965f" rel="noopener"> <strong class="lb iu"> 15个你可能不知道的Python包退出</strong> </a></li><li id="0496" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated"><a class="ae ky" href="https://medium.com/pythoneers/10-killer-websites-every-developer-should-try-170b365a0590" rel="noopener"> <strong class="lb iu">每个开发者必去的10个黑仔网站</strong> </a></li><li id="a520" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated"><a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/30-python-hacks-every-developer-should-know-11d4b5f95be5"> <strong class="lb iu"> 30 Python Hacks每个开发者都应该知道的</strong> </a></li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><h1 id="df13" class="mx lw it bd lx my pm na ma nb pn nd md jz po ka mg kc pp kd mj kf pq kg mm nh bi translated">分级编码</h1><p id="38f4" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">感谢您成为我们社区的一员！在你离开之前:</p><ul class=""><li id="7d7b" class="od oe it lb b lc ld lf lg li of lm og lq oh lu pe oj ok ol bi translated">👏为故事鼓掌，跟着作者走👉</li><li id="eb08" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated">📰查看<a class="ae ky" href="https://levelup.gitconnected.com/?utm_source=pub&amp;utm_medium=post" rel="noopener ugc nofollow" target="_blank">升级编码出版物</a>中的更多内容</li><li id="1688" class="od oe it lb b lc om lf on li oo lm op lq oq lu pe oj ok ol bi translated">🔔关注我们:<a class="ae ky" href="https://twitter.com/gitconnected" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae ky" href="https://www.linkedin.com/company/gitconnected" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae ky" href="https://newsletter.levelup.dev" rel="noopener ugc nofollow" target="_blank">时事通讯</a></li></ul><p id="cf6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">🚀👉<a class="ae ky" href="https://jobs.levelup.dev/talent/welcome?referral=true" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">加入升级人才集体，找到一份神奇的工作</strong> </a></p></div></div>    
</body>
</html>