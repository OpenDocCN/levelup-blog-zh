<html>
<head>
<title>Fundamentals of Reinforcement Learning: Monte Carlo Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的基础:蒙特卡罗算法</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/fundamental-of-reinforcement-learning-monte-carlo-algorithm-85428dc77f76?source=collection_archive---------5-----------------------#2021-05-31">https://levelup.gitconnected.com/fundamental-of-reinforcement-learning-monte-carlo-algorithm-85428dc77f76?source=collection_archive---------5-----------------------#2021-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0a78" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第三部分:阐述了无模型RL算法的基本原理:蒙特卡罗算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/855824dcb6ac9e253ecef07ab2a5d0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CoPcNVd-nBeU6Trm"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">卢卡斯·本杰明在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="b008" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下在<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/fundamental-of-reinforcement-learning-markov-decision-process-8ba98fa66060">第一部分</a>中介绍的智能体-环境界面，观察是智能体对环境的感知，行动将改变环境的状态，奖励是一个标量值，表示智能体在步骤t做得有多好，智能体的目标是最大化累积奖励。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/01b35c4888a5fd4a0229ebde859aaf60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kinHLnRkJNlVOXqP.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图1:代理-环境界面。来源——斯坦福大学CME 241<a class="ae ky" href="https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/5-6-MDPs.pdf" rel="noopener ugc nofollow" target="_blank">讲座</a>。</figcaption></figure><p id="1f0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习不同于其他机器学习范式是因为代理的行为影响了它接收到的后续数据，没有监督(标签数据)，只有奖励信号。</p><p id="42b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/fundamentals-of-reinforcement-learning-value-iteration-and-policy-iteration-with-tutorials-a7ad0049c84f">第2部分</a>中，我们计算了价值函数，并找到了转移概率已知的最优策略。然而，在大多数情况下<strong class="lb iu">转移概率是未知的</strong>，我们需要<strong class="lb iu">学习</strong>价值函数并从经验中找到最优策略<strong class="lb iu">。</strong></p><p id="7c5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将使用采样方法解释蒙特卡罗算法，这是一种无模型RL算法。</p><h1 id="01b2" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">蒙特卡洛</h1><p id="c4f8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">先说什么是蒙特卡罗(MC)？<br/> MC是一种利用随机性解决问题的方法，即通过生成合适的随机数并观察符合某些特性的数字分数来解决问题。使用MC计算π的示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/cab9895f1fa908004591465c66d11a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*L6VfNA9edLr6ss2I499VXA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图2:为《𝑛=3000时报》在正方形上随机放置圆点</figcaption></figure><p id="527e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图可以看出，给定r = 1，红色面积= 1/4 πr，正方形面积= r。<br/> ∴ π ≈ 4 ×红色区域的点数/总点数</p><h1 id="9ea5" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">法国蒙特卡洛</h1><p id="6234" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">同样，在RL中，我们使用MC对许多轨迹进行随机采样，并在此基础上尝试估计不同状态的值——回想一下，价值函数是从s: V𝜋(𝑠)=𝔼𝜋[𝐺ₜ|𝑆ₜ=𝑠]开始的预期收益，而q值函数是从𝑠开始的预期收益，采取行动𝑎: 𝑄𝜋(𝑠,𝑎)=𝔼𝜋[𝐺ₜ|𝑆ₜ=𝑠，𝐴ₜ=𝑎].轨迹是从开始状态到结束状态的状态序列。例如(s₁，a₁，r₂，s₂，a₂，r₃，…，rₜ).)</p><h2 id="eaff" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">首次访问蒙特卡洛政策评估</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/190d4be16233ed3cdfcfc164eacfb63e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fD8x6IfbP4Y__qhvkWhSUw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图3:首次访问蒙特卡洛政策评估—伪代码[3]</figcaption></figure><p id="9b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们用经验来学习一个经验状态值函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/80ce0f97166a6ab8671ee85210aa56ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*VobK146LgLsfkL7jpKZkBw.png"/></div></figure><p id="ee7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当计算价值函数时，它对访问(𝑠,𝑎).)后观察到的回报进行平均首次访问MC意味着平均回报率只有第一次(𝑠,𝑎)被访问的轨迹。</p><p id="bb74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过一个简单的例子来了解一下。假设下面提到的一维网格世界环境。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/320d4c4a56ad3edd6f80b53f6dc947ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*FzncRd7_NZoUMhHsukodaQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图4:示例</figcaption></figure><p id="9fb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代理处于1x4网格世界中，状态s ∈ {cell₁、cell₂、cell₃、cell₄}，动作a ∈{向左移动，向右移动)。在这种环境下，代理人如果到达目的地将获得10分奖励，每个动作将花费1分。</p><p id="e3b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设以下是3个轨迹的表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/02feb0ce8cdf2976dd76f7c157b24a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5aTydlmBS7ig6xMA8xljcg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图5:随机采样的3条轨迹</figcaption></figure><p id="efd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定这3条轨迹，我们可以计算所有非终点状态的价值函数:cell₁、cell₂、cell₃.首先，我们计算𝛾=0.9.每集的回报</p><p id="c25c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于第一集和第二集没有涉及cell₁，<br/><strong class="lb iu"/>的经验值函数为−1×0.9⁰−1×0.9 +10×0.9 = 6.2</p><p id="37b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一集cell₂:gₜ=−1×0.9⁰+10×0.9 = 8<br/>第二集gₜ=−1×0.9⁰−1×0.9 1×0.9+10×0.9 = 4.58<br/>第三集gₜ=−1×0.9⁰−1×0.9 1×0.9+10×0.9 = 4.58<br/>cell₂<strong class="lb iu">的经验值函数为(8+4.58+4.58)/3=5.72</strong></p><p id="96a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一集cell₃:gₜ=10×0.9⁰=10<br/>第二集Gₜ=−1×0.9⁰−1×0.9 +10×0.9 =6.2 <br/>第三集gₜ=10×0.9⁰=10<br/>cell₃的经验值函数为(10+6.2+10)/3=8.73</p><blockquote class="nk nl nm"><p id="9d8b" class="kz la nn lb b lc ld ju le lf lg jx lh no lj lk ll np ln lo lp nq lr ls lt lu im bi translated">我们可以用更多的情节或轨迹得到更精确的价值函数。</p></blockquote><h2 id="91e9" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">蒙特卡罗控制算法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/3d64db351bb3958a8cc9020d7c59e860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsE_pEn8Daf7UlgQsdwZcA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图6:蒙特卡罗控制算法——伪代码[3]</figcaption></figure><p id="c7eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们有了给定政策的所有状态的价值函数。我们需要改进政策，使之更好。评估和改进的过程重复进行，直到策略没有改变或达到最优策略。这个过程类似于我们在<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/fundamentals-of-reinforcement-learning-value-iteration-and-policy-iteration-with-tutorials-a7ad0049c84f"> <strong class="lb iu">策略迭代</strong> </a>中所做的。但是，光有价值函数是不够的，我们需要知道一个动作停留在一个状态下有多好(Q值)。使用上面的例子，让我们给定一个状态和一个动作来填写Q表。估计Q函数与估计值函数略有不同。它是一集里国家行动对(𝑠,𝑎)被访问的平均回报。</p><p id="9a13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于第一集和第二集都没有(cell₁，右)<br/><strong class="lb iu">(cell₁，右)</strong>的经验q值函数为−1×0.9⁰−1×0.9 +10×0.9 = 6.2 <br/>由于所有集都没有(cell₁，左)<br/><strong class="lb iu">(cell₁，左)</strong>的经验q值函数为0</p><p id="64cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第一集(cell₂，右)来说:Gₜ =−1×0.9⁰ +10×0.9 =8 <br/>第二集gₜ=−1×0.9⁰−1×0.9 1×0.9+10×0.9 = 4.58<br/>第三集Gₜ=−1×0.9⁰+10×0.9 =8 <br/>对于<strong class="lb iu"> (cell₂，右)</strong>的经验q值函数为(8+4.58+8)/3=6.86 <br/>由于第一集和第二集都有no(cell₂(左)参与，<br/>经验q值函数</p><p id="86a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于(cell₃，右)第一集:Gₜ =10×0.9⁰=10 <br/>第二集Gₜ=10×0.9⁰=10 <br/>第三集gₜ=10×0.9⁰=10<br/><strong class="lb iu">(cell₃，右)</strong>的经验q值函数为(10+10+10)/3=10 <br/>由于第一集和第三集都有no(cell₃(左)<br/><strong class="lb iu">(cell₃，左)</strong>的经验q值函数为−1×0.9⁰−1×0.9 +10×0.9 =6.2</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b848583d1e142bdbf520a1c33771dcdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*tZ7bWvAJPYGP8BPVvc5nug.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图7:完成的问题表</figcaption></figure><p id="4ca1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每种状态下的最优行动是具有最大Q(s，a)的行动。从结果来看，cell₁、cell₂、cell₃的最优行动是向右移动。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="119d" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">推荐阅读</h1><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/fundamental-of-reinforcement-learning-markov-decision-process-8ba98fa66060"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">强化学习的基础:马尔可夫决策过程</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">第1部分:解释马尔可夫决策过程和贝尔曼方程的概念</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ks oi"/></div></div></a></div><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/fundamentals-of-reinforcement-learning-value-iteration-and-policy-iteration-with-tutorials-a7ad0049c84f"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">强化学习的基础:价值迭代和政策迭代教程</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">第二部分:解释用于解决MDP问题的价值迭代和策略迭代的概念。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">levelup.gitconnected.com</p></div></div><div class="or l"><div class="ox l ot ou ov or ow ks oi"/></div></div></a></div></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="524e" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">参考</h1><p id="331f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] <a class="ae ky" href="https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/5-6-MDPs.pdf" rel="noopener ugc nofollow" target="_blank">斯坦福大学CME241讲座:金融中随机控制问题的强化学习</a>，2021</p><p id="8ed1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]“圆周率的蒙特卡罗模拟——奥尔登堡大学。”【在线】。可用:<a class="ae ky" href="https://uol.de/en/lcs/probabilistic-programming/webchurch-and-openbugs/pi-by-monte-carlo-simulation" rel="noopener ugc nofollow" target="_blank">https://uol . de/en/LCS/probabilical-programming/web church-and-open bugs/pi-by-Monte-Carlo-simulation</a></p><p id="581e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] R. S .萨顿和A. G .巴尔托，“强化学习:导论，第二版，进行中。”</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="d8a4" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">分级编码</h1><p id="2956" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">感谢您成为我们社区的一员！<a class="ae ky" href="https://www.youtube.com/channel/UC3v9kBR_ab4UHXXdknz8Fbg?sub_confirmation=1" rel="noopener ugc nofollow" target="_blank">订阅我们的YouTube频道</a>或者加入<a class="ae ky" href="https://skilled.dev/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Skilled.dev编码面试课程</strong> </a>。</p><div class="of og gp gr oh oi"><a href="https://skilled.dev" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">dreamus114.com</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk">The course to master the coding interview</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk">skilled.dev</p></div></div><div class="or l"><div class="oy l ot ou ov or ow ks oi"/></div></div></a></div></div></div>    
</body>
</html>