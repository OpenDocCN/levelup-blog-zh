<html>
<head>
<title>SciBERT Wins: 5 Improvements Over BERT, Simply Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SciBERT胜出:比BERT提高了5点，简单解释</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/scibert-wins-5-improvements-over-bert-simply-explained-a965b71d9a96?source=collection_archive---------8-----------------------#2022-08-11">https://levelup.gitconnected.com/scibert-wins-5-improvements-over-bert-simply-explained-a965b71d9a96?source=collection_archive---------8-----------------------#2022-08-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="1192" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SciBERT与BERT，及其应用程序、最佳实践，以及它如何超越BERT。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0b06b3fde5ee6e42cd78342481311bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LR4QKimz1_thgqPIx1Ydug.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">你好，我是来自Unsplash的Nik</figcaption></figure><p id="cfe2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我正在寻找实现一个基于科学领域见解的NLP管道，我偶然发现了SciBERT，一个预先训练好的用于科学文本的上下文化嵌入模型[2]。在确认它是基于BERT的基础上，它变得更有吸引力。稍后会详细介绍。</p><p id="3d92" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它在计算机科学和生物医学领域见解的大型出版物语料库上进行训练，目的是为科学文本提供有根据的单词表示，可以应用于各种下游任务，如文本分类、信息检索和问题回答。</p><p id="45cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，需要强调的是，SciBERT是由艾伦人工智能研究所(AI2) [7]开发的开源项目，艾伦人工智能研究所是一个以“高影响力人工智能研究和工程”而闻名的非营利组织[7]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lf"><img src="../Images/df93c5a3b521116c1d4e93e942db7a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Y_psIhsvqwinvif8s-q5Q.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由来自Unsplash的<a class="ae le" href="https://unsplash.com/@thisisengineering" rel="noopener ugc nofollow" target="_blank">this engineering RAEng</a></figcaption></figure><h1 id="5ab5" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">其训练根源</strong></h1><p id="b4b4" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">基于来自ELMO [4]、GPT [5]和伯特[6]的语言模型[2]的无监督预训练，观察到NLP任务[2]的显著性能改善。为了将它与SciBERT联系起来，有必要强调训练SciBERT的内容的绝对规模(在一个大的计划中，这个陈述对于这种类型的NLP任务非常重要):</p><h2 id="8918" class="mj lh it bd li mk ml dn lm mm mn dp lq kb mo mp lu kf mq mr ly kj ms mt mc mu bi translated">1.14M纸，3.1B代币[7]。</h2><p id="24f3" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">虽然SciBert是自然语言处理(NLP)算法方法的一部分，并且是专门为科学应用设计的，但它在核心上是Bert的变体:</p><blockquote class="mv"><p id="bfdc" class="mw mx it bd my mz na nb nc nd ne kn dk translated">SciBERT是一个预先训练好的基于BERT的语言模型，旨在执行科学任务。</p></blockquote><p id="55f4" class="pw-post-body-paragraph jq jr it js b jt nf jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj nj kl km kn im bi translated">为了了解BERT，我将在底部简单地链接与之相关的研究论文[9]。</p><p id="cc70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考虑使用它的搜索功能来分析相关的科学论文或查找任何研究任务所需的数据集。此外，SciBERT可以帮助您开发假设，并根据可用数据对其进行测试。回想一下，BERT是它的根，SciBERT就是通过它建立的；因此，许多可以跨BERT实现应用的功能也适用于这种情况，比如使用它来创建结果的可视化。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/564f323c74bd535011aedec060e44de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0unm9iBbfCe6rEtKS558Nw.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来自Unsplash的Shane Rounce</figcaption></figure><h1 id="b6f0" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">为什么赛伯特会发光</strong></h1><p id="dfbe" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">它提高了文本分类和信息提取任务的准确性。在科学任务的应用中，SciBERT受到其概括能力的影响。也就是说，它可以从更少的训练样本中学习，但仍然可以在看不见的数据上获得良好的性能。</p><p id="9dfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SciBERT还提供了一些比其他语言模型更容易理解和解释的特性，因为它针对分析跨科学任务实现的特定任务进行了优化。</p><p id="70e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将自动化视为建立跨科学文献搜索的任务，整个人工智能社区正在努力开发专门用于从PubMed或arXiv等大型数字图书馆检索相关论文的神经网络模型。挑战并不明显(从技术上来说)，就像与长尾查询(即只有几个文档相关的查询)的斗争一样，因为从有限的数据中学习不同的概念很困难。像SciBERT这样的transformer模型的泛化能力在这一领域提供了潜在的前景，提供了一种在没有人工监督的情况下从零开始在任何给定的文本数据集上有效学习信息检索启发式的方法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nl"><img src="../Images/658d061a20cad10b300c36e04e6b5865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KhAwZR9hPYSKZrxLHNmRAQ.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来自Unsplash的Alain Pham</figcaption></figure><p id="a15e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最先进的性能是凭经验获得的(至少是为了在AI社区中使用它的目的)。在基于文献的发现[10]的背景下，基于文献的发现(LBD) [11]是通过读取和挖掘大量文本数据来发现实体之间新关系的过程。后一项工作传统上由专家研究人员手动执行，但近年来已经看到了基于机器学习和自然语言处理算法的自动化方法的兴起。这些方法，像SciBERT一样，有可能在简单的LBD任务中展示有希望的结果。SciBERT处理长文档的机会和它从小数据集学习的能力使它成为一种可能的、有前途的处理更复杂的LBD任务的方法。</p><p id="6e8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为一项任务，科学问答可以结合机器学习的应用来回答关于文本(如研究论文或期刊文章)中存在的信息的问题。这是一个具有挑战性的用例，因为需要深入理解文本段落和问题，并且可用于解释的上下文通常有限。已经提出了几个神经网络模型来处理这项任务，但是像SciBERT这样的变压器模型可以很好地解决这个问题空间，特别是实现最先进的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/9b71a8ae31f8ddcc3f6c40f07f29a0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qhHOEfUwY0u88haa29smQ.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">这个工程来自Unsplash</figcaption></figure><h1 id="d4cc" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">SciBERT有哪些潜在的应用？</strong></h1><p id="d150" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">让我们先详细列举这些(BLUF):</p><p id="16b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">—最先进的文本处理:凭借其强大的文本处理能力，SciBERT可以帮助最终用户从冗长复杂的文本(如研究论文或法律文档)中提取含义。当一项任务需要快速消化大量信息时，利用一项功能的机会，尤其是当它们是开源的时候，尤其有影响力。</p><p id="5654" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">—文本数据挖掘:凭借其在文本数据中识别模式的能力，SciBERT可被实现用于各种与文本数据挖掘相关的任务(由于采用了BERT的能力和功能)。举例来说，它可以被构建成按照主题或流派对文章进行自动分类，或者识别科学文献中的新趋势。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/ad197a0a7f15d1a3a102f16ee4d2c046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_CA1wk-xk2FhaF2D7ZrFw.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由Unsplash的this engineering RAEng</figcaption></figure><p id="09dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">—进行大规模研究:SciBERT快速处理大量数据的能力使其成为进行大规模研究的理想选择。例如，最终用户可以实现SciBERT来分析任何数据，显然是为了在科学源内容中应用它。</p><p id="7559" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">—支持基于证据的决策:数据的日益普及意味着越来越多的决策需要基于证据做出(基于数据的决策比基于数据的决策更多)。SciBERT可以帮助管理者和决策者筛选堆积如山的信息，找到做出明智决策所需的真知灼见。</p><p id="767f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">—增强人机交互:SciBERT的自然语言处理能力可以在人类和计算机之间建立更好的界面。考虑如何部署它来开发聊天机器人，以提供与用户或虚拟助理更自然和有效的交流，从而更好地理解用户的需求并准确地满足他们的请求。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/c76c153010a0ad52569c9973f0a7af05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qZnY8obU0B3ydpfmBqq84g.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由来自Unsplash的<a class="ae le" href="https://unsplash.com/@bravoprince" rel="noopener ugc nofollow" target="_blank">大卫王子</a></figcaption></figure><h1 id="ea38" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">离别的思念</strong></h1><p id="0a04" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">BERT为我们在人工智能方面的许多能力奠定了基础，特别是基于自然语言处理和机器学习的开源方法的实现。此外，SciBERT建立在BERT基础之上，允许进行专门的分析，这是我每天在科学任务中激活的一个用例(总的来说，这要感谢BERT)。</p><p id="e1c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你对这篇文章的编辑有任何建议，或者对进一步扩展这个主题领域有什么建议，请和我分享你的想法。</p><h2 id="0f8d" class="mj lh it bd li mk ml dn lm mm mn dp lq kb mo mp lu kf mq mr ly kj ms mt mc mu bi translated">另外，请考虑订阅我的每周简讯:</h2><div class="no np gp gr nq nr"><a href="https://pventures.substack.com" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">产品。风险时事通讯</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">产品和人工智能交汇处的想法。点击阅读产品。投资时事通讯，作者安尼尔…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">pventures.substack.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ky nr"/></div></div></a></div><p id="bcf8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og">参考文献:</em></p><p id="cde7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 1。安特吉尼，m .，德索萨，j .，桑托斯，V. A. P. M .多斯，&amp;# 38；奥尔，S. (2020年9月16日)。开放研究知识图中基于SciBERT的生物测定语义化。ArXiv.Org。</em><a class="ae le" href="https://arxiv.org/abs/2009.08801" rel="noopener ugc nofollow" target="_blank"><em class="og"/></a></p><p id="a8cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 2。Beltagy，I .，Lo，k .，&amp;# 38；Cohan，A. (2019年3月26日)。SciBERT:科学文本的预训练语言模型。ArXiv.Org。</em><a class="ae le" href="https://arxiv.org/abs/1903.10676" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1903.10676</em></a></p><p id="8ee0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 3。马赫什瓦里，h .辛格，b .，&amp;# 38；瓦尔马，v .(未注明)。引文上下文分类的SciBERT句子表示。ACL选集。检索到2022年8月9日，来自</em><a class="ae le" href="https://aclanthology.org/2021.sdp-1.17/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://aclanthology.org/2021.sdp-1.17/</em></a></p><p id="bc25" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 4。马修·e·彼得斯、马克·纽曼、莫希特·伊耶、马特·加德纳、克里斯托弗·克拉克、肯顿·李和卢克·s·塞特勒莫耶。2018.深层语境化的词语表达。在NAACL-HLT。</em></p><p id="e8cf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 5。亚历克·拉德福德、卡蒂克·纳拉辛汉、蒂姆·萨利曼斯和伊利亚·苏茨基弗。2018.通过生成性预训练提高语言理解能力。</em></p><p id="72e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">6。雅各布·德夫林、张明蔚、肯顿·李和克里斯蒂娜·图塔诺娃。2019.BERT:用于语言理解的深度双向转换器的预训练。在NAACL-HLT。</p><p id="586b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">7。阿莱奈。(未注明)。GitHub — Allenai/scibert:科学文本的bert模型。GitHub。检索到2022年8月9日，来自<a class="ae le" href="https://github.com/allenai/scibert/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://github.com/allenai/scibert/</em></a></p><p id="cdaf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 8。allenai/scibert _ scivocab _ uncased拥抱脸。(未注明)。检索到2022年8月9日，来自</em><a class="ae le" href="https://huggingface.co/allenai/scibert_scivocab_uncased" rel="noopener ugc nofollow" target="_blank"><em class="og">https://huggingface.co/allenai/scibert_scivocab_uncased</em></a></p><p id="63d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 9。Devlin等人BERT:用于语言理解的深度双向转换器的预训练。</em><a class="ae le" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/pdf/1810.04805.pdf</em></a></p><p id="cd28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 10。本施。(2021年1月1日)。基于文献的发现的新方法。</em><a class="ae le" href="https://elib.dlr.de/147246/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://elib.dlr.de/147246/</em></a></p><p id="10a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="og"> 11。基于文献的发现:模型、方法和趋势。(未注明)。生物医学信息学杂志，74，20–32。</em><a class="ae le" href="https://doi.org/10.1016/j.jbi.2017.08.011" rel="noopener ugc nofollow" target="_blank"><em class="og"/></a></p></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><h1 id="941e" class="lg lh it bd li lj oo ll lm ln op lp lq lr oq lt lu lv or lx ly lz os mb mc md bi translated">分级编码</h1><p id="37a9" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">感谢您成为我们社区的一员！在你离开之前:</p><ul class=""><li id="b33b" class="ot ou it js b jt ju jx jy kb ov kf ow kj ox kn oy oz pa pb bi translated">👏为故事鼓掌，跟着作者走👉</li><li id="208e" class="ot ou it js b jt pc jx pd kb pe kf pf kj pg kn oy oz pa pb bi translated">📰查看<a class="ae le" href="https://levelup.gitconnected.com/?utm_source=pub&amp;utm_medium=post" rel="noopener ugc nofollow" target="_blank">升级编码出版物</a>中的更多内容</li><li id="3990" class="ot ou it js b jt pc jx pd kb pe kf pf kj pg kn oy oz pa pb bi translated">🔔关注我们:<a class="ae le" href="https://twitter.com/gitconnected" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae le" href="https://www.linkedin.com/company/gitconnected" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae le" href="https://newsletter.levelup.dev" rel="noopener ugc nofollow" target="_blank">时事通讯</a></li></ul><p id="1a4d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">🚀👉<a class="ae le" href="https://jobs.levelup.dev/talent/welcome?referral=true" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">加入升级人才集体，找到一份神奇的工作</strong> </a></p></div></div>    
</body>
</html>