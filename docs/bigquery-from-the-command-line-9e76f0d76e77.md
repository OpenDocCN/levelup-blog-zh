# 来自命令行的 BigQuery

> 原文：<https://levelup.gitconnected.com/bigquery-from-the-command-line-9e76f0d76e77>

从命令行管理 BigQuery 可以节省大量时间。以下是如何设置这一点。

![](img/dfc3a89d6324950c806bbef2846a1806.png)

照片由[艾莉·约翰逊](https://unsplash.com/@lejo?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄

使用 BigQuery 可能非常复杂。云控制台(Google 对基于网络的云管理界面的命名)功能强大，但任务很快变得重复和繁琐。实现 API 客户端、Composer DAGs 和类似的应用程序通常是多余的，尤其是对于相当基本和附带的操作。幸运的是，Google 还提供了一个很棒的工具包，其中包括许多 Google Cloud APIs 的命令行客户端。这些客户端的优势在于，简单的任务可以以简单的方式执行，而复杂的任务仍然是可能的。命令可以被复制和粘贴、编写脚本、在 cron 表中定期执行、放入版本控制等等。所有这些都可以从您的一个 GCP 项目中的 VM 实例中获得，也可以从您在街角咖啡店的笔记本中获得。

# 入门指南

显然，您首先需要的是一个命令行。对于本文，我假设您有可用的命令行。Linux 和 macOS 用户会懂我的意思，Windows 用户可能不懂。我们将要安装的工具包将为您提供一个 Windows 命令行，但是 shell 有些受限。作为一种选择，WSL (Windows Subsystem for Linux)将为您提供一个您选择的基本但非常实用的 Linux 安装，它包括一个合适的终端，您可以在其中运行一个 shell。所有的例子都是在运行 bash shell 的 Linux 终端中执行时给出的，但是这些例子很容易被转换成不同的 shell。

下一步是安装[云 SDK](https://cloud.google.com/sdk/docs/) 。这为你提供了一套与云平台交互的工具。遵循在线文档中的说明，包括初始化 SDK 的步骤。这将允许您使用您的 GCP 帐户进行身份验证，该帐户当然应该有足够的权限来管理 BigQuery 和云存储。

# 一点探索

安装完 Cloud SDK 后，启动终端并键入:

这将向您显示 GCP 可用的各种子系统的大量信息，按组进行组织。就本文而言，`auth`组尤其重要:

这显示了初始化 SDK 时使用的帐户。通过输入以下内容添加另一个帐户:

按照指示去做。然后，您可以在帐户之间切换。

另一个有用的选项是`config`组，它允许您管理一些基本设置，例如活动帐户和项目。使用`gcloud help config`获取所有可用参数的完整列表。使用以下方法检查活动设置:

# 回避云存储

云存储作为 BigQuery 导入或导出数据的存储位置非常方便，所以让我们稍微回避一下从命令行管理云存储。这个实用程序是`gsutil`，它以类似于`gcloud`的方式接受子命令。列出可用存储桶的过程如下:

使用选项可以获得更详细的信息:

使用`cp`子命令将数据复制到云存储中或从云存储中复制数据:

不需要创建文件夹，因为路径映射到云存储中的对象。使用`gsutil`可以控制的还有很多。有关更多信息，请查看内置帮助和联机文档。

除了使用`gsutil`之外，云存储上的对象也可以通过使用`gcsfuse`挂载桶作为本地文件系统来为一些操作系统进行管理，如这里的[所解释的](https://medium.com/swlh/three-strategies-for-accessing-google-cloud-storage-from-php-d2ea5f07c7d2)。

# 管理 BigQuery

使用`bq`命令完成与 BigQuery 的交互。与其他云 SDK 工具一样，帮助内置于:

让我们通过几个步骤来演示您在日常工作中可能使用 BigQuery 执行的基本操作。首先，我们创建一个数据集，我们可以在不影响重要数据的情况下进行实验。

`mk`子命令用于创建各种资源，包括数据集和表格。相反，`rm`子命令用于删除资源。

在继续创建表之前，我们需要决定如何提供表的模式。对于简单的表，可以在命令行上使用`--schema`选项提供模式，但是该选项也可以用于指定模式文件，允许更复杂的表定义。作为一个例子，我将使用下面的模式来表示一个 order 对象，它有一个或多个 order 行，保存在一个名为`order.json`的文件中:

创建表格的命令是:

请注意，有更多的选项可以帮助您控制创建什么，以及至少同样重要的是，在哪里创建它。同样，de`bq`utility 中的内置帮助是一个很好的起点。

现在我们已经创建了一个表，下一个明显的步骤是将一些数据放入其中，这是使用`load`子命令实现的。这允许导入各种格式的数据，包括 CSV 和 JSON。您可能已经注意到，我创建了一个包含重复记录的表，因此 CSV 不是一个选项，但 JSON 是。假设我有一个包含订单信息的文件，其中每个订单看起来有点像这样:

请注意，JSON 文件需要格式化，以便每个记录只占一行，所以漂亮打印的 JSON 将无法工作。假设我已经在一个文件中收集了一组订单，每行一个完整的订单文档，我可以使用以下命令导入这些订单:

现在可以在云控制台中找到导入作业，在那里可以使用由`bq`命令打印的作业 id 来识别它。在这种情况下，我导入的文件是一个本地文件，但它也可能是之前使用`gsutil`放在云存储空间中的文件。

进一步考虑导入，如果我必须导入一组文件，我可以使用一个循环在一个命令中完成:

这里我使用 UNIX 风格的换行(一个反斜杠作为一行的最后一个字符)来提高可读性。对文件列表命令的结果进行循环，将每个 CSV 文件加载到我们之前创建的数据集中的一个假想的`shipment`表中。我使用额外的选项来控制导入，比如不替换现有的表格内容(`--noreplace`)，跳过 CSV 标题(`--skip_leading_rows=1`)。结果将是一个作业 id 列表，每个导入的 CSV 文件一个。请注意，其他 shells 的循环语法可能略有不同。

需要解决的最后一个基本步骤是从 BigQuery 中获取数据。我想演示两个场景，第一个是按原样导出表格内容。最简单的形式是，将表内容转储到文件中，如下所示:

提取只能在云存储中完成，这就是为什么目的地 URI 指向一个桶。如前所述，使用`gsutil`将文件下载到您的本地系统。默认的导出格式是 CSV，它不支持嵌套模式。对于上面的`order`表，我们需要指定一种支持嵌套的格式，例如:

也可以压缩输出。更多信息见`bq help extract`。

一个更复杂的场景是我们想要导出查询的结果。除了用`bq`命令打印结果之外，没有直接导出查询结果的方法，这对于较大的数据集来说不太实用。因此，这个过程必须分成几个步骤。首先，我们执行查询，将结果写入表中，然后提取数据，最后，我们删除包含查询结果的表。可以按如下方式创建结果表:

这里，我将一个相当简单的查询赋给了一个变量，就像在脚本中一样。更常见的是编写这些过程的脚本，以便它们可以无人值守地运行，在这种情况下，可以通过使用临时表来使过程更加简洁。事实是，每个查询的结果都会免费存储一段时间(目前大约是 24 小时)，这些结果集也可以用来提取数据。为了做到这一点，我们需要数据被写入的表的信息。我们可以在运行查询的作业中找到以下信息:

以上是上一份工作可用的大量信息的一个小例子。临时目标表中的数据集和表 id 可用于导出数据:

几句话到位。首先，应该检查输出的作业状态。如果工作状态不是“完成”，那么继续下去就没有意义。此外，如果使用临时表的作业并发运行，验证临时表是由正确的作业创建的非常重要。如果流程变得像这样复杂，这可能是一个切换到比命令行更合适的环境的好时机，例如适当的 Google API 客户端或成熟的工作流管理器，如 Airflow 或 Composer (Google 对其基于 Airflow 的服务的名称)和类似的解决方案。

我希望已经让您对如何从命令行和在基本的 shell 脚本中与 BigQuery 进行交互有了基本的了解，而不必立即开发 API 客户机。能够使用一次性导入或简单的 cron 作业来完成这项工作可以节省大量时间。