<html>
<head>
<title>The Linear Algebra View of Least-Squares Regression: Supplements for the Novice Reader</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最小二乘回归的线性代数观点:对新手读者的补充</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/the-linear-algebra-view-of-least-squares-regression-supplements-for-the-novice-reader-94092b60bd25?source=collection_archive---------11-----------------------#2020-02-16">https://levelup.gitconnected.com/the-linear-algebra-view-of-least-squares-regression-supplements-for-the-novice-reader-94092b60bd25?source=collection_archive---------11-----------------------#2020-02-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="febf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章基于Andrew Chamberlain在他的文章“最小二乘回归的线性代数观点”中提出的材料。他出色地完成了在线性代数模型中呈现线性回归的工作，在这里，我试图填补我第一次阅读他的帖子时遇到的一些缺失。单凭这篇文章决不会让你接触到张伯伦提出的核心观点，所以我鼓励你先阅读他的文章，并以此作为补充参考。我所涉及的概念是面向那些对线性代数经验相对较少，但仍有灵感并想学习更多的人的。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="5e53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">用矩阵形式定义直线方程</strong></p><p id="37e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能记得高中时学过直线的<a class="ae kl" href="https://www.mathsisfun.com/equation_of_line.html" rel="noopener ugc nofollow" target="_blank">方程</a>。不管方程中的变量取了什么名字，方程总是三个东西的函数:<br/> 1)解释变量<br/> 2)直线的斜率<br/> 3)它是y轴截距</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi kt"><img src="../Images/ac01d38647eadd2373119f6aa2660272.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*LiQ7_dD7wc1oTlXIYvsYwg.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">直线的方程式</figcaption></figure><p id="5828" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给定一个解释变量<em class="lf"> x </em>，让我们假设<em class="lf"> x </em>只取三个值。即<em class="lf"> x₁、</em>x₂、和<em class="lf"> x₃ </em>。直线方程将预测每个<em class="lf"> x </em> ᵢ ∈ <em class="lf"> x、</em>的y值，结果将存储在与<em class="lf"> x </em>大小相同的向量b中。我们可以将直线方程改写为Ah = b形式的线性系统，其中a是设计矩阵，h是分别代表截距和斜率的两个参数h₀和h₁的列向量。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/39a42bf39687af1408fd2ab72edb460e.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*0Wsf26d714dxZ1HzD33Wiw.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">改写为线性系统Ah = b的直线方程</figcaption></figure><p id="279c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了证明将前面的方程改写为上面的线性系统是正确的，让我们评估点积A ⋅ h。得到的方程系统是我们的初始直线方程，对于<em class="lf"> x. </em>的每个不同值，该方程被分解为单独的方程</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lh"><img src="../Images/41f6c464b5d3590b8c341549afb707cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qxBUG2sCh-O3VxtRzvK_gQ.png"/></div></div></figure><p id="6535" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b" rel="noopener"> Chamberlain的帖子</a>中，我们已经开始定义矩阵形式Ax = b的线性系统。接着，Chamberlain使用二维平面上的向量符号来表示我们的搜索空间，用一个错误的向量来表示实际观察到的数据，从而很好地可视化了最小二乘回归模型。再次，看帖子。</p><p id="03a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我在‘求解回归系数’时问的问题</strong></p><blockquote class="lm ln lo"><p id="9e4c" class="jn jo lf jp b jq jr js jt ju jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj kk ij bi translated">‘A的转置是从哪里来的？’</p></blockquote><p id="8019" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在第一个等式之后，当用<em class="lf"> b </em> - <em class="lf"> p </em>交换<em class="lf"> e </em>时，引入了<em class="lf"> A </em>的转置，其中<em class="lf"> p </em>是<em class="lf"> A </em> * <em class="lf"> x-hat </em>。他为什么移调A？如果你的直觉告诉你这和点积有关，那你就对了！我发现在处理需要许多矩阵运算的方程时，一个有用的技巧是只使用矩阵的形状而不是它们的内容来遍历方程。例如，由于我们的设计矩阵A有3行2列，我将用A = (3，2)来解方程。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ls"><img src="../Images/639825e98c5864f81263e96c47a04f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AKSSbicb0neQalj1lAujvQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">换位A的理由，以及为什么A本身在取点积时不起作用</figcaption></figure><p id="ba92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们看到，转置a对于计算点积<em class="lf"> A ⋅ e </em>是必要的。</p><blockquote class="lm ln lo"><p id="bb75" class="jn jo lf jp b jq jr js jt ju jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj kk ij bi translated">求解回归系数的中间步骤是什么？</p></blockquote><p id="7984" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先让我们讨论来自向量加法的错误向量<em class="lf"> b </em>、投影<em class="lf"> p </em>和错误向量<em class="lf"> e. </em>之间的关系，如果我们有一个向量<em class="lf"> A </em>和另一个向量<em class="lf"> B </em>，其来源是向量<em class="lf"> A </em>的头， 然后向量和<em class="lf"> C </em> = <em class="lf"> A+B </em>由一个向量得到，该向量的源是<em class="lf"> A的源，而</em>的头与<em class="lf"> B的头相遇</em>因此给定两个向量<em class="lf"> A </em>和<em class="lf"> C </em>，我们可以推理出<em class="lf">B = C-A</em>我们可以将相同的逻辑应用于<em class="lf"> b </em>和<em class="lf"> p</em></p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lt"><img src="../Images/228ba113f1180bed22ab1d4e1abe6d9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RI_tZimm-mqJJbR5filE8g.png"/></div></div></figure><p id="8ef8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，下面展示了我为解决x-hat所做的工作。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lu"><img src="../Images/eef51cca9a530a336266b0e1349013f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbtpHYohHIjPi5aSWjJCrw.png"/></div></div></figure><p id="74e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然这篇文章没有带来任何新的东西，但它揭示了最初文章中没有详细讨论的一些更基本的概念。</p></div></div>    
</body>
</html>