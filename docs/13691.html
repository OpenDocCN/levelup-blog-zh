<html>
<head>
<title>Predicting Emojis with the Help of OpenAI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">借助OpenAI预测表情符号</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/predicting-emojis-with-the-help-of-openai-331a3a91f27b?source=collection_archive---------13-----------------------#2022-09-27">https://levelup.gitconnected.com/predicting-emojis-with-the-help-of-openai-331a3a91f27b?source=collection_archive---------13-----------------------#2022-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f664" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我如何使用OpenAI的CLIP深度学习模型来预测给定推文的表情符号。检查有无训练数据的表现，并亲自与模型打交道！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a9d16ae6e48a765ac99cac16a71f7a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwf4NODGHbH0g0s6pMtwxA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">我们数据集中的32个表情符号。</figcaption></figure><p id="9754" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">表情符号是一种给句子带来情感的方式。由于表情符号是图像，<br/>我想知道Open AI的CLIP模型预测一条推文的表情符号有多好。我们从预测表情符号的片段开始。后来，我们对每个表情符号的1、5…多达500个样本进行微调。</p><blockquote class="lu lv lw"><p id="bfbd" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">向下滚动到<strong class="la iu">试用型号</strong>！👇<br/>对<strong class="la iu">码感兴趣？</strong>留下评论直接联系我！</p></blockquote><h1 id="139d" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">🤖—什么是CLIP？</h1><p id="675a" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated"><a class="ae my" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> OpenAI的CLIP </a>(对比语言-图像预训练)是在互联网上找到的4亿个图像-描述组合上进行训练的，擅长选择最能描述一句话的图像。</p><p id="599a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> CLIP与</strong>不同，因为它采用两种数据格式作为输入:语言和图像。而“传统方法”使用一种类型的数据格式；文本、图像或音频…</p><p id="8ce8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">采用两种或两种以上数据格式或模态作为输入的模型也被称为<em class="lx">多模态模型</em>。</p><h1 id="cc45" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">🧑‍🔬—有什么新内容？</h1><p id="a8a6" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">网上很多模型都试图根据一句话来预测一个表情符号。表情符号是一个标签，由一个数字描述，而不是表情符号的图像。</p><p id="a1f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法的新特点是我们给了模型文本和表情图像。它将试图找到文本和图像中的模式，并将它们结合起来，找出最能描述文本中情感的表情符号。</p><h1 id="fbb4" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">📈—数字</h1><ul class=""><li id="e070" class="mz na it la b lb mt le mu lh nb ll nc lp nd lt ne nf ng nh bi translated">没有微调的情况下，CLIP有多好？</li><li id="f9d4" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">有几个样本呢？</li><li id="0a6f" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">我们能走多远？</li></ul><h2 id="31de" class="nn mc it bd md no np dn mh nq nr dp ml lh ns nt mn ll nu nv mp lp nw nx mr ny bi translated">在我们开始之前…</h2><p id="266c" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">我们使用<strong class="la iu"> precision </strong>作为模型性能的KPI。精度是介于0和1之间的数字，其中0表示所有预测都是错误的，1表示所有预测都是正确的。</p><p id="d801" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们用于微调的<a class="ae my" href="https://huggingface.co/datasets/vincentclaes/emoji-predictor" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">数据集</strong> </a>有两列；</p><ul class=""><li id="40c7" class="mz na it la b lb lc le lf lh nz ll oa lp ob lt ne nf ng nh bi translated">有原始推文的专栏；</li><li id="9e79" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">带有数字的标签列，该数字连接到表情符号的图像。</li></ul><p id="4a92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与标签编号相对应的图像可以在这里找到<a class="ae my" href="https://public-assets-vincent-claes.s3.eu-west-1.amazonaws.com/emoji-precitor/emojis.zip" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="8bf3" class="nn mc it bd md no np dn mh nq nr dp ml lh ns nt mn ll nu nv mp lp nw nx mr ny bi translated">零射击学习</h2><p id="4a87" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">我们从架子上取下<a class="ae my" href="https://huggingface.co/openai/clip-vit-base-patch32" rel="noopener ugc nofollow" target="_blank">剪辑预训练模型</a>，并将其应用于<a class="ae my" href="https://huggingface.co/datasets/vincentclaes/emoji-predictor/viewer/vincentclaes--emoji-predictor/test" rel="noopener ugc nofollow" target="_blank">测试数据集</a>。这被称为“零射击”学习，因为我们没有采取任何训练数据来微调剪辑。</p><p id="b1ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 1预测</strong> <br/>通过零镜头学习，我们得到预测正确表情符号的精度为0.13。相比之下，随机选择一个表情符号的精度为1/32或0.0325。不错吧？</p><p id="efb0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是一个片段，展示了如何使用zero-shot进行预测:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="31cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于对于一条推文的正确表情符号可能存在一些困惑，我也尝试提出了4条建议。如果4个建议中有一个与标签相同，我认为这是一个有效的预测。</p><p id="6625" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当提供4个建议时，我们得到的精度是0.33，其中<strong class="la iu"> <br/> </strong> 4/32或0.12是随机选择4个建议的精度。</p><p id="95d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面你会看到这些建议的混乱矩阵。对角线上的瓷砖越亮，我们的模型对表情符号的预测就越好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/74553812ea2e6512d8f3baaa3cdfb8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NRQiJjGYbUwPNTQ4fho6hA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">用于建议表情符号的混淆矩阵(精度为0.33)。</figcaption></figure><blockquote class="lu lv lw"><p id="693b" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">我们从中学到了什么？</p><p id="aed6" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">CLIP无需从数据集中学习模式，就可以识别推文中的情绪，并将其分配给正确的表情符号。CLIP在如此大量的图像-标题组合上进行了预训练，看起来它学会了识别快乐或悲伤的推文，并可以识别表情图像中的这些情绪。</p></blockquote><h2 id="8efd" class="nn mc it bd md no np dn mh nq nr dp ml lh ns nt mn ll nu nv mp lp nw nx mr ny bi translated">少拍和多拍</h2><p id="74da" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">如果我们让CLIP有机会学习我们数据集中的模式会怎么样？我们获取训练数据集，并逐步向片段提供数据。</p><p id="51e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在零射击旁边，你有“一次射击”和“几次射击”学习，这意味着你在一个或几个样本上训练。<br/>我们从1个样本开始微调，逐渐将样本增加到5个、10个、15个……每个表情符号最多500个。</p><ul class=""><li id="833e" class="mz na it la b lb lc le lf lh nz ll oa lp ob lt ne nf ng nh bi translated">我们用的是原始推文(不清洗)；</li><li id="b9cc" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">不调音；</li><li id="e522" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">批量大小为32，因为我们有32个表情符号；</li><li id="e930" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">训练了4个时代。</li></ul><p id="6609" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了一系列样本的预测和建议的精确度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/95e9297f377bf5f25b307ad55f5baff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ElaiHjHnP93AFLSEDW3dA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">每个表情符号的一系列样本的预测和建议的精确度。</figcaption></figure><ul class=""><li id="dd6f" class="mz na it la b lb lc le lf lh nz ll oa lp ob lt ne nf ng nh bi translated">😅1次拍摄比0次拍摄效果差。</li><li id="da78" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">🚀从5个样本开始，精确度开始上升。</li><li id="60b6" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">⚠️:在15个样本中，建议值开始稳定在0.51。</li><li id="c107" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">🐌精度逐渐增加到最大值0.63。</li></ul><h1 id="3975" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">📢—带走</h1><p id="45ca" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">在看不到任何训练数据的情况下，CLIP已经表现得很好了。它可以识别句子中的情绪，并将其与表情符号联系起来。</p><p id="c74b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">微调CLIP进一步提高了性能，每个表情符号只有15个样本，性能显著提高。</p><p id="e55e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这很好，因为组装一个小数据集相对便宜！像CLIP这样的深度学习神经网络可以为没有任何或没有大量数据进行训练，或者没有资源收集大量训练数据的企业普及人工智能。</p><p id="6f07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想推荐图像、图画或绘画等视觉效果，CLIP是一个很好的开始。如果你需要帮助，你可以<a class="ae my" href="https://www.linkedin.com/in/vincent-claes-0b346337/" rel="noopener ugc nofollow" target="_blank">联系我</a>。</p><h2 id="308f" class="nn mc it bd md no np dn mh nq nr dp ml lh ns nt mn ll nu nv mp lp nw nx mr ny bi translated">⚡️Try自己拿出了模型！</h2><p id="75ac" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">【https://huggingface.co/spaces/vincentclaes/emoji-predictor】→<a class="ae my" href="https://huggingface.co/spaces/vincentclaes/emoji-predictor" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"/></a><strong class="la iu">↓</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/2454378dd9b6bc2beb45917e8bdce1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ta3XorNDFQ_nuPqdVIFVyw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">我的huggingface空间上gradio应用程序的标题。</figcaption></figure><h1 id="db0f" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">链接</h1><ul class=""><li id="6597" class="mz na it la b lb mt le mu lh nb ll nc lp nd lt ne nf ng nh bi translated">型号:<a class="ae my" href="https://huggingface.co/vincentclaes/emoji-predictor" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/vincentclaes/emoji-predictor</a></li><li id="e4f2" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">数据集:<a class="ae my" href="https://huggingface.co/datasets/vincentclaes/emoji-predictor" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/datasets/Vincent claes/e moji-predictor</a></li></ul><h1 id="8419" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">文森特·克拉斯</h1><p id="285d" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">👋在<a class="ae my" href="https://medium.com/@vincentclaes_43752" rel="noopener"> Medium </a>、<a class="ae my" href="https://www.linkedin.com/in/vincent-claes-0b346337/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>和<a class="ae my" href="https://twitter.com/VincentClaes1" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我，阅读更多关于ML工程和ML管道的内容。</p><h2 id="419b" class="nn mc it bd md no np dn mh nq nr dp ml lh ns nt mn ll nu nv mp lp nw nx mr ny bi translated">来源</h2><ul class=""><li id="0d33" class="mz na it la b lb mt le mu lh nb ll nc lp nd lt ne nf ng nh bi translated">[1]:打开艾的剪辑首页:<a class="ae my" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/clip/</a></li><li id="189a" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">[2]:文章描述剪辑:<a class="ae my" href="https://towardsdatascience.com/openais-dall-e-and-clip-101-a-brief-introduction-3a4367280d4e" rel="noopener" target="_blank">https://towards data science . com/open ais-Dall-e-and-CLIP-101-a-brief-introduction-3a 4367280 d4e</a></li><li id="e428" class="mz na it la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">[3]:什么是精度:<a class="ae my" href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/crash-course/classification/precision-and-recall</a></li></ul></div></div>    
</body>
</html>