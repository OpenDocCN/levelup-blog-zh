<html>
<head>
<title>Reinforcement Learning — Teaching the Machine to Gamble with Q-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——教机器用Q学习赌博</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/reinforcement-learning-teaching-the-machine-to-gamble-with-q-learning-bc6790ee66c2?source=collection_archive---------2-----------------------#2021-01-15">https://levelup.gitconnected.com/reinforcement-learning-teaching-the-machine-to-gamble-with-q-learning-bc6790ee66c2?source=collection_archive---------2-----------------------#2021-01-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="5f82" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">增强学习是人工智能和机器学习的一个领域，它涉及模拟许多场景以优化结果。强化学习中最常用的方法之一是Q学习方法。在Q-learning中，创建了一个模拟环境，算法包括每个模拟场景的一组“S”状态、一组“A”动作和一个代理，该代理采取这些动作以渗透到状态中。</p><p id="863b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每当代理采取集合“A”中的动作“A ”,它就从一个状态转换到环境中的另一个状态。在环境中的特定状态下执行一个动作会给代理人返回一个奖励，这个奖励可以是好的也可以是坏的。该模型的目标始终是找到一组行动，使回报最大化，并以对环境最好的方式发展。在一组强化学习算法中有几种不同的技术，从具有定义的策略的数学模型到更复杂的模型，如进化模型和深度学习模型，如深度强化学习。</p><p id="41ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在上一篇文章中，你可以找到下面的链接，我写了如何使用机器学习来预测真实的足球比赛结果，现在我将写如何使用强化学习模型来对真实的足球比赛进行下注和优化。</p><p id="ed91" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kx" href="https://octaviobomfim.medium.com/predicting-real-soccer-matches-using-fantasy-game-scouts-a3b388edb8aa" rel="noopener">https://octaviobomfim . medium . com/predicting-real-football-matches-using-fantasy-game-scouts-a3b 388 EDB 8 aa</a></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/60110fc870531f48d3f9be3797937a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*vAysTdrHqINisDbs_NW_pQ.jpeg"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">瓢虫</figcaption></figure><h1 id="ca67" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">q学习</h1><p id="648a" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">Q-learning是一个基于策略的强化学习模型，旨在根据当前状态找到要采取的最佳行动，在这种情况下没有定义行动策略。这种模型被认为是一种非策略模型，因为Q-learning功能通过当前策略之外的动作来学习，换句话说，它的学习以探索的方式进行，通过采取随机动作来创建使该集的总回报最大化的动作策略。</p><p id="707f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">为什么问？这个政策是什么？</strong></p><p id="cd7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">字母Q代表质量，学习模型基于Q表(质量表), Q表是模型可以在每个状态的环境中使用的动作策略。因此，我们有一个表[state，action]来表示一个策略，其中每个动作对于每个状态都有一个质量值(Q值)。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/2dd3fb7670510aa2430fd65df2088750.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*LJPHo3Kj_0DKvqjm63Upeg.png"/></div></figure><p id="7690" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">动作是环境的一部分，在上面的例子中，我们有向北、向南、向东、向西等行走的动作。状态的概念是一组变量，表示模型在模拟环境中的演变。</p><p id="3a16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随着所采取的每个动作，Q值通过值迭代的概念来更新，该值迭代遵循被称为马尔可夫决策过程(MDP)和贝尔曼方程的决策过程。该等式由所采取的动作的旧Q值以及动作的奖励和新状态的最大Q值组成，两者都从“学习率”中扣除，该“学习率”衡量当前值和新值之间的质量。因此，该模型仅依赖于[状态，行动]和在环境中采取的行动中观察到的回报。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/56b2b56088b783916f8b348d81c07281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wIGmhpK30BFqXo2RTIK3w.png"/></div></div></figure><h1 id="a526" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">模型开发—代码</h1><p id="cbfa" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">在经历了概念之后，我将继续代码部分和模型的结果。该解决方案的完整代码可以通过下面的链接在我的GitHub上找到:</p><div class="mt mu gp gr mv mw"><a href="https://github.com/octavio-santiago/Reinforcement_Learning_Bet/blob/master/RL%20Q%20learning%20value%20iteration.py" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">奥克塔维奥-圣地亚哥/强化_学习_打赌</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">强化_学习_下注。通过创建一个社区，为Octavio-Santiago/Reinforcement _ Learning _ Bet的发展做出贡献</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">github.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk le mw"/></div></div></a></div><p id="aea5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用于开发的库，根据它们的导入:</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="74c4" class="nq ll it nm b gy nr ns l nt nu">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pickle<br/>from matplotlib import style<br/>import time<br/>	<br/>import pandas as pd</span></pre><p id="906d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们定义了贝尔曼方程、情节和学习率的一些值，以及模拟下注环境的初始值，包括每局的投资值和初始投资组合值。</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="ecbd" class="nq ll it nm b gy nr ns l nt nu">HM_EPISODES = 45000<br/>epsilon = 0.9<br/>EPS_DECAY = 0.9998<br/>LEARNING_RATE = 0.1<br/>DISCOUNT = 0.95</span></pre><ul class=""><li id="2177" class="nv nw it js b jt ju jx jy kb nx kf ny kj nz kn oa ob oc od bi translated">学习率定义了随着质量(Q)值的更新，探索中奖励的权重。</li><li id="4259" class="nv nw it js b jt oe jx of kb og kf oh kj oi kn oa ob oc od bi translated">起初，我使用了45000集，这是模型将遵循的迭代次数。</li><li id="4f5f" class="nv nw it js b jt oe jx of kb og kf oh kj oi kn oa ob oc od bi translated">ε和eps衰减表示初始开采系数及其随迭代的衰减</li><li id="02d5" class="nv nw it js b jt oe jx of kb og kf oh kj oi kn oa ob oc od bi translated">贴现表示行动后Q的未来价值的权重</li></ul><p id="db7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在定义之后，如果必要的话，模型加载先前的Q表，或者启动新的Q表来开始迭代。</p><p id="ab99" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每一集都会经过之前选择的每一个游戏进行迭代。</p><h2 id="798b" class="nq ll it bd lm oj ok dn lq ol om dp lu kb on oo ly kf op oq mc kj or os mg ot bi translated">创建模拟环境</h2><p id="32af" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">模拟环境有3种不同的功能:</p><ul class=""><li id="0e0f" class="nv nw it js b jt ju jx jy kb nx kf ny kj nz kn oa ob oc od bi translated">action()通过选择策略和赌注来执行模型定义的操作</li><li id="453d" class="nv nw it js b jt oe jx of kb og kf oh kj oi kn oa ob oc od bi translated">strategy()从action()函数接收动作，并有效地执行先前定义的策略。在这个模型中，我们有3个策略:“Min”选择具有最低odd的团队，“Max”选择具有最高odd的团队，“ML”根据另一个时间开发的机器学习模型选择团队。</li><li id="0931" class="nv nw it js b jt oe jx of kb og kf oh kj oi kn oa ob oc od bi translated">bet()接收action()和strategy()值，并使用游戏的真实值应用数据集中的策略，从而返回赌注的结果值，无论它是成功还是失败。</li></ul><p id="1114" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">紧接着，我们在剧集和游戏的循环中定义了剥削政策:</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="cd3a" class="nq ll it nm b gy nr ns l nt nu">if np.random.random() &gt; epsilon:<br/>   action_n = np.argmax(q_table[obs])<br/>else:<br/>   action_n = np.random.randint(0,high=2)</span></pre><p id="fe60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们有一个部分执行每一次下注的利润和回报计算，更新累计利润，并设置一个新的观察状态，一个由投资组合的价值和累计利润表示的状态。</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="1b2a" class="nq ll it nm b gy nr ns l nt nu">j = result[0][0]<br/>	        if j == -1:<br/>	            erros += 1<br/>	            l_tot = 0<br/>	        elif j == 0:<br/>	            l_tot = 0  <br/>	        elif j == 1:<br/>	            l_tot = result[1][0]<br/>	            <br/>	        lucro = (l_tot*invest) - invest        <br/>	        reward = lucro<br/>	<br/><br/>	        lucro0 += lucro<br/>	 <br/><br/>new_obs = (carteira,lucro0) # get new state</span></pre><p id="f745" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，应用贝尔曼方程来更新该状态的Q值。</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="2bb3" class="nq ll it nm b gy nr ns l nt nu">try:<br/>	  max_future_q = np.max(q_table[new_obs])<br/>except:<br/>	  q_table[new_obs] = [0 for i in range(val)]<br/>	  max_future_q = np.max(q_table[new_obs])<br/>	            <br/>	        <br/>current_q = q_table[obs][action_n] <br/>	 <br/>new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)<br/><br/>q_table[obs][action_n] = new_q #update actual q</span></pre><h1 id="d089" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">结果</h1><p id="15a3" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">该模型被执行，并通过Matplotlib库，生成一些图形来跟踪模型的性能和带有匹配结果的日志。</p><p id="ee40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下图在顶部显示了每300集的平均利润，在底部，蓝色显示了每次下注的结果，红色显示了初始投资组合价值。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ou"><img src="../Images/03c2ee13bfcd31af96cbf6c77c6a9a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iIcdIRbbJzKgakucXZKEFQ.png"/></div></div></figure><p id="5be9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过情节分析模型的性能，我们看到，在开始时，模型非常不稳定，因为它受探索的影响很大，它不知道如何做最好的押注，因此模型执行许多随机移动，并在一段时间内达到负面结果。沿着蓝线，我们看到波动性降低，赌注的总价值稳定在非常正的值，直到它超过最初的投资组合本身，从而使投资翻倍，并最终稳定在80.00美元的累计<strong class="js iu">利润范围内。</strong></p><p id="7951" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，模型对这些匹配的决策策略是:</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="b703" class="nq ll it nm b gy nr ns l nt nu">['ml', 'max', 'ml', 'ml', 'max', 'max', 'max', 'max', 'ml', 'max', 'max', 'ml', 'min', 'ml', 'min', 'ml', 'min', 'ml', 'ml', 'ml', 'ml', 'ml', 'ml', 'max', 'max', 'ml', 'ml', 'min', 'ml', 'ml']</span></pre><p id="70da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型57%的时间使用ML算法建议，30%的时间使用最高odd，13%的时间使用最低odd。这向我们展示了所开发的ML模型的效率，以及最大值的第二个选择，它具有更大的风险，但提供了更高的回报，最终弥补了损失。</p><p id="3a51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最终结果是:</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="b8c5" class="nq ll it nm b gy nr ns l nt nu">Final Wallet: $ 142.15999999999997, Profit: $ 82.15999999999998</span></pre><p id="3577" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模拟环境有30个游戏，投资额为2美元，初始投资组合为60美元。累计的<strong class="js iu">利润是82.16美元</strong>，这代表了<strong class="js iu">最初投资组合</strong>的136.93%，这个模型使这些智能下注的投资额增加了一倍多！</p><p id="38d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是对每场比赛结果和最后一集模型性能的下注详情:</p><pre class="kz la lb lc gt nl nm nn no aw np bi"><span id="5817" class="nq ll it nm b gy nr ns l nt nu">Profit 1.9 Profit ac 1.9 erros 0 action 0 ...............<br/>Profit 4.2 Profit ac 6.1 erros 0 action 0 ...............<br/>Profit 4.4 Profit ac 10.5 erros 0 action 0 ...............<br/>Profit 0.8399999999999999 Profit ac 11.34 erros 0 action 0 .......<br/>Profit -2 Profit ac 9.34 erros 1 action 0 ...............<br/>Profit 13.5 Profit ac 22.84 erros 0 action 1 ...............<br/>Profit -2 Profit ac 20.84 erros 1 action 0 ...............<br/>Profit 4.2 Profit ac 25.04 erros 0 action 1 ...............<br/>Profit 5.0 Profit ac 30.04 erros 0 action 1 ...............<br/>Profit 3.9000000000000004 Profit ac 33.94 erros 0 action 1 ........<br/>Profit 2.3 Profit ac 36.239999999999995 erros 0 action 2 .........<br/>Profit 0.79999999999999 Profit ac 37.03999999999999 erros 0 action 0 <br/>Profit 2.40000000000000 Profit ac 39.43999999999999 erros 0 action 2 <br/>Profit 0.799999999999999 lucro ac 40.23999999999999 erros 0 action 0 <br/>Profit -2 Profit ac 38.23999999999999 erros 1 action 0 ............<br/>Profit 5.0 Profit ac 43.23999999999999 erros 0 action 0 ............<br/>Profit 1.2999999999999 Profit ac 44.539999999999985 erros 0 action 0 <br/>Profit 3.3 Profit ac 47.83999999999998 erros 0 action 0 ............<br/>Profit 3.4000000000000 Profit ac 51.23999999999998 erros 0 action 0 <br/>Profit -2 Profit ac 49.23999999999998 erros 1 action 0 ............<br/>Profit 0.56 Profit ac 49.79999999999998 erros 0 action 0 ..........<br/>Profit 5.4 Profit ac 55.19999999999998 erros 0 action 0 ...........<br/>Profit 2.3 Profit ac 57.49999999999998 erros 0 action 0 ...........<br/>Profit 5.8 Profit ac 63.299999999999976 erros 0 action 1 ..........<br/>Profit 5.0 Profit ac 68.29999999999998 erros 0 action 1 ...........<br/>Profit -2 Profit ac 66.29999999999998 erros 1 action 0 .............<br/>Profit 1.14000000000000 Profit ac 67.43999999999998 erros 0 action 0 <br/>Profit -2 Profit ac 65.43999999999998 erros 1 action 1 .............<br/>Profit 0.72000000000000 Profit ac 66.15999999999998 erros 0 action 0 <br/>Profit 16.0 Profit ac 82.15999999999998 erros 0 action 1 ...........</span></pre><p id="719c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看到详细的赌注我们看到模型错过了一些游戏，但总的来说，它结束得非常积极。解释模型不能完美匹配的主要原因之一是，我传递给它的策略是基于最小和最大的赔率或基于机器学习模型，这不允许它探索每场比赛的所有可能情况。</p><h1 id="90db" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">结论</h1><p id="6cc5" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">根据给出的结果，可以得出结论，强化学习模型成功地学会了支配下注环境并为每场比赛选择最佳策略。对于我来说，在没有任何前期政策编程的情况下，使用机器学习对足球比赛进行投注，并且仍然在投注中大获成功地完成模拟，这是一件非常有趣的事情。对于真实的情况，我们将处理概率，因为在它发生之前我们没有匹配的结果，但是，由于模拟，我们可以找到一种更优化的方式来下注。</p><p id="8cfa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在RL领域还有几个更复杂的策略，我将在未来介绍，我写这篇文章的目的是将我在该领域的一些经验带给那些感兴趣的人，并为社区做出贡献，我希望我已经帮助理解了这个主题，并且我可以随时联系任何人。感谢阅读。</p><p id="a9fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你想了解更多，请随时在LinkedIn联系我！<a class="ae kx" href="https://www.linkedin.com/in/octavio-b-santiago/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/octavio-b-santiago/</a></p></div></div>    
</body>
</html>