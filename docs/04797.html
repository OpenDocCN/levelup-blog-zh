<html>
<head>
<title>Scikit-Learn (Python): 6 Useful Tricks for Data Scientists</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scikit-Learn (Python):数据科学家的6个有用技巧</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/scikit-learn-python-6-useful-tricks-for-data-scientists-1a0a502a6aa3?source=collection_archive---------1-----------------------#2020-07-16">https://levelup.gitconnected.com/scikit-learn-python-6-useful-tricks-for-data-scientists-1a0a502a6aa3?source=collection_archive---------1-----------------------#2020-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="33b7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用scikit-learn (sklearn)改进Python中的机器学习模型的技巧</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5f8671ef7889e78aa036c4a1e61952da.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*vddwWe8WbpPWZzcPQmaQMg.png"/></div></figure><p id="4d4b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> Scikit-learn (sklearn) </strong>是一个强大的开源<strong class="ks iu">机器学习库</strong>构建在Python编程语言之上。这个库包含许多用于机器学习和统计建模的高效工具，包括各种分类、回归和聚类算法。</p><p id="886b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在本文中，我将展示关于scikit-learn库的6个技巧，以使某些编程实践变得简单一些。</p><h1 id="fc49" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">1.生成随机虚拟数据</h1><p id="1580" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">为了生成随机的“虚拟”数据，我们可以在<strong class="ks iu">分类数据</strong>的情况下使用<code class="fe mj mk ml mm b">make_classification()</code>函数，在<strong class="ks iu">回归数据</strong>的情况下使用<code class="fe mj mk ml mm b">make_regression()</code>函数。这在某些情况下非常有用，例如在调试时，或者当您想要在(小的)随机数据集上尝试某些东西时。</p><p id="f0af" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下面，我们生成10个分类数据点，包括4个特征(在X中找到)和一个类标签(在y中找到)，其中数据点属于负类(0)或正类(1):</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="aa05" class="mr ln it mm b gy ms mt l mu mv">from sklearn.datasets import make_classification<br/>import pandas as pd</span><span id="d2d4" class="mr ln it mm b gy mw mt l mu mv">X, y = make_classification(n_samples=10, n_features=4, n_classes=2, random_state=123)</span></pre><p id="86c8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这里，X由生成的数据点的4个特征列组成:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="532e" class="mr ln it mm b gy ms mt l mu mv">pd.DataFrame(X, columns=['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/50a419c52644547d122b84452c941309.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*DOj28YtNA1RLkSNyVGTzag.png"/></div></figure><p id="b71b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">y包含每个数据点的相应标签:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="b25e" class="mr ln it mm b gy ms mt l mu mv">pd.DataFrame(y, columns=['Label'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/d7bdfedcf8d3df08a794aa52c74af661.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/format:webp/1*e_lV7s0T6mA4HSD3i1ZnJg.png"/></div></figure><h1 id="1cc6" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">2.估算缺失值</h1><p id="9d6d" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">Scikit-learn提供了多种方法来<strong class="ks iu">估算</strong>缺失值。这里，我们考虑两种方法。<code class="fe mj mk ml mm b">SimpleImputer</code>类提供了输入缺失值的基本策略(例如通过平均值或中值)。更复杂的方法是<code class="fe mj mk ml mm b">KNNImputer</code>类，它使用<strong class="ks iu">K-最近邻</strong>方法提供填补缺失值的插补。使用具有特定要素值的<code class="fe mj mk ml mm b">n_neighbors</code>最近邻的值来估算每个缺失值。邻居的值被均匀地平均或根据到每个邻居的距离加权。</p><p id="7870" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下面，我们展示了一个使用两种插补方法的应用示例:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="34b9" class="mr ln it mm b gy ms mt l mu mv">from sklearn.experimental import enable_iterative_imputer<br/>from sklearn.impute import SimpleImputer, KNNImputer<br/>from sklearn.datasets import make_classification<br/>import pandas as pd</span><span id="aaff" class="mr ln it mm b gy mw mt l mu mv">X, y = make_classification(n_samples=5, n_features=4, n_classes=2, random_state=123)<br/>X = pd.DataFrame(X, columns=['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4'])</span><span id="caa9" class="mr ln it mm b gy mw mt l mu mv">print(X.iloc[1,2])</span></pre><p id="2f40" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi">&gt;&gt;&gt; 2.21298305</p><p id="ea81" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">将X[1，2]转换为缺失值:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="b378" class="mr ln it mm b gy ms mt l mu mv">X.iloc[1, 2] = float('NaN')</span><span id="60b1" class="mr ln it mm b gy mw mt l mu mv">X</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/bd5e9d52afc796e0b2bf9febd3c39119.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*ibPA0HbxWpxv8CjvCADq9A.png"/></div></figure><p id="acca" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">首先我们使用<strong class="ks iu">简单估算器</strong>:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="0299" class="mr ln it mm b gy ms mt l mu mv">imputer_simple = SimpleImputer()<br/><br/>pd.DataFrame(imputer_simple.fit_transform(X))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/4b7795532334bf6b98363b92717c1960.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*sceJUbPTq9pf90CDpTIL2A.png"/></div></figure><p id="482b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">导致值为<strong class="ks iu"> -0.143476 </strong>。</p><p id="dd89" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">接下来，我们尝试使用<strong class="ks iu"> KNN估算器</strong>，其中考虑了两个最近的邻居，并对这些邻居进行了统一加权:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="101b" class="mr ln it mm b gy ms mt l mu mv">imputer_KNN = KNNImputer(n_neighbors=2, weights="uniform")</span><span id="df22" class="mr ln it mm b gy mw mt l mu mv">pd.DataFrame(imputer_KNN.fit_transform(X))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/ce371c15f9580f2fb7e2d44da14e5170.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*WZSrGdDG5NFf66YyC30wRg.png"/></div></div></figure><p id="51e0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">导致值为<strong class="ks iu">0.997105</strong>(= 0.5 *(1.904188+0.090022))。</p><h1 id="bc4d" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak"> 3。利用管道将多个步骤链接在一起</strong></h1><p id="55d7" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">scikit-learn中的<strong class="ks iu">管道</strong>工具对于简化你的机器学习模型非常有帮助。管道可以用来将多个步骤连接成一个步骤，这样数据将通过一个固定的步骤序列。因此，管道将所有步骤连接成一个系统，而不是分别调用每个步骤。为了创建这样的管道，我们使用了<code class="fe mj mk ml mm b">make_pipeline</code>函数。</p><p id="5038" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下面显示了一个简单的示例，其中管道由一个估算器和一个逻辑回归分类器组成，估算器用于估算缺失值(如果有)。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="e21a" class="mr ln it mm b gy ms mt l mu mv">from sklearn.model_selection import train_test_split<br/>from sklearn.impute import SimpleImputer<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.datasets import make_classification<br/>import pandas as pd</span><span id="0898" class="mr ln it mm b gy mw mt l mu mv">X, y = make_classification(n_samples=25, n_features=4, n_classes=2, random_state=123)<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)<br/><br/>imputer = SimpleImputer()<br/>clf = LogisticRegression()<br/><br/>pipe = make_pipeline(imputer, clf)</span></pre><p id="976c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我们可以使用管道来拟合我们的训练数据，并对测试数据进行预测。首先，训练数据经过imputer，然后它开始使用逻辑回归分类器进行训练。然后，我们能够预测测试数据的类别:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="605a" class="mr ln it mm b gy ms mt l mu mv">pipe.fit(X_train, y_train)<br/><br/>y_pred = pipe.predict(X_test)</span><span id="1207" class="mr ln it mm b gy mw mt l mu mv">pd.DataFrame({'Prediction': y_pred, 'True': y_test})</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6205b0d6d281354d7f7c8f310326f74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*ZK-EtmhNXt5a_emO0qReEw.png"/></div></figure><h1 id="9e07" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">4.使用joblib保存管道模型</h1><p id="2024" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">通过使用<strong class="ks iu"> joblib </strong>可以很容易地保存通过scikit-learn创建的管道模型。如果您的模型包含大型数据数组，每个数组都存储在一个单独的文件中。一旦保存在本地，用户就可以轻松地加载(或恢复)他们的模型，以便在新的应用程序中使用。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="3f05" class="mr ln it mm b gy ms mt l mu mv">from sklearn.model_selection import train_test_split<br/>from sklearn.impute import SimpleImputer<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.datasets import make_classification<br/>import joblib</span><span id="78d3" class="mr ln it mm b gy mw mt l mu mv">X, y = make_classification(n_samples=20, n_features=4, n_classes=2, random_state=123)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)<br/><br/>imputer = SimpleImputer()<br/>clf = LogisticRegression()<br/><br/>pipe = make_pipeline(imputer, clf)<br/><br/>pipe.fit(X_train, y_train)</span><span id="63e5" class="mr ln it mm b gy mw mt l mu mv">joblib.dump(pipe, 'pipe.joblib')</span></pre><p id="192b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，拟合的管线模型通过<code class="fe mj mk ml mm b">joblib.dump</code>保存(转储)在您的计算机上。该模型通过<code class="fe mj mk ml mm b">joblib.load</code>恢复，之后可以照常应用:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="ba9a" class="mr ln it mm b gy ms mt l mu mv">new_pipe = joblib.load('.../pipe.joblib')</span><span id="a1fc" class="mr ln it mm b gy mw mt l mu mv">new_pipe.predict(X_test)</span></pre><h1 id="dbcb" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">5.绘制混淆矩阵</h1><p id="d2c7" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated"><strong class="ks iu">混淆矩阵</strong>是用于描述分类器对一组测试数据的性能的表格。这里，我们关注一个<strong class="ks iu">二元分类问题</strong>，即观察值可能属于两个可能的类别:“是”(1)和“否”(0)。</p><p id="04fd" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们利用<code class="fe mj mk ml mm b">plot_confusion_matrix</code>函数创建一个二进制分类问题的例子，并显示相应的混淆矩阵:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="e570" class="mr ln it mm b gy ms mt l mu mv">from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import plot_confusion_matrix<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.datasets import make_classification</span><span id="d3e9" class="mr ln it mm b gy mw mt l mu mv">X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=123)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)<br/><br/>clf = LogisticRegression()<br/><br/>clf.fit(X_train, y_train)<br/><br/>confmat = plot_confusion_matrix(clf, X_test, y_test, cmap="Blues")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/e7a960bc62c0af68a9c3e012f1625a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*RbEl-2gt5pQnYHUf714Rqg.png"/></div></figure><p id="dd94" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这里，我们通过混淆矩阵以一种很好的方式可视化了:</p><ul class=""><li id="c691" class="nh ni it ks b kt ku kw kx kz nj ld nk lh nl ll nm nn no np bi translated">93 <strong class="ks iu">真阳性(TP)；</strong></li><li id="c71c" class="nh ni it ks b kt nq kw nr kz ns ld nt lh nu ll nm nn no np bi translated">97 <strong class="ks iu">真底片(TN)；</strong></li><li id="e3d9" class="nh ni it ks b kt nq kw nr kz ns ld nt lh nu ll nm nn no np bi translated">3 <strong class="ks iu">误报(FP)</strong>；</li><li id="e8e3" class="nh ni it ks b kt nq kw nr kz ns ld nt lh nu ll nm nn no np bi translated">7 <strong class="ks iu">假阴性(FN) </strong>。</li></ul><p id="32e8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">所以，我们达到了(93+97)/200 = 95%的准确率。</p><h1 id="05f4" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">6.可视化决策树</h1><p id="f3e6" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">最著名的分类算法之一是<strong class="ks iu">决策树</strong>，其特点是<strong class="ks iu"> </strong>其树状可视化非常直观。决策树的思想是根据描述性特征将数据分割成更小的区域。然后，在测试观察所属的区域中的训练观察中最常出现的类别是预测。为了决定如何将数据分割成区域，必须应用<strong class="ks iu">分割度量</strong>来确定每个特征的相关性和重要性。一些众所周知的分裂措施是信息增益，基尼指数和交叉熵。</p><p id="a1d6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下面，我们展示了一个如何利用scikit-learn中的<code class="fe mj mk ml mm b">plot_tree</code>函数的示例:</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="e002" class="mr ln it mm b gy ms mt l mu mv">from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier, plot_tree<br/>from sklearn.datasets import make_classification<br/><br/>X, y = make_classification(n_samples=50, n_features=4, n_classes=2, random_state=123)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)<br/><br/>clf = DecisionTreeClassifier()<br/><br/>clf.fit(X_train, y_train)<br/><br/>plot_tree(clf, filled=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1d3d773f544f4268c332a3bf499973f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ZUryjl_GFxG1SwZm3sdClA.png"/></div></figure><p id="9b7d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这个例子中，我们在40个训练观察值上拟合决策树，这些观察值属于否定类(0)或肯定类(1)，因此我们正在处理一个<strong class="ks iu">二元分类问题</strong>。在树中，我们有两种节点，即<strong class="ks iu">内部节点</strong>(预测器空间被进一步分割的节点)或<strong class="ks iu">终端节点</strong>(端点)。连接两个节点的树段被称为<strong class="ks iu">分支</strong>。</p><p id="7912" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们仔细看看为决策树中的每个节点提供的信息:</p><ul class=""><li id="514b" class="nh ni it ks b kt ku kw kx kz nj ld nk lh nl ll nm nn no np bi translated">在特定节点中使用的<strong class="ks iu">分裂标准</strong>显示为例如‘F2&lt;=-0.052’。这意味着满足第二特征的值低于-0.052的条件的每个数据点属于左边新形成的区域，而不满足该条件的数据点属于内部节点右边的区域。</li><li id="d430" class="nh ni it ks b kt nq kw nr kz ns ld nt lh nu ll nm nn no np bi translated"><strong class="ks iu">基尼指数</strong>在这里被用作分割指标。基尼指数(称为杂质的量度)衡量的是随机选择的特定元素被错误分类的程度或概率。</li><li id="6005" class="nh ni it ks b kt nq kw nr kz ns ld nt lh nu ll nm nn no np bi translated">节点的“样本”指示在特定节点中找到了多少训练观测值。</li><li id="2847" class="nh ni it ks b kt nq kw nr kz ns ld nt lh nu ll nm nn no np bi translated">节点的“值”指示分别在负类(0)和正类(1)中找到的训练观察的数量。因此，value=[19，21]意味着在该特定节点中，19个观察值属于负类，21个观察值属于正类。</li></ul><h1 id="3cb4" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">结论</h1><p id="bec9" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">本文介绍了6个有用的scikit-learn技巧，以改进sklearn中的机器学习模型。我希望这些技巧在某种程度上对您有所帮助，并祝您在使用scikit-learn库的下一个项目中好运！</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h1 id="a55c" class="lm ln it bd lo lp oc lr ls lt od lv lw jz oe ka ly kc of kd ma kf og kg mc md bi translated">分级编码</h1><p id="4c7c" class="pw-post-body-paragraph kq kr it ks b kt me ju kv kw mf jx ky kz mg lb lc ld mh lf lg lh mi lj lk ll im bi translated">感谢您成为我们社区的一员！<a class="ae oh" href="https://www.youtube.com/channel/UC3v9kBR_ab4UHXXdknz8Fbg?sub_confirmation=1" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu">订阅我们的YouTube频道</strong> </a>或者加入<a class="ae oh" href="https://skilled.dev/" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu"> Skilled.dev编码面试课程</strong> </a>。</p><div class="oi oj gp gr ok ol"><a href="https://skilled.dev" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">编写面试问题</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">掌握编码面试的过程</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">技术开发</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ko ol"/></div></div></a></div></div></div>    
</body>
</html>