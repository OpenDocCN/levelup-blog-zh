<html>
<head>
<title>DQN from Scratch with TensorFlow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DQN从零开始使用TensorFlow 2</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/dqn-from-scratch-with-tensorflow-2-eb0541151049?source=collection_archive---------0-----------------------#2020-07-15">https://levelup.gitconnected.com/dqn-from-scratch-with-tensorflow-2-eb0541151049?source=collection_archive---------0-----------------------#2020-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="66e9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个高度可行的计划，以建立自己的DQN</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a57b91162b7eefaab7ac5e3e8c253af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxZ7TgR8a-HuJhrCnFh4YA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Max Kleinen 在<a class="ae ky" href="https://unsplash.com/s/photos/robot?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片(这是我最喜欢的游戏中的一个角色。猜猜那是什么？)</figcaption></figure><p id="3638" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">自从大肆宣传Alpha Zero以来，深度强化学习已经遍布新闻。</p><p id="b1ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然A3C正在成为RL的首选方法，但对于初学者来说，DQN仍然是了解RL基础的更好方法。</p><p id="85f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，很少有教程适合有抱负的RL开发人员。</p><p id="4dc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大量现有的深度强化学习教程(分为1个或更多):</p><ul class=""><li id="cd8b" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">没有代码:(</li><li id="5736" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">使用抽象出必要概念的高级包，如<code class="fe ms mt mu mv b">tf_agent</code></li><li id="d066" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">不要解释高层概念和实现细节之间的联系</li></ul><p id="7343" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章旨在成为一个<strong class="lb iu">高度可操作的</strong>循序渐进的教程:</p><ul class=""><li id="14f9" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">演示如何构建一个完整的DQN模型</li><li id="294a" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">不用比TensorFlow 2和OpenAI Gym更高级的东西</li><li id="2285" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">解释如何从高级概念到实现，一次一步，与真实的开发流程保持一致</li></ul><p id="de86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于阅读代码比阅读帖子更好的代码忍者来说，代码在<a class="ae ky" href="https://github.com/ultronify/dqn-from-scratch-with-tf2" rel="noopener ugc nofollow" target="_blank">这个库</a>中，更复杂的版本在<a class="ae ky" href="https://github.com/ultronify/cartpole-tf-dqn" rel="noopener ugc nofollow" target="_blank">这个库</a>中。</p><p id="00c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于喜欢从实时编码会话中学习的人:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h1 id="863f" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">快速回顾一下DQN是如何工作的</h1><p id="6e07" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">由于这篇文章主要关注的是建造DQN的可行计划，我们将只讨论这个理论的过于简化的版本:</p><ul class=""><li id="aa21" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">强化学习:从其<strong class="lb iu">与<strong class="lb iu">环境</strong>交互</strong>的<strong class="lb iu">后果</strong>中学习。</li><li id="de72" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">Q学习:预测对给定状态采取行动的<strong class="lb iu">预期未来回报</strong>，即Q值。</li><li id="1575" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">深度强化学习(DQN): Q学习，但是使用<strong class="lb iu">深度神经网络</strong>。</li></ul><p id="9ff2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在DQN，我们希望通过预测可以对状态执行的所有可能行动的预期长期回报(Q值)来指导给定状态下的行动选择，因此关键是通过训练让Q值预测越来越接近“真理”。</p><p id="8c9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，DQN训练的核心是得到一个状态-动作对的下一个“更可取的”Q值(深度神经网络训练中的“标签”)。我们称之为目标Q值。</p><p id="dd52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于终端和非终端状态，不同地计算目标Q值的值:</p><p id="31f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当非终结状态转换发生时，如下图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a5d475b37a2b5f773c739072ef67893d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*jMJTFrqMO_lp2ZxbtvzVwg.png"/></div></figure><p id="592b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当前状态和动作对的目标Q值(最优长期报酬)是即时报酬和下一个状态的贴现Q值之和(动作导致的贴现最优长期报酬):<code class="fe ms mt mu mv b">Q_next = gamma * Q_current + r</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/84d54064af5ae8e953a29f1f34764781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*sLprg72Pn2tb1u-POmikmg.png"/></div></figure><p id="696b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，由于下一个状态不存在，当前状态和动作的下一个目标Q值将只是直接的奖励。</p><p id="d984" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经知道了什么是DQN以及它是如何工作的，让我们开始构建一个。</p><h1 id="c312" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">代码框架</h1><p id="bef3" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">正如我所承诺的，我不会用500行实现细节来淹没你。</p><p id="4c74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，我们将从高级组件开始，逐步填充更多的代码。</p><p id="2150" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在DQN，我们需要如图所示的以下组件:</p><ul class=""><li id="3833" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">DQN特工:负责学习如何玩游戏</li><li id="c7a2" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">重放缓冲器:负责存储游戏体验</li><li id="ad28" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">环境:代表游戏规则</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e6a81cab9e8ecbb6bbf32b177bb644a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*dKEemi16NSFX2lw9JRiw0g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">高层设计</figcaption></figure><p id="277a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义了所需的组件后，我们将把上面的组件转换成代码。</p><h2 id="bb36" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">DQN代理商</h2><p id="685b" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">DQN代理的工作是:</p><ol class=""><li id="d6ba" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu oj mk ml mm bi translated">通过预测所有可能动作的Q值来选择给定状态下要采取的动作，并选择具有最高Q值的动作</li><li id="4cce" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu oj mk ml mm bi translated">从过去的游戏经验中学习如何更好地预测Q值</li></ol><p id="4093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，DQN代理需要具备:</p><ol class=""><li id="3ad1" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu oj mk ml mm bi translated">给定当前环境状态产生动作的方法。我们称之为政策。</li><li id="fc85" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu oj mk ml mm bi translated">一种让代理从经验中学习的方法。我们称之为<code class="fe ms mt mu mv b">train</code>,它接受一批游戏体验并输出一个<code class="fe ms mt mu mv b">loss</code>来表示训练的有效性。</li></ol><p id="04c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码将上述要求转化为一个类接口:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><h2 id="c574" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">重放缓冲器</h2><p id="f86e" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">由于我们将进行游戏回放训练，我们需要一个回放缓冲区，以便更容易地存储和检索游戏体验:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="e16e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">游戏体验稍后用于训练DQN代理，因此它需要所有信息训练需求:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/52f5df93f7d58636356965ab38e1e2e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMIABb_Q_8lbXtWDlDHpcg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://drive.google.com/file/d/0BxXI_RttTZAhVUhpbDhiSUFFNjg/view" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="8606" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的DQN更新公式(第一行)中，我们可以看到，对于每个转换，它需要:</p><ul class=""><li id="30a0" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">当前状态</li><li id="ef02" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">对当前状态采取的操作</li><li id="a07a" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">对当前状态采取行动带来的回报</li><li id="9f28" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">采取行动后的状态</li></ul><p id="a697" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们对应于我们实现的<code class="fe ms mt mu mv b">store_gameplay_experience</code>的输入参数。</p><h2 id="2049" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">环境</h2><p id="245e" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">我们将使用OpenAI Gym中的环境，它具有以下API:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="5a68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:我选择使用OpenAI Gym环境，而不是构建一个，因为它的API足够通用，可以在任何未来的RL任务中使用，并且没有RL相关的复杂性。</p><h2 id="80f6" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">将碎片拼在一起</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e6a81cab9e8ecbb6bbf32b177bb644a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*dKEemi16NSFX2lw9JRiw0g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">高层设计</figcaption></figure><p id="47a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义了核心组件后，我们可以按照上图实现训练循环:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><h1 id="60b5" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">填写细节</h1><p id="0930" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">有了上面的高级代码，我们的代码已经是端到端可测试的了，这对初学者来说很重要。</p><p id="0ccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从存根代码开始，让我们填充实现细节。</p><h2 id="b6ab" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">DQN代理商</h2><p id="1197" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">要让Q学习“深度”，我们需要一个深度的神经网络。</p><p id="48ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于推车杆子不是一个复杂的游戏，我们将使用一个小密网:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/897998d5a1a788f9532af297d60c1e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkEyr0Z3QpIbgvNSRYjLJg.png"/></div></div></figure><p id="67f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用下面的代码在DQN代理类中实现一个助手函数来生成上面的网络:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="dc29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了上面的帮助器函数，我们就可以在DQN代理类构造函数中初始化Q网络和目标Q网络:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="464f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:由于用于训练的目标Q值也来自Q网络，为了不让Q网络追逐移动的目标，我们仅在一轮训练完成后更新Q网络。在上面的代码中，<code class="fe ms mt mu mv b">q_net</code>是被主动训练的那个，<code class="fe ms mt mu mv b">target_q_net</code>是训练后才改变的那个。下图显示了这两个网络如何相互作用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1fd6b640f7aba9883416fa141dcf40b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*maBd8nkhn6mKqD_saRc86A.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="3ee4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在网络已经就绪，我们可以使用它通过Q网络运行状态，并选择具有最高Q值的动作，从而从状态生成动作:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="caf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是添加一个模型训练方法，该方法严格遵循上面DQN重述部分中的公式:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/52f5df93f7d58636356965ab38e1e2e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMIABb_Q_8lbXtWDlDHpcg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://drive.google.com/file/d/0BxXI_RttTZAhVUhpbDhiSUFFNjg/view" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="ae0b" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">重放缓冲器</h2><p id="b3f5" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">replay buffer类只是一个固定长度的先进先出队列，不会消耗太多内存，并清除过时的游戏体验:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="1913" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:有许多方法可以实现replay buffer类。要有创意:)</p><h2 id="f348" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">训练循环</h2><p id="f07c" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">由于组件被很好地定义，训练循环保持不变。</p><p id="fdc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过执行训练循环，我们可以训练一个玩得很好的DQN代理:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">训练前的游戏录制样本</figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">训练后的游戏录制示例</figcaption></figure><h1 id="d2f1" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">让培训变得更容易</h1><p id="208f" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">如果你还在这里，恭喜你！您刚刚成功实现了一个全功能的DQN，但是您可能会发现DQN很难训练。别担心，我们只需要最后的润色。</p><h2 id="3c1c" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">额外探索</h2><p id="18c0" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">如果你用上面的代码训练DQN，你可能会发现训练非常慢，有时甚至不收敛。</p><p id="9d3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个探索与利用的问题，这意味着模型不太可能探索未知的状态，因此，它可能会陷入已知的状态。</p><p id="7685" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决方案是使用以下代码将随机性添加到策略中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="1264" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:使用上面的代码，有5%的变化DQN代理会绕过网络，随机产生动作，以Q网络不会的方式探索游戏。</p><h2 id="5b50" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">绩效跟踪</h2><p id="05e0" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">由于培训可能需要一段时间(大约30分钟)，理想情况下，我们希望跟踪模型的性能，以确保培训的进展。</p><p id="97e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码使用训练有素的DQN代理播放10集游戏，取平均分作为DQN表现:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="8ade" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用上面的助手函数，我们可以在训练循环中使用它来监控训练期间的表现:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><h2 id="8000" class="nx mz it bd na ny nz dn ne oa ob dp ni li oc od nk lm oe of nm lq og oh no oi bi translated">模型保存和加载</h2><p id="4c28" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">虽然训练这个DQN只需要大约30分钟，但训练一个更复杂的DQN可能需要几天，在理想情况下，我们希望保留训练检查点和最佳性能模型，以减少像电脑被猫拔掉插头这样的事故(这发生在我身上……):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok mx l"/></div></figure><p id="ee9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，应该在更新网络后立即在训练循环中调用方法<code class="fe ms mt mu mv b">save_checkpoint</code>,该网络保存了最近10个训练结果以防意外。</p><p id="d38a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只有当一个新的性能最佳的模型出现时，才应该调用<code class="fe ms mt mu mv b">save_model</code>,因此<code class="fe ms mt mu mv b">save_model</code>导出的模型将是生产中使用的模型(或者是实验项目的测试)。</p><p id="0535" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了变化，培训体验应该会好很多。</p><p id="a086" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，这只猫拔掉了我电脑的电源:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/431eaa75b3e0a931aa278d88bf944a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s_86p1zVU30r3i2KDEyn8A.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">认识一下Monhchi:编码猫</figcaption></figure></div></div>    
</body>
</html>