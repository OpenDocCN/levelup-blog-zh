<html>
<head>
<title>6 solutions to prevent model overfitting !</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">防止模型过度拟合的6个解决方案！</h1>
<blockquote>原文：<a href="https://levelup.gitconnected.com/6-solutions-prevent-model-overfitting-9c73aa026c1?source=collection_archive---------5-----------------------#2022-09-17">https://levelup.gitconnected.com/6-solutions-prevent-model-overfitting-9c73aa026c1?source=collection_archive---------5-----------------------#2022-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/977461a8f3beb2563fe27d5491510e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QMQvZHiHaCoVlwtr"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">约书亚·索蒂诺在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><pre class="kg kh ki kj gt kk kl km kn aw ko bi"><span id="ce25" class="kp kq it kl b gy kr ks l kt ku">Source: Deep Learning Basics and Advanced, Extreme Market Platform</span><span id="be53" class="kp kq it kl b gy kv ks l kt ku">This article <strong class="kl iu">is about 2700 words</strong> , it is recommended to read <strong class="kl iu">6 minutes</strong></span><span id="eb03" class="kp kq it kl b gy kv ks l kt ku">This article summarizes and explains in detail several commonly used methods to prevent model overfitting.</span></pre><p id="b783" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其实正则化的本质很简单，它是一种手段或操作，对一个问题施加先验的限制或约束，以达到特定的目的。在算法中使用正则化的目的是防止模型过度拟合。谈到正则化，许多学生可能会立即想到常用的L1常模和L2常模。在总结之前，我们先来看看LP范数是什么？</p><h1 id="15ba" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">LP范数</h1><p id="53a5" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">范数可以简单理解为表示向量空间中的距离，距离的定义很抽象。只要满足非负、自反、三角不等式，就可以称为距离。<br/>LP规范不是一个规范，而是一组规范，定义如下:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/731db0c0e39ccd2db708ea0040025dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/0*oYItTlK0jdLYeOXt"/></div></figure><p id="769f" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">p的取值范围是[1，∞)。p不是定义在范围(0，1)中的范数，因为违反了三角形不等式。<br/>根据pp的变化，定额也有不同的变化。借用P范数的一个经典变化图如下:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/2c2888dedf270a88d5072d0566676734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2lXGQWufs4F9n7PR"/></div></div></figure><p id="bfb7" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图显示了当p从0变化到正无穷大时，单位球是如何变化的。P范数下定义的单位球都是凸集，但当0 <p the="" unit="" balls="" under="" this="" definition="" are="" not="" convex="" sets="" we="" mentioned="" earlier="" when="" is="" a="" set="" norm=""/>问题是，L0范数是什么？<br/>L0范数表示向量中非零元素的个数，用公式表示如下:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi my"><img src="../Images/392a32eeb3b1e01a041afe4f92c775e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/0*TDDf_p7AP9E665mb"/></div></figure><p id="eb8b" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过最小化L0范数来找到最优稀疏特征项。但遗憾的是，L0范数的优化问题是一个NP难问题(L0范数也是非凸的)。因此，在实际应用中，我们经常对L0进行凸松弛。从理论上证明了L1范数是L0范数的最优凸逼近，所以通常使用L1范数而不是直接优化L0范数。</p><h1 id="b9d0" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">L1常模</h1><p id="b2e6" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">根据LP范数的定义，我们很容易得到L1范数的数学形式:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/1be25e61354bcbf23cf2ee74e8b94992.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/0*aUAGZ2qdduGDULJE"/></div></figure><p id="1fad" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上式可以看出，L1范数是向量的元素的绝对值之和，也称为“稀疏正则化算子”(Lasso正则化)。所以问题是，我们为什么要稀疏？稀疏化有很多好处，其中两个最直接:</p><ul class=""><li id="0409" class="na nb it ky b kz la ld le lh nc ll nd lp ne lt nf ng nh ni bi translated">特征选择</li><li id="0318" class="na nb it ky b kz nj ld nk lh nl ll nm lp nn lt nf ng nh ni bi translated">可解释性</li></ul><h1 id="f442" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">L2常模</h1><p id="96ac" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">L2范数是最熟悉的，它是欧几里德距离，公式如下:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8ba4a475da32dfefdec345f8d6f0c2ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/0*aHt5U8o-TyWPF4I_"/></div></figure><p id="219e" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">L2规范有许多名称。有人称其回归为“岭回归”，有人称其为“体重衰减”。用L2范数作为正则项可以得到稠密解，即每个特征对应的参数ww很小，接近0但不为0；此外，作为正则化项的L2范数会阻止模型适应训练集。过于复杂会造成过拟合，从而提高模型的泛化能力。</p><h1 id="be2c" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">L1常模和L2常模的区别</h1><p id="ae26" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">介绍一个PRML的经典图来说明L1和L2范数的区别，如下图所示:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/06da77935305bfa9d807f93befba0e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2yoJoJi3XNFJ8H5T"/></div></div></figure><p id="c174" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上图所示，蓝色圆圈代表问题的可能解范围，橙色圆圈代表正则项的可能解范围。而整个目标函数(原问题+正则项)有解当且仅当两个解范围相切。从上图中很容易看出，由于L2范数解的范围是一个圆，切点极有可能不在坐标轴上，而由于L1范数是一个菱形(顶点是凸的)，它的相位切点更有可能在坐标轴上，坐标轴上的点有一个特点，只有一个坐标分量不为零，其他坐标分量都为零，即稀疏。由此得出结论，L1范数可以导致稀疏解，而L2范数可以导致稠密解。<br/>从贝叶斯先验的角度来看，在训练一个模型的时候，仅仅依靠当前的训练数据集是不够的。为了达到更好的泛化能力，往往需要加入一个先验项，加入一个正则项就相当于加入了一个先验。</p><ul class=""><li id="a7ba" class="na nb it ky b kz la ld le lh nc ll nd lp ne lt nf ng nh ni bi translated">L1范数相当于增加了一个拉普拉斯先验；</li><li id="8593" class="na nb it ky b kz nj ld nk lh nl ll nm lp nn lt nf ng nh ni bi translated">L2范数相当于增加了一个高斯先验。</li></ul><p id="ec5d" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如下图所示:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/ebc4e6be7e213b5efaca6f85dd53be5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UJ1CHoLj_1P0ZIRO"/></div></div></figure><h1 id="9a91" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">拒绝传统社会的人</h1><p id="ca5f" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">Dropout是深度学习中经常使用的正则化方法。其做法可以简单理解为在DNNs训练过程中丢弃一些概率为pp的神经元，即使被丢弃神经元的输出为0。可以按如下方式实例化Dropout:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/d94adede0219909ce31f8c18c02eacc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/0*BLMTw7qSVfMsqDpf"/></div></div></figure><p id="e8e2" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以从两个方面直观地理解辍学的正则化效应:</p><ul class=""><li id="ac32" class="na nb it ky b kz la ld le lh nc ll nd lp ne lt nf ng nh ni bi translated">每轮辍学训练中随机丢失神经元的操作相当于对多个dnn进行平均，所以用于预测时具有投票的效果。</li><li id="a492" class="na nb it ky b kz nj ld nk lh nl ll nm lp nn lt nf ng nh ni bi translated">减少神经元之间复杂的共同适应。当隐层神经元被随机删除时，全连通网络具有一定程度的稀疏性，从而有效降低了不同特征的协同效应。也就是说，一些特征可能依赖于固定关系的隐式节点的共同作用，通过Dropout，它有效地组织了一些特征只有在其他特征存在的情况下才有效的情况，增加了神经网络的鲁棒性。太棒了。</li></ul><h1 id="de97" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">批量标准化</h1><p id="702d" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">批量归一化严格来说是一种归一化方法，主要用于加速网络的收敛，但也有一定程度的正则化效果。<br/>以下是魏秀三博士知乎回答(https://www.zhihu.com/question/38102762)中关于协变量移位的解释<br/>注:以下内容引自魏秀三博士知乎回答。大家都知道统计机器学习的一个经典假设是“源域和目标域的数据分布是一致的。”。如果没有，那么新的机器学习问题就产生了，比如迁移学习/领域适应等。协变量移位是不一致分布假设下的一个分支问题。这意味着源空间和目标空间的条件概率相同，但它们的边际概率不同。你想一想就会发现，的确，对于神经网络的每一层的输出，由于它们都经过了层内运算，所以它们的分布与每一层对应的输入信号的分布是明显不同的，而且这种差异会随着网络的深度而增大。大，但它们所能“指示”的样本标签仍然不变，符合协变量移位的定义。<br/>BN的基本思想其实挺直观的，因为非线性变换前的神经网络的激活输入值(X=WU+B，U为输入)随着网络深度的加深而逐渐偏移或变化(即上述的协变量偏移)。训练收敛慢的原因是总体分布逐渐接近非线性函数的取值范围的上下限(对于sigmoid函数，意味着激活输入值X=WU+B是一个较大的负值或正值)，所以这导致了在反向传播时低层神经网络的梯度消失，这是训练一个深度神经网络收敛越来越慢的本质原因。BN是通过一定的归一化方法，迫使神经网络各层中任意一个神经元的输入值的分布回归到均值为0、方差为1的标准正态分布，从而避免激活函数带来的梯度分散问题。所以，与其说BN的作用是缓解协变量偏移，不如说BN可以缓解梯度分散的问题。</p><h1 id="7e7f" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">规范化、标准化和正规化</h1><p id="66ee" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">我们已经提到了正规化，这里简单提一下正规化和标准化。归一化:归一化的目标是找到一定的映射关系，将原始数据映射到[a，b]区间。一般来说，a，b会取[1，1]，[0，1]这些组合。<br/>一般有两种应用场景:</p><ul class=""><li id="4744" class="na nb it ky b kz la ld le lh nc ll nd lp ne lt nf ng nh ni bi translated">将数字转换为(0，1)之间的小数</li><li id="6b24" class="na nb it ky b kz nj ld nk lh nl ll nm lp nn lt nf ng nh ni bi translated">将量纲数转换成无量纲数</li></ul><p id="08eb" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">常见最小-最大归一化:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/311c5e9a5d5ba68955922592214c81ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/0*vUhUvhLvXQZHuEiS"/></div></figure><p id="f7cd" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标准化:使用大数法则将数据转换成标准的正态分布。标准化公式为:</p><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/36d873fa69d1e2160ba18ea4bca41213.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/0*0lsVLMCSsElUqMas"/></div></figure><h1 id="f05b" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">规范化和正常化的区别:</h1><p id="d8d2" class="pw-post-body-paragraph kw kx it ky b kz mr lb lc ld ms lf lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">我们可以这样简单的解释:<br/>归一化标度是“扁平化”的，统一到一个区间(仅由极值决定)，而归一化标度更具“弹性”和“动态性”，与整体样本的分布有很大关系。<br/>值得注意的是:</p><ul class=""><li id="4106" class="na nb it ky b kz la ld le lh nc ll nd lp ne lt nf ng nh ni bi translated">标准化:缩放只与最大值和最小值之间的差异有关。</li><li id="1ac2" class="na nb it ky b kz nj ld nk lh nl ll nm lp nn lt nf ng nh ni bi translated">标准化:缩放与每个点相关，体现在方差中。与归一化相反，归一化中所有数据点都有贡献(通过平均值和标准偏差来贡献)。</li></ul><h1 id="a532" class="lu kq it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">为什么要标准化和常态化？</h1><ul class=""><li id="58f6" class="na nb it ky b kz mr ld ms lh nt ll nu lp nv lt nf ng nh ni bi translated">提高模型精度:归一化后，不同维度之间的特征在数值上具有可比性，可以大大提高分类器的精度。</li><li id="4bd1" class="na nb it ky b kz nj ld nk lh nl ll nm lp nn lt nf ng nh ni bi translated">加速模型收敛:标准化后，最优解的优化过程会明显变得更加平滑，更容易正确收敛到最优解。如下图所示:</li></ul><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f1c5925ea5e8eac65020c81caca9255c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/0*-nxd0B6F_tMyFvsK"/></div></figure><figure class="kg kh ki kj gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1abdf73e0251c130f0aed71fdd62051f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/0*OC2YgdE0lM2wSSwH"/></div></figure><p id="d946" class="pw-post-body-paragraph kw kx it ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">喜欢这篇文章吗？成为一个媒介成员，通过无限制的阅读继续学习。如果你使用<a class="ae kf" href="https://machinelearningabc.medium.com/membership" rel="noopener">这个链接</a>成为会员，你将支持我，不需要额外的费用。提前感谢，再见！</p></div></div>    
</body>
</html>